{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from data_preprocessing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading of the data : done\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"loading of the data : done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio_of_training):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    p = np.random.permutation(np.arange(y.shape[0]))\n",
    "    n = int(y.shape[0] * ratio_of_training)\n",
    "    return  x[p][:n], x[p][n:], y[p][:n], y[p][n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFyCAYAAAAu+3oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ5IkqVUMJ5IkqVX+PyRetkpZgguTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d455a7358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "\n",
    "from ipywidgets import IntSlider, interact\n",
    "from IPython.display import display\n",
    "\n",
    "def plot_hist(tx, i, transformation=None):\n",
    "    plt.figure()\n",
    "    if transformation is None:\n",
    "        plt.hist(tx[:, i], bins=100)\n",
    "    else:\n",
    "        plt.hist(transformation(tx[:, i]), bins=100)\n",
    "    plt.title(\"feature %i\" % i)\n",
    "    plt.show()\n",
    "\n",
    "interact(lambda x:plot_hist(tX, x), x=IntSlider(min=0, max=29))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Good Features                   : [7, 10, 14, 15, 17, 18, 20\n",
    "    Suffering from outlier          : [3, 8, 19, 23, 26, 29]\n",
    "    Suffering from outlier (skewed) : [1, 2, 5, 9, 13, 16, 21]\n",
    "    missing values                  : [0, 4, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "    categorical                     : [11, 12, 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devide Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Features_Good            = [7, 10, 14, 15, 17, 18, 20]\n",
    "Features_with_outlier    = [3, 8, 19, 23, 26, 29]\n",
    "Features_skewed          = [1, 2, 5, 9, 13, 16, 21]\n",
    "Features_missing_entry   = [0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "Features_categorical     = [11, 12, 22]\n",
    "Features_non_categorical = [x for x in range(30) if x not in Features_categorical]\n",
    "Features_using_log       = np.union1d(Features_with_outlier, Features_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devide Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completeness_status_tX = np.sum(tX == -999, axis=1)\n",
    "non_missing_tX         = tX[completeness_status_tX==0, :]\n",
    "missing_tX             = tX[completeness_status_tX!=0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filled_tX_median     = fill_na(tX, np.median)\n",
    "filled_tX_mean       = fill_na(tX, np.mean)\n",
    "missing_indicator_tX = missing_indicator(tX)\n",
    "log_tX               = logs_of_features(filled_tX_median, Features_using_log)\n",
    "decomposed_tX        = decompose_categorical_features(filled_tX_median)\n",
    "inverse_tX           = inver_terms(filled_tX_median, Features_using_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 351)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_tX = mixed_features(filled_tX_median, Features_non_categorical)\n",
    "mixed_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Features_denominate = Features_using_log\n",
    "Features_numerator  = [x for x in Features_non_categorical if x not in Features_denominate]\n",
    "\n",
    "def mixed_inverse_features(tx, features_denominate, features_numerator):\n",
    "    foo = np.zeros(tx.shape[0])\n",
    "    for i in features_denominate:\n",
    "        for j in features_numerator:\n",
    "            foo = np.c_[foo, tx[:, i]/(tx[:, j] + 1e-8)]\n",
    "            \n",
    "    return foo[:, 1:]\n",
    "\n",
    "mixed_invese_tX = mixed_inverse_features(filled_tX_median, Features_denominate, Features_numerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 182)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_invese_tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# low degree but more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 478)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For non categorical features, build polynomials\n",
    "degree = 3\n",
    "poly_tX = build_polynomial_without_mixed_term(filled_tX_median[:, Features_non_categorical], degree=degree)\n",
    "log_poly_tX = build_polynomial_without_mixed_term(log_tX, degree=degree)\n",
    "# inv_poly_tX = build_polynomial_without_mixed_term(inverse_tX, degree=degree)\n",
    "\n",
    "# Build a design matrix\n",
    "design_matrix = np.c_[poly_tX, decomposed_tX, missing_indicator_tX, log_poly_tX, mixed_tX]\n",
    "tX9, mean_x, std_x = standardize(design_matrix)\n",
    "training_ratio = 0.9\n",
    "y9 = transform_y(y)\n",
    "train_tX9, cv_tX9, train_y9, cv_y9 = split_data(tX9, y9, training_ratio)\n",
    "cv_tX9.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_corr(tx):\n",
    "    print (tx.shape[1])\n",
    "    if (tx.shape[0] < 20000):\n",
    "        print (np.linalg.matrix_rank(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 39)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_poly_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n",
      "437\n"
     ]
    }
   ],
   "source": [
    "detect_corr(tX9[:10000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_var(y, tx, K, func, seed, fpred, faccuracy, max_fold):\n",
    "    \"\"\"Cross validation (varied version)\n",
    "    \n",
    "    When the datasets are large and we want to train on a small subset of data,\n",
    "    and validate them on the rest of datasets. We will first divide the the data\n",
    "    into K parts and train on only one of them, then test them on the rest K-1\n",
    "    parts.\n",
    "    \n",
    "    input:\n",
    "        y   : 1-D array (processed).\n",
    "        tx  : 2-D design matrix (processed).\n",
    "        K   : K-folded cross validation\n",
    "        func: function which takes (y, tx) as argument\n",
    "        seed: random seed\n",
    "        \n",
    "        fpred: prediction function\n",
    "        faccuracy: function for calculating accuracy\n",
    "        \n",
    "    return:\n",
    "        ave_acc : averaged accuracy\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    n = len(y)\n",
    "    tr_size = n // K\n",
    "    perm  = np.random.permutation(np.arange(n)).reshape(K, tr_size)\n",
    "    idx = np.ma.array(np.arange(K), mask=False)\n",
    "                                    \n",
    "    def get_index():\n",
    "        i = 0\n",
    "        while i < K:\n",
    "            idx.mask[i] = True\n",
    "            yield perm[i], perm[idx].flatten()\n",
    "            idx.mask[i] = False\n",
    "            i = i + 1\n",
    "    \n",
    "    tr_accs = []\n",
    "    cv_accs = []\n",
    "    for i, (tr_idx, cv_idx) in zip(np.arange(max_fold), get_index()):\n",
    "        start   = time.time()\n",
    "\n",
    "        w, cost = func(y[tr_idx], tx[tr_idx, :])\n",
    "        end     = time.time()\n",
    "        print (\"Time for {i:2}th cross validation = {t:3.6}s\".format(i=i, t=end-start))\n",
    "        \n",
    "        pred_y  = fpred(tx[cv_idx], w)\n",
    "        cv_acc  = faccuracy(pred_y, y[cv_idx])\n",
    "        pred_y  = fpred(tx[tr_idx], w)\n",
    "        tr_acc  = faccuracy(pred_y, y[tr_idx])\n",
    "        \n",
    "        print (\"Training Accuracy         = {a:6.10}\".format(a=tr_acc))\n",
    "        print (\"Cross Validation Accuracy = {a:6.10}\".format(a=cv_acc))\n",
    "        tr_accs.append(tr_acc)\n",
    "        cv_accs.append(cv_acc)\n",
    "        \n",
    "    return tr_accs, cv_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "               max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n",
    "\n",
    "cross_validation_var(y9, tX9, \n",
    "                     K=25, \n",
    "                     func=set_up_f, \n",
    "                     seed=3, \n",
    "                     fpred=predict_cv,\n",
    "                     faccuracy=accuracy,\n",
    "                     max_fold=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 6821.17151835445\n",
      "Losgistic Regression(     100/10000): loss= 4544.61276932469\n",
      "Losgistic Regression(     200/10000): loss= 4270.87902890875\n",
      "Losgistic Regression(     300/10000): loss= 4136.31849876117\n",
      "Losgistic Regression(     400/10000): loss= 4050.29705059181\n",
      "Losgistic Regression(     500/10000): loss= 3988.49090212743\n",
      "Losgistic Regression(     600/10000): loss= 3943.35903686592\n",
      "Losgistic Regression(     700/10000): loss= 3909.81930111496\n",
      "Losgistic Regression(     800/10000): loss= 3883.24686723897\n",
      "Losgistic Regression(     900/10000): loss= 3861.92395576994\n",
      "Losgistic Regression(    1000/10000): loss= 3844.11086419099\n",
      "Losgistic Regression(    1100/10000): loss= 3828.55174370073\n",
      "Losgistic Regression(    1200/10000): loss= 3814.97516728427\n",
      "Losgistic Regression(    1300/10000): loss= 3803.32463602277\n",
      "Losgistic Regression(    1400/10000): loss= 3793.47852120653\n",
      "Losgistic Regression(    1500/10000): loss= 3785.77659681995\n",
      "Losgistic Regression(    1600/10000): loss= 3779.62877646789\n",
      "Losgistic Regression(    1700/10000): loss= 3773.75118672646\n",
      "Losgistic Regression(    1800/10000): loss= 3768.68044831659\n",
      "Losgistic Regression(    1900/10000): loss= 3763.94138072544\n",
      "Losgistic Regression(    2000/10000): loss= 3760.64417754173\n",
      "Losgistic Regression(    2100/10000): loss= 3757.02012747612\n",
      "Losgistic Regression(    2200/10000): loss= 3753.20645176296\n",
      "Losgistic Regression(    2300/10000): loss= 3749.7145332604\n",
      "Losgistic Regression(    2400/10000): loss= 3746.20468791786\n",
      "Losgistic Regression(    2500/10000): loss= 3743.8349542535\n",
      "Losgistic Regression(    2600/10000): loss= 3741.39386586718\n",
      "Losgistic Regression(    2700/10000): loss= 3739.4868013759\n",
      "Losgistic Regression(    2800/10000): loss= 3737.83976350938\n",
      "Losgistic Regression(    2900/10000): loss= 3736.3970647501\n",
      "Losgistic Regression(    3000/10000): loss= 3735.10126867578\n",
      "Losgistic Regression(    3100/10000): loss= 3734.11172957363\n",
      "Losgistic Regression(    3200/10000): loss= 3733.09911052162\n",
      "Losgistic Regression(    3300/10000): loss= 3732.28750277501\n",
      "Losgistic Regression(    3400/10000): loss= 3731.61532850037\n",
      "Losgistic Regression(    3500/10000): loss= 3730.95700031378\n",
      "Losgistic Regression(    3600/10000): loss= 3730.24830265371\n",
      "Losgistic Regression(    3700/10000): loss= 3729.36354473536\n",
      "Losgistic Regression(    3800/10000): loss= 3728.46027016086\n",
      "Losgistic Regression(    3900/10000): loss= 3727.7599342903\n",
      "Losgistic Regression(    4000/10000): loss= 3727.16397669262\n",
      "Losgistic Regression(    4100/10000): loss= 3726.72127101203\n",
      "Losgistic Regression(    4200/10000): loss= 3726.21397041575\n",
      "Losgistic Regression(    4300/10000): loss= 3725.77445543366\n",
      "Losgistic Regression(    4400/10000): loss= 3725.4681742968\n",
      "Losgistic Regression(    4500/10000): loss= 3725.45189388825\n",
      "Totoal number of iterations =  4500\n",
      "Loss                        =  3725.45189389\n",
      "Time for  0th cross validation = 149.784s\n",
      "Training Accuracy         = 0.8382\n",
      "Cross Validation Accuracy = 0.814204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81420400000000004"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "               max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n",
    "\n",
    "def predict_cv(tx, w):\n",
    "    return 1 * ((tx @ w) > 0)\n",
    "\n",
    "cross_validation_var(y9, tX9, K=25, \n",
    "                     func=set_up_f, \n",
    "                     seed=3,\n",
    "                     fpred=predict_cv, \n",
    "                     faccuracy=accuracy, \n",
    "                     max_fold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(n, lambda_):\n",
    "    set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "                   max_iters = 10000, lambda_=lambda_, regularizor=regularizor_lasso)\n",
    "\n",
    "    return cross_validation_var(y9, tX9, K=(250000//n), \n",
    "                         func=set_up_f, \n",
    "                         seed=3,\n",
    "                         fpred=predict_cv, \n",
    "                         faccuracy=accuracy, \n",
    "                         max_fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 32495.5055050499\n",
      "Losgistic Regression(     100/10000): loss= 21564.8229086324\n",
      "Losgistic Regression(     200/10000): loss= 20662.8263215795\n",
      "Losgistic Regression(     300/10000): loss= 20224.9277174388\n",
      "Losgistic Regression(     400/10000): loss= 19944.7098363013\n",
      "Losgistic Regression(     500/10000): loss= 19817.4213630915\n",
      "Losgistic Regression(     600/10000): loss= 19733.4073985364\n",
      "Losgistic Regression(     700/10000): loss= 19680.1023139437\n",
      "Losgistic Regression(     800/10000): loss= 19630.6836645785\n",
      "Losgistic Regression(     900/10000): loss= 19601.2017266897\n",
      "Losgistic Regression(    1000/10000): loss= 19574.9996255812\n",
      "Losgistic Regression(    1100/10000): loss= 19543.7313412869\n",
      "Losgistic Regression(    1200/10000): loss= 19519.4291790914\n",
      "Losgistic Regression(    1300/10000): loss= 19499.7704212502\n",
      "Losgistic Regression(    1400/10000): loss= 19481.2780787485\n",
      "Losgistic Regression(    1500/10000): loss= 19466.3310751705\n",
      "Losgistic Regression(    1600/10000): loss= 19456.2925233962\n",
      "Losgistic Regression(    1700/10000): loss= 19446.7968686921\n",
      "Losgistic Regression(    1800/10000): loss= 19430.1296293733\n",
      "Losgistic Regression(    1900/10000): loss= 19414.8794986715\n",
      "Losgistic Regression(    2000/10000): loss= 19401.2403779494\n",
      "Losgistic Regression(    2100/10000): loss= 19388.4422266236\n",
      "Losgistic Regression(    2200/10000): loss= 19376.0394609109\n",
      "Losgistic Regression(    2300/10000): loss= 19364.4424184994\n",
      "Losgistic Regression(    2400/10000): loss= 19353.1680009544\n",
      "Losgistic Regression(    2500/10000): loss= 19343.5466977175\n",
      "Losgistic Regression(    2600/10000): loss= 19334.76313084\n",
      "Losgistic Regression(    2700/10000): loss= 19326.5405118335\n",
      "Losgistic Regression(    2800/10000): loss= 19319.6975070728\n",
      "Losgistic Regression(    2900/10000): loss= 19313.4791654836\n",
      "Losgistic Regression(    3000/10000): loss= 19307.7669963319\n",
      "Losgistic Regression(    3100/10000): loss= 19302.9922517587\n",
      "Losgistic Regression(    3200/10000): loss= 19298.5263101605\n",
      "Losgistic Regression(    3300/10000): loss= 19294.2872487719\n",
      "Losgistic Regression(    3400/10000): loss= 19290.510474663\n",
      "Losgistic Regression(    3500/10000): loss= 19287.0232206097\n",
      "Losgistic Regression(    3600/10000): loss= 19283.9838039791\n",
      "Losgistic Regression(    3700/10000): loss= 19281.2497295935\n",
      "Losgistic Regression(    3800/10000): loss= 19278.4036747805\n",
      "Losgistic Regression(    3900/10000): loss= 19275.6320763331\n",
      "Losgistic Regression(    4000/10000): loss= 19273.2294646009\n",
      "Losgistic Regression(    4100/10000): loss= 19271.2126615413\n",
      "Losgistic Regression(    4200/10000): loss= 19269.4212656785\n",
      "Losgistic Regression(    4300/10000): loss= 19267.7510856254\n",
      "Losgistic Regression(    4400/10000): loss= 19266.2355860609\n",
      "Losgistic Regression(    4500/10000): loss= 19264.8376037201\n",
      "Losgistic Regression(    4600/10000): loss= 19263.6093655765\n",
      "Losgistic Regression(    4700/10000): loss= 19262.360608453\n",
      "Losgistic Regression(    4800/10000): loss= 19261.2118432683\n",
      "Losgistic Regression(    4900/10000): loss= 19260.2864330224\n",
      "Losgistic Regression(    5000/10000): loss= 19259.0530503888\n",
      "Losgistic Regression(    5100/10000): loss= 19257.7007585288\n",
      "Losgistic Regression(    5200/10000): loss= 19256.3629636172\n",
      "Losgistic Regression(    5300/10000): loss= 19255.0469901238\n",
      "Losgistic Regression(    5400/10000): loss= 19253.7089056797\n",
      "Losgistic Regression(    5500/10000): loss= 19252.2646709865\n",
      "Losgistic Regression(    5600/10000): loss= 19250.7508678765\n",
      "Losgistic Regression(    5700/10000): loss= 19249.2361377556\n",
      "Losgistic Regression(    5800/10000): loss= 19247.7305356632\n",
      "Losgistic Regression(    5900/10000): loss= 19246.1477649683\n",
      "Losgistic Regression(    6000/10000): loss= 19244.5239920809\n",
      "Losgistic Regression(    6100/10000): loss= 19242.9414615399\n",
      "Losgistic Regression(    6200/10000): loss= 19241.3361522153\n",
      "Losgistic Regression(    6300/10000): loss= 19239.6404644808\n",
      "Losgistic Regression(    6400/10000): loss= 19237.8592857005\n",
      "Losgistic Regression(    6500/10000): loss= 19236.0538789776\n",
      "Losgistic Regression(    6600/10000): loss= 19234.2839598031\n",
      "Losgistic Regression(    6700/10000): loss= 19232.6105693946\n",
      "Losgistic Regression(    6800/10000): loss= 19230.9839074922\n",
      "Losgistic Regression(    6900/10000): loss= 19229.3938640386\n",
      "Losgistic Regression(    7000/10000): loss= 19227.8695328024\n",
      "Losgistic Regression(    7100/10000): loss= 19226.2865725753\n",
      "Losgistic Regression(    7200/10000): loss= 19224.6469802925\n",
      "Losgistic Regression(    7300/10000): loss= 19223.0301257956\n",
      "Losgistic Regression(    7400/10000): loss= 19221.4663956349\n",
      "Losgistic Regression(    7500/10000): loss= 19219.9457605995\n",
      "Losgistic Regression(    7600/10000): loss= 19218.4364582447\n",
      "Losgistic Regression(    7700/10000): loss= 19216.9385404067\n",
      "Losgistic Regression(    7800/10000): loss= 19215.4377534477\n",
      "Losgistic Regression(    7900/10000): loss= 19213.9260851938\n",
      "Losgistic Regression(    8000/10000): loss= 19212.4007157631\n",
      "Losgistic Regression(    8100/10000): loss= 19210.8495470755\n",
      "Losgistic Regression(    8200/10000): loss= 19209.2925796874\n",
      "Losgistic Regression(    8300/10000): loss= 19207.749606543\n",
      "Losgistic Regression(    8400/10000): loss= 19206.227883763\n",
      "Losgistic Regression(    8500/10000): loss= 19204.6941767014\n",
      "Losgistic Regression(    8600/10000): loss= 19203.1360697029\n",
      "Losgistic Regression(    8700/10000): loss= 19201.5847527178\n",
      "Losgistic Regression(    8800/10000): loss= 19200.0574891227\n",
      "Losgistic Regression(    8900/10000): loss= 19198.5758543315\n",
      "Losgistic Regression(    9000/10000): loss= 19197.11025292\n",
      "Losgistic Regression(    9100/10000): loss= 19195.6365615841\n",
      "Losgistic Regression(    9200/10000): loss= 19194.1550204319\n",
      "Losgistic Regression(    9300/10000): loss= 19192.6729795754\n",
      "Losgistic Regression(    9400/10000): loss= 19191.1922809511\n",
      "Losgistic Regression(    9500/10000): loss= 19189.7031778519\n",
      "Losgistic Regression(    9600/10000): loss= 19188.1876812741\n",
      "Losgistic Regression(    9700/10000): loss= 19186.6593824943\n",
      "Losgistic Regression(    9800/10000): loss= 19185.1509804011\n",
      "Losgistic Regression(    9900/10000): loss= 19183.6786951657\n",
      "Time for  0th cross validation = 1572.77s\n",
      "Training Accuracy         = 0.83044\n",
      "Cross Validation Accuracy = 0.825748\n",
      "Losgistic Regression(       0/10000): loss= 32430.8544326402\n",
      "Losgistic Regression(     100/10000): loss= 21599.04124445\n",
      "Losgistic Regression(     200/10000): loss= 20684.7509556007\n",
      "Losgistic Regression(     300/10000): loss= 20318.0835353782\n",
      "Losgistic Regression(     400/10000): loss= 20083.6498651339\n",
      "Losgistic Regression(     500/10000): loss= 19948.6502334014\n",
      "Losgistic Regression(     600/10000): loss= 19858.1024034387\n",
      "Losgistic Regression(     700/10000): loss= 19801.9004766224\n",
      "Losgistic Regression(     800/10000): loss= 19761.1016338108\n",
      "Losgistic Regression(     900/10000): loss= 19730.7388107109\n",
      "Losgistic Regression(    1000/10000): loss= 19707.4077864401\n",
      "Losgistic Regression(    1100/10000): loss= 19686.7018257475\n",
      "Losgistic Regression(    1200/10000): loss= 19661.3332051964\n",
      "Losgistic Regression(    1300/10000): loss= 19638.9430366569\n",
      "Losgistic Regression(    1400/10000): loss= 19619.9674781839\n",
      "Losgistic Regression(    1500/10000): loss= 19604.2506533007\n",
      "Losgistic Regression(    1600/10000): loss= 19591.4402459986\n",
      "Losgistic Regression(    1700/10000): loss= 19579.2677435103\n",
      "Losgistic Regression(    1800/10000): loss= 19567.8663033832\n",
      "Losgistic Regression(    1900/10000): loss= 19559.0501147331\n",
      "Losgistic Regression(    2000/10000): loss= 19551.9113326159\n",
      "Losgistic Regression(    2100/10000): loss= 19545.6312536513\n",
      "Losgistic Regression(    2200/10000): loss= 19540.7460035617\n",
      "Losgistic Regression(    2300/10000): loss= 19537.192415809\n",
      "Losgistic Regression(    2400/10000): loss= 19534.0120381479\n",
      "Losgistic Regression(    2500/10000): loss= 19531.2160441612\n",
      "Losgistic Regression(    2600/10000): loss= 19529.2940500073\n",
      "Losgistic Regression(    2700/10000): loss= 19527.4855490358\n",
      "Losgistic Regression(    2800/10000): loss= 19525.6461239361\n",
      "Losgistic Regression(    2900/10000): loss= 19523.8167067752\n",
      "Losgistic Regression(    3000/10000): loss= 19522.1200255428\n",
      "Losgistic Regression(    3100/10000): loss= 19520.3599119183\n",
      "Losgistic Regression(    3200/10000): loss= 19518.5072965437\n",
      "Losgistic Regression(    3300/10000): loss= 19516.7193299281\n",
      "Losgistic Regression(    3400/10000): loss= 19514.9972773305\n",
      "Losgistic Regression(    3500/10000): loss= 19513.1453568799\n",
      "Losgistic Regression(    3600/10000): loss= 19511.1678434039\n",
      "Losgistic Regression(    3700/10000): loss= 19509.1157468419\n",
      "Losgistic Regression(    3800/10000): loss= 19507.1123795788\n",
      "Losgistic Regression(    3900/10000): loss= 19505.3079178442\n",
      "Losgistic Regression(    4000/10000): loss= 19503.7623416966\n",
      "Losgistic Regression(    4100/10000): loss= 19502.4232547224\n",
      "Losgistic Regression(    4200/10000): loss= 19501.1270749691\n",
      "Losgistic Regression(    4300/10000): loss= 19499.9366733309\n",
      "Losgistic Regression(    4400/10000): loss= 19498.8945842839\n",
      "Losgistic Regression(    4500/10000): loss= 19497.8740349547\n",
      "Losgistic Regression(    4600/10000): loss= 19496.8193865171\n",
      "Losgistic Regression(    4700/10000): loss= 19495.7336653013\n",
      "Losgistic Regression(    4800/10000): loss= 19494.6510244573\n",
      "Losgistic Regression(    4900/10000): loss= 19493.5319267062\n",
      "Losgistic Regression(    5000/10000): loss= 19492.3184969805\n",
      "Losgistic Regression(    5100/10000): loss= 19490.9955074223\n",
      "Losgistic Regression(    5200/10000): loss= 19489.6262335187\n",
      "Losgistic Regression(    5300/10000): loss= 19488.2744658504\n",
      "Losgistic Regression(    5400/10000): loss= 19486.9410118277\n",
      "Losgistic Regression(    5500/10000): loss= 19485.6047547885\n",
      "Losgistic Regression(    5600/10000): loss= 19484.2797492952\n",
      "Losgistic Regression(    5700/10000): loss= 19482.9804151696\n",
      "Losgistic Regression(    5800/10000): loss= 19481.7534268032\n",
      "Losgistic Regression(    5900/10000): loss= 19480.6432090077\n",
      "Losgistic Regression(    6000/10000): loss= 19479.6009673465\n",
      "Losgistic Regression(    6100/10000): loss= 19478.5904876223\n",
      "Losgistic Regression(    6200/10000): loss= 19477.620079243\n",
      "Losgistic Regression(    6300/10000): loss= 19476.6636047198\n",
      "Losgistic Regression(    6400/10000): loss= 19475.6500009469\n",
      "Losgistic Regression(    6500/10000): loss= 19474.5727221311\n",
      "Losgistic Regression(    6600/10000): loss= 19473.4857607022\n",
      "Losgistic Regression(    6700/10000): loss= 19472.4010049531\n",
      "Losgistic Regression(    6800/10000): loss= 19471.2820526971\n",
      "Losgistic Regression(    6900/10000): loss= 19470.1176334237\n",
      "Losgistic Regression(    7000/10000): loss= 19468.9265500786\n",
      "Losgistic Regression(    7100/10000): loss= 19467.6983083102\n",
      "Losgistic Regression(    7200/10000): loss= 19466.4479615605\n",
      "Losgistic Regression(    7300/10000): loss= 19465.1906315022\n",
      "Losgistic Regression(    7400/10000): loss= 19463.9348039651\n",
      "Losgistic Regression(    7500/10000): loss= 19462.6811983829\n",
      "Losgistic Regression(    7600/10000): loss= 19461.4282867996\n",
      "Losgistic Regression(    7700/10000): loss= 19460.1934522517\n",
      "Losgistic Regression(    7800/10000): loss= 19458.9788507323\n",
      "Losgistic Regression(    7900/10000): loss= 19457.7784534106\n",
      "Losgistic Regression(    8000/10000): loss= 19456.594652873\n",
      "Losgistic Regression(    8100/10000): loss= 19455.4209508398\n",
      "Losgistic Regression(    8200/10000): loss= 19454.2350373318\n",
      "Losgistic Regression(    8300/10000): loss= 19453.019423413\n",
      "Losgistic Regression(    8400/10000): loss= 19451.7616202877\n",
      "Losgistic Regression(    8500/10000): loss= 19450.4504596768\n",
      "Losgistic Regression(    8600/10000): loss= 19449.0947115199\n",
      "Losgistic Regression(    8700/10000): loss= 19447.7253669106\n",
      "Losgistic Regression(    8800/10000): loss= 19446.3568607434\n",
      "Losgistic Regression(    8900/10000): loss= 19444.9912535781\n",
      "Losgistic Regression(    9000/10000): loss= 19443.633732636\n",
      "Losgistic Regression(    9100/10000): loss= 19442.2794324418\n",
      "Losgistic Regression(    9200/10000): loss= 19440.9176085427\n",
      "Losgistic Regression(    9300/10000): loss= 19439.5558748685\n",
      "Losgistic Regression(    9400/10000): loss= 19438.2056360286\n",
      "Losgistic Regression(    9500/10000): loss= 19436.8553153137\n",
      "Losgistic Regression(    9600/10000): loss= 19435.4905117289\n",
      "Losgistic Regression(    9700/10000): loss= 19434.1086196866\n",
      "Losgistic Regression(    9800/10000): loss= 19432.719653036\n",
      "Losgistic Regression(    9900/10000): loss= 19431.332054208\n",
      "Time for  1th cross validation = 1565.21s\n",
      "Training Accuracy         = 0.82926\n",
      "Cross Validation Accuracy = 0.825644\n",
      "Losgistic Regression(       0/10000): loss= 32484.361937325\n",
      "Losgistic Regression(     100/10000): loss= 21538.9442937084\n",
      "Losgistic Regression(     200/10000): loss= 20575.8311316921\n",
      "Losgistic Regression(     300/10000): loss= 20209.7026941608\n",
      "Losgistic Regression(     400/10000): loss= 20012.6369425414\n",
      "Losgistic Regression(     500/10000): loss= 19853.121028882\n",
      "Losgistic Regression(     600/10000): loss= 19715.1013911901\n",
      "Losgistic Regression(     700/10000): loss= 19662.0674526783\n",
      "Losgistic Regression(     800/10000): loss= 19633.0034955672\n",
      "Losgistic Regression(     900/10000): loss= 19597.9735716074\n",
      "Losgistic Regression(    1000/10000): loss= 19573.4131690464\n",
      "Losgistic Regression(    1100/10000): loss= 19548.4977381441\n",
      "Losgistic Regression(    1200/10000): loss= 19531.3968325107\n",
      "Losgistic Regression(    1300/10000): loss= 19518.9583564156\n",
      "Losgistic Regression(    1400/10000): loss= 19506.8629717855\n",
      "Losgistic Regression(    1500/10000): loss= 19495.3731409293\n",
      "Losgistic Regression(    1600/10000): loss= 19486.0400701787\n",
      "Losgistic Regression(    1700/10000): loss= 19478.8080753245\n",
      "Losgistic Regression(    1800/10000): loss= 19472.9537104905\n",
      "Losgistic Regression(    1900/10000): loss= 19468.0731428088\n",
      "Losgistic Regression(    2000/10000): loss= 19463.5378629271\n",
      "Losgistic Regression(    2100/10000): loss= 19458.7673362077\n",
      "Losgistic Regression(    2200/10000): loss= 19455.1643927968\n",
      "Losgistic Regression(    2300/10000): loss= 19452.0862111491\n",
      "Losgistic Regression(    2400/10000): loss= 19450.2558239669\n",
      "Losgistic Regression(    2500/10000): loss= 19448.6462799523\n",
      "Losgistic Regression(    2600/10000): loss= 19447.0286706156\n",
      "Losgistic Regression(    2700/10000): loss= 19445.9116019734\n",
      "Losgistic Regression(    2800/10000): loss= 19444.6769142309\n",
      "Losgistic Regression(    2900/10000): loss= 19443.7515794275\n",
      "Losgistic Regression(    3000/10000): loss= 19442.9889807223\n",
      "Losgistic Regression(    3100/10000): loss= 19442.0176534249\n",
      "Losgistic Regression(    3200/10000): loss= 19441.4209142132\n",
      "Losgistic Regression(    3300/10000): loss= 19440.9483728526\n",
      "Losgistic Regression(    3400/10000): loss= 19440.4628895411\n",
      "Losgistic Regression(    3500/10000): loss= 19440.1413454046\n",
      "Losgistic Regression(    3600/10000): loss= 19439.7835446785\n",
      "Losgistic Regression(    3700/10000): loss= 19439.353229629\n",
      "Losgistic Regression(    3800/10000): loss= 19438.9897752017\n",
      "Losgistic Regression(    3900/10000): loss= 19438.7191879495\n",
      "Losgistic Regression(    4000/10000): loss= 19437.9948619698\n",
      "Losgistic Regression(    4100/10000): loss= 19436.9495969903\n",
      "Losgistic Regression(    4200/10000): loss= 19435.6801380499\n",
      "Losgistic Regression(    4300/10000): loss= 19434.1401740793\n",
      "Losgistic Regression(    4400/10000): loss= 19432.6719426484\n",
      "Losgistic Regression(    4500/10000): loss= 19431.1190732386\n",
      "Losgistic Regression(    4600/10000): loss= 19429.4490213616\n",
      "Losgistic Regression(    4700/10000): loss= 19427.7223050537\n",
      "Losgistic Regression(    4800/10000): loss= 19425.7714745321\n",
      "Losgistic Regression(    4900/10000): loss= 19423.7757845546\n",
      "Losgistic Regression(    5000/10000): loss= 19421.7800362859\n",
      "Losgistic Regression(    5100/10000): loss= 19419.3393982303\n",
      "Losgistic Regression(    5200/10000): loss= 19416.8102444281\n",
      "Losgistic Regression(    5300/10000): loss= 19414.3284219976\n",
      "Losgistic Regression(    5400/10000): loss= 19411.8254396992\n",
      "Losgistic Regression(    5500/10000): loss= 19409.410734204\n",
      "Losgistic Regression(    5600/10000): loss= 19407.1637133804\n",
      "Losgistic Regression(    5700/10000): loss= 19405.0174127015\n",
      "Losgistic Regression(    5800/10000): loss= 19402.9017947908\n",
      "Losgistic Regression(    5900/10000): loss= 19400.8641155746\n",
      "Losgistic Regression(    6000/10000): loss= 19398.8299978341\n",
      "Losgistic Regression(    6100/10000): loss= 19396.7641817568\n",
      "Losgistic Regression(    6200/10000): loss= 19394.7340557142\n",
      "Losgistic Regression(    6300/10000): loss= 19392.7255077411\n",
      "Losgistic Regression(    6400/10000): loss= 19390.7099513855\n",
      "Losgistic Regression(    6500/10000): loss= 19388.7357912997\n",
      "Losgistic Regression(    6600/10000): loss= 19386.8156873077\n",
      "Losgistic Regression(    6700/10000): loss= 19384.9118144942\n",
      "Losgistic Regression(    6800/10000): loss= 19383.0387385382\n",
      "Losgistic Regression(    6900/10000): loss= 19381.1874066391\n",
      "Losgistic Regression(    7000/10000): loss= 19379.2576314146\n",
      "Losgistic Regression(    7100/10000): loss= 19377.2426614692\n",
      "Losgistic Regression(    7200/10000): loss= 19375.2477264512\n",
      "Losgistic Regression(    7300/10000): loss= 19373.266153681\n",
      "Losgistic Regression(    7400/10000): loss= 19371.2416695617\n",
      "Losgistic Regression(    7500/10000): loss= 19369.2251417709\n",
      "Losgistic Regression(    7600/10000): loss= 19367.2601323437\n",
      "Losgistic Regression(    7700/10000): loss= 19365.3192188943\n",
      "Losgistic Regression(    7800/10000): loss= 19363.3863785686\n",
      "Losgistic Regression(    7900/10000): loss= 19361.4308000576\n",
      "Losgistic Regression(    8000/10000): loss= 19359.4271418403\n",
      "Losgistic Regression(    8100/10000): loss= 19357.4270371815\n",
      "Losgistic Regression(    8200/10000): loss= 19355.4750915226\n",
      "Losgistic Regression(    8300/10000): loss= 19353.5649251633\n",
      "Losgistic Regression(    8400/10000): loss= 19351.6708951204\n",
      "Losgistic Regression(    8500/10000): loss= 19349.7591068579\n",
      "Losgistic Regression(    8600/10000): loss= 19347.8471723066\n",
      "Losgistic Regression(    8700/10000): loss= 19345.9713048355\n",
      "Losgistic Regression(    8800/10000): loss= 19344.1224012095\n",
      "Losgistic Regression(    8900/10000): loss= 19342.2834165722\n",
      "Losgistic Regression(    9000/10000): loss= 19340.4541345703\n",
      "Losgistic Regression(    9100/10000): loss= 19338.6098361892\n",
      "Losgistic Regression(    9200/10000): loss= 19336.7407319066\n",
      "Losgistic Regression(    9300/10000): loss= 19334.8819758221\n",
      "Losgistic Regression(    9400/10000): loss= 19333.0463120563\n",
      "Losgistic Regression(    9500/10000): loss= 19331.2249260427\n",
      "Losgistic Regression(    9600/10000): loss= 19329.4373537606\n",
      "Losgistic Regression(    9700/10000): loss= 19327.6857901904\n",
      "Losgistic Regression(    9800/10000): loss= 19325.9467255736\n",
      "Losgistic Regression(    9900/10000): loss= 19324.2241508384\n",
      "Time for  2th cross validation = 1564.62s\n",
      "Training Accuracy         = 0.82714\n",
      "Cross Validation Accuracy = 0.825188\n",
      "Losgistic Regression(       0/10000): loss= 32421.0239276688\n",
      "Losgistic Regression(     100/10000): loss= 21290.0972754909\n",
      "Losgistic Regression(     200/10000): loss= 20324.8288503923\n",
      "Losgistic Regression(     300/10000): loss= 19918.0354346579\n",
      "Losgistic Regression(     400/10000): loss= 19711.8976719302\n",
      "Losgistic Regression(     500/10000): loss= 19583.0658325858\n",
      "Losgistic Regression(     600/10000): loss= 19487.3879415566\n",
      "Losgistic Regression(     700/10000): loss= 19412.859150738\n",
      "Losgistic Regression(     800/10000): loss= 19352.375866439\n",
      "Losgistic Regression(     900/10000): loss= 19297.952099343\n",
      "Losgistic Regression(    1000/10000): loss= 19253.1876050299\n",
      "Losgistic Regression(    1100/10000): loss= 19211.7328219639\n",
      "Losgistic Regression(    1200/10000): loss= 19176.9623563348\n",
      "Losgistic Regression(    1300/10000): loss= 19147.9928463556\n",
      "Losgistic Regression(    1400/10000): loss= 19123.9797169901\n",
      "Losgistic Regression(    1500/10000): loss= 19104.3946911485\n",
      "Losgistic Regression(    1600/10000): loss= 19085.6019753314\n",
      "Losgistic Regression(    1700/10000): loss= 19069.7453286016\n",
      "Losgistic Regression(    1800/10000): loss= 19057.2954450652\n",
      "Losgistic Regression(    1900/10000): loss= 19046.7006659654\n",
      "Losgistic Regression(    2000/10000): loss= 19038.1171217579\n",
      "Losgistic Regression(    2100/10000): loss= 19032.4554888733\n",
      "Losgistic Regression(    2200/10000): loss= 19027.8955835587\n",
      "Losgistic Regression(    2300/10000): loss= 19024.4668821426\n",
      "Losgistic Regression(    2400/10000): loss= 19022.0122064944\n",
      "Losgistic Regression(    2500/10000): loss= 19019.4573812493\n",
      "Losgistic Regression(    2600/10000): loss= 19018.1646827277\n",
      "Losgistic Regression(    2700/10000): loss= 19017.7112226037\n",
      "Losgistic Regression(    2800/10000): loss= 19017.1319591682\n",
      "Losgistic Regression(    2900/10000): loss= 19017.1239213613\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  19017.1239214\n",
      "Time for  3th cross validation = 459.049s\n",
      "Training Accuracy         = 0.83356\n",
      "Cross Validation Accuracy = 0.82726\n",
      "Losgistic Regression(       0/10000): loss= 32417.8016499762\n",
      "Losgistic Regression(     100/10000): loss= 21596.8062272154\n",
      "Losgistic Regression(     200/10000): loss= 20706.1146669791\n",
      "Losgistic Regression(     300/10000): loss= 20304.4523962884\n",
      "Losgistic Regression(     400/10000): loss= 20045.9719824263\n",
      "Losgistic Regression(     500/10000): loss= 19890.9194411855\n",
      "Losgistic Regression(     600/10000): loss= 19804.1860952967\n",
      "Losgistic Regression(     700/10000): loss= 19752.9251982065\n",
      "Losgistic Regression(     800/10000): loss= 19711.5037365382\n",
      "Losgistic Regression(     900/10000): loss= 19678.0203392381\n",
      "Losgistic Regression(    1000/10000): loss= 19646.1373816486\n",
      "Losgistic Regression(    1100/10000): loss= 19620.8876680739\n",
      "Losgistic Regression(    1200/10000): loss= 19599.4807333582\n",
      "Losgistic Regression(    1300/10000): loss= 19578.765860511\n",
      "Losgistic Regression(    1400/10000): loss= 19560.5088901117\n",
      "Losgistic Regression(    1500/10000): loss= 19546.0614661924\n",
      "Losgistic Regression(    1600/10000): loss= 19533.8880874613\n",
      "Losgistic Regression(    1700/10000): loss= 19523.534357563\n",
      "Losgistic Regression(    1800/10000): loss= 19515.4597240794\n",
      "Losgistic Regression(    1900/10000): loss= 19509.2060139335\n",
      "Losgistic Regression(    2000/10000): loss= 19504.5643721765\n",
      "Losgistic Regression(    2100/10000): loss= 19501.576850658\n",
      "Losgistic Regression(    2200/10000): loss= 19497.1968574958\n",
      "Losgistic Regression(    2300/10000): loss= 19493.1520798459\n",
      "Losgistic Regression(    2400/10000): loss= 19489.5483639593\n",
      "Losgistic Regression(    2500/10000): loss= 19486.2248316363\n",
      "Losgistic Regression(    2600/10000): loss= 19483.1856873363\n",
      "Losgistic Regression(    2700/10000): loss= 19480.5195172191\n",
      "Losgistic Regression(    2800/10000): loss= 19478.2976238164\n",
      "Losgistic Regression(    2900/10000): loss= 19476.1162660082\n",
      "Losgistic Regression(    3000/10000): loss= 19473.8636982499\n",
      "Losgistic Regression(    3100/10000): loss= 19471.5660460609\n",
      "Losgistic Regression(    3200/10000): loss= 19469.5581251878\n",
      "Losgistic Regression(    3300/10000): loss= 19467.9538517228\n",
      "Losgistic Regression(    3400/10000): loss= 19466.5755614918\n",
      "Losgistic Regression(    3500/10000): loss= 19465.3406722238\n",
      "Losgistic Regression(    3600/10000): loss= 19464.2026304868\n",
      "Losgistic Regression(    3700/10000): loss= 19463.0815980198\n",
      "Losgistic Regression(    3800/10000): loss= 19461.6887045151\n",
      "Losgistic Regression(    3900/10000): loss= 19460.4377434662\n",
      "Losgistic Regression(    4000/10000): loss= 19459.2302902246\n",
      "Losgistic Regression(    4100/10000): loss= 19457.8618792081\n",
      "Losgistic Regression(    4200/10000): loss= 19456.1574462483\n",
      "Losgistic Regression(    4300/10000): loss= 19454.4812579397\n",
      "Losgistic Regression(    4400/10000): loss= 19452.9272381468\n",
      "Losgistic Regression(    4500/10000): loss= 19451.3600527487\n",
      "Losgistic Regression(    4600/10000): loss= 19449.6812916465\n",
      "Losgistic Regression(    4700/10000): loss= 19447.9629420371\n",
      "Losgistic Regression(    4800/10000): loss= 19446.2120439491\n",
      "Losgistic Regression(    4900/10000): loss= 19444.3116967411\n",
      "Losgistic Regression(    5000/10000): loss= 19442.3324439365\n",
      "Losgistic Regression(    5100/10000): loss= 19440.3778609905\n",
      "Losgistic Regression(    5200/10000): loss= 19438.4307659251\n",
      "Losgistic Regression(    5300/10000): loss= 19436.4662582813\n",
      "Losgistic Regression(    5400/10000): loss= 19434.472858109\n",
      "Losgistic Regression(    5500/10000): loss= 19432.43118615\n",
      "Losgistic Regression(    5600/10000): loss= 19430.3647922463\n",
      "Losgistic Regression(    5700/10000): loss= 19428.321342216\n",
      "Losgistic Regression(    5800/10000): loss= 19426.3374289903\n",
      "Losgistic Regression(    5900/10000): loss= 19424.4762239091\n",
      "Losgistic Regression(    6000/10000): loss= 19422.7315956635\n",
      "Losgistic Regression(    6100/10000): loss= 19420.9793350522\n",
      "Losgistic Regression(    6200/10000): loss= 19419.2404585467\n",
      "Losgistic Regression(    6300/10000): loss= 19417.5926559392\n",
      "Losgistic Regression(    6400/10000): loss= 19415.981013043\n",
      "Losgistic Regression(    6500/10000): loss= 19414.3945310661\n",
      "Losgistic Regression(    6600/10000): loss= 19412.8327079024\n",
      "Losgistic Regression(    6700/10000): loss= 19411.2463695409\n",
      "Losgistic Regression(    6800/10000): loss= 19409.6346475727\n",
      "Losgistic Regression(    6900/10000): loss= 19407.8529706486\n",
      "Losgistic Regression(    7000/10000): loss= 19406.0014994338\n",
      "Losgistic Regression(    7100/10000): loss= 19404.108520918\n",
      "Losgistic Regression(    7200/10000): loss= 19402.1588819238\n",
      "Losgistic Regression(    7300/10000): loss= 19400.1864517289\n",
      "Losgistic Regression(    7400/10000): loss= 19398.224044968\n",
      "Losgistic Regression(    7500/10000): loss= 19396.291790151\n",
      "Losgistic Regression(    7600/10000): loss= 19394.3609644204\n",
      "Losgistic Regression(    7700/10000): loss= 19392.4115743509\n",
      "Losgistic Regression(    7800/10000): loss= 19390.4800788593\n",
      "Losgistic Regression(    7900/10000): loss= 19388.591723337\n",
      "Losgistic Regression(    8000/10000): loss= 19386.7179383853\n",
      "Losgistic Regression(    8100/10000): loss= 19384.8200226443\n",
      "Losgistic Regression(    8200/10000): loss= 19382.9041905644\n",
      "Losgistic Regression(    8300/10000): loss= 19380.9909623974\n",
      "Losgistic Regression(    8400/10000): loss= 19379.0689488139\n",
      "Losgistic Regression(    8500/10000): loss= 19377.1188839603\n",
      "Losgistic Regression(    8600/10000): loss= 19375.1517563732\n",
      "Losgistic Regression(    8700/10000): loss= 19373.1777428547\n",
      "Losgistic Regression(    8800/10000): loss= 19371.1994226595\n",
      "Losgistic Regression(    8900/10000): loss= 19369.2359466874\n",
      "Losgistic Regression(    9000/10000): loss= 19367.2997020075\n",
      "Losgistic Regression(    9100/10000): loss= 19365.3976705077\n",
      "Losgistic Regression(    9200/10000): loss= 19363.5212669917\n",
      "Losgistic Regression(    9300/10000): loss= 19361.6554803801\n",
      "Losgistic Regression(    9400/10000): loss= 19359.7878011779\n",
      "Losgistic Regression(    9500/10000): loss= 19357.9075675084\n",
      "Losgistic Regression(    9600/10000): loss= 19356.0186738403\n",
      "Losgistic Regression(    9700/10000): loss= 19354.1179969646\n",
      "Losgistic Regression(    9800/10000): loss= 19352.1965247422\n",
      "Losgistic Regression(    9900/10000): loss= 19350.256329904\n",
      "Time for  4th cross validation = 1684.07s\n",
      "Training Accuracy         = 0.8291\n",
      "Cross Validation Accuracy = 0.825568\n",
      "*************** ([0.83043999999999996, 0.82926, 0.82713999999999999, 0.83355999999999997, 0.82909999999999995], [0.82574800000000004, 0.82564400000000004, 0.82518800000000003, 0.82726, 0.82556799999999997])\n",
      "Losgistic Regression(       0/10000): loss= 32495.5338517655\n",
      "Losgistic Regression(     100/10000): loss= 21565.9185576866\n",
      "Losgistic Regression(     200/10000): loss= 20664.7369350125\n",
      "Losgistic Regression(     300/10000): loss= 20227.441085223\n",
      "Losgistic Regression(     400/10000): loss= 19947.8159830081\n",
      "Losgistic Regression(     500/10000): loss= 19820.9621864405\n",
      "Losgistic Regression(     600/10000): loss= 19737.5322217533\n",
      "Losgistic Regression(     700/10000): loss= 19684.5143223687\n",
      "Losgistic Regression(     800/10000): loss= 19635.3634658876\n",
      "Losgistic Regression(     900/10000): loss= 19606.096615829\n",
      "Losgistic Regression(    1000/10000): loss= 19580.0575225466\n",
      "Losgistic Regression(    1100/10000): loss= 19548.967610773\n",
      "Losgistic Regression(    1200/10000): loss= 19524.737690304\n",
      "Losgistic Regression(    1300/10000): loss= 19505.1881864485\n",
      "Losgistic Regression(    1400/10000): loss= 19486.7929873234\n",
      "Losgistic Regression(    1500/10000): loss= 19471.9885240641\n",
      "Losgistic Regression(    1600/10000): loss= 19462.1887576097\n",
      "Losgistic Regression(    1700/10000): loss= 19453.0121325785\n",
      "Losgistic Regression(    1800/10000): loss= 19436.8140416209\n",
      "Losgistic Regression(    1900/10000): loss= 19421.7640484339\n",
      "Losgistic Regression(    2000/10000): loss= 19408.2692216567\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-478-bb74acb4624f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccu_50000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***************\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccu_50000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-472-59aa1226bca6>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(n, lambda_)\u001b[0m\n\u001b[1;32m      8\u001b[0m                          \u001b[0mfpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_cv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                          \u001b[0mfaccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                          max_fold=5)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-457-77a042919e9f>\u001b[0m in \u001b[0;36mcross_validation_var\u001b[0;34m(y, tx, K, func, seed, fpred, faccuracy, max_fold)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mstart\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mend\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Time for {i:2}th cross validation = {t:3.6}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-472-59aa1226bca6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n\u001b[0;32m----> 3\u001b[0;31m                    max_iters = 10000, lambda_=lambda_, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return cross_validation_var(y9, tX9, K=(250000//n), \n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_AGDR\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Restart if the loss of new weight is larger than the original one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accu_50000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(50000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_50000.append(tmp)\n",
    "\n",
    "accu_50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 16721.0280439384\n",
      "Losgistic Regression(     100/10000): loss= 10932.1251833644\n",
      "Losgistic Regression(     200/10000): loss= 10407.2173779291\n",
      "Losgistic Regression(     300/10000): loss= 10146.086705963\n",
      "Losgistic Regression(     400/10000): loss= 9983.54157283347\n",
      "Losgistic Regression(     500/10000): loss= 9871.30073461271\n",
      "Losgistic Regression(     600/10000): loss= 9784.79324806552\n",
      "Losgistic Regression(     700/10000): loss= 9713.16026138604\n",
      "Losgistic Regression(     800/10000): loss= 9661.16929158904\n",
      "Losgistic Regression(     900/10000): loss= 9626.34455776342\n",
      "Losgistic Regression(    1000/10000): loss= 9598.86646649527\n",
      "Losgistic Regression(    1100/10000): loss= 9579.13566108009\n",
      "Losgistic Regression(    1200/10000): loss= 9563.33988045476\n",
      "Losgistic Regression(    1300/10000): loss= 9551.09499138008\n",
      "Losgistic Regression(    1400/10000): loss= 9540.77399486591\n",
      "Losgistic Regression(    1500/10000): loss= 9527.90641571044\n",
      "Losgistic Regression(    1600/10000): loss= 9518.19280008319\n",
      "Losgistic Regression(    1700/10000): loss= 9507.02890365546\n",
      "Losgistic Regression(    1800/10000): loss= 9494.12967261127\n",
      "Losgistic Regression(    1900/10000): loss= 9480.87049530409\n",
      "Losgistic Regression(    2000/10000): loss= 9468.9080013743\n",
      "Losgistic Regression(    2100/10000): loss= 9457.29856634738\n",
      "Losgistic Regression(    2200/10000): loss= 9446.8428943709\n",
      "Losgistic Regression(    2300/10000): loss= 9436.77907482099\n",
      "Losgistic Regression(    2400/10000): loss= 9427.56197755646\n",
      "Losgistic Regression(    2500/10000): loss= 9419.46080765679\n",
      "Losgistic Regression(    2600/10000): loss= 9412.4065635616\n",
      "Losgistic Regression(    2700/10000): loss= 9406.31978101675\n",
      "Losgistic Regression(    2800/10000): loss= 9401.31538337997\n",
      "Losgistic Regression(    2900/10000): loss= 9397.37332053804\n",
      "Losgistic Regression(    3000/10000): loss= 9393.94328517541\n",
      "Losgistic Regression(    3100/10000): loss= 9390.83831188612\n",
      "Losgistic Regression(    3200/10000): loss= 9388.4933381221\n",
      "Losgistic Regression(    3300/10000): loss= 9387.19360881593\n",
      "Losgistic Regression(    3400/10000): loss= 9386.82497816026\n",
      "Losgistic Regression(    3500/10000): loss= 9386.82975164646\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9386.82975165\n",
      "Time for  0th cross validation = 284.766s\n",
      "Training Accuracy         = 0.83272\n",
      "Cross Validation Accuracy = 0.821352\n",
      "Losgistic Regression(       0/10000): loss= 16695.4133420063\n",
      "Losgistic Regression(     100/10000): loss= 11024.8493125464\n",
      "Losgistic Regression(     200/10000): loss= 10527.9254877966\n",
      "Losgistic Regression(     300/10000): loss= 10280.7722145521\n",
      "Losgistic Regression(     400/10000): loss= 10117.1777811684\n",
      "Losgistic Regression(     500/10000): loss= 9989.43397523596\n",
      "Losgistic Regression(     600/10000): loss= 9903.08525417924\n",
      "Losgistic Regression(     700/10000): loss= 9842.80252060514\n",
      "Losgistic Regression(     800/10000): loss= 9798.60328737496\n",
      "Losgistic Regression(     900/10000): loss= 9758.73156174187\n",
      "Losgistic Regression(    1000/10000): loss= 9726.26550303955\n",
      "Losgistic Regression(    1100/10000): loss= 9698.29450818791\n",
      "Losgistic Regression(    1200/10000): loss= 9676.83477793912\n",
      "Losgistic Regression(    1300/10000): loss= 9659.68839709488\n",
      "Losgistic Regression(    1400/10000): loss= 9644.96212442297\n",
      "Losgistic Regression(    1500/10000): loss= 9635.36018099662\n",
      "Losgistic Regression(    1600/10000): loss= 9630.10245104386\n",
      "Losgistic Regression(    1700/10000): loss= 9622.36598304255\n",
      "Losgistic Regression(    1800/10000): loss= 9613.1053851671\n",
      "Losgistic Regression(    1900/10000): loss= 9605.52831353813\n",
      "Losgistic Regression(    2000/10000): loss= 9598.37863763116\n",
      "Losgistic Regression(    2100/10000): loss= 9592.45807121214\n",
      "Losgistic Regression(    2200/10000): loss= 9586.81676312802\n",
      "Losgistic Regression(    2300/10000): loss= 9581.94081897188\n",
      "Losgistic Regression(    2400/10000): loss= 9577.69378573401\n",
      "Losgistic Regression(    2500/10000): loss= 9574.23823948438\n",
      "Losgistic Regression(    2600/10000): loss= 9571.37162260243\n",
      "Losgistic Regression(    2700/10000): loss= 9568.72458366685\n",
      "Losgistic Regression(    2800/10000): loss= 9566.6012452941\n",
      "Losgistic Regression(    2900/10000): loss= 9565.07701807539\n",
      "Losgistic Regression(    3000/10000): loss= 9564.21563849379\n",
      "Losgistic Regression(    3100/10000): loss= 9563.53966545955\n",
      "Losgistic Regression(    3200/10000): loss= 9562.66519939677\n",
      "Losgistic Regression(    3300/10000): loss= 9561.77341405994\n",
      "Losgistic Regression(    3400/10000): loss= 9560.46584688226\n",
      "Losgistic Regression(    3500/10000): loss= 9559.17962151433\n",
      "Losgistic Regression(    3600/10000): loss= 9557.78114467458\n",
      "Losgistic Regression(    3700/10000): loss= 9556.68886710333\n",
      "Losgistic Regression(    3800/10000): loss= 9555.66432627887\n",
      "Losgistic Regression(    3900/10000): loss= 9554.77829550035\n",
      "Losgistic Regression(    4000/10000): loss= 9554.25193688196\n",
      "Losgistic Regression(    4100/10000): loss= 9553.79904748376\n",
      "Losgistic Regression(    4200/10000): loss= 9553.42069502146\n",
      "Losgistic Regression(    4300/10000): loss= 9552.89696137457\n",
      "Losgistic Regression(    4400/10000): loss= 9552.26965785784\n",
      "Losgistic Regression(    4500/10000): loss= 9551.63393153141\n",
      "Losgistic Regression(    4600/10000): loss= 9551.0836998664\n",
      "Losgistic Regression(    4700/10000): loss= 9550.49482972752\n",
      "Losgistic Regression(    4800/10000): loss= 9549.92868559249\n",
      "Losgistic Regression(    4900/10000): loss= 9549.50212308435\n",
      "Losgistic Regression(    5000/10000): loss= 9549.20003865299\n",
      "Losgistic Regression(    5100/10000): loss= 9549.02811984574\n",
      "Losgistic Regression(    5200/10000): loss= 9548.86049096284\n",
      "Losgistic Regression(    5300/10000): loss= 9548.61393139534\n",
      "Losgistic Regression(    5400/10000): loss= 9548.22989806208\n",
      "Losgistic Regression(    5500/10000): loss= 9547.74841211251\n",
      "Losgistic Regression(    5600/10000): loss= 9547.18284095746\n",
      "Losgistic Regression(    5700/10000): loss= 9546.56393815477\n",
      "Losgistic Regression(    5800/10000): loss= 9545.95371159331\n",
      "Losgistic Regression(    5900/10000): loss= 9545.37779461361\n",
      "Losgistic Regression(    6000/10000): loss= 9544.86890721462\n",
      "Losgistic Regression(    6100/10000): loss= 9544.45881774947\n",
      "Losgistic Regression(    6200/10000): loss= 9544.09995009084\n",
      "Losgistic Regression(    6300/10000): loss= 9543.76305441762\n",
      "Losgistic Regression(    6400/10000): loss= 9543.46521390689\n",
      "Losgistic Regression(    6500/10000): loss= 9543.20097016025\n",
      "Losgistic Regression(    6600/10000): loss= 9542.9928154358\n",
      "Losgistic Regression(    6700/10000): loss= 9542.83757263533\n",
      "Losgistic Regression(    6800/10000): loss= 9542.70898806803\n",
      "Losgistic Regression(    6900/10000): loss= 9542.58440889526\n",
      "Losgistic Regression(    7000/10000): loss= 9542.51651164391\n",
      "Totoal number of iterations =  7000\n",
      "Loss                        =  9542.51651164\n",
      "Time for  1th cross validation = 563.123s\n",
      "Training Accuracy         = 0.83068\n",
      "Cross Validation Accuracy = 0.822732\n",
      "Losgistic Regression(       0/10000): loss= 16686.5652133638\n",
      "Losgistic Regression(     100/10000): loss= 11087.2056284524\n",
      "Losgistic Regression(     200/10000): loss= 10554.7831890192\n",
      "Losgistic Regression(     300/10000): loss= 10275.4472594309\n",
      "Losgistic Regression(     400/10000): loss= 10109.4905212616\n",
      "Losgistic Regression(     500/10000): loss= 10004.0330360775\n",
      "Losgistic Regression(     600/10000): loss= 9935.57473006465\n",
      "Losgistic Regression(     700/10000): loss= 9885.4894909498\n",
      "Losgistic Regression(     800/10000): loss= 9842.4850305777\n",
      "Losgistic Regression(     900/10000): loss= 9809.57508334043\n",
      "Losgistic Regression(    1000/10000): loss= 9784.4086002437\n",
      "Losgistic Regression(    1100/10000): loss= 9763.52936433341\n",
      "Losgistic Regression(    1200/10000): loss= 9745.57510848865\n",
      "Losgistic Regression(    1300/10000): loss= 9731.03778363196\n",
      "Losgistic Regression(    1400/10000): loss= 9720.01557565414\n",
      "Losgistic Regression(    1500/10000): loss= 9712.2085780036\n",
      "Losgistic Regression(    1600/10000): loss= 9706.12952008095\n",
      "Losgistic Regression(    1700/10000): loss= 9701.35953879297\n",
      "Losgistic Regression(    1800/10000): loss= 9698.33031116415\n",
      "Losgistic Regression(    1900/10000): loss= 9695.52185461822\n",
      "Losgistic Regression(    2000/10000): loss= 9692.94766145774\n",
      "Losgistic Regression(    2100/10000): loss= 9689.94131733353\n",
      "Losgistic Regression(    2200/10000): loss= 9686.60787544773\n",
      "Losgistic Regression(    2300/10000): loss= 9683.50248068297\n",
      "Losgistic Regression(    2400/10000): loss= 9680.22042262246\n",
      "Losgistic Regression(    2500/10000): loss= 9676.99700913357\n",
      "Losgistic Regression(    2600/10000): loss= 9674.08819040773\n",
      "Losgistic Regression(    2700/10000): loss= 9671.41371781924\n",
      "Losgistic Regression(    2800/10000): loss= 9668.80153507162\n",
      "Losgistic Regression(    2900/10000): loss= 9665.24557464973\n",
      "Losgistic Regression(    3000/10000): loss= 9661.68645513005\n",
      "Losgistic Regression(    3100/10000): loss= 9658.20248541139\n",
      "Losgistic Regression(    3200/10000): loss= 9654.80145749821\n",
      "Losgistic Regression(    3300/10000): loss= 9651.62232077611\n",
      "Losgistic Regression(    3400/10000): loss= 9648.55407604312\n",
      "Losgistic Regression(    3500/10000): loss= 9645.40099544163\n",
      "Losgistic Regression(    3600/10000): loss= 9642.34608321495\n",
      "Losgistic Regression(    3700/10000): loss= 9639.53976587508\n",
      "Losgistic Regression(    3800/10000): loss= 9636.89709201946\n",
      "Losgistic Regression(    3900/10000): loss= 9634.26173958574\n",
      "Losgistic Regression(    4000/10000): loss= 9631.77957921416\n",
      "Losgistic Regression(    4100/10000): loss= 9629.56422075222\n",
      "Losgistic Regression(    4200/10000): loss= 9627.40287645155\n",
      "Losgistic Regression(    4300/10000): loss= 9625.24081396458\n",
      "Losgistic Regression(    4400/10000): loss= 9623.23978160724\n",
      "Losgistic Regression(    4500/10000): loss= 9621.43169090652\n",
      "Losgistic Regression(    4600/10000): loss= 9619.81295387361\n",
      "Losgistic Regression(    4700/10000): loss= 9618.47810744658\n",
      "Losgistic Regression(    4800/10000): loss= 9617.40125849227\n",
      "Losgistic Regression(    4900/10000): loss= 9616.47518474468\n",
      "Losgistic Regression(    5000/10000): loss= 9615.69656820779\n",
      "Losgistic Regression(    5100/10000): loss= 9615.1093672392\n",
      "Losgistic Regression(    5200/10000): loss= 9614.57657532777\n",
      "Losgistic Regression(    5300/10000): loss= 9614.01873768287\n",
      "Losgistic Regression(    5400/10000): loss= 9613.46214384261\n",
      "Losgistic Regression(    5500/10000): loss= 9612.91385074185\n",
      "Losgistic Regression(    5600/10000): loss= 9612.37917512868\n",
      "Losgistic Regression(    5700/10000): loss= 9611.87533785296\n",
      "Losgistic Regression(    5800/10000): loss= 9611.40597594536\n",
      "Losgistic Regression(    5900/10000): loss= 9610.96614936513\n",
      "Losgistic Regression(    6000/10000): loss= 9610.58587048752\n",
      "Losgistic Regression(    6100/10000): loss= 9610.35317225968\n",
      "Losgistic Regression(    6200/10000): loss= 9610.28100524076\n",
      "Totoal number of iterations =  6200\n",
      "Loss                        =  9610.28100524\n",
      "Time for  2th cross validation = 497.753s\n",
      "Training Accuracy         = 0.82776\n",
      "Cross Validation Accuracy = 0.823048\n",
      "Losgistic Regression(       0/10000): loss= 16690.1693326792\n",
      "Losgistic Regression(     100/10000): loss= 10979.5159762389\n",
      "Losgistic Regression(     200/10000): loss= 10418.9388205117\n",
      "Losgistic Regression(     300/10000): loss= 10171.7054110725\n",
      "Losgistic Regression(     400/10000): loss= 10039.0545799237\n",
      "Losgistic Regression(     500/10000): loss= 9954.64772694487\n",
      "Losgistic Regression(     600/10000): loss= 9892.91761976932\n",
      "Losgistic Regression(     700/10000): loss= 9842.1088375306\n",
      "Losgistic Regression(     800/10000): loss= 9797.81404989021\n",
      "Losgistic Regression(     900/10000): loss= 9758.48794737446\n",
      "Losgistic Regression(    1000/10000): loss= 9728.25367270769\n",
      "Losgistic Regression(    1100/10000): loss= 9707.3345845467\n",
      "Losgistic Regression(    1200/10000): loss= 9686.96608261127\n",
      "Losgistic Regression(    1300/10000): loss= 9672.74443639289\n",
      "Losgistic Regression(    1400/10000): loss= 9659.64506256327\n",
      "Losgistic Regression(    1500/10000): loss= 9648.06103597778\n",
      "Losgistic Regression(    1600/10000): loss= 9637.83079102741\n",
      "Losgistic Regression(    1700/10000): loss= 9628.788310467\n",
      "Losgistic Regression(    1800/10000): loss= 9620.82630474579\n",
      "Losgistic Regression(    1900/10000): loss= 9612.93118523668\n",
      "Losgistic Regression(    2000/10000): loss= 9605.1249307962\n",
      "Losgistic Regression(    2100/10000): loss= 9597.99543555041\n",
      "Losgistic Regression(    2200/10000): loss= 9591.05431930921\n",
      "Losgistic Regression(    2300/10000): loss= 9584.00742195772\n",
      "Losgistic Regression(    2400/10000): loss= 9577.31902479083\n",
      "Losgistic Regression(    2500/10000): loss= 9572.38957038244\n",
      "Losgistic Regression(    2600/10000): loss= 9569.09708242644\n",
      "Losgistic Regression(    2700/10000): loss= 9565.86116606872\n",
      "Losgistic Regression(    2800/10000): loss= 9561.97944128419\n",
      "Losgistic Regression(    2900/10000): loss= 9557.99463821147\n",
      "Losgistic Regression(    3000/10000): loss= 9554.43864242975\n",
      "Losgistic Regression(    3100/10000): loss= 9551.53774922746\n",
      "Losgistic Regression(    3200/10000): loss= 9549.16805757507\n",
      "Losgistic Regression(    3300/10000): loss= 9547.1413365329\n",
      "Losgistic Regression(    3400/10000): loss= 9545.3998331123\n",
      "Losgistic Regression(    3500/10000): loss= 9543.7583113791\n",
      "Losgistic Regression(    3600/10000): loss= 9541.99825680293\n",
      "Losgistic Regression(    3700/10000): loss= 9540.38144436245\n",
      "Losgistic Regression(    3800/10000): loss= 9538.93848008997\n",
      "Losgistic Regression(    3900/10000): loss= 9537.75982455805\n",
      "Losgistic Regression(    4000/10000): loss= 9536.57010697605\n",
      "Losgistic Regression(    4100/10000): loss= 9535.24729157959\n",
      "Losgistic Regression(    4200/10000): loss= 9534.03747361881\n",
      "Losgistic Regression(    4300/10000): loss= 9532.98161344499\n",
      "Losgistic Regression(    4400/10000): loss= 9531.84002202419\n",
      "Losgistic Regression(    4500/10000): loss= 9530.5423148372\n",
      "Losgistic Regression(    4600/10000): loss= 9529.19315115943\n",
      "Losgistic Regression(    4700/10000): loss= 9527.79438656501\n",
      "Losgistic Regression(    4800/10000): loss= 9526.2141124816\n",
      "Losgistic Regression(    4900/10000): loss= 9524.51309717104\n",
      "Losgistic Regression(    5000/10000): loss= 9522.79267180633\n",
      "Losgistic Regression(    5100/10000): loss= 9521.07146548451\n",
      "Losgistic Regression(    5200/10000): loss= 9519.28666452641\n",
      "Losgistic Regression(    5300/10000): loss= 9517.37854936752\n",
      "Losgistic Regression(    5400/10000): loss= 9515.46218771572\n",
      "Losgistic Regression(    5500/10000): loss= 9513.63635110087\n",
      "Losgistic Regression(    5600/10000): loss= 9511.94108202126\n",
      "Losgistic Regression(    5700/10000): loss= 9510.36263287728\n",
      "Losgistic Regression(    5800/10000): loss= 9508.89625092476\n",
      "Losgistic Regression(    5900/10000): loss= 9507.47834944432\n",
      "Losgistic Regression(    6000/10000): loss= 9505.96934252285\n",
      "Losgistic Regression(    6100/10000): loss= 9504.42006731406\n",
      "Losgistic Regression(    6200/10000): loss= 9502.97518882886\n",
      "Losgistic Regression(    6300/10000): loss= 9501.62168753329\n",
      "Losgistic Regression(    6400/10000): loss= 9500.26667724121\n",
      "Losgistic Regression(    6500/10000): loss= 9498.8557189934\n",
      "Losgistic Regression(    6600/10000): loss= 9497.46049658491\n",
      "Losgistic Regression(    6700/10000): loss= 9496.21976729171\n",
      "Losgistic Regression(    6800/10000): loss= 9495.17262144168\n",
      "Losgistic Regression(    6900/10000): loss= 9494.20892376746\n",
      "Losgistic Regression(    7000/10000): loss= 9493.21935392627\n",
      "Losgistic Regression(    7100/10000): loss= 9492.14373712686\n",
      "Losgistic Regression(    7200/10000): loss= 9490.9430519648\n",
      "Losgistic Regression(    7300/10000): loss= 9489.64256881825\n",
      "Losgistic Regression(    7400/10000): loss= 9488.31631456605\n",
      "Losgistic Regression(    7500/10000): loss= 9486.9436834399\n",
      "Losgistic Regression(    7600/10000): loss= 9485.5568884501\n",
      "Losgistic Regression(    7700/10000): loss= 9484.15908637886\n",
      "Losgistic Regression(    7800/10000): loss= 9482.68791122197\n",
      "Losgistic Regression(    7900/10000): loss= 9481.06281881004\n",
      "Losgistic Regression(    8000/10000): loss= 9479.25139327432\n",
      "Losgistic Regression(    8100/10000): loss= 9477.28360638952\n",
      "Losgistic Regression(    8200/10000): loss= 9475.23273799423\n",
      "Losgistic Regression(    8300/10000): loss= 9473.18384816998\n",
      "Losgistic Regression(    8400/10000): loss= 9471.15839990888\n",
      "Losgistic Regression(    8500/10000): loss= 9469.10821597562\n",
      "Losgistic Regression(    8600/10000): loss= 9467.00998836247\n",
      "Losgistic Regression(    8700/10000): loss= 9464.88562612693\n",
      "Losgistic Regression(    8800/10000): loss= 9462.75030636073\n",
      "Losgistic Regression(    8900/10000): loss= 9460.590086386\n",
      "Losgistic Regression(    9000/10000): loss= 9458.40958751683\n",
      "Losgistic Regression(    9100/10000): loss= 9456.24915811995\n",
      "Losgistic Regression(    9200/10000): loss= 9454.16019776404\n",
      "Losgistic Regression(    9300/10000): loss= 9452.01435410186\n",
      "Losgistic Regression(    9400/10000): loss= 9449.94721802146\n",
      "Losgistic Regression(    9500/10000): loss= 9447.93586722961\n",
      "Losgistic Regression(    9600/10000): loss= 9445.96730703219\n",
      "Losgistic Regression(    9700/10000): loss= 9444.02636761827\n",
      "Losgistic Regression(    9800/10000): loss= 9442.1256695905\n",
      "Losgistic Regression(    9900/10000): loss= 9440.29066802287\n",
      "Time for  3th cross validation = 800.495s\n",
      "Training Accuracy         = 0.83224\n",
      "Cross Validation Accuracy = 0.822012\n",
      "Losgistic Regression(       0/10000): loss= 16714.9352168016\n",
      "Losgistic Regression(     100/10000): loss= 10877.4068136371\n",
      "Losgistic Regression(     200/10000): loss= 10336.663718486\n",
      "Losgistic Regression(     300/10000): loss= 10056.4744083637\n",
      "Losgistic Regression(     400/10000): loss= 9894.25179536515\n",
      "Losgistic Regression(     500/10000): loss= 9804.35986047491\n",
      "Losgistic Regression(     600/10000): loss= 9733.96853983712\n",
      "Losgistic Regression(     700/10000): loss= 9663.225499949\n",
      "Losgistic Regression(     800/10000): loss= 9588.31055740017\n",
      "Losgistic Regression(     900/10000): loss= 9517.21791325575\n",
      "Losgistic Regression(    1000/10000): loss= 9458.66564005038\n",
      "Losgistic Regression(    1100/10000): loss= 9417.90534222798\n",
      "Losgistic Regression(    1200/10000): loss= 9395.82932222514\n",
      "Losgistic Regression(    1300/10000): loss= 9389.56665158512\n",
      "Losgistic Regression(    1400/10000): loss= 9389.70092713105\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  9389.70092713\n",
      "Time for  4th cross validation = 115.26s\n",
      "Training Accuracy         = 0.82672\n",
      "Cross Validation Accuracy = 0.82068\n",
      "*************** ([0.83272000000000002, 0.83067999999999997, 0.82776000000000005, 0.83223999999999998, 0.82672000000000001], [0.82135199999999997, 0.82273200000000002, 0.823048, 0.82201199999999996, 0.82067999999999997])\n",
      "Losgistic Regression(       0/10000): loss= 16721.0421576809\n",
      "Losgistic Regression(     100/10000): loss= 10933.0023648913\n",
      "Losgistic Regression(     200/10000): loss= 10408.8168137154\n",
      "Losgistic Regression(     300/10000): loss= 10148.2448093156\n",
      "Losgistic Regression(     400/10000): loss= 9986.16449739506\n",
      "Losgistic Regression(     500/10000): loss= 9874.34573879399\n",
      "Losgistic Regression(     600/10000): loss= 9788.25099483393\n",
      "Losgistic Regression(     700/10000): loss= 9717.04281795243\n",
      "Losgistic Regression(     800/10000): loss= 9665.48673809017\n",
      "Losgistic Regression(     900/10000): loss= 9630.91201303741\n",
      "Losgistic Regression(    1000/10000): loss= 9603.66714220648\n",
      "Losgistic Regression(    1100/10000): loss= 9584.13614968008\n",
      "Losgistic Regression(    1200/10000): loss= 9568.51955831975\n",
      "Losgistic Regression(    1300/10000): loss= 9556.43415892366\n",
      "Losgistic Regression(    1400/10000): loss= 9546.25870158415\n",
      "Losgistic Regression(    1500/10000): loss= 9533.58841715284\n",
      "Losgistic Regression(    1600/10000): loss= 9524.07890166127\n",
      "Losgistic Regression(    1700/10000): loss= 9513.02990232278\n",
      "Losgistic Regression(    1800/10000): loss= 9500.27703523255\n",
      "Losgistic Regression(    1900/10000): loss= 9487.15482382217\n",
      "Losgistic Regression(    2000/10000): loss= 9475.32179850338\n",
      "Losgistic Regression(    2100/10000): loss= 9463.85053804286\n",
      "Losgistic Regression(    2200/10000): loss= 9453.55992039426\n",
      "Losgistic Regression(    2300/10000): loss= 9443.67541294906\n",
      "Losgistic Regression(    2400/10000): loss= 9434.60532973884\n",
      "Losgistic Regression(    2500/10000): loss= 9426.63240357161\n",
      "Losgistic Regression(    2600/10000): loss= 9419.68903426864\n",
      "Losgistic Regression(    2700/10000): loss= 9413.73780964354\n",
      "Losgistic Regression(    2800/10000): loss= 9408.87294451709\n",
      "Losgistic Regression(    2900/10000): loss= 9405.05996385695\n",
      "Losgistic Regression(    3000/10000): loss= 9401.74179673284\n",
      "Losgistic Regression(    3100/10000): loss= 9398.7087800385\n",
      "Losgistic Regression(    3200/10000): loss= 9396.39369736093\n",
      "Losgistic Regression(    3300/10000): loss= 9395.10096778915\n",
      "Losgistic Regression(    3400/10000): loss= 9394.74497754403\n",
      "Losgistic Regression(    3500/10000): loss= 9394.75012869785\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9394.7501287\n",
      "Time for  0th cross validation = 282.152s\n",
      "Training Accuracy         = 0.83264\n",
      "Cross Validation Accuracy = 0.821412\n",
      "Losgistic Regression(       0/10000): loss= 16695.4278216259\n",
      "Losgistic Regression(     100/10000): loss= 11025.7085863454\n",
      "Losgistic Regression(     200/10000): loss= 10529.4213367516\n",
      "Losgistic Regression(     300/10000): loss= 10282.8303102075\n",
      "Losgistic Regression(     400/10000): loss= 10119.7876855842\n",
      "Losgistic Regression(     500/10000): loss= 9992.56547277029\n",
      "Losgistic Regression(     600/10000): loss= 9906.68306899201\n",
      "Losgistic Regression(     700/10000): loss= 9846.74419002081\n",
      "Losgistic Regression(     800/10000): loss= 9802.92079249081\n",
      "Losgistic Regression(     900/10000): loss= 9763.35544155573\n",
      "Losgistic Regression(    1000/10000): loss= 9731.23076848458\n",
      "Losgistic Regression(    1100/10000): loss= 9703.58778112456\n",
      "Losgistic Regression(    1200/10000): loss= 9682.45756572605\n",
      "Losgistic Regression(    1300/10000): loss= 9665.60056025506\n",
      "Losgistic Regression(    1400/10000): loss= 9651.14719527853\n",
      "Losgistic Regression(    1500/10000): loss= 9641.78412154364\n",
      "Losgistic Regression(    1600/10000): loss= 9636.71600148426\n",
      "Losgistic Regression(    1700/10000): loss= 9628.85306225774\n",
      "Losgistic Regression(    1800/10000): loss= 9619.82057178049\n",
      "Losgistic Regression(    1900/10000): loss= 9612.46212673555\n",
      "Losgistic Regression(    2000/10000): loss= 9605.52874032473\n",
      "Losgistic Regression(    2100/10000): loss= 9599.74240765766\n",
      "Losgistic Regression(    2200/10000): loss= 9594.19724589564\n",
      "Losgistic Regression(    2300/10000): loss= 9589.41510386927\n",
      "Losgistic Regression(    2400/10000): loss= 9585.26371006755\n",
      "Losgistic Regression(    2500/10000): loss= 9581.89178575116\n",
      "Losgistic Regression(    2600/10000): loss= 9579.09066067271\n",
      "Losgistic Regression(    2700/10000): loss= 9576.5359048911\n",
      "Losgistic Regression(    2800/10000): loss= 9574.43838601375\n",
      "Losgistic Regression(    2900/10000): loss= 9572.90356086495\n",
      "Losgistic Regression(    3000/10000): loss= 9572.01586654096\n",
      "Losgistic Regression(    3100/10000): loss= 9571.26684875474\n",
      "Losgistic Regression(    3200/10000): loss= 9570.33117973201\n",
      "Losgistic Regression(    3300/10000): loss= 9569.36651549529\n",
      "Losgistic Regression(    3400/10000): loss= 9568.01943652614\n",
      "Losgistic Regression(    3500/10000): loss= 9566.68821715583\n",
      "Losgistic Regression(    3600/10000): loss= 9565.25069570449\n",
      "Losgistic Regression(    3700/10000): loss= 9564.17172744081\n",
      "Losgistic Regression(    3800/10000): loss= 9563.13343217954\n",
      "Losgistic Regression(    3900/10000): loss= 9562.29480860953\n",
      "Losgistic Regression(    4000/10000): loss= 9561.7668972331\n",
      "Losgistic Regression(    4100/10000): loss= 9561.32792469389\n",
      "Losgistic Regression(    4200/10000): loss= 9560.98103173438\n",
      "Losgistic Regression(    4300/10000): loss= 9560.48757688918\n",
      "Losgistic Regression(    4400/10000): loss= 9559.93025937728\n",
      "Losgistic Regression(    4500/10000): loss= 9559.3683467058\n",
      "Losgistic Regression(    4600/10000): loss= 9558.89635107345\n",
      "Losgistic Regression(    4700/10000): loss= 9558.39252021453\n",
      "Losgistic Regression(    4800/10000): loss= 9557.93914269809\n",
      "Losgistic Regression(    4900/10000): loss= 9557.63021986005\n",
      "Losgistic Regression(    5000/10000): loss= 9557.47426490812\n",
      "Losgistic Regression(    5100/10000): loss= 9557.43954326042\n",
      "Totoal number of iterations =  5100\n",
      "Loss                        =  9557.43954326\n",
      "Time for  1th cross validation = 412.263s\n",
      "Training Accuracy         = 0.83056\n",
      "Cross Validation Accuracy = 0.822716\n",
      "Losgistic Regression(       0/10000): loss= 16686.5796193506\n",
      "Losgistic Regression(     100/10000): loss= 11088.0837504582\n",
      "Losgistic Regression(     200/10000): loss= 10556.3733438348\n",
      "Losgistic Regression(     300/10000): loss= 10277.6674871079\n",
      "Losgistic Regression(     400/10000): loss= 10112.2241386922\n",
      "Losgistic Regression(     500/10000): loss= 10007.1209266958\n",
      "Losgistic Regression(     600/10000): loss= 9939.03216319392\n",
      "Losgistic Regression(     700/10000): loss= 9889.24226311191\n",
      "Losgistic Regression(     800/10000): loss= 9846.48471149141\n",
      "Losgistic Regression(     900/10000): loss= 9813.80335268373\n",
      "Losgistic Regression(    1000/10000): loss= 9788.89443910435\n",
      "Losgistic Regression(    1100/10000): loss= 9768.18893189943\n",
      "Losgistic Regression(    1200/10000): loss= 9750.47180171595\n",
      "Losgistic Regression(    1300/10000): loss= 9736.13689653235\n",
      "Losgistic Regression(    1400/10000): loss= 9725.232890057\n",
      "Losgistic Regression(    1500/10000): loss= 9717.53751302159\n",
      "Losgistic Regression(    1600/10000): loss= 9711.514503594\n",
      "Losgistic Regression(    1700/10000): loss= 9706.78690301911\n",
      "Losgistic Regression(    1800/10000): loss= 9703.81232869219\n",
      "Losgistic Regression(    1900/10000): loss= 9701.02309190103\n",
      "Losgistic Regression(    2000/10000): loss= 9698.46237672667\n",
      "Losgistic Regression(    2100/10000): loss= 9695.47954503396\n",
      "Losgistic Regression(    2200/10000): loss= 9692.16964160209\n",
      "Losgistic Regression(    2300/10000): loss= 9689.11922708213\n",
      "Losgistic Regression(    2400/10000): loss= 9685.8722132956\n",
      "Losgistic Regression(    2500/10000): loss= 9682.66252139504\n",
      "Losgistic Regression(    2600/10000): loss= 9679.73535232115\n",
      "Losgistic Regression(    2700/10000): loss= 9677.04390705188\n",
      "Losgistic Regression(    2800/10000): loss= 9674.44786774357\n",
      "Losgistic Regression(    2900/10000): loss= 9670.66107310514\n",
      "Losgistic Regression(    3000/10000): loss= 9667.15397362024\n",
      "Losgistic Regression(    3100/10000): loss= 9663.74408169828\n",
      "Losgistic Regression(    3200/10000): loss= 9660.43095231369\n",
      "Losgistic Regression(    3300/10000): loss= 9657.36669126088\n",
      "Losgistic Regression(    3400/10000): loss= 9654.40684018234\n",
      "Losgistic Regression(    3500/10000): loss= 9651.35426536782\n",
      "Losgistic Regression(    3600/10000): loss= 9648.37907601146\n",
      "Losgistic Regression(    3700/10000): loss= 9645.62861036523\n",
      "Losgistic Regression(    3800/10000): loss= 9643.04416421516\n",
      "Losgistic Regression(    3900/10000): loss= 9640.47769309787\n",
      "Losgistic Regression(    4000/10000): loss= 9638.07700062413\n",
      "Losgistic Regression(    4100/10000): loss= 9635.92361870277\n",
      "Losgistic Regression(    4200/10000): loss= 9633.82345605112\n",
      "Losgistic Regression(    4300/10000): loss= 9631.72843947436\n",
      "Losgistic Regression(    4400/10000): loss= 9629.79960970847\n",
      "Losgistic Regression(    4500/10000): loss= 9628.10038406764\n",
      "Losgistic Regression(    4600/10000): loss= 9626.62573169338\n",
      "Losgistic Regression(    4700/10000): loss= 9625.44561501992\n",
      "Losgistic Regression(    4800/10000): loss= 9624.54116037393\n",
      "Losgistic Regression(    4900/10000): loss= 9623.81368936401\n",
      "Losgistic Regression(    5000/10000): loss= 9623.20965136125\n",
      "Losgistic Regression(    5100/10000): loss= 9622.74238161199\n",
      "Losgistic Regression(    5200/10000): loss= 9622.30339354479\n",
      "Losgistic Regression(    5300/10000): loss= 9621.85064941348\n",
      "Losgistic Regression(    5400/10000): loss= 9621.40954646163\n",
      "Losgistic Regression(    5500/10000): loss= 9620.99964269766\n",
      "Losgistic Regression(    5600/10000): loss= 9620.62098026767\n",
      "Losgistic Regression(    5700/10000): loss= 9620.2585085731\n",
      "Losgistic Regression(    5800/10000): loss= 9619.92072355267\n",
      "Losgistic Regression(    5900/10000): loss= 9619.60269689478\n",
      "Losgistic Regression(    6000/10000): loss= 9619.33320948524\n",
      "Losgistic Regression(    6100/10000): loss= 9619.18646732037\n",
      "Losgistic Regression(    6200/10000): loss= 9619.17151900179\n",
      "Totoal number of iterations =  6200\n",
      "Loss                        =  9619.171519\n",
      "Time for  2th cross validation = 500.717s\n",
      "Training Accuracy         = 0.8278\n",
      "Cross Validation Accuracy = 0.823104\n",
      "Losgistic Regression(       0/10000): loss= 16690.1837477403\n",
      "Losgistic Regression(     100/10000): loss= 10980.3798588872\n",
      "Losgistic Regression(     200/10000): loss= 10420.5116168448\n",
      "Losgistic Regression(     300/10000): loss= 10173.8261621578\n",
      "Losgistic Regression(     400/10000): loss= 10041.5811885348\n",
      "Losgistic Regression(     500/10000): loss= 9957.58080645194\n",
      "Losgistic Regression(     600/10000): loss= 9896.25354992434\n",
      "Losgistic Regression(     700/10000): loss= 9845.79244007287\n",
      "Losgistic Regression(     800/10000): loss= 9801.84866806661\n",
      "Losgistic Regression(     900/10000): loss= 9762.85422127077\n",
      "Losgistic Regression(    1000/10000): loss= 9732.86632051771\n",
      "Losgistic Regression(    1100/10000): loss= 9712.19160145377\n",
      "Losgistic Regression(    1200/10000): loss= 9692.0482876422\n",
      "Losgistic Regression(    1300/10000): loss= 9678.04402683495\n",
      "Losgistic Regression(    1400/10000): loss= 9665.1881998807\n",
      "Losgistic Regression(    1500/10000): loss= 9653.80849733323\n",
      "Losgistic Regression(    1600/10000): loss= 9643.85031723867\n",
      "Losgistic Regression(    1700/10000): loss= 9635.05932541047\n",
      "Losgistic Regression(    1800/10000): loss= 9627.31847852033\n",
      "Losgistic Regression(    1900/10000): loss= 9619.77626222667\n",
      "Losgistic Regression(    2000/10000): loss= 9612.17742407279\n",
      "Losgistic Regression(    2100/10000): loss= 9605.22662542327\n",
      "Losgistic Regression(    2200/10000): loss= 9598.44868782878\n",
      "Losgistic Regression(    2300/10000): loss= 9591.53269759336\n",
      "Losgistic Regression(    2400/10000): loss= 9585.16933309722\n",
      "Losgistic Regression(    2500/10000): loss= 9580.35134973012\n",
      "Losgistic Regression(    2600/10000): loss= 9577.1340562341\n",
      "Losgistic Regression(    2700/10000): loss= 9573.94673215025\n",
      "Losgistic Regression(    2800/10000): loss= 9570.11637815423\n",
      "Losgistic Regression(    2900/10000): loss= 9566.21043001911\n",
      "Losgistic Regression(    3000/10000): loss= 9562.76024956759\n",
      "Losgistic Regression(    3100/10000): loss= 9560.00189091251\n",
      "Losgistic Regression(    3200/10000): loss= 9557.85226463326\n",
      "Losgistic Regression(    3300/10000): loss= 9556.00995232541\n",
      "Losgistic Regression(    3400/10000): loss= 9554.26433867799\n",
      "Losgistic Regression(    3500/10000): loss= 9552.49724589246\n",
      "Losgistic Regression(    3600/10000): loss= 9550.79117712974\n",
      "Losgistic Regression(    3700/10000): loss= 9549.22941993512\n",
      "Losgistic Regression(    3800/10000): loss= 9547.85037441519\n",
      "Losgistic Regression(    3900/10000): loss= 9546.75651839106\n",
      "Losgistic Regression(    4000/10000): loss= 9545.60991729142\n",
      "Losgistic Regression(    4100/10000): loss= 9544.33776044445\n",
      "Losgistic Regression(    4200/10000): loss= 9543.16056905719\n",
      "Losgistic Regression(    4300/10000): loss= 9542.12748508555\n",
      "Losgistic Regression(    4400/10000): loss= 9541.00292085815\n",
      "Losgistic Regression(    4500/10000): loss= 9539.74449981691\n",
      "Losgistic Regression(    4600/10000): loss= 9538.46083894593\n",
      "Losgistic Regression(    4700/10000): loss= 9537.10950429994\n",
      "Losgistic Regression(    4800/10000): loss= 9535.58144871012\n",
      "Losgistic Regression(    4900/10000): loss= 9533.97552709521\n",
      "Losgistic Regression(    5000/10000): loss= 9532.39826658095\n",
      "Losgistic Regression(    5100/10000): loss= 9530.84685330232\n",
      "Losgistic Regression(    5200/10000): loss= 9529.29773681176\n",
      "Losgistic Regression(    5300/10000): loss= 9527.70443514096\n",
      "Losgistic Regression(    5400/10000): loss= 9526.07615179907\n",
      "Losgistic Regression(    5500/10000): loss= 9524.49357352857\n",
      "Losgistic Regression(    5600/10000): loss= 9523.03529654325\n",
      "Losgistic Regression(    5700/10000): loss= 9521.69295289985\n",
      "Losgistic Regression(    5800/10000): loss= 9520.46101118708\n",
      "Losgistic Regression(    5900/10000): loss= 9519.24446130326\n",
      "Losgistic Regression(    6000/10000): loss= 9517.9355491897\n",
      "Losgistic Regression(    6100/10000): loss= 9516.5981279653\n",
      "Losgistic Regression(    6200/10000): loss= 9515.38128813542\n",
      "Losgistic Regression(    6300/10000): loss= 9514.24648991327\n",
      "Losgistic Regression(    6400/10000): loss= 9513.10880943702\n",
      "Losgistic Regression(    6500/10000): loss= 9511.93225065026\n",
      "Losgistic Regression(    6600/10000): loss= 9510.77176039691\n",
      "Losgistic Regression(    6700/10000): loss= 9509.7613289093\n",
      "Losgistic Regression(    6800/10000): loss= 9508.92828591733\n",
      "Losgistic Regression(    6900/10000): loss= 9508.15164511961\n",
      "Losgistic Regression(    7000/10000): loss= 9507.31217912346\n",
      "Losgistic Regression(    7100/10000): loss= 9506.36592481769\n",
      "Losgistic Regression(    7200/10000): loss= 9505.30815284533\n",
      "Losgistic Regression(    7300/10000): loss= 9504.20816491387\n",
      "Losgistic Regression(    7400/10000): loss= 9503.11962053664\n",
      "Losgistic Regression(    7500/10000): loss= 9502.09657933542\n",
      "Losgistic Regression(    7600/10000): loss= 9501.08068471695\n",
      "Losgistic Regression(    7700/10000): loss= 9500.00776171984\n",
      "Losgistic Regression(    7800/10000): loss= 9498.80592652385\n",
      "Losgistic Regression(    7900/10000): loss= 9497.42727283992\n",
      "Losgistic Regression(    8000/10000): loss= 9495.82961061232\n",
      "Losgistic Regression(    8100/10000): loss= 9494.01851444303\n",
      "Losgistic Regression(    8200/10000): loss= 9492.17208298815\n",
      "Losgistic Regression(    8300/10000): loss= 9490.31018165395\n",
      "Losgistic Regression(    8400/10000): loss= 9488.44760895114\n",
      "Losgistic Regression(    8500/10000): loss= 9486.57772316667\n",
      "Losgistic Regression(    8600/10000): loss= 9484.65064326267\n",
      "Losgistic Regression(    8700/10000): loss= 9482.70081878749\n",
      "Losgistic Regression(    8800/10000): loss= 9480.76410571921\n",
      "Losgistic Regression(    8900/10000): loss= 9478.82942165348\n",
      "Losgistic Regression(    9000/10000): loss= 9476.8854254138\n",
      "Losgistic Regression(    9100/10000): loss= 9474.97229872542\n",
      "Losgistic Regression(    9200/10000): loss= 9473.13454240482\n",
      "Losgistic Regression(    9300/10000): loss= 9471.36201661531\n",
      "Losgistic Regression(    9400/10000): loss= 9469.64239467118\n",
      "Losgistic Regression(    9500/10000): loss= 9467.93944021414\n",
      "Losgistic Regression(    9600/10000): loss= 9466.14479077979\n",
      "Losgistic Regression(    9700/10000): loss= 9464.33764166155\n",
      "Losgistic Regression(    9800/10000): loss= 9462.56793118899\n",
      "Losgistic Regression(    9900/10000): loss= 9460.84762816548\n",
      "Time for  3th cross validation = 803.074s\n",
      "Training Accuracy         = 0.83252\n",
      "Cross Validation Accuracy = 0.82216\n",
      "Losgistic Regression(       0/10000): loss= 16714.9493482786\n",
      "Losgistic Regression(     100/10000): loss= 10878.4204629477\n",
      "Losgistic Regression(     200/10000): loss= 10338.6248930628\n",
      "Losgistic Regression(     300/10000): loss= 10059.2102892487\n",
      "Losgistic Regression(     400/10000): loss= 9897.19572573546\n",
      "Losgistic Regression(     500/10000): loss= 9807.33207808054\n",
      "Losgistic Regression(     600/10000): loss= 9737.15110445913\n",
      "Losgistic Regression(     700/10000): loss= 9666.47400771273\n",
      "Losgistic Regression(     800/10000): loss= 9591.35084381282\n",
      "Losgistic Regression(     900/10000): loss= 9520.09600754885\n",
      "Losgistic Regression(    1000/10000): loss= 9461.51416730307\n",
      "Losgistic Regression(    1100/10000): loss= 9421.07191396316\n",
      "Losgistic Regression(    1200/10000): loss= 9399.65001480346\n",
      "Losgistic Regression(    1300/10000): loss= 9394.36784136525\n",
      "Losgistic Regression(    1400/10000): loss= 9394.4936517976\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  9394.4936518\n",
      "Time for  4th cross validation = 112.324s\n",
      "Training Accuracy         = 0.82672\n",
      "Cross Validation Accuracy = 0.82072\n",
      "*************** ([0.83264000000000005, 0.83055999999999996, 0.82779999999999998, 0.83252000000000004, 0.82672000000000001], [0.82141200000000003, 0.822716, 0.82310399999999995, 0.82216, 0.82072000000000001])\n",
      "Losgistic Regression(       0/10000): loss= 16721.091304587\n",
      "Losgistic Regression(     100/10000): loss= 10936.0526058619\n",
      "Losgistic Regression(     200/10000): loss= 10414.3726339898\n",
      "Losgistic Regression(     300/10000): loss= 10155.731123023\n",
      "Losgistic Regression(     400/10000): loss= 9995.25526194143\n",
      "Losgistic Regression(     500/10000): loss= 9884.88871887163\n",
      "Losgistic Regression(     600/10000): loss= 9800.21526053378\n",
      "Losgistic Regression(     700/10000): loss= 9730.48444233151\n",
      "Losgistic Regression(     800/10000): loss= 9680.41176015505\n",
      "Losgistic Regression(     900/10000): loss= 9646.67598515329\n",
      "Losgistic Regression(    1000/10000): loss= 9620.21795128593\n",
      "Losgistic Regression(    1100/10000): loss= 9601.36225469408\n",
      "Losgistic Regression(    1200/10000): loss= 9586.3109368133\n",
      "Losgistic Regression(    1300/10000): loss= 9574.73710927979\n",
      "Losgistic Regression(    1400/10000): loss= 9565.04250766908\n",
      "Losgistic Regression(    1500/10000): loss= 9553.03923890673\n",
      "Losgistic Regression(    1600/10000): loss= 9544.21224917404\n",
      "Losgistic Regression(    1700/10000): loss= 9533.56917571797\n",
      "Losgistic Regression(    1800/10000): loss= 9521.2810988079\n",
      "Losgistic Regression(    1900/10000): loss= 9508.58689512414\n",
      "Losgistic Regression(    2000/10000): loss= 9497.115137197\n",
      "Losgistic Regression(    2100/10000): loss= 9486.05205660678\n",
      "Losgistic Regression(    2200/10000): loss= 9476.22746031768\n",
      "Losgistic Regression(    2300/10000): loss= 9466.88378208882\n",
      "Losgistic Regression(    2400/10000): loss= 9458.32806016362\n",
      "Losgistic Regression(    2500/10000): loss= 9450.81572373807\n",
      "Losgistic Regression(    2600/10000): loss= 9444.30279257935\n",
      "Losgistic Regression(    2700/10000): loss= 9438.85145771383\n",
      "Losgistic Regression(    2800/10000): loss= 9434.44436250407\n",
      "Losgistic Regression(    2900/10000): loss= 9431.09032869824\n",
      "Losgistic Regression(    3000/10000): loss= 9427.96888898309\n",
      "Losgistic Regression(    3100/10000): loss= 9425.08195501731\n",
      "Losgistic Regression(    3200/10000): loss= 9422.85641870474\n",
      "Losgistic Regression(    3300/10000): loss= 9421.58429139092\n",
      "Losgistic Regression(    3400/10000): loss= 9421.19623502915\n",
      "Losgistic Regression(    3500/10000): loss= 9421.20240352749\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9421.20240353\n",
      "Time for  0th cross validation = 277.441s\n",
      "Training Accuracy         = 0.83244\n",
      "Cross Validation Accuracy = 0.821492\n",
      "Losgistic Regression(       0/10000): loss= 16695.4782425898\n",
      "Losgistic Regression(     100/10000): loss= 11028.6959061135\n",
      "Losgistic Regression(     200/10000): loss= 10534.6178848483\n",
      "Losgistic Regression(     300/10000): loss= 10289.9709971647\n",
      "Losgistic Regression(     400/10000): loss= 10128.8361743364\n",
      "Losgistic Regression(     500/10000): loss= 10003.4063266209\n",
      "Losgistic Regression(     600/10000): loss= 9919.12681267107\n",
      "Losgistic Regression(     700/10000): loss= 9860.36951793835\n",
      "Losgistic Regression(     800/10000): loss= 9817.65100796121\n",
      "Losgistic Regression(     900/10000): loss= 9779.30441802105\n",
      "Losgistic Regression(    1000/10000): loss= 9748.35483031871\n",
      "Losgistic Regression(    1100/10000): loss= 9721.85252943847\n",
      "Losgistic Regression(    1200/10000): loss= 9701.85626528388\n",
      "Losgistic Regression(    1300/10000): loss= 9685.98714090103\n",
      "Losgistic Regression(    1400/10000): loss= 9672.43560679753\n",
      "Losgistic Regression(    1500/10000): loss= 9663.88692151256\n",
      "Losgistic Regression(    1600/10000): loss= 9659.45222815212\n",
      "Losgistic Regression(    1700/10000): loss= 9651.11521807579\n",
      "Losgistic Regression(    1800/10000): loss= 9642.86702957717\n",
      "Losgistic Regression(    1900/10000): loss= 9636.17046772785\n",
      "Losgistic Regression(    2000/10000): loss= 9629.84752257046\n",
      "Losgistic Regression(    2100/10000): loss= 9624.46943559927\n",
      "Losgistic Regression(    2200/10000): loss= 9619.23317303885\n",
      "Losgistic Regression(    2300/10000): loss= 9614.69536527383\n",
      "Losgistic Regression(    2400/10000): loss= 9610.80337267044\n",
      "Losgistic Regression(    2500/10000): loss= 9607.62988361923\n",
      "Losgistic Regression(    2600/10000): loss= 9604.96581204463\n",
      "Losgistic Regression(    2700/10000): loss= 9602.44048598588\n",
      "Losgistic Regression(    2800/10000): loss= 9600.42974581809\n",
      "Losgistic Regression(    2900/10000): loss= 9598.74263158923\n",
      "Losgistic Regression(    3000/10000): loss= 9597.68237610166\n",
      "Losgistic Regression(    3100/10000): loss= 9596.74103344191\n",
      "Losgistic Regression(    3200/10000): loss= 9595.68743001474\n",
      "Losgistic Regression(    3300/10000): loss= 9594.56688465504\n",
      "Losgistic Regression(    3400/10000): loss= 9593.07559433585\n",
      "Losgistic Regression(    3500/10000): loss= 9591.52044426908\n",
      "Losgistic Regression(    3600/10000): loss= 9589.85917070778\n",
      "Losgistic Regression(    3700/10000): loss= 9588.61624206253\n",
      "Losgistic Regression(    3800/10000): loss= 9587.3747349568\n",
      "Losgistic Regression(    3900/10000): loss= 9586.562394016\n",
      "Losgistic Regression(    4000/10000): loss= 9586.0452037337\n",
      "Losgistic Regression(    4100/10000): loss= 9585.77425206892\n",
      "Losgistic Regression(    4200/10000): loss= 9585.57331821204\n",
      "Losgistic Regression(    4300/10000): loss= 9585.2642041944\n",
      "Losgistic Regression(    4400/10000): loss= 9584.94698109085\n",
      "Losgistic Regression(    4500/10000): loss= 9584.55901761962\n",
      "Losgistic Regression(    4600/10000): loss= 9584.27031797266\n",
      "Losgistic Regression(    4700/10000): loss= 9583.91697156737\n",
      "Losgistic Regression(    4800/10000): loss= 9583.69034729638\n",
      "Losgistic Regression(    4900/10000): loss= 9583.64878160017\n",
      "Totoal number of iterations =  4900\n",
      "Loss                        =  9583.6487816\n",
      "Time for  1th cross validation = 385.513s\n",
      "Training Accuracy         = 0.83072\n",
      "Cross Validation Accuracy = 0.8228\n",
      "Losgistic Regression(       0/10000): loss= 16686.6297839103\n",
      "Losgistic Regression(     100/10000): loss= 11091.1370922306\n",
      "Losgistic Regression(     200/10000): loss= 10561.8973491022\n",
      "Losgistic Regression(     300/10000): loss= 10285.3693008534\n",
      "Losgistic Regression(     400/10000): loss= 10121.6978193796\n",
      "Losgistic Regression(     500/10000): loss= 10017.8078487521\n",
      "Losgistic Regression(     600/10000): loss= 9950.98977296483\n",
      "Losgistic Regression(     700/10000): loss= 9902.20468131666\n",
      "Losgistic Regression(     800/10000): loss= 9860.2952267051\n",
      "Losgistic Regression(     900/10000): loss= 9828.39410585365\n",
      "Losgistic Regression(    1000/10000): loss= 9804.36224561317\n",
      "Losgistic Regression(    1100/10000): loss= 9784.25516644986\n",
      "Losgistic Regression(    1200/10000): loss= 9767.30003506198\n",
      "Losgistic Regression(    1300/10000): loss= 9753.60640731326\n",
      "Losgistic Regression(    1400/10000): loss= 9743.10890830768\n",
      "Losgistic Regression(    1500/10000): loss= 9735.79825875322\n",
      "Losgistic Regression(    1600/10000): loss= 9729.96627013441\n",
      "Losgistic Regression(    1700/10000): loss= 9725.35717018079\n",
      "Losgistic Regression(    1800/10000): loss= 9722.50721341525\n",
      "Losgistic Regression(    1900/10000): loss= 9719.76497637012\n",
      "Losgistic Regression(    2000/10000): loss= 9717.20007324784\n",
      "Losgistic Regression(    2100/10000): loss= 9714.26386903721\n",
      "Losgistic Regression(    2200/10000): loss= 9711.07187849286\n",
      "Losgistic Regression(    2300/10000): loss= 9708.10567201388\n",
      "Losgistic Regression(    2400/10000): loss= 9704.84093907163\n",
      "Losgistic Regression(    2500/10000): loss= 9701.56660895622\n",
      "Losgistic Regression(    2600/10000): loss= 9698.53970115352\n",
      "Losgistic Regression(    2700/10000): loss= 9695.84813462532\n",
      "Losgistic Regression(    2800/10000): loss= 9692.60293296007\n",
      "Losgistic Regression(    2900/10000): loss= 9688.82835481766\n",
      "Losgistic Regression(    3000/10000): loss= 9685.74341747396\n",
      "Losgistic Regression(    3100/10000): loss= 9682.8135080163\n",
      "Losgistic Regression(    3200/10000): loss= 9679.98043050356\n",
      "Losgistic Regression(    3300/10000): loss= 9677.39248432264\n",
      "Losgistic Regression(    3400/10000): loss= 9674.94037128921\n",
      "Losgistic Regression(    3500/10000): loss= 9672.28259002137\n",
      "Losgistic Regression(    3600/10000): loss= 9669.62880875658\n",
      "Losgistic Regression(    3700/10000): loss= 9667.32347990844\n",
      "Losgistic Regression(    3800/10000): loss= 9665.23147726253\n",
      "Losgistic Regression(    3900/10000): loss= 9663.1021198243\n",
      "Losgistic Regression(    4000/10000): loss= 9661.1406508041\n",
      "Losgistic Regression(    4100/10000): loss= 9659.31003936511\n",
      "Losgistic Regression(    4200/10000): loss= 9657.4692203962\n",
      "Losgistic Regression(    4300/10000): loss= 9655.75799543275\n",
      "Losgistic Regression(    4400/10000): loss= 9654.25970192411\n",
      "Losgistic Regression(    4500/10000): loss= 9652.96829581873\n",
      "Losgistic Regression(    4600/10000): loss= 9651.71888664003\n",
      "Losgistic Regression(    4700/10000): loss= 9650.5628976475\n",
      "Losgistic Regression(    4800/10000): loss= 9649.64311171651\n",
      "Losgistic Regression(    4900/10000): loss= 9648.76644836411\n",
      "Losgistic Regression(    5000/10000): loss= 9647.90089133798\n",
      "Losgistic Regression(    5100/10000): loss= 9647.19659389657\n",
      "Losgistic Regression(    5200/10000): loss= 9646.5562786354\n",
      "Losgistic Regression(    5300/10000): loss= 9645.94069246872\n",
      "Losgistic Regression(    5400/10000): loss= 9645.46772781944\n",
      "Losgistic Regression(    5500/10000): loss= 9645.0172709149\n",
      "Losgistic Regression(    5600/10000): loss= 9644.67519188592\n",
      "Losgistic Regression(    5700/10000): loss= 9644.57457374211\n",
      "Losgistic Regression(    5800/10000): loss= 9644.57835508777\n",
      "Totoal number of iterations =  5800\n",
      "Loss                        =  9644.57835509\n",
      "Time for  2th cross validation = 458.499s\n",
      "Training Accuracy         = 0.82804\n",
      "Cross Validation Accuracy = 0.823088\n",
      "Losgistic Regression(       0/10000): loss= 16690.2339438988\n",
      "Losgistic Regression(     100/10000): loss= 10983.3840439224\n",
      "Losgistic Regression(     200/10000): loss= 10425.9755236729\n",
      "Losgistic Regression(     300/10000): loss= 10181.182842698\n",
      "Losgistic Regression(     400/10000): loss= 10050.3367271457\n",
      "Losgistic Regression(     500/10000): loss= 9967.73633191565\n",
      "Losgistic Regression(     600/10000): loss= 9907.80214528752\n",
      "Losgistic Regression(     700/10000): loss= 9858.52880200592\n",
      "Losgistic Regression(     800/10000): loss= 9815.77338683402\n",
      "Losgistic Regression(     900/10000): loss= 9777.90640403298\n",
      "Losgistic Regression(    1000/10000): loss= 9748.74180785145\n",
      "Losgistic Regression(    1100/10000): loss= 9728.87945769567\n",
      "Losgistic Regression(    1200/10000): loss= 9709.50658864753\n",
      "Losgistic Regression(    1300/10000): loss= 9696.23544563214\n",
      "Losgistic Regression(    1400/10000): loss= 9684.17715961754\n",
      "Losgistic Regression(    1500/10000): loss= 9673.53999844441\n",
      "Losgistic Regression(    1600/10000): loss= 9664.50977039053\n",
      "Losgistic Regression(    1700/10000): loss= 9656.54973148011\n",
      "Losgistic Regression(    1800/10000): loss= 9649.57488337599\n",
      "Losgistic Regression(    1900/10000): loss= 9642.83985378839\n",
      "Losgistic Regression(    2000/10000): loss= 9636.30892236921\n",
      "Losgistic Regression(    2100/10000): loss= 9629.88351191363\n",
      "Losgistic Regression(    2200/10000): loss= 9623.52509683449\n",
      "Losgistic Regression(    2300/10000): loss= 9616.92249464744\n",
      "Losgistic Regression(    2400/10000): loss= 9610.98162252214\n",
      "Losgistic Regression(    2500/10000): loss= 9606.66879484613\n",
      "Losgistic Regression(    2600/10000): loss= 9603.64519501404\n",
      "Losgistic Regression(    2700/10000): loss= 9600.58243372121\n",
      "Losgistic Regression(    2800/10000): loss= 9596.96841944911\n",
      "Losgistic Regression(    2900/10000): loss= 9593.4989669053\n",
      "Losgistic Regression(    3000/10000): loss= 9590.60389109848\n",
      "Losgistic Regression(    3100/10000): loss= 9588.37471499348\n",
      "Losgistic Regression(    3200/10000): loss= 9586.69080937532\n",
      "Losgistic Regression(    3300/10000): loss= 9585.17261888798\n",
      "Losgistic Regression(    3400/10000): loss= 9583.49977525948\n",
      "Losgistic Regression(    3500/10000): loss= 9581.80063135496\n",
      "Losgistic Regression(    3600/10000): loss= 9580.07653240336\n",
      "Losgistic Regression(    3700/10000): loss= 9578.58475653974\n",
      "Losgistic Regression(    3800/10000): loss= 9577.29993386328\n",
      "Losgistic Regression(    3900/10000): loss= 9576.35913214205\n",
      "Losgistic Regression(    4000/10000): loss= 9575.42579540014\n",
      "Losgistic Regression(    4100/10000): loss= 9574.35304386991\n",
      "Losgistic Regression(    4200/10000): loss= 9573.25816862389\n",
      "Losgistic Regression(    4300/10000): loss= 9572.16478093979\n",
      "Losgistic Regression(    4400/10000): loss= 9571.02435014099\n",
      "Losgistic Regression(    4500/10000): loss= 9569.89111168101\n",
      "Losgistic Regression(    4600/10000): loss= 9568.84420309492\n",
      "Losgistic Regression(    4700/10000): loss= 9567.84902383676\n",
      "Losgistic Regression(    4800/10000): loss= 9566.61782887522\n",
      "Losgistic Regression(    4900/10000): loss= 9565.41108778656\n",
      "Losgistic Regression(    5000/10000): loss= 9564.30228054102\n",
      "Losgistic Regression(    5100/10000): loss= 9563.28144853639\n",
      "Losgistic Regression(    5200/10000): loss= 9562.28773029316\n",
      "Losgistic Regression(    5300/10000): loss= 9561.24128981942\n",
      "Losgistic Regression(    5400/10000): loss= 9560.24569712098\n",
      "Losgistic Regression(    5500/10000): loss= 9559.39204930765\n",
      "Losgistic Regression(    5600/10000): loss= 9558.65266230111\n",
      "Losgistic Regression(    5700/10000): loss= 9558.0828314444\n",
      "Losgistic Regression(    5800/10000): loss= 9557.70667980397\n",
      "Losgistic Regression(    5900/10000): loss= 9557.41331342192\n",
      "Losgistic Regression(    6000/10000): loss= 9556.96926128826\n",
      "Losgistic Regression(    6100/10000): loss= 9556.42335186172\n",
      "Losgistic Regression(    6200/10000): loss= 9555.94641490751\n",
      "Losgistic Regression(    6300/10000): loss= 9555.50467552407\n",
      "Losgistic Regression(    6400/10000): loss= 9555.03960316595\n",
      "Losgistic Regression(    6500/10000): loss= 9554.5180790282\n",
      "Losgistic Regression(    6600/10000): loss= 9553.93553835943\n",
      "Losgistic Regression(    6700/10000): loss= 9553.31592109706\n",
      "Losgistic Regression(    6800/10000): loss= 9552.7332412116\n",
      "Losgistic Regression(    6900/10000): loss= 9552.25930700968\n",
      "Losgistic Regression(    7000/10000): loss= 9551.80281191108\n",
      "Losgistic Regression(    7100/10000): loss= 9551.33307683714\n",
      "Losgistic Regression(    7200/10000): loss= 9550.51318127614\n",
      "Losgistic Regression(    7300/10000): loss= 9549.46685442684\n",
      "Losgistic Regression(    7400/10000): loss= 9548.43867386808\n",
      "Losgistic Regression(    7500/10000): loss= 9547.35783612892\n",
      "Losgistic Regression(    7600/10000): loss= 9546.28878756737\n",
      "Losgistic Regression(    7700/10000): loss= 9545.25672416815\n",
      "Losgistic Regression(    7800/10000): loss= 9544.24562738953\n",
      "Losgistic Regression(    7900/10000): loss= 9543.0561909878\n",
      "Losgistic Regression(    8000/10000): loss= 9541.76019525054\n",
      "Losgistic Regression(    8100/10000): loss= 9540.60086324227\n",
      "Losgistic Regression(    8200/10000): loss= 9539.66486082162\n",
      "Losgistic Regression(    8300/10000): loss= 9538.90550482484\n",
      "Losgistic Regression(    8400/10000): loss= 9538.26633414399\n",
      "Losgistic Regression(    8500/10000): loss= 9537.59893596293\n",
      "Losgistic Regression(    8600/10000): loss= 9536.57226351879\n",
      "Losgistic Regression(    8700/10000): loss= 9535.32076374909\n",
      "Losgistic Regression(    8800/10000): loss= 9534.10131707739\n",
      "Losgistic Regression(    8900/10000): loss= 9533.11898746135\n",
      "Losgistic Regression(    9000/10000): loss= 9532.33143501366\n",
      "Losgistic Regression(    9100/10000): loss= 9531.65546746468\n",
      "Losgistic Regression(    9200/10000): loss= 9530.89571133369\n",
      "Losgistic Regression(    9300/10000): loss= 9530.03671908566\n",
      "Losgistic Regression(    9400/10000): loss= 9529.21592199698\n",
      "Losgistic Regression(    9500/10000): loss= 9528.60748752368\n",
      "Losgistic Regression(    9600/10000): loss= 9528.19642599625\n",
      "Losgistic Regression(    9700/10000): loss= 9527.91008093668\n",
      "Losgistic Regression(    9800/10000): loss= 9527.57513982141\n",
      "Losgistic Regression(    9900/10000): loss= 9527.06792242297\n",
      "Time for  3th cross validation = 787.663s\n",
      "Training Accuracy         = 0.83252\n",
      "Cross Validation Accuracy = 0.822112\n",
      "Losgistic Regression(       0/10000): loss= 16714.9985569395\n",
      "Losgistic Regression(     100/10000): loss= 10881.943169028\n",
      "Losgistic Regression(     200/10000): loss= 10345.4206920151\n",
      "Losgistic Regression(     300/10000): loss= 10068.5885117552\n",
      "Losgistic Regression(     400/10000): loss= 9907.19782143131\n",
      "Losgistic Regression(     500/10000): loss= 9817.43912537262\n",
      "Losgistic Regression(     600/10000): loss= 9747.65690067111\n",
      "Losgistic Regression(     700/10000): loss= 9676.80727181613\n",
      "Losgistic Regression(     800/10000): loss= 9601.276430175\n",
      "Losgistic Regression(     900/10000): loss= 9529.74985783295\n",
      "Losgistic Regression(    1000/10000): loss= 9471.7342897017\n",
      "Losgistic Regression(    1100/10000): loss= 9432.98012582129\n",
      "Losgistic Regression(    1200/10000): loss= 9414.56611718086\n",
      "Losgistic Regression(    1300/10000): loss= 9412.36424032938\n",
      "Losgistic Regression(    1400/10000): loss= 9412.45436237425\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  9412.45436237\n",
      "Time for  4th cross validation = 113.345s\n",
      "Training Accuracy         = 0.82668\n",
      "Cross Validation Accuracy = 0.82078\n",
      "*************** ([0.83243999999999996, 0.83072000000000001, 0.82804, 0.83252000000000004, 0.82667999999999997], [0.821492, 0.82279999999999998, 0.82308800000000004, 0.82211199999999995, 0.82077999999999995])\n",
      "Losgistic Regression(       0/10000): loss= 16721.2624440539\n",
      "Losgistic Regression(     100/10000): loss= 10946.6251665517\n",
      "Losgistic Regression(     200/10000): loss= 10433.5457267884\n",
      "Losgistic Regression(     300/10000): loss= 10181.4447805036\n",
      "Losgistic Regression(     400/10000): loss= 10026.3788350419\n",
      "Losgistic Regression(     500/10000): loss= 9920.88186922522\n",
      "Losgistic Regression(     600/10000): loss= 9840.98818002754\n",
      "Losgistic Regression(     700/10000): loss= 9776.31574511151\n",
      "Losgistic Regression(     800/10000): loss= 9730.52778806981\n",
      "Losgistic Regression(     900/10000): loss= 9699.76261636863\n",
      "Losgistic Regression(    1000/10000): loss= 9675.7208903328\n",
      "Losgistic Regression(    1100/10000): loss= 9658.78122942283\n",
      "Losgistic Regression(    1200/10000): loss= 9645.30029126262\n",
      "Losgistic Regression(    1300/10000): loss= 9635.30700773646\n",
      "Losgistic Regression(    1400/10000): loss= 9627.25474097081\n",
      "Losgistic Regression(    1500/10000): loss= 9617.33507502821\n",
      "Losgistic Regression(    1600/10000): loss= 9610.57424935072\n",
      "Losgistic Regression(    1700/10000): loss= 9600.61284926307\n",
      "Losgistic Regression(    1800/10000): loss= 9589.37306119173\n",
      "Losgistic Regression(    1900/10000): loss= 9578.0445296365\n",
      "Losgistic Regression(    2000/10000): loss= 9567.62524890282\n",
      "Losgistic Regression(    2100/10000): loss= 9557.24534755544\n",
      "Losgistic Regression(    2200/10000): loss= 9548.34224705974\n",
      "Losgistic Regression(    2300/10000): loss= 9540.17346380781\n",
      "Losgistic Regression(    2400/10000): loss= 9532.54524373905\n",
      "Losgistic Regression(    2500/10000): loss= 9525.65934633094\n",
      "Losgistic Regression(    2600/10000): loss= 9519.58224257528\n",
      "Losgistic Regression(    2700/10000): loss= 9515.40580248406\n",
      "Losgistic Regression(    2800/10000): loss= 9512.39395079248\n",
      "Losgistic Regression(    2900/10000): loss= 9510.19202630449\n",
      "Losgistic Regression(    3000/10000): loss= 9508.1827091303\n",
      "Losgistic Regression(    3100/10000): loss= 9505.96711384184\n",
      "Losgistic Regression(    3200/10000): loss= 9504.1876318013\n",
      "Losgistic Regression(    3300/10000): loss= 9503.15579728043\n",
      "Losgistic Regression(    3400/10000): loss= 9502.8216728384\n",
      "Losgistic Regression(    3500/10000): loss= 9502.76894286725\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9502.76894287\n",
      "Time for  0th cross validation = 277.884s\n",
      "Training Accuracy         = 0.83248\n",
      "Cross Validation Accuracy = 0.821712\n",
      "Losgistic Regression(       0/10000): loss= 16695.653818584\n",
      "Losgistic Regression(     100/10000): loss= 11039.0430777893\n",
      "Losgistic Regression(     200/10000): loss= 10552.5696178489\n",
      "Losgistic Regression(     300/10000): loss= 10314.5341629487\n",
      "Losgistic Regression(     400/10000): loss= 10159.8667369206\n",
      "Losgistic Regression(     500/10000): loss= 10040.3718578313\n",
      "Losgistic Regression(     600/10000): loss= 9960.88147836901\n",
      "Losgistic Regression(     700/10000): loss= 9906.61815954285\n",
      "Losgistic Regression(     800/10000): loss= 9867.36338090964\n",
      "Losgistic Regression(     900/10000): loss= 9833.16692382987\n",
      "Losgistic Regression(    1000/10000): loss= 9806.18649388618\n",
      "Losgistic Regression(    1100/10000): loss= 9783.53922922653\n",
      "Losgistic Regression(    1200/10000): loss= 9767.03125666039\n",
      "Losgistic Regression(    1300/10000): loss= 9754.25389361999\n",
      "Losgistic Regression(    1400/10000): loss= 9743.60845844768\n",
      "Losgistic Regression(    1500/10000): loss= 9737.55239215542\n",
      "Losgistic Regression(    1600/10000): loss= 9731.12789017477\n",
      "Losgistic Regression(    1700/10000): loss= 9724.22653678499\n",
      "Losgistic Regression(    1800/10000): loss= 9718.05969231096\n",
      "Losgistic Regression(    1900/10000): loss= 9712.75164684079\n",
      "Losgistic Regression(    2000/10000): loss= 9707.40944379963\n",
      "Losgistic Regression(    2100/10000): loss= 9702.20185578917\n",
      "Losgistic Regression(    2200/10000): loss= 9696.96538694731\n",
      "Losgistic Regression(    2300/10000): loss= 9692.77512904693\n",
      "Losgistic Regression(    2400/10000): loss= 9689.24431211395\n",
      "Losgistic Regression(    2500/10000): loss= 9686.03071307643\n",
      "Losgistic Regression(    2600/10000): loss= 9682.95296289568\n",
      "Losgistic Regression(    2700/10000): loss= 9679.87855167727\n",
      "Losgistic Regression(    2800/10000): loss= 9677.69971910872\n",
      "Losgistic Regression(    2900/10000): loss= 9675.86016989673\n",
      "Losgistic Regression(    3000/10000): loss= 9674.4740064768\n",
      "Losgistic Regression(    3100/10000): loss= 9672.96650317727\n",
      "Losgistic Regression(    3200/10000): loss= 9671.5870819644\n",
      "Losgistic Regression(    3300/10000): loss= 9670.46734218973\n",
      "Losgistic Regression(    3400/10000): loss= 9669.40237459971\n",
      "Losgistic Regression(    3500/10000): loss= 9668.74428200433\n",
      "Losgistic Regression(    3600/10000): loss= 9668.2205159927\n",
      "Losgistic Regression(    3700/10000): loss= 9667.5996216833\n",
      "Losgistic Regression(    3800/10000): loss= 9667.14503035138\n",
      "Losgistic Regression(    3900/10000): loss= 9666.46140412358\n",
      "Losgistic Regression(    4000/10000): loss= 9666.05632010942\n",
      "Losgistic Regression(    4100/10000): loss= 9665.76507741594\n",
      "Losgistic Regression(    4200/10000): loss= 9665.75732850197\n",
      "Totoal number of iterations =  4200\n",
      "Loss                        =  9665.7573285\n",
      "Time for  1th cross validation = 333.306s\n",
      "Training Accuracy         = 0.82984\n",
      "Cross Validation Accuracy = 0.822668\n",
      "Losgistic Regression(       0/10000): loss= 16686.8044670533\n",
      "Losgistic Regression(     100/10000): loss= 11101.7212188433\n",
      "Losgistic Regression(     200/10000): loss= 10580.9739921932\n",
      "Losgistic Regression(     300/10000): loss= 10311.8329899163\n",
      "Losgistic Regression(     400/10000): loss= 10153.9262938186\n",
      "Losgistic Regression(     500/10000): loss= 10054.2423057729\n",
      "Losgistic Regression(     600/10000): loss= 9991.57924478226\n",
      "Losgistic Regression(     700/10000): loss= 9946.0944708421\n",
      "Losgistic Regression(     800/10000): loss= 9906.98631553382\n",
      "Losgistic Regression(     900/10000): loss= 9877.66378725193\n",
      "Losgistic Regression(    1000/10000): loss= 9856.28150960937\n",
      "Losgistic Regression(    1100/10000): loss= 9837.73625677029\n",
      "Losgistic Regression(    1200/10000): loss= 9823.11146970019\n",
      "Losgistic Regression(    1300/10000): loss= 9811.42178752724\n",
      "Losgistic Regression(    1400/10000): loss= 9802.38166326734\n",
      "Losgistic Regression(    1500/10000): loss= 9796.26372734218\n",
      "Losgistic Regression(    1600/10000): loss= 9791.12840539899\n",
      "Losgistic Regression(    1700/10000): loss= 9786.26395625285\n",
      "Losgistic Regression(    1800/10000): loss= 9783.43483106129\n",
      "Losgistic Regression(    1900/10000): loss= 9780.3643771672\n",
      "Losgistic Regression(    2000/10000): loss= 9777.26105260389\n",
      "Losgistic Regression(    2100/10000): loss= 9773.93867538115\n",
      "Losgistic Regression(    2200/10000): loss= 9770.59199678838\n",
      "Losgistic Regression(    2300/10000): loss= 9767.76275706632\n",
      "Losgistic Regression(    2400/10000): loss= 9765.34595851585\n",
      "Losgistic Regression(    2500/10000): loss= 9762.73033173488\n",
      "Losgistic Regression(    2600/10000): loss= 9759.15042198031\n",
      "Losgistic Regression(    2700/10000): loss= 9754.94501172304\n",
      "Losgistic Regression(    2800/10000): loss= 9751.9384615807\n",
      "Losgistic Regression(    2900/10000): loss= 9749.7709576034\n",
      "Losgistic Regression(    3000/10000): loss= 9747.57432721668\n",
      "Losgistic Regression(    3100/10000): loss= 9745.5322289777\n",
      "Losgistic Regression(    3200/10000): loss= 9743.84379352834\n",
      "Losgistic Regression(    3300/10000): loss= 9742.82882427208\n",
      "Losgistic Regression(    3400/10000): loss= 9741.7400916429\n",
      "Losgistic Regression(    3500/10000): loss= 9740.32174095624\n",
      "Losgistic Regression(    3600/10000): loss= 9739.11690565591\n",
      "Losgistic Regression(    3700/10000): loss= 9737.38649143465\n",
      "Losgistic Regression(    3800/10000): loss= 9735.7595277348\n",
      "Losgistic Regression(    3900/10000): loss= 9734.47096085053\n",
      "Losgistic Regression(    4000/10000): loss= 9733.02781717829\n",
      "Losgistic Regression(    4100/10000): loss= 9731.39908121793\n",
      "Losgistic Regression(    4200/10000): loss= 9730.66609779005\n",
      "Losgistic Regression(    4300/10000): loss= 9729.76205668208\n",
      "Losgistic Regression(    4400/10000): loss= 9728.17902969481\n",
      "Losgistic Regression(    4500/10000): loss= 9726.82927160023\n",
      "Losgistic Regression(    4600/10000): loss= 9725.43705966827\n",
      "Losgistic Regression(    4700/10000): loss= 9724.02061815094\n",
      "Losgistic Regression(    4800/10000): loss= 9723.01990414636\n",
      "Losgistic Regression(    4900/10000): loss= 9722.40775488239\n",
      "Losgistic Regression(    5000/10000): loss= 9721.60971891114\n",
      "Losgistic Regression(    5100/10000): loss= 9721.10410541109\n",
      "Losgistic Regression(    5200/10000): loss= 9720.83440529371\n",
      "Losgistic Regression(    5300/10000): loss= 9720.84078148852\n",
      "Totoal number of iterations =  5300\n",
      "Loss                        =  9720.84078149\n",
      "Time for  2th cross validation = 420.676s\n",
      "Training Accuracy         = 0.82784\n",
      "Cross Validation Accuracy = 0.823372\n",
      "Losgistic Regression(       0/10000): loss= 16690.4087370749\n",
      "Losgistic Regression(     100/10000): loss= 10993.7966473165\n",
      "Losgistic Regression(     200/10000): loss= 10444.8320383217\n",
      "Losgistic Regression(     300/10000): loss= 10206.4503261246\n",
      "Losgistic Regression(     400/10000): loss= 10080.2855216729\n",
      "Losgistic Regression(     500/10000): loss= 10002.3880517233\n",
      "Losgistic Regression(     600/10000): loss= 9946.78761105693\n",
      "Losgistic Regression(     700/10000): loss= 9901.66328284575\n",
      "Losgistic Regression(     800/10000): loss= 9862.60284198492\n",
      "Losgistic Regression(     900/10000): loss= 9828.27290311745\n",
      "Losgistic Regression(    1000/10000): loss= 9801.53467292411\n",
      "Losgistic Regression(    1100/10000): loss= 9783.98575962645\n",
      "Losgistic Regression(    1200/10000): loss= 9767.28484929203\n",
      "Losgistic Regression(    1300/10000): loss= 9756.11631900788\n",
      "Losgistic Regression(    1400/10000): loss= 9746.76648288565\n",
      "Losgistic Regression(    1500/10000): loss= 9738.86495500157\n",
      "Losgistic Regression(    1600/10000): loss= 9732.5025347369\n",
      "Losgistic Regression(    1700/10000): loss= 9726.66640842661\n",
      "Losgistic Regression(    1800/10000): loss= 9721.76083732983\n",
      "Losgistic Regression(    1900/10000): loss= 9716.91100682346\n",
      "Losgistic Regression(    2000/10000): loss= 9711.78777051042\n",
      "Losgistic Regression(    2100/10000): loss= 9706.75639929927\n",
      "Losgistic Regression(    2200/10000): loss= 9701.91685439848\n",
      "Losgistic Regression(    2300/10000): loss= 9697.30665358594\n",
      "Losgistic Regression(    2400/10000): loss= 9693.5462925947\n",
      "Losgistic Regression(    2500/10000): loss= 9690.84153728487\n",
      "Losgistic Regression(    2600/10000): loss= 9688.65510416599\n",
      "Losgistic Regression(    2700/10000): loss= 9686.6358084802\n",
      "Losgistic Regression(    2800/10000): loss= 9684.5941301104\n",
      "Losgistic Regression(    2900/10000): loss= 9682.70071473235\n",
      "Losgistic Regression(    3000/10000): loss= 9681.07896924447\n",
      "Losgistic Regression(    3100/10000): loss= 9679.23307873352\n",
      "Losgistic Regression(    3200/10000): loss= 9676.78423927811\n",
      "Losgistic Regression(    3300/10000): loss= 9674.16294943758\n",
      "Losgistic Regression(    3400/10000): loss= 9672.09772875954\n",
      "Losgistic Regression(    3500/10000): loss= 9669.97960079299\n",
      "Losgistic Regression(    3600/10000): loss= 9668.19159693515\n",
      "Losgistic Regression(    3700/10000): loss= 9666.7142968788\n",
      "Losgistic Regression(    3800/10000): loss= 9665.27657149272\n",
      "Losgistic Regression(    3900/10000): loss= 9664.45480365733\n",
      "Losgistic Regression(    4000/10000): loss= 9664.26861371216\n",
      "Losgistic Regression(    4100/10000): loss= 9664.27477187301\n",
      "Totoal number of iterations =  4100\n",
      "Loss                        =  9664.27477187\n",
      "Time for  3th cross validation = 325.603s\n",
      "Training Accuracy         =  0.831\n",
      "Cross Validation Accuracy = 0.821792\n",
      "Losgistic Regression(       0/10000): loss= 16715.1699114494\n",
      "Losgistic Regression(     100/10000): loss= 10894.1525817255\n",
      "Losgistic Regression(     200/10000): loss= 10368.7133476968\n",
      "Losgistic Regression(     300/10000): loss= 10099.743000909\n",
      "Losgistic Regression(     400/10000): loss= 9939.54997824465\n",
      "Losgistic Regression(     500/10000): loss= 9848.26063301553\n",
      "Losgistic Regression(     600/10000): loss= 9776.66057259851\n",
      "Losgistic Regression(     700/10000): loss= 9706.93623228905\n",
      "Losgistic Regression(     800/10000): loss= 9632.43176509744\n",
      "Losgistic Regression(     900/10000): loss= 9563.58321165782\n",
      "Losgistic Regression(    1000/10000): loss= 9514.5858343743\n",
      "Losgistic Regression(    1100/10000): loss= 9492.83569919\n",
      "Losgistic Regression(    1200/10000): loss= 9489.23250764509\n",
      "Losgistic Regression(    1300/10000): loss= 9489.34635129625\n",
      "Totoal number of iterations =  1300\n",
      "Loss                        =  9489.3463513\n",
      "Time for  4th cross validation = 105.325s\n",
      "Training Accuracy         = 0.82632\n",
      "Cross Validation Accuracy = 0.820936\n",
      "*************** ([0.83248, 0.82984000000000002, 0.82784000000000002, 0.83099999999999996, 0.82632000000000005], [0.821712, 0.82266799999999995, 0.82337199999999999, 0.82179199999999997, 0.820936])\n",
      "Losgistic Regression(       0/10000): loss= 16721.8583862913\n",
      "Losgistic Regression(     100/10000): loss= 10982.8521313403\n",
      "Losgistic Regression(     200/10000): loss= 10498.1457217551\n",
      "Losgistic Regression(     300/10000): loss= 10266.598613569\n",
      "Losgistic Regression(     400/10000): loss= 10128.3875469559\n",
      "Losgistic Regression(     500/10000): loss= 10037.6485379787\n",
      "Losgistic Regression(     600/10000): loss= 9972.51416980651\n",
      "Losgistic Regression(     700/10000): loss= 9921.7797199019\n",
      "Losgistic Regression(     800/10000): loss= 9885.36426891207\n",
      "Losgistic Regression(     900/10000): loss= 9860.41068750564\n",
      "Losgistic Regression(    1000/10000): loss= 9842.67850954585\n",
      "Losgistic Regression(    1100/10000): loss= 9831.00283804039\n",
      "Losgistic Regression(    1200/10000): loss= 9820.83749641587\n",
      "Losgistic Regression(    1300/10000): loss= 9814.89773969567\n",
      "Losgistic Regression(    1400/10000): loss= 9810.33412097896\n",
      "Losgistic Regression(    1500/10000): loss= 9805.1387502044\n",
      "Losgistic Regression(    1600/10000): loss= 9801.91006163809\n",
      "Losgistic Regression(    1700/10000): loss= 9793.91804066177\n",
      "Losgistic Regression(    1800/10000): loss= 9785.09384149965\n",
      "Losgistic Regression(    1900/10000): loss= 9776.25067142524\n",
      "Losgistic Regression(    2000/10000): loss= 9767.08279903104\n",
      "Losgistic Regression(    2100/10000): loss= 9757.20891395547\n",
      "Losgistic Regression(    2200/10000): loss= 9749.17852497934\n",
      "Losgistic Regression(    2300/10000): loss= 9744.56082471362\n",
      "Losgistic Regression(    2400/10000): loss= 9740.32319806407\n",
      "Losgistic Regression(    2500/10000): loss= 9735.36586976199\n",
      "Losgistic Regression(    2600/10000): loss= 9730.88486812033\n",
      "Losgistic Regression(    2700/10000): loss= 9726.66067084185\n",
      "Losgistic Regression(    2800/10000): loss= 9723.26363679378\n",
      "Losgistic Regression(    2900/10000): loss= 9721.04231781269\n",
      "Losgistic Regression(    3000/10000): loss= 9719.96047554111\n",
      "Losgistic Regression(    3100/10000): loss= 9718.11593903565\n",
      "Losgistic Regression(    3200/10000): loss= 9716.535568413\n",
      "Losgistic Regression(    3300/10000): loss= 9715.29602100419\n",
      "Losgistic Regression(    3400/10000): loss= 9714.92471244568\n",
      "Losgistic Regression(    3500/10000): loss= 9714.31305621\n",
      "Losgistic Regression(    3600/10000): loss= 9714.31697198307\n",
      "Totoal number of iterations =  3600\n",
      "Loss                        =  9714.31697198\n",
      "Time for  0th cross validation = 286.066s\n",
      "Training Accuracy         = 0.82992\n",
      "Cross Validation Accuracy = 0.821944\n",
      "Losgistic Regression(       0/10000): loss= 16696.2652097066\n",
      "Losgistic Regression(     100/10000): loss= 11074.3995197931\n",
      "Losgistic Regression(     200/10000): loss= 10613.2089343278\n",
      "Losgistic Regression(     300/10000): loss= 10396.3787980569\n",
      "Losgistic Regression(     400/10000): loss= 10261.5568289762\n",
      "Losgistic Regression(     500/10000): loss= 10159.6302706345\n",
      "Losgistic Regression(     600/10000): loss= 10092.1520244811\n",
      "Losgistic Regression(     700/10000): loss= 10049.4618350641\n",
      "Losgistic Regression(     800/10000): loss= 10017.4053551599\n",
      "Losgistic Regression(     900/10000): loss= 9994.16788149977\n",
      "Losgistic Regression(    1000/10000): loss= 9976.25344793681\n",
      "Losgistic Regression(    1100/10000): loss= 9963.87391142412\n",
      "Losgistic Regression(    1200/10000): loss= 9954.09470279626\n",
      "Losgistic Regression(    1300/10000): loss= 9945.65045354374\n",
      "Losgistic Regression(    1400/10000): loss= 9937.83886898157\n",
      "Losgistic Regression(    1500/10000): loss= 9931.77136892229\n",
      "Losgistic Regression(    1600/10000): loss= 9927.53754518025\n",
      "Losgistic Regression(    1700/10000): loss= 9923.58920263851\n",
      "Losgistic Regression(    1800/10000): loss= 9919.11052787436\n",
      "Losgistic Regression(    1900/10000): loss= 9915.4751536306\n",
      "Losgistic Regression(    2000/10000): loss= 9912.57690174935\n",
      "Losgistic Regression(    2100/10000): loss= 9909.55474587976\n",
      "Losgistic Regression(    2200/10000): loss= 9906.97001962981\n",
      "Losgistic Regression(    2300/10000): loss= 9905.13940322704\n",
      "Losgistic Regression(    2400/10000): loss= 9903.4613292668\n",
      "Losgistic Regression(    2500/10000): loss= 9902.11610735553\n",
      "Losgistic Regression(    2600/10000): loss= 9900.84321169425\n",
      "Losgistic Regression(    2700/10000): loss= 9899.64549619643\n",
      "Losgistic Regression(    2800/10000): loss= 9898.40314957304\n",
      "Losgistic Regression(    2900/10000): loss= 9897.64453719621\n",
      "Losgistic Regression(    3000/10000): loss= 9896.342845697\n",
      "Losgistic Regression(    3100/10000): loss= 9895.96952097528\n",
      "Losgistic Regression(    3200/10000): loss= 9895.7604573934\n",
      "Losgistic Regression(    3300/10000): loss= 9895.76820495099\n",
      "Totoal number of iterations =  3300\n",
      "Loss                        =  9895.76820495\n",
      "Time for  1th cross validation = 262.325s\n",
      "Training Accuracy         = 0.82904\n",
      "Cross Validation Accuracy = 0.8226\n",
      "Losgistic Regression(       0/10000): loss= 16687.4127490871\n",
      "Losgistic Regression(     100/10000): loss= 11137.9803014237\n",
      "Losgistic Regression(     200/10000): loss= 10645.3405010559\n",
      "Losgistic Regression(     300/10000): loss= 10399.6383187369\n",
      "Losgistic Regression(     400/10000): loss= 10259.4654057609\n",
      "Losgistic Regression(     500/10000): loss= 10172.0681850022\n",
      "Losgistic Regression(     600/10000): loss= 10119.1758345097\n",
      "Losgistic Regression(     700/10000): loss= 10083.7860578297\n",
      "Losgistic Regression(     800/10000): loss= 10052.5892444755\n",
      "Losgistic Regression(     900/10000): loss= 10029.4338682771\n",
      "Losgistic Regression(    1000/10000): loss= 10013.9356023682\n",
      "Losgistic Regression(    1100/10000): loss= 10000.8523207217\n",
      "Losgistic Regression(    1200/10000): loss= 9991.23678950141\n",
      "Losgistic Regression(    1300/10000): loss= 9982.14376490506\n",
      "Losgistic Regression(    1400/10000): loss= 9974.73347206288\n",
      "Losgistic Regression(    1500/10000): loss= 9969.49998806045\n",
      "Losgistic Regression(    1600/10000): loss= 9966.36705041716\n",
      "Losgistic Regression(    1700/10000): loss= 9963.68362768783\n",
      "Losgistic Regression(    1800/10000): loss= 9962.63029981149\n",
      "Losgistic Regression(    1900/10000): loss= 9961.50983674224\n",
      "Losgistic Regression(    2000/10000): loss= 9958.15940045101\n",
      "Losgistic Regression(    2100/10000): loss= 9954.95688455108\n",
      "Losgistic Regression(    2200/10000): loss= 9951.61720347619\n",
      "Losgistic Regression(    2300/10000): loss= 9948.67610742012\n",
      "Losgistic Regression(    2400/10000): loss= 9944.58084007753\n",
      "Losgistic Regression(    2500/10000): loss= 9940.85073871997\n",
      "Losgistic Regression(    2600/10000): loss= 9936.90140996474\n",
      "Losgistic Regression(    2700/10000): loss= 9932.69845835253\n",
      "Losgistic Regression(    2800/10000): loss= 9930.32514749126\n",
      "Losgistic Regression(    2900/10000): loss= 9929.93378497696\n",
      "Losgistic Regression(    3000/10000): loss= 9929.92932527883\n",
      "Totoal number of iterations =  3000\n",
      "Loss                        =  9929.92932528\n",
      "Time for  2th cross validation = 237.261s\n",
      "Training Accuracy         = 0.82604\n",
      "Cross Validation Accuracy = 0.822168\n",
      "Losgistic Regression(       0/10000): loss= 16691.0174022664\n",
      "Losgistic Regression(     100/10000): loss= 11029.4345427207\n",
      "Losgistic Regression(     200/10000): loss= 10508.4197418987\n",
      "Losgistic Regression(     300/10000): loss= 10290.1427736607\n",
      "Losgistic Regression(     400/10000): loss= 10178.5436430266\n",
      "Losgistic Regression(     500/10000): loss= 10114.8907440946\n",
      "Losgistic Regression(     600/10000): loss= 10070.1881155992\n",
      "Losgistic Regression(     700/10000): loss= 10035.3747189551\n",
      "Losgistic Regression(     800/10000): loss= 10006.5112331344\n",
      "Losgistic Regression(     900/10000): loss= 9981.64866280908\n",
      "Losgistic Regression(    1000/10000): loss= 9960.63524987647\n",
      "Losgistic Regression(    1100/10000): loss= 9948.42619182902\n",
      "Losgistic Regression(    1200/10000): loss= 9937.33512826621\n",
      "Losgistic Regression(    1300/10000): loss= 9930.59862203345\n",
      "Losgistic Regression(    1400/10000): loss= 9926.22613004531\n",
      "Losgistic Regression(    1500/10000): loss= 9922.99835284452\n",
      "Losgistic Regression(    1600/10000): loss= 9920.06298058787\n",
      "Losgistic Regression(    1700/10000): loss= 9917.74619456122\n",
      "Losgistic Regression(    1800/10000): loss= 9915.77953218314\n",
      "Losgistic Regression(    1900/10000): loss= 9913.7752373408\n",
      "Losgistic Regression(    2000/10000): loss= 9911.48738140065\n",
      "Losgistic Regression(    2100/10000): loss= 9909.11297684184\n",
      "Losgistic Regression(    2200/10000): loss= 9906.58803827215\n",
      "Losgistic Regression(    2300/10000): loss= 9904.4006983915\n",
      "Losgistic Regression(    2400/10000): loss= 9902.08639542949\n",
      "Losgistic Regression(    2500/10000): loss= 9900.16510618406\n",
      "Losgistic Regression(    2600/10000): loss= 9899.42263703858\n",
      "Losgistic Regression(    2700/10000): loss= 9898.30276586366\n",
      "Losgistic Regression(    2800/10000): loss= 9896.21762122402\n",
      "Losgistic Regression(    2900/10000): loss= 9894.65451408487\n",
      "Losgistic Regression(    3000/10000): loss= 9893.41297821304\n",
      "Losgistic Regression(    3100/10000): loss= 9892.16713621958\n",
      "Losgistic Regression(    3200/10000): loss= 9891.10606796329\n",
      "Losgistic Regression(    3300/10000): loss= 9890.09669740604\n",
      "Losgistic Regression(    3400/10000): loss= 9889.26989368818\n",
      "Losgistic Regression(    3500/10000): loss= 9888.37868193138\n",
      "Losgistic Regression(    3600/10000): loss= 9887.96207291368\n",
      "Losgistic Regression(    3700/10000): loss= 9887.96361859201\n",
      "Totoal number of iterations =  3700\n",
      "Loss                        =  9887.96361859\n",
      "Time for  3th cross validation = 293.184s\n",
      "Training Accuracy         = 0.82876\n",
      "Cross Validation Accuracy = 0.821312\n",
      "Losgistic Regression(       0/10000): loss= 16715.7666025098\n",
      "Losgistic Regression(     100/10000): loss= 10935.8687864398\n",
      "Losgistic Regression(     200/10000): loss= 10444.6894392419\n",
      "Losgistic Regression(     300/10000): loss= 10187.4856572566\n",
      "Losgistic Regression(     400/10000): loss= 10025.0904348039\n",
      "Losgistic Regression(     500/10000): loss= 9937.92191388982\n",
      "Losgistic Regression(     600/10000): loss= 9880.94743986352\n",
      "Losgistic Regression(     700/10000): loss= 9857.12597211467\n",
      "Losgistic Regression(     800/10000): loss= 9834.35034776617\n",
      "Losgistic Regression(     900/10000): loss= 9815.01992652649\n",
      "Losgistic Regression(    1000/10000): loss= 9808.88757563303\n",
      "Losgistic Regression(    1100/10000): loss= 9809.06431710969\n",
      "Totoal number of iterations =  1100\n",
      "Loss                        =  9809.06431711\n",
      "Time for  4th cross validation = 88.62s\n",
      "Training Accuracy         = 0.82412\n",
      "Cross Validation Accuracy = 0.820236\n",
      "*************** ([0.82991999999999999, 0.82904, 0.82604, 0.82876000000000005, 0.82411999999999996], [0.82194400000000001, 0.8226, 0.82216800000000001, 0.82131200000000004, 0.82023599999999997])\n",
      "Losgistic Regression(       0/10000): loss= 16723.9335776932\n",
      "Losgistic Regression(     100/10000): loss= 11101.75650462\n",
      "Losgistic Regression(     200/10000): loss= 10697.2328224722\n",
      "Losgistic Regression(     300/10000): loss= 10516.5684081286\n",
      "Losgistic Regression(     400/10000): loss= 10417.0179563702\n",
      "Losgistic Regression(     500/10000): loss= 10358.3983655369\n",
      "Losgistic Regression(     600/10000): loss= 10319.5718426809\n",
      "Losgistic Regression(     700/10000): loss= 10290.5642963996\n",
      "Losgistic Regression(     800/10000): loss= 10266.6160933921\n",
      "Losgistic Regression(     900/10000): loss= 10253.8211365203\n",
      "Losgistic Regression(    1000/10000): loss= 10244.2828769461\n",
      "Losgistic Regression(    1100/10000): loss= 10238.5360631278\n",
      "Losgistic Regression(    1200/10000): loss= 10232.0326073113\n",
      "Losgistic Regression(    1300/10000): loss= 10226.5820252357\n",
      "Losgistic Regression(    1400/10000): loss= 10221.7884079877\n",
      "Losgistic Regression(    1500/10000): loss= 10219.248141659\n",
      "Losgistic Regression(    1600/10000): loss= 10217.914876272\n",
      "Losgistic Regression(    1700/10000): loss= 10217.2990819287\n",
      "Losgistic Regression(    1800/10000): loss= 10216.877927515\n",
      "Losgistic Regression(    1900/10000): loss= 10216.5637045813\n",
      "Losgistic Regression(    2000/10000): loss= 10216.1711575193\n",
      "Losgistic Regression(    2100/10000): loss= 10215.6964131659\n",
      "Losgistic Regression(    2200/10000): loss= 10215.1612709599\n",
      "Losgistic Regression(    2300/10000): loss= 10214.6055580156\n",
      "Losgistic Regression(    2400/10000): loss= 10214.0129434474\n",
      "Losgistic Regression(    2500/10000): loss= 10213.3608775448\n",
      "Losgistic Regression(    2600/10000): loss= 10212.6377882087\n",
      "Losgistic Regression(    2700/10000): loss= 10211.96768926\n",
      "Losgistic Regression(    2800/10000): loss= 10211.4255234487\n",
      "Losgistic Regression(    2900/10000): loss= 10211.4278803163\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  10211.4278803\n",
      "Time for  0th cross validation = 230.608s\n",
      "Training Accuracy         = 0.82524\n",
      "Cross Validation Accuracy = 0.819088\n",
      "Losgistic Regression(       0/10000): loss= 16698.3941972512\n",
      "Losgistic Regression(     100/10000): loss= 11189.6547118671\n",
      "Losgistic Regression(     200/10000): loss= 10800.8730242779\n",
      "Losgistic Regression(     300/10000): loss= 10636.0700217868\n",
      "Losgistic Regression(     400/10000): loss= 10544.7433436366\n",
      "Losgistic Regression(     500/10000): loss= 10476.6088027393\n",
      "Losgistic Regression(     600/10000): loss= 10423.7428553438\n",
      "Losgistic Regression(     700/10000): loss= 10391.1800173886\n",
      "Losgistic Regression(     800/10000): loss= 10372.3436553313\n",
      "Losgistic Regression(     900/10000): loss= 10360.5130703321\n",
      "Losgistic Regression(    1000/10000): loss= 10352.6698851795\n",
      "Losgistic Regression(    1100/10000): loss= 10349.0653350035\n",
      "Losgistic Regression(    1200/10000): loss= 10345.4208437305\n",
      "Losgistic Regression(    1300/10000): loss= 10343.3990026023\n",
      "Losgistic Regression(    1400/10000): loss= 10341.2909437123\n",
      "Losgistic Regression(    1500/10000): loss= 10339.1295065299\n",
      "Losgistic Regression(    1600/10000): loss= 10337.2251021201\n",
      "Losgistic Regression(    1700/10000): loss= 10335.7797786728\n",
      "Losgistic Regression(    1800/10000): loss= 10334.3237634777\n",
      "Losgistic Regression(    1900/10000): loss= 10332.8718931793\n",
      "Losgistic Regression(    2000/10000): loss= 10331.6540401918\n",
      "Losgistic Regression(    2100/10000): loss= 10330.4480563478\n",
      "Losgistic Regression(    2200/10000): loss= 10329.9406544527\n",
      "Losgistic Regression(    2300/10000): loss= 10329.8970353859\n",
      "Totoal number of iterations =  2300\n",
      "Loss                        =  10329.8970354\n",
      "Time for  1th cross validation = 182.321s\n",
      "Training Accuracy         = 0.82508\n",
      "Cross Validation Accuracy = 0.81982\n",
      "Losgistic Regression(       0/10000): loss= 16689.5309101559\n",
      "Losgistic Regression(     100/10000): loss= 11256.8009245689\n",
      "Losgistic Regression(     200/10000): loss= 10843.1970988731\n",
      "Losgistic Regression(     300/10000): loss= 10657.1425586428\n",
      "Losgistic Regression(     400/10000): loss= 10559.3172484275\n",
      "Losgistic Regression(     500/10000): loss= 10495.0619466027\n",
      "Losgistic Regression(     600/10000): loss= 10457.6816038635\n",
      "Losgistic Regression(     700/10000): loss= 10436.3808740446\n",
      "Losgistic Regression(     800/10000): loss= 10420.9619495324\n",
      "Losgistic Regression(     900/10000): loss= 10411.2143612425\n",
      "Losgistic Regression(    1000/10000): loss= 10407.7890089172\n",
      "Losgistic Regression(    1100/10000): loss= 10401.6644678745\n",
      "Losgistic Regression(    1200/10000): loss= 10399.1578836176\n",
      "Losgistic Regression(    1300/10000): loss= 10395.0763492884\n",
      "Losgistic Regression(    1400/10000): loss= 10394.6557484098\n",
      "Losgistic Regression(    1500/10000): loss= 10394.0672045146\n",
      "Losgistic Regression(    1600/10000): loss= 10393.4581322249\n",
      "Losgistic Regression(    1700/10000): loss= 10392.940632816\n",
      "Losgistic Regression(    1800/10000): loss= 10392.642972619\n",
      "Losgistic Regression(    1900/10000): loss= 10392.6181668703\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  10392.6181669\n",
      "Time for  2th cross validation = 151.563s\n",
      "Training Accuracy         = 0.82176\n",
      "Cross Validation Accuracy = 0.818972\n",
      "Losgistic Regression(       0/10000): loss= 16693.1368975679\n",
      "Losgistic Regression(     100/10000): loss= 11146.2893883318\n",
      "Losgistic Regression(     200/10000): loss= 10704.706745316\n",
      "Losgistic Regression(     300/10000): loss= 10532.9030088531\n",
      "Losgistic Regression(     400/10000): loss= 10452.5210062322\n",
      "Losgistic Regression(     500/10000): loss= 10410.4498428393\n",
      "Losgistic Regression(     600/10000): loss= 10380.9612189497\n",
      "Losgistic Regression(     700/10000): loss= 10361.8785881401\n",
      "Losgistic Regression(     800/10000): loss= 10345.5708127807\n",
      "Losgistic Regression(     900/10000): loss= 10332.1169449937\n",
      "Losgistic Regression(    1000/10000): loss= 10321.7396276716\n",
      "Losgistic Regression(    1100/10000): loss= 10314.2812955952\n",
      "Losgistic Regression(    1200/10000): loss= 10307.7875291368\n",
      "Losgistic Regression(    1300/10000): loss= 10301.8362259316\n",
      "Losgistic Regression(    1400/10000): loss= 10298.5981724748\n",
      "Losgistic Regression(    1500/10000): loss= 10295.4852951154\n",
      "Losgistic Regression(    1600/10000): loss= 10292.5068458326\n",
      "Losgistic Regression(    1700/10000): loss= 10290.0754206187\n",
      "Losgistic Regression(    1800/10000): loss= 10288.1181652019\n",
      "Losgistic Regression(    1900/10000): loss= 10286.7952492177\n",
      "Losgistic Regression(    2000/10000): loss= 10286.3245437701\n",
      "Losgistic Regression(    2100/10000): loss= 10286.0407337739\n",
      "Losgistic Regression(    2200/10000): loss= 10285.825689834\n",
      "Losgistic Regression(    2300/10000): loss= 10285.8211931824\n",
      "Totoal number of iterations =  2300\n",
      "Loss                        =  10285.8211932\n",
      "Time for  3th cross validation = 181.319s\n",
      "Training Accuracy         = 0.82424\n",
      "Cross Validation Accuracy = 0.818072\n",
      "Losgistic Regression(       0/10000): loss= 16717.8444014649\n",
      "Losgistic Regression(     100/10000): loss= 11071.6421280721\n",
      "Losgistic Regression(     200/10000): loss= 10656.5628533349\n",
      "Losgistic Regression(     300/10000): loss= 10461.522117538\n",
      "Losgistic Regression(     400/10000): loss= 10356.8793627645\n",
      "Losgistic Regression(     500/10000): loss= 10325.4689155993\n",
      "Losgistic Regression(     600/10000): loss= 10325.3706081141\n",
      "Totoal number of iterations =  600\n",
      "Loss                        =  10325.3706081\n",
      "Time for  4th cross validation = 48.6226s\n",
      "Training Accuracy         = 0.81832\n",
      "Cross Validation Accuracy = 0.816936\n",
      "*************** ([0.82523999999999997, 0.82508000000000004, 0.82176000000000005, 0.82423999999999997, 0.81832000000000005], [0.81908800000000004, 0.81981999999999999, 0.81897200000000003, 0.81807200000000002, 0.816936])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.83272000000000002,\n",
       "   0.83067999999999997,\n",
       "   0.82776000000000005,\n",
       "   0.83223999999999998,\n",
       "   0.82672000000000001],\n",
       "  [0.82135199999999997,\n",
       "   0.82273200000000002,\n",
       "   0.823048,\n",
       "   0.82201199999999996,\n",
       "   0.82067999999999997]),\n",
       " ([0.83264000000000005,\n",
       "   0.83055999999999996,\n",
       "   0.82779999999999998,\n",
       "   0.83252000000000004,\n",
       "   0.82672000000000001],\n",
       "  [0.82141200000000003,\n",
       "   0.822716,\n",
       "   0.82310399999999995,\n",
       "   0.82216,\n",
       "   0.82072000000000001]),\n",
       " ([0.83243999999999996,\n",
       "   0.83072000000000001,\n",
       "   0.82804,\n",
       "   0.83252000000000004,\n",
       "   0.82667999999999997],\n",
       "  [0.821492,\n",
       "   0.82279999999999998,\n",
       "   0.82308800000000004,\n",
       "   0.82211199999999995,\n",
       "   0.82077999999999995]),\n",
       " ([0.83248,\n",
       "   0.82984000000000002,\n",
       "   0.82784000000000002,\n",
       "   0.83099999999999996,\n",
       "   0.82632000000000005],\n",
       "  [0.821712,\n",
       "   0.82266799999999995,\n",
       "   0.82337199999999999,\n",
       "   0.82179199999999997,\n",
       "   0.820936]),\n",
       " ([0.82991999999999999,\n",
       "   0.82904,\n",
       "   0.82604,\n",
       "   0.82876000000000005,\n",
       "   0.82411999999999996],\n",
       "  [0.82194400000000001,\n",
       "   0.8226,\n",
       "   0.82216800000000001,\n",
       "   0.82131200000000004,\n",
       "   0.82023599999999997]),\n",
       " ([0.82523999999999997,\n",
       "   0.82508000000000004,\n",
       "   0.82176000000000005,\n",
       "   0.82423999999999997,\n",
       "   0.81832000000000005],\n",
       "  [0.81908800000000004,\n",
       "   0.81981999999999999,\n",
       "   0.81897200000000003,\n",
       "   0.81807200000000002,\n",
       "   0.816936])]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_25000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(25000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_25000.append(tmp)\n",
    "\n",
    "accu_25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 6821.09725390924\n",
      "Losgistic Regression(     100/10000): loss= 4536.47032779892\n",
      "Losgistic Regression(     200/10000): loss= 4256.02807380162\n",
      "Losgistic Regression(     300/10000): loss= 4115.29060027381\n",
      "Losgistic Regression(     400/10000): loss= 4023.8808452216\n",
      "Losgistic Regression(     500/10000): loss= 3957.02846304881\n",
      "Losgistic Regression(     600/10000): loss= 3907.54728747435\n",
      "Losgistic Regression(     700/10000): loss= 3870.04094953132\n",
      "Losgistic Regression(     800/10000): loss= 3839.50710073522\n",
      "Losgistic Regression(     900/10000): loss= 3814.22003875712\n",
      "Losgistic Regression(    1000/10000): loss= 3792.47865657709\n",
      "Losgistic Regression(    1100/10000): loss= 3773.58087112654\n",
      "Losgistic Regression(    1200/10000): loss= 3756.89606693645\n",
      "Losgistic Regression(    1300/10000): loss= 3742.30381021611\n",
      "Losgistic Regression(    1400/10000): loss= 3729.82165615005\n",
      "Losgistic Regression(    1500/10000): loss= 3719.83789594026\n",
      "Losgistic Regression(    1600/10000): loss= 3710.99462178656\n",
      "Losgistic Regression(    1700/10000): loss= 3702.29339867727\n",
      "Losgistic Regression(    1800/10000): loss= 3695.755161003\n",
      "Losgistic Regression(    1900/10000): loss= 3689.33076819024\n",
      "Losgistic Regression(    2000/10000): loss= 3684.86096185856\n",
      "Losgistic Regression(    2100/10000): loss= 3679.79067934966\n",
      "Losgistic Regression(    2200/10000): loss= 3674.93497346774\n",
      "Losgistic Regression(    2300/10000): loss= 3669.90642676627\n",
      "Losgistic Regression(    2400/10000): loss= 3665.12627576681\n",
      "Losgistic Regression(    2500/10000): loss= 3661.16530540809\n",
      "Losgistic Regression(    2600/10000): loss= 3657.27739629252\n",
      "Losgistic Regression(    2700/10000): loss= 3653.96210973395\n",
      "Losgistic Regression(    2800/10000): loss= 3650.8281992305\n",
      "Losgistic Regression(    2900/10000): loss= 3647.94956665106\n",
      "Losgistic Regression(    3000/10000): loss= 3645.31631841196\n",
      "Losgistic Regression(    3100/10000): loss= 3643.04937775782\n",
      "Losgistic Regression(    3200/10000): loss= 3640.78775678797\n",
      "Losgistic Regression(    3300/10000): loss= 3638.54637834018\n",
      "Losgistic Regression(    3400/10000): loss= 3636.40201747567\n",
      "Losgistic Regression(    3500/10000): loss= 3634.48092454769\n",
      "Losgistic Regression(    3600/10000): loss= 3632.631419508\n",
      "Losgistic Regression(    3700/10000): loss= 3630.84329722812\n",
      "Losgistic Regression(    3800/10000): loss= 3628.98309351774\n",
      "Losgistic Regression(    3900/10000): loss= 3627.29894866134\n",
      "Losgistic Regression(    4000/10000): loss= 3625.60554449185\n",
      "Losgistic Regression(    4100/10000): loss= 3624.10031183926\n",
      "Losgistic Regression(    4200/10000): loss= 3622.74834818769\n",
      "Losgistic Regression(    4300/10000): loss= 3621.39496344953\n",
      "Losgistic Regression(    4400/10000): loss= 3620.09329014787\n",
      "Losgistic Regression(    4500/10000): loss= 3618.92331191956\n",
      "Losgistic Regression(    4600/10000): loss= 3617.84728202779\n",
      "Losgistic Regression(    4700/10000): loss= 3616.95270242473\n",
      "Losgistic Regression(    4800/10000): loss= 3616.15819005866\n",
      "Losgistic Regression(    4900/10000): loss= 3615.51223707674\n",
      "Losgistic Regression(    5000/10000): loss= 3614.8823724289\n",
      "Losgistic Regression(    5100/10000): loss= 3614.24509644526\n",
      "Losgistic Regression(    5200/10000): loss= 3613.56177492293\n",
      "Losgistic Regression(    5300/10000): loss= 3612.79506976239\n",
      "Losgistic Regression(    5400/10000): loss= 3612.08630029261\n",
      "Losgistic Regression(    5500/10000): loss= 3611.48697727197\n",
      "Losgistic Regression(    5600/10000): loss= 3611.00664126175\n",
      "Losgistic Regression(    5700/10000): loss= 3610.5840587415\n",
      "Losgistic Regression(    5800/10000): loss= 3610.26477349265\n",
      "Losgistic Regression(    5900/10000): loss= 3609.9827613366\n",
      "Losgistic Regression(    6000/10000): loss= 3609.75422714179\n",
      "Losgistic Regression(    6100/10000): loss= 3609.59084586038\n",
      "Losgistic Regression(    6200/10000): loss= 3609.41598140494\n",
      "Losgistic Regression(    6300/10000): loss= 3609.26029849406\n",
      "Losgistic Regression(    6400/10000): loss= 3609.12889285022\n",
      "Losgistic Regression(    6500/10000): loss= 3608.98397581842\n",
      "Losgistic Regression(    6600/10000): loss= 3608.87600186244\n",
      "Losgistic Regression(    6700/10000): loss= 3608.80921433873\n",
      "Losgistic Regression(    6800/10000): loss= 3608.7761722824\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  3608.77617228\n",
      "Time for  0th cross validation = 218.209s\n",
      "Training Accuracy         = 0.8411\n",
      "Cross Validation Accuracy = 0.813632\n",
      "Losgistic Regression(       0/10000): loss= 6827.55281501584\n",
      "Losgistic Regression(     100/10000): loss= 4486.15730921931\n",
      "Losgistic Regression(     200/10000): loss= 4210.37151528254\n",
      "Losgistic Regression(     300/10000): loss= 4065.61704434822\n",
      "Losgistic Regression(     400/10000): loss= 3967.90899469504\n",
      "Losgistic Regression(     500/10000): loss= 3897.39882322951\n",
      "Losgistic Regression(     600/10000): loss= 3845.96648130426\n",
      "Losgistic Regression(     700/10000): loss= 3806.91929532483\n",
      "Losgistic Regression(     800/10000): loss= 3775.94729305364\n",
      "Losgistic Regression(     900/10000): loss= 3751.03823401674\n",
      "Losgistic Regression(    1000/10000): loss= 3731.25705310498\n",
      "Losgistic Regression(    1100/10000): loss= 3715.54691324431\n",
      "Losgistic Regression(    1200/10000): loss= 3702.76954914785\n",
      "Losgistic Regression(    1300/10000): loss= 3691.98547310976\n",
      "Losgistic Regression(    1400/10000): loss= 3682.56822303035\n",
      "Losgistic Regression(    1500/10000): loss= 3674.56037162838\n",
      "Losgistic Regression(    1600/10000): loss= 3667.34142769706\n",
      "Losgistic Regression(    1700/10000): loss= 3660.65497715595\n",
      "Losgistic Regression(    1800/10000): loss= 3654.45179650949\n",
      "Losgistic Regression(    1900/10000): loss= 3648.68421249811\n",
      "Losgistic Regression(    2000/10000): loss= 3643.37050406782\n",
      "Losgistic Regression(    2100/10000): loss= 3638.43197587157\n",
      "Losgistic Regression(    2200/10000): loss= 3633.88387769689\n",
      "Losgistic Regression(    2300/10000): loss= 3629.69290576271\n",
      "Losgistic Regression(    2400/10000): loss= 3625.80766753003\n",
      "Losgistic Regression(    2500/10000): loss= 3622.00556007863\n",
      "Losgistic Regression(    2600/10000): loss= 3618.22065993334\n",
      "Losgistic Regression(    2700/10000): loss= 3614.75623876174\n",
      "Losgistic Regression(    2800/10000): loss= 3611.49796894757\n",
      "Losgistic Regression(    2900/10000): loss= 3608.36382995066\n",
      "Losgistic Regression(    3000/10000): loss= 3605.3133201999\n",
      "Losgistic Regression(    3100/10000): loss= 3602.39292826917\n",
      "Losgistic Regression(    3200/10000): loss= 3598.70577144588\n",
      "Losgistic Regression(    3300/10000): loss= 3593.8280994607\n",
      "Losgistic Regression(    3400/10000): loss= 3588.87820012485\n",
      "Losgistic Regression(    3500/10000): loss= 3583.93631592543\n",
      "Losgistic Regression(    3600/10000): loss= 3579.11863341078\n",
      "Losgistic Regression(    3700/10000): loss= 3574.41817056885\n",
      "Losgistic Regression(    3800/10000): loss= 3569.86883768702\n",
      "Losgistic Regression(    3900/10000): loss= 3565.55532024963\n",
      "Losgistic Regression(    4000/10000): loss= 3561.50247450286\n",
      "Losgistic Regression(    4100/10000): loss= 3557.6274618016\n",
      "Losgistic Regression(    4200/10000): loss= 3553.88456657269\n",
      "Losgistic Regression(    4300/10000): loss= 3550.22473277582\n",
      "Losgistic Regression(    4400/10000): loss= 3546.70768318694\n",
      "Losgistic Regression(    4500/10000): loss= 3543.29013972685\n",
      "Losgistic Regression(    4600/10000): loss= 3539.98498969817\n",
      "Losgistic Regression(    4700/10000): loss= 3536.76037758526\n",
      "Losgistic Regression(    4800/10000): loss= 3533.70158200311\n",
      "Losgistic Regression(    4900/10000): loss= 3530.85602940552\n",
      "Losgistic Regression(    5000/10000): loss= 3528.13876831843\n",
      "Losgistic Regression(    5100/10000): loss= 3525.53319846317\n",
      "Losgistic Regression(    5200/10000): loss= 3522.95568608759\n",
      "Losgistic Regression(    5300/10000): loss= 3520.52642809706\n",
      "Losgistic Regression(    5400/10000): loss= 3518.29208366415\n",
      "Losgistic Regression(    5500/10000): loss= 3516.19272383074\n",
      "Losgistic Regression(    5600/10000): loss= 3514.27084905422\n",
      "Losgistic Regression(    5700/10000): loss= 3512.60325449367\n",
      "Losgistic Regression(    5800/10000): loss= 3511.1641461549\n",
      "Losgistic Regression(    5900/10000): loss= 3509.9506000589\n",
      "Losgistic Regression(    6000/10000): loss= 3508.90109067525\n",
      "Losgistic Regression(    6100/10000): loss= 3508.08916943377\n",
      "Losgistic Regression(    6200/10000): loss= 3507.43293601175\n",
      "Losgistic Regression(    6300/10000): loss= 3506.55747595855\n",
      "Losgistic Regression(    6400/10000): loss= 3506.0211938821\n",
      "Losgistic Regression(    6500/10000): loss= 3506.02568399804\n",
      "Totoal number of iterations =  6500\n",
      "Loss                        =  3506.025684\n",
      "Time for  1th cross validation = 206.051s\n",
      "Training Accuracy         = 0.8424\n",
      "Cross Validation Accuracy = 0.81584\n",
      "Losgistic Regression(       0/10000): loss= 6825.28848387494\n",
      "Losgistic Regression(     100/10000): loss= 4452.38912978921\n",
      "Losgistic Regression(     200/10000): loss= 4192.93522375195\n",
      "Losgistic Regression(     300/10000): loss= 4053.63139725418\n",
      "Losgistic Regression(     400/10000): loss= 3957.84343818041\n",
      "Losgistic Regression(     500/10000): loss= 3887.43008272319\n",
      "Losgistic Regression(     600/10000): loss= 3831.09951329145\n",
      "Losgistic Regression(     700/10000): loss= 3785.67876784513\n",
      "Losgistic Regression(     800/10000): loss= 3749.62133913377\n",
      "Losgistic Regression(     900/10000): loss= 3719.55318080366\n",
      "Losgistic Regression(    1000/10000): loss= 3698.64407103227\n",
      "Losgistic Regression(    1100/10000): loss= 3682.88934419654\n",
      "Losgistic Regression(    1200/10000): loss= 3671.05493523719\n",
      "Losgistic Regression(    1300/10000): loss= 3662.60991519184\n",
      "Losgistic Regression(    1400/10000): loss= 3655.59128981526\n",
      "Losgistic Regression(    1500/10000): loss= 3650.72440878683\n",
      "Losgistic Regression(    1600/10000): loss= 3647.11301674684\n",
      "Losgistic Regression(    1700/10000): loss= 3644.81737212364\n",
      "Losgistic Regression(    1800/10000): loss= 3642.11874682446\n",
      "Losgistic Regression(    1900/10000): loss= 3639.68663508657\n",
      "Losgistic Regression(    2000/10000): loss= 3636.91828724213\n",
      "Losgistic Regression(    2100/10000): loss= 3634.62202900534\n",
      "Losgistic Regression(    2200/10000): loss= 3628.37146400405\n",
      "Losgistic Regression(    2300/10000): loss= 3622.59354242267\n",
      "Losgistic Regression(    2400/10000): loss= 3616.87134141163\n",
      "Losgistic Regression(    2500/10000): loss= 3611.45329303555\n",
      "Losgistic Regression(    2600/10000): loss= 3606.29151852791\n",
      "Losgistic Regression(    2700/10000): loss= 3601.24202391751\n",
      "Losgistic Regression(    2800/10000): loss= 3596.61265939686\n",
      "Losgistic Regression(    2900/10000): loss= 3592.27899917709\n",
      "Losgistic Regression(    3000/10000): loss= 3588.34345986879\n",
      "Losgistic Regression(    3100/10000): loss= 3584.59064072915\n",
      "Losgistic Regression(    3200/10000): loss= 3581.07800204054\n",
      "Losgistic Regression(    3300/10000): loss= 3577.66007886299\n",
      "Losgistic Regression(    3400/10000): loss= 3574.45813416066\n",
      "Losgistic Regression(    3500/10000): loss= 3571.68231932675\n",
      "Losgistic Regression(    3600/10000): loss= 3569.11641094898\n",
      "Losgistic Regression(    3700/10000): loss= 3566.40647634032\n",
      "Losgistic Regression(    3800/10000): loss= 3563.76137895912\n",
      "Losgistic Regression(    3900/10000): loss= 3561.3432672318\n",
      "Losgistic Regression(    4000/10000): loss= 3559.26088895059\n",
      "Losgistic Regression(    4100/10000): loss= 3557.35824456816\n",
      "Losgistic Regression(    4200/10000): loss= 3554.55935312491\n",
      "Losgistic Regression(    4300/10000): loss= 3551.2812640693\n",
      "Losgistic Regression(    4400/10000): loss= 3548.31300294579\n",
      "Losgistic Regression(    4500/10000): loss= 3545.52612079283\n",
      "Losgistic Regression(    4600/10000): loss= 3543.01661761467\n",
      "Losgistic Regression(    4700/10000): loss= 3540.70716659172\n",
      "Losgistic Regression(    4800/10000): loss= 3538.57879690862\n",
      "Losgistic Regression(    4900/10000): loss= 3536.61207217966\n",
      "Losgistic Regression(    5000/10000): loss= 3534.70696391758\n",
      "Losgistic Regression(    5100/10000): loss= 3532.87563838046\n",
      "Losgistic Regression(    5200/10000): loss= 3531.18100624496\n",
      "Losgistic Regression(    5300/10000): loss= 3529.54695814995\n",
      "Losgistic Regression(    5400/10000): loss= 3527.97111244586\n",
      "Losgistic Regression(    5500/10000): loss= 3526.46458690127\n",
      "Losgistic Regression(    5600/10000): loss= 3525.02381876062\n",
      "Losgistic Regression(    5700/10000): loss= 3523.68524274419\n",
      "Losgistic Regression(    5800/10000): loss= 3522.4552978419\n",
      "Losgistic Regression(    5900/10000): loss= 3521.23707895158\n",
      "Losgistic Regression(    6000/10000): loss= 3519.91712971573\n",
      "Losgistic Regression(    6100/10000): loss= 3518.54305761564\n",
      "Losgistic Regression(    6200/10000): loss= 3517.24078699671\n",
      "Losgistic Regression(    6300/10000): loss= 3516.02207400009\n",
      "Losgistic Regression(    6400/10000): loss= 3514.81217516427\n",
      "Losgistic Regression(    6500/10000): loss= 3513.61947393014\n",
      "Losgistic Regression(    6600/10000): loss= 3512.52191735784\n",
      "Losgistic Regression(    6700/10000): loss= 3511.55589978075\n",
      "Losgistic Regression(    6800/10000): loss= 3510.70158199986\n",
      "Losgistic Regression(    6900/10000): loss= 3509.92982186972\n",
      "Losgistic Regression(    7000/10000): loss= 3509.21151050509\n",
      "Losgistic Regression(    7100/10000): loss= 3508.5054005912\n",
      "Losgistic Regression(    7200/10000): loss= 3507.78892735464\n",
      "Losgistic Regression(    7300/10000): loss= 3507.05872097227\n",
      "Losgistic Regression(    7400/10000): loss= 3506.32259408663\n",
      "Losgistic Regression(    7500/10000): loss= 3505.60363729734\n",
      "Losgistic Regression(    7600/10000): loss= 3504.90038851805\n",
      "Losgistic Regression(    7700/10000): loss= 3504.23828550184\n",
      "Losgistic Regression(    7800/10000): loss= 3503.60794752634\n",
      "Losgistic Regression(    7900/10000): loss= 3502.96934269664\n",
      "Losgistic Regression(    8000/10000): loss= 3502.34079768002\n",
      "Losgistic Regression(    8100/10000): loss= 3501.78040171295\n",
      "Losgistic Regression(    8200/10000): loss= 3501.31146211217\n",
      "Losgistic Regression(    8300/10000): loss= 3500.92591456353\n",
      "Losgistic Regression(    8400/10000): loss= 3500.6092895109\n",
      "Losgistic Regression(    8500/10000): loss= 3500.3358423528\n",
      "Losgistic Regression(    8600/10000): loss= 3500.05026780842\n",
      "Losgistic Regression(    8700/10000): loss= 3499.72387016441\n",
      "Losgistic Regression(    8800/10000): loss= 3499.39579825961\n",
      "Losgistic Regression(    8900/10000): loss= 3499.10830162825\n",
      "Losgistic Regression(    9000/10000): loss= 3498.87291275335\n",
      "Losgistic Regression(    9100/10000): loss= 3498.6944176382\n",
      "Losgistic Regression(    9200/10000): loss= 3498.57453425655\n",
      "Losgistic Regression(    9300/10000): loss= 3498.51736415954\n",
      "Losgistic Regression(    9400/10000): loss= 3498.49679580909\n",
      "Totoal number of iterations =  9400\n",
      "Loss                        =  3498.49679581\n",
      "Time for  2th cross validation = 301.883s\n",
      "Training Accuracy         = 0.8474\n",
      "Cross Validation Accuracy = 0.81414\n",
      "Losgistic Regression(       0/10000): loss= 6822.87301767037\n",
      "Losgistic Regression(     100/10000): loss= 4535.5752645038\n",
      "Losgistic Regression(     200/10000): loss= 4275.6250702742\n",
      "Losgistic Regression(     300/10000): loss= 4133.78984297124\n",
      "Losgistic Regression(     400/10000): loss= 4044.52656870019\n",
      "Losgistic Regression(     500/10000): loss= 3978.58344075658\n",
      "Losgistic Regression(     600/10000): loss= 3923.93600735\n",
      "Losgistic Regression(     700/10000): loss= 3878.669209997\n",
      "Losgistic Regression(     800/10000): loss= 3848.14671580426\n",
      "Losgistic Regression(     900/10000): loss= 3822.95943095151\n",
      "Losgistic Regression(    1000/10000): loss= 3804.89485684971\n",
      "Losgistic Regression(    1100/10000): loss= 3789.63604642773\n",
      "Losgistic Regression(    1200/10000): loss= 3777.63780234782\n",
      "Losgistic Regression(    1300/10000): loss= 3765.37886524585\n",
      "Losgistic Regression(    1400/10000): loss= 3752.94265416502\n",
      "Losgistic Regression(    1500/10000): loss= 3741.34352515083\n",
      "Losgistic Regression(    1600/10000): loss= 3731.27079585311\n",
      "Losgistic Regression(    1700/10000): loss= 3721.74478273051\n",
      "Losgistic Regression(    1800/10000): loss= 3712.58885846474\n",
      "Losgistic Regression(    1900/10000): loss= 3704.26627072269\n",
      "Losgistic Regression(    2000/10000): loss= 3696.59973364103\n",
      "Losgistic Regression(    2100/10000): loss= 3689.12875975084\n",
      "Losgistic Regression(    2200/10000): loss= 3681.55605407147\n",
      "Losgistic Regression(    2300/10000): loss= 3673.96704365038\n",
      "Losgistic Regression(    2400/10000): loss= 3666.65413815554\n",
      "Losgistic Regression(    2500/10000): loss= 3659.70127059471\n",
      "Losgistic Regression(    2600/10000): loss= 3652.92289989249\n",
      "Losgistic Regression(    2700/10000): loss= 3646.15145029826\n",
      "Losgistic Regression(    2800/10000): loss= 3639.47796203109\n",
      "Losgistic Regression(    2900/10000): loss= 3633.07589900235\n",
      "Losgistic Regression(    3000/10000): loss= 3627.00917501861\n",
      "Losgistic Regression(    3100/10000): loss= 3621.32683317696\n",
      "Losgistic Regression(    3200/10000): loss= 3616.00487860234\n",
      "Losgistic Regression(    3300/10000): loss= 3611.05309833332\n",
      "Losgistic Regression(    3400/10000): loss= 3606.46226247929\n",
      "Losgistic Regression(    3500/10000): loss= 3602.10050688913\n",
      "Losgistic Regression(    3600/10000): loss= 3597.96725891524\n",
      "Losgistic Regression(    3700/10000): loss= 3594.15440610075\n",
      "Losgistic Regression(    3800/10000): loss= 3590.66041122675\n",
      "Losgistic Regression(    3900/10000): loss= 3587.45134672074\n",
      "Losgistic Regression(    4000/10000): loss= 3584.40780880247\n",
      "Losgistic Regression(    4100/10000): loss= 3581.44441846038\n",
      "Losgistic Regression(    4200/10000): loss= 3578.49593950712\n",
      "Losgistic Regression(    4300/10000): loss= 3575.59433478113\n",
      "Losgistic Regression(    4400/10000): loss= 3572.79109108423\n",
      "Losgistic Regression(    4500/10000): loss= 3570.1567765734\n",
      "Losgistic Regression(    4600/10000): loss= 3567.68994333009\n",
      "Losgistic Regression(    4700/10000): loss= 3565.28247319566\n",
      "Losgistic Regression(    4800/10000): loss= 3562.86697746271\n",
      "Losgistic Regression(    4900/10000): loss= 3560.48811226756\n",
      "Losgistic Regression(    5000/10000): loss= 3558.19408389371\n",
      "Losgistic Regression(    5100/10000): loss= 3556.04905990489\n",
      "Losgistic Regression(    5200/10000): loss= 3554.02318113895\n",
      "Losgistic Regression(    5300/10000): loss= 3552.07453529846\n",
      "Losgistic Regression(    5400/10000): loss= 3550.21432087675\n",
      "Losgistic Regression(    5500/10000): loss= 3548.41387731671\n",
      "Losgistic Regression(    5600/10000): loss= 3546.71906688348\n",
      "Losgistic Regression(    5700/10000): loss= 3545.12543300751\n",
      "Losgistic Regression(    5800/10000): loss= 3543.64316718264\n",
      "Losgistic Regression(    5900/10000): loss= 3542.29995588667\n",
      "Losgistic Regression(    6000/10000): loss= 3541.05565823854\n",
      "Losgistic Regression(    6100/10000): loss= 3539.85540329267\n",
      "Losgistic Regression(    6200/10000): loss= 3538.67137680455\n",
      "Losgistic Regression(    6300/10000): loss= 3537.49438495402\n",
      "Losgistic Regression(    6400/10000): loss= 3536.3393125164\n",
      "Losgistic Regression(    6500/10000): loss= 3535.21057526574\n",
      "Losgistic Regression(    6600/10000): loss= 3534.10355295302\n",
      "Losgistic Regression(    6700/10000): loss= 3532.93273587926\n",
      "Losgistic Regression(    6800/10000): loss= 3531.75784639114\n",
      "Losgistic Regression(    6900/10000): loss= 3530.59988328037\n",
      "Losgistic Regression(    7000/10000): loss= 3529.46183846758\n",
      "Losgistic Regression(    7100/10000): loss= 3528.34744830693\n",
      "Losgistic Regression(    7200/10000): loss= 3527.23339565186\n",
      "Losgistic Regression(    7300/10000): loss= 3526.10480807012\n",
      "Losgistic Regression(    7400/10000): loss= 3524.92770945445\n",
      "Losgistic Regression(    7500/10000): loss= 3523.70682237709\n",
      "Losgistic Regression(    7600/10000): loss= 3522.51875957879\n",
      "Losgistic Regression(    7700/10000): loss= 3521.35672485011\n",
      "Losgistic Regression(    7800/10000): loss= 3520.20705210703\n",
      "Losgistic Regression(    7900/10000): loss= 3519.07688760096\n",
      "Losgistic Regression(    8000/10000): loss= 3517.96825349937\n",
      "Losgistic Regression(    8100/10000): loss= 3516.88038664028\n",
      "Losgistic Regression(    8200/10000): loss= 3515.8045286569\n",
      "Losgistic Regression(    8300/10000): loss= 3514.7381709091\n",
      "Losgistic Regression(    8400/10000): loss= 3513.70719868308\n",
      "Losgistic Regression(    8500/10000): loss= 3512.7368634915\n",
      "Losgistic Regression(    8600/10000): loss= 3511.83165280684\n",
      "Losgistic Regression(    8700/10000): loss= 3510.98627235701\n",
      "Losgistic Regression(    8800/10000): loss= 3510.18588410985\n",
      "Losgistic Regression(    8900/10000): loss= 3509.43039113572\n",
      "Losgistic Regression(    9000/10000): loss= 3508.73318107809\n",
      "Losgistic Regression(    9100/10000): loss= 3508.09569184624\n",
      "Losgistic Regression(    9200/10000): loss= 3507.51547562535\n",
      "Losgistic Regression(    9300/10000): loss= 3506.996016696\n",
      "Losgistic Regression(    9400/10000): loss= 3506.52661741855\n",
      "Losgistic Regression(    9500/10000): loss= 3506.09040923293\n",
      "Losgistic Regression(    9600/10000): loss= 3505.67637962338\n",
      "Losgistic Regression(    9700/10000): loss= 3505.28836376236\n",
      "Losgistic Regression(    9800/10000): loss= 3504.92940617249\n",
      "Losgistic Regression(    9900/10000): loss= 3504.60502845513\n",
      "Time for  3th cross validation = 316.615s\n",
      "Training Accuracy         =  0.845\n",
      "Cross Validation Accuracy = 0.81414\n",
      "Losgistic Regression(       0/10000): loss= 6820.81257140681\n",
      "Losgistic Regression(     100/10000): loss= 4549.08737631688\n",
      "Losgistic Regression(     200/10000): loss= 4304.07914827624\n",
      "Losgistic Regression(     300/10000): loss= 4187.0563956635\n",
      "Losgistic Regression(     400/10000): loss= 4113.86147606653\n",
      "Losgistic Regression(     500/10000): loss= 4059.88786775868\n",
      "Losgistic Regression(     600/10000): loss= 4013.19119597406\n",
      "Losgistic Regression(     700/10000): loss= 3973.81369196939\n",
      "Losgistic Regression(     800/10000): loss= 3939.81536905797\n",
      "Losgistic Regression(     900/10000): loss= 3911.09815501102\n",
      "Losgistic Regression(    1000/10000): loss= 3887.80597497603\n",
      "Losgistic Regression(    1100/10000): loss= 3867.34712626692\n",
      "Losgistic Regression(    1200/10000): loss= 3849.59421962292\n",
      "Losgistic Regression(    1300/10000): loss= 3834.62515970259\n",
      "Losgistic Regression(    1400/10000): loss= 3823.32699882631\n",
      "Losgistic Regression(    1500/10000): loss= 3812.90189812583\n",
      "Losgistic Regression(    1600/10000): loss= 3802.987247851\n",
      "Losgistic Regression(    1700/10000): loss= 3792.71551629668\n",
      "Losgistic Regression(    1800/10000): loss= 3783.56281283207\n",
      "Losgistic Regression(    1900/10000): loss= 3775.40178739603\n",
      "Losgistic Regression(    2000/10000): loss= 3768.46311256192\n",
      "Losgistic Regression(    2100/10000): loss= 3761.97108960724\n",
      "Losgistic Regression(    2200/10000): loss= 3755.43429155067\n",
      "Losgistic Regression(    2300/10000): loss= 3749.36034727366\n",
      "Losgistic Regression(    2400/10000): loss= 3744.47302208722\n",
      "Losgistic Regression(    2500/10000): loss= 3737.8383013543\n",
      "Losgistic Regression(    2600/10000): loss= 3730.29523815014\n",
      "Losgistic Regression(    2700/10000): loss= 3722.94047241162\n",
      "Losgistic Regression(    2800/10000): loss= 3716.11126468113\n",
      "Losgistic Regression(    2900/10000): loss= 3709.16304992761\n",
      "Losgistic Regression(    3000/10000): loss= 3702.09539189451\n",
      "Losgistic Regression(    3100/10000): loss= 3694.7670295661\n",
      "Losgistic Regression(    3200/10000): loss= 3686.6057669833\n",
      "Losgistic Regression(    3300/10000): loss= 3678.13873361311\n",
      "Losgistic Regression(    3400/10000): loss= 3669.73863352366\n",
      "Losgistic Regression(    3500/10000): loss= 3661.99941436619\n",
      "Losgistic Regression(    3600/10000): loss= 3654.4159493689\n",
      "Losgistic Regression(    3700/10000): loss= 3647.49764041543\n",
      "Losgistic Regression(    3800/10000): loss= 3641.23378139557\n",
      "Losgistic Regression(    3900/10000): loss= 3635.673522877\n",
      "Losgistic Regression(    4000/10000): loss= 3630.75804161485\n",
      "Losgistic Regression(    4100/10000): loss= 3626.12354626119\n",
      "Losgistic Regression(    4200/10000): loss= 3621.80678128647\n",
      "Losgistic Regression(    4300/10000): loss= 3617.45440453766\n",
      "Losgistic Regression(    4400/10000): loss= 3613.36209574162\n",
      "Losgistic Regression(    4500/10000): loss= 3609.23929787846\n",
      "Losgistic Regression(    4600/10000): loss= 3605.46240818641\n",
      "Losgistic Regression(    4700/10000): loss= 3601.72455644528\n",
      "Losgistic Regression(    4800/10000): loss= 3598.14340512103\n",
      "Losgistic Regression(    4900/10000): loss= 3594.70765790642\n",
      "Losgistic Regression(    5000/10000): loss= 3591.35720713416\n",
      "Losgistic Regression(    5100/10000): loss= 3587.78556431035\n",
      "Losgistic Regression(    5200/10000): loss= 3584.31196049074\n",
      "Losgistic Regression(    5300/10000): loss= 3580.87640757516\n",
      "Losgistic Regression(    5400/10000): loss= 3577.4974843726\n",
      "Losgistic Regression(    5500/10000): loss= 3574.42891550958\n",
      "Losgistic Regression(    5600/10000): loss= 3571.55450744441\n",
      "Losgistic Regression(    5700/10000): loss= 3568.90770312022\n",
      "Losgistic Regression(    5800/10000): loss= 3566.28157065168\n",
      "Losgistic Regression(    5900/10000): loss= 3563.66741949419\n",
      "Losgistic Regression(    6000/10000): loss= 3561.36826248488\n",
      "Losgistic Regression(    6100/10000): loss= 3559.33910243898\n",
      "Losgistic Regression(    6200/10000): loss= 3557.56729990133\n",
      "Losgistic Regression(    6300/10000): loss= 3556.04514961189\n",
      "Losgistic Regression(    6400/10000): loss= 3554.89412038511\n",
      "Losgistic Regression(    6500/10000): loss= 3553.95929615754\n",
      "Losgistic Regression(    6600/10000): loss= 3553.15121412892\n",
      "Losgistic Regression(    6700/10000): loss= 3552.58578004309\n",
      "Losgistic Regression(    6800/10000): loss= 3551.87482990902\n",
      "Losgistic Regression(    6900/10000): loss= 3551.18674513383\n",
      "Losgistic Regression(    7000/10000): loss= 3550.57865476888\n",
      "Losgistic Regression(    7100/10000): loss= 3550.10731129597\n",
      "Losgistic Regression(    7200/10000): loss= 3549.67142265136\n",
      "Losgistic Regression(    7300/10000): loss= 3549.243687084\n",
      "Losgistic Regression(    7400/10000): loss= 3548.92136028517\n",
      "Losgistic Regression(    7500/10000): loss= 3548.63403249947\n",
      "Losgistic Regression(    7600/10000): loss= 3548.23704217327\n",
      "Losgistic Regression(    7700/10000): loss= 3547.80363010904\n",
      "Losgistic Regression(    7800/10000): loss= 3547.36375536062\n",
      "Losgistic Regression(    7900/10000): loss= 3546.88968800852\n",
      "Losgistic Regression(    8000/10000): loss= 3546.35562592364\n",
      "Losgistic Regression(    8100/10000): loss= 3545.81103672763\n",
      "Losgistic Regression(    8200/10000): loss= 3545.19569799344\n",
      "Losgistic Regression(    8300/10000): loss= 3544.40423518826\n",
      "Losgistic Regression(    8400/10000): loss= 3543.52113374772\n",
      "Losgistic Regression(    8500/10000): loss= 3542.5203384672\n",
      "Losgistic Regression(    8600/10000): loss= 3541.36756304777\n",
      "Losgistic Regression(    8700/10000): loss= 3540.16044721759\n",
      "Losgistic Regression(    8800/10000): loss= 3538.94753290106\n",
      "Losgistic Regression(    8900/10000): loss= 3537.71449950819\n",
      "Losgistic Regression(    9000/10000): loss= 3536.46976818941\n",
      "Losgistic Regression(    9100/10000): loss= 3535.19426958348\n",
      "Losgistic Regression(    9200/10000): loss= 3533.8915690139\n",
      "Losgistic Regression(    9300/10000): loss= 3532.49137249303\n",
      "Losgistic Regression(    9400/10000): loss= 3531.01275612247\n",
      "Losgistic Regression(    9500/10000): loss= 3529.51853116795\n",
      "Losgistic Regression(    9600/10000): loss= 3527.96901763913\n",
      "Losgistic Regression(    9700/10000): loss= 3526.35288085581\n",
      "Losgistic Regression(    9800/10000): loss= 3524.73018731022\n",
      "Losgistic Regression(    9900/10000): loss= 3523.0964202242\n",
      "Time for  4th cross validation = 315.235s\n",
      "Training Accuracy         = 0.8353\n",
      "Cross Validation Accuracy = 0.815752\n",
      "*************** ([0.84109999999999996, 0.84240000000000004, 0.84740000000000004, 0.84499999999999997, 0.83530000000000004], [0.81363200000000002, 0.81584000000000001, 0.81413999999999997, 0.81413999999999997, 0.81575200000000003])\n",
      "Losgistic Regression(       0/10000): loss= 6821.10320034063\n",
      "Losgistic Regression(     100/10000): loss= 4537.12604080686\n",
      "Losgistic Regression(     200/10000): loss= 4257.22888395114\n",
      "Losgistic Regression(     300/10000): loss= 4116.99743899103\n",
      "Losgistic Regression(     400/10000): loss= 4026.0334837952\n",
      "Losgistic Regression(     500/10000): loss= 3959.60742953169\n",
      "Losgistic Regression(     600/10000): loss= 3910.48813453314\n",
      "Losgistic Regression(     700/10000): loss= 3873.32274673794\n",
      "Losgistic Regression(     800/10000): loss= 3843.1270691045\n",
      "Losgistic Regression(     900/10000): loss= 3818.18013609114\n",
      "Losgistic Regression(    1000/10000): loss= 3796.78591672888\n",
      "Losgistic Regression(    1100/10000): loss= 3778.20543966407\n",
      "Losgistic Regression(    1200/10000): loss= 3761.80020038578\n",
      "Losgistic Regression(    1300/10000): loss= 3747.47628341895\n",
      "Losgistic Regression(    1400/10000): loss= 3735.25312186498\n",
      "Losgistic Regression(    1500/10000): loss= 3725.60243230438\n",
      "Losgistic Regression(    1600/10000): loss= 3716.95884052755\n",
      "Losgistic Regression(    1700/10000): loss= 3708.46699230361\n",
      "Losgistic Regression(    1800/10000): loss= 3702.11059780219\n",
      "Losgistic Regression(    1900/10000): loss= 3695.84656716069\n",
      "Losgistic Regression(    2000/10000): loss= 3691.49624330236\n",
      "Losgistic Regression(    2100/10000): loss= 3686.57258912659\n",
      "Losgistic Regression(    2200/10000): loss= 3681.84462218514\n",
      "Losgistic Regression(    2300/10000): loss= 3676.96366849071\n",
      "Losgistic Regression(    2400/10000): loss= 3672.32474608363\n",
      "Losgistic Regression(    2500/10000): loss= 3668.53925421463\n",
      "Losgistic Regression(    2600/10000): loss= 3664.80143213296\n",
      "Losgistic Regression(    2700/10000): loss= 3661.63987475747\n",
      "Losgistic Regression(    2800/10000): loss= 3658.66698685367\n",
      "Losgistic Regression(    2900/10000): loss= 3655.95207527631\n",
      "Losgistic Regression(    3000/10000): loss= 3653.46178996921\n",
      "Losgistic Regression(    3100/10000): loss= 3651.34892121807\n",
      "Losgistic Regression(    3200/10000): loss= 3649.24596392702\n",
      "Losgistic Regression(    3300/10000): loss= 3647.16348754646\n",
      "Losgistic Regression(    3400/10000): loss= 3645.17442845754\n",
      "Losgistic Regression(    3500/10000): loss= 3643.41248228478\n",
      "Losgistic Regression(    3600/10000): loss= 3641.71439628295\n",
      "Losgistic Regression(    3700/10000): loss= 3640.09686353183\n",
      "Losgistic Regression(    3800/10000): loss= 3638.42533242078\n",
      "Losgistic Regression(    3900/10000): loss= 3636.91743333129\n",
      "Losgistic Regression(    4000/10000): loss= 3635.40151881901\n",
      "Losgistic Regression(    4100/10000): loss= 3634.0495684487\n",
      "Losgistic Regression(    4200/10000): loss= 3632.81304579602\n",
      "Losgistic Regression(    4300/10000): loss= 3631.56889876559\n",
      "Losgistic Regression(    4400/10000): loss= 3630.36754114809\n",
      "Losgistic Regression(    4500/10000): loss= 3629.29809275446\n",
      "Losgistic Regression(    4600/10000): loss= 3628.34016733721\n",
      "Losgistic Regression(    4700/10000): loss= 3627.56582936312\n",
      "Losgistic Regression(    4800/10000): loss= 3626.89140975789\n",
      "Losgistic Regression(    4900/10000): loss= 3626.39147944546\n",
      "Losgistic Regression(    5000/10000): loss= 3625.8719506205\n",
      "Losgistic Regression(    5100/10000): loss= 3625.33207923379\n",
      "Losgistic Regression(    5200/10000): loss= 3624.77860876813\n",
      "Losgistic Regression(    5300/10000): loss= 3624.23328574074\n",
      "Losgistic Regression(    5400/10000): loss= 3623.74638209885\n",
      "Losgistic Regression(    5500/10000): loss= 3623.34523892728\n",
      "Losgistic Regression(    5600/10000): loss= 3622.96564881513\n",
      "Losgistic Regression(    5700/10000): loss= 3622.635778493\n",
      "Losgistic Regression(    5800/10000): loss= 3622.38910082147\n",
      "Losgistic Regression(    5900/10000): loss= 3622.18734298869\n",
      "Losgistic Regression(    6000/10000): loss= 3622.03092226599\n",
      "Losgistic Regression(    6100/10000): loss= 3621.94302499184\n",
      "Losgistic Regression(    6200/10000): loss= 3621.83702002586\n",
      "Losgistic Regression(    6300/10000): loss= 3621.7406397971\n",
      "Losgistic Regression(    6400/10000): loss= 3621.65624896022\n",
      "Losgistic Regression(    6500/10000): loss= 3621.57368095895\n",
      "Losgistic Regression(    6600/10000): loss= 3621.53952198526\n",
      "Totoal number of iterations =  6600\n",
      "Loss                        =  3621.53952199\n",
      "Time for  0th cross validation = 207.727s\n",
      "Training Accuracy         = 0.8405\n",
      "Cross Validation Accuracy = 0.813684\n",
      "Losgistic Regression(       0/10000): loss= 6827.55841056485\n",
      "Losgistic Regression(     100/10000): loss= 4486.79934276993\n",
      "Losgistic Regression(     200/10000): loss= 4211.53404631562\n",
      "Losgistic Regression(     300/10000): loss= 4067.26244127669\n",
      "Losgistic Regression(     400/10000): loss= 3970.00447028755\n",
      "Losgistic Regression(     500/10000): loss= 3899.93643162957\n",
      "Losgistic Regression(     600/10000): loss= 3848.89194903242\n",
      "Losgistic Regression(     700/10000): loss= 3810.18365775755\n",
      "Losgistic Regression(     800/10000): loss= 3779.53474821211\n",
      "Losgistic Regression(     900/10000): loss= 3754.95448578011\n",
      "Losgistic Regression(    1000/10000): loss= 3735.5033093954\n",
      "Losgistic Regression(    1100/10000): loss= 3720.09666972197\n",
      "Losgistic Regression(    1200/10000): loss= 3707.59875401394\n",
      "Losgistic Regression(    1300/10000): loss= 3697.0770546328\n",
      "Losgistic Regression(    1400/10000): loss= 3687.90635999333\n",
      "Losgistic Regression(    1500/10000): loss= 3680.11725184779\n",
      "Losgistic Regression(    1600/10000): loss= 3673.09816442947\n",
      "Losgistic Regression(    1700/10000): loss= 3666.60894680289\n",
      "Losgistic Regression(    1800/10000): loss= 3660.59063388618\n",
      "Losgistic Regression(    1900/10000): loss= 3655.00897649625\n",
      "Losgistic Regression(    2000/10000): loss= 3649.88367898672\n",
      "Losgistic Regression(    2100/10000): loss= 3645.12594152167\n",
      "Losgistic Regression(    2200/10000): loss= 3640.75058685289\n",
      "Losgistic Regression(    2300/10000): loss= 3636.73127386408\n",
      "Losgistic Regression(    2400/10000): loss= 3633.01751106433\n",
      "Losgistic Regression(    2500/10000): loss= 3629.58123760419\n",
      "Losgistic Regression(    2600/10000): loss= 3626.00735133652\n",
      "Losgistic Regression(    2700/10000): loss= 3622.7061853556\n",
      "Losgistic Regression(    2800/10000): loss= 3619.61003462154\n",
      "Losgistic Regression(    2900/10000): loss= 3616.63430358069\n",
      "Losgistic Regression(    3000/10000): loss= 3613.74576116612\n",
      "Losgistic Regression(    3100/10000): loss= 3610.97933607324\n",
      "Losgistic Regression(    3200/10000): loss= 3607.68247958944\n",
      "Losgistic Regression(    3300/10000): loss= 3602.95375111659\n",
      "Losgistic Regression(    3400/10000): loss= 3598.15566474635\n",
      "Losgistic Regression(    3500/10000): loss= 3593.38450858773\n",
      "Losgistic Regression(    3600/10000): loss= 3588.74867595204\n",
      "Losgistic Regression(    3700/10000): loss= 3584.2453926548\n",
      "Losgistic Regression(    3800/10000): loss= 3579.8963750055\n",
      "Losgistic Regression(    3900/10000): loss= 3575.7929056014\n",
      "Losgistic Regression(    4000/10000): loss= 3571.93926486994\n",
      "Losgistic Regression(    4100/10000): loss= 3568.24373175751\n",
      "Losgistic Regression(    4200/10000): loss= 3564.64260638992\n",
      "Losgistic Regression(    4300/10000): loss= 3561.08478122176\n",
      "Losgistic Regression(    4400/10000): loss= 3557.6470348205\n",
      "Losgistic Regression(    4500/10000): loss= 3554.34378156298\n",
      "Losgistic Regression(    4600/10000): loss= 3551.22391901735\n",
      "Losgistic Regression(    4700/10000): loss= 3548.27955791508\n",
      "Losgistic Regression(    4800/10000): loss= 3545.41791556375\n",
      "Losgistic Regression(    4900/10000): loss= 3542.65502338705\n",
      "Losgistic Regression(    5000/10000): loss= 3539.98372524614\n",
      "Losgistic Regression(    5100/10000): loss= 3537.41568845685\n",
      "Losgistic Regression(    5200/10000): loss= 3534.98558236292\n",
      "Losgistic Regression(    5300/10000): loss= 3532.71353218993\n",
      "Losgistic Regression(    5400/10000): loss= 3530.61777880853\n",
      "Losgistic Regression(    5500/10000): loss= 3528.61995434254\n",
      "Losgistic Regression(    5600/10000): loss= 3526.70704641848\n",
      "Losgistic Regression(    5700/10000): loss= 3525.00931270624\n",
      "Losgistic Regression(    5800/10000): loss= 3523.51140921885\n",
      "Losgistic Regression(    5900/10000): loss= 3522.22113580624\n",
      "Losgistic Regression(    6000/10000): loss= 3521.10853650939\n",
      "Losgistic Regression(    6100/10000): loss= 3520.2001390732\n",
      "Losgistic Regression(    6200/10000): loss= 3519.4680822387\n",
      "Losgistic Regression(    6300/10000): loss= 3518.44332174321\n",
      "Losgistic Regression(    6400/10000): loss= 3517.93386118074\n",
      "Losgistic Regression(    6500/10000): loss= 3517.93431806882\n",
      "Totoal number of iterations =  6500\n",
      "Loss                        =  3517.93431807\n",
      "Time for  1th cross validation = 205.318s\n",
      "Training Accuracy         = 0.8417\n",
      "Cross Validation Accuracy = 0.8159\n",
      "Losgistic Regression(       0/10000): loss= 6825.29433723592\n",
      "Losgistic Regression(     100/10000): loss= 4453.05105807843\n",
      "Losgistic Regression(     200/10000): loss= 4194.11104561502\n",
      "Losgistic Regression(     300/10000): loss= 4055.27529336639\n",
      "Losgistic Regression(     400/10000): loss= 3959.93422566799\n",
      "Losgistic Regression(     500/10000): loss= 3889.96439156754\n",
      "Losgistic Regression(     600/10000): loss= 3834.05858246469\n",
      "Losgistic Regression(     700/10000): loss= 3789.05074464555\n",
      "Losgistic Regression(     800/10000): loss= 3753.37531677857\n",
      "Losgistic Regression(     900/10000): loss= 3723.68531919097\n",
      "Losgistic Regression(    1000/10000): loss= 3703.10371301037\n",
      "Losgistic Regression(    1100/10000): loss= 3687.64743466905\n",
      "Losgistic Regression(    1200/10000): loss= 3676.07027402628\n",
      "Losgistic Regression(    1300/10000): loss= 3667.83623220377\n",
      "Losgistic Regression(    1400/10000): loss= 3661.22378527716\n",
      "Losgistic Regression(    1500/10000): loss= 3656.60140943531\n",
      "Losgistic Regression(    1600/10000): loss= 3653.24259794237\n",
      "Losgistic Regression(    1700/10000): loss= 3651.11055640743\n",
      "Losgistic Regression(    1800/10000): loss= 3648.5353721041\n",
      "Losgistic Regression(    1900/10000): loss= 3646.28892273601\n",
      "Losgistic Regression(    2000/10000): loss= 3643.72285085812\n",
      "Losgistic Regression(    2100/10000): loss= 3641.65783446504\n",
      "Losgistic Regression(    2200/10000): loss= 3636.25689682407\n",
      "Losgistic Regression(    2300/10000): loss= 3630.72448876221\n",
      "Losgistic Regression(    2400/10000): loss= 3625.44726571805\n",
      "Losgistic Regression(    2500/10000): loss= 3620.27167376393\n",
      "Losgistic Regression(    2600/10000): loss= 3615.34860388951\n",
      "Losgistic Regression(    2700/10000): loss= 3610.57474544961\n",
      "Losgistic Regression(    2800/10000): loss= 3606.1777436806\n",
      "Losgistic Regression(    2900/10000): loss= 3602.07186668596\n",
      "Losgistic Regression(    3000/10000): loss= 3598.3121816075\n",
      "Losgistic Regression(    3100/10000): loss= 3594.7098122159\n",
      "Losgistic Regression(    3200/10000): loss= 3591.417159207\n",
      "Losgistic Regression(    3300/10000): loss= 3588.18672899738\n",
      "Losgistic Regression(    3400/10000): loss= 3585.18199419997\n",
      "Losgistic Regression(    3500/10000): loss= 3582.52982571415\n",
      "Losgistic Regression(    3600/10000): loss= 3580.16695919752\n",
      "Losgistic Regression(    3700/10000): loss= 3577.68010968439\n",
      "Losgistic Regression(    3800/10000): loss= 3575.22530581421\n",
      "Losgistic Regression(    3900/10000): loss= 3572.92493144482\n",
      "Losgistic Regression(    4000/10000): loss= 3570.93792876208\n",
      "Losgistic Regression(    4100/10000): loss= 3569.16381297399\n",
      "Losgistic Regression(    4200/10000): loss= 3567.04604290247\n",
      "Losgistic Regression(    4300/10000): loss= 3563.91558418164\n",
      "Losgistic Regression(    4400/10000): loss= 3561.07794487578\n",
      "Losgistic Regression(    4500/10000): loss= 3558.40845263389\n",
      "Losgistic Regression(    4600/10000): loss= 3556.01277926068\n",
      "Losgistic Regression(    4700/10000): loss= 3553.82050658679\n",
      "Losgistic Regression(    4800/10000): loss= 3551.80979064927\n",
      "Losgistic Regression(    4900/10000): loss= 3549.96760305575\n",
      "Losgistic Regression(    5000/10000): loss= 3548.19705992147\n",
      "Losgistic Regression(    5100/10000): loss= 3546.46636389727\n",
      "Losgistic Regression(    5200/10000): loss= 3544.8421851928\n",
      "Losgistic Regression(    5300/10000): loss= 3543.30785951412\n",
      "Losgistic Regression(    5400/10000): loss= 3541.84409486957\n",
      "Losgistic Regression(    5500/10000): loss= 3540.45786913891\n",
      "Losgistic Regression(    5600/10000): loss= 3539.13658503739\n",
      "Losgistic Regression(    5700/10000): loss= 3537.92026047549\n",
      "Losgistic Regression(    5800/10000): loss= 3536.82204769838\n",
      "Losgistic Regression(    5900/10000): loss= 3535.75767774555\n",
      "Losgistic Regression(    6000/10000): loss= 3534.62756380828\n",
      "Losgistic Regression(    6100/10000): loss= 3533.43845248306\n",
      "Losgistic Regression(    6200/10000): loss= 3532.27943639934\n",
      "Losgistic Regression(    6300/10000): loss= 3531.17248468666\n",
      "Losgistic Regression(    6400/10000): loss= 3530.08291227213\n",
      "Losgistic Regression(    6500/10000): loss= 3529.01752516448\n",
      "Losgistic Regression(    6600/10000): loss= 3528.03098495074\n",
      "Losgistic Regression(    6700/10000): loss= 3527.1648570088\n",
      "Losgistic Regression(    6800/10000): loss= 3526.4071125748\n",
      "Losgistic Regression(    6900/10000): loss= 3525.72887454435\n",
      "Losgistic Regression(    7000/10000): loss= 3525.10520016655\n",
      "Losgistic Regression(    7100/10000): loss= 3524.4676552736\n",
      "Losgistic Regression(    7200/10000): loss= 3523.8137860684\n",
      "Losgistic Regression(    7300/10000): loss= 3523.16496251243\n",
      "Losgistic Regression(    7400/10000): loss= 3522.52021336607\n",
      "Losgistic Regression(    7500/10000): loss= 3521.89334251771\n",
      "Losgistic Regression(    7600/10000): loss= 3521.28692238574\n",
      "Losgistic Regression(    7700/10000): loss= 3520.70588734179\n",
      "Losgistic Regression(    7800/10000): loss= 3520.14935462887\n",
      "Losgistic Regression(    7900/10000): loss= 3519.57614850131\n",
      "Losgistic Regression(    8000/10000): loss= 3519.01356975164\n",
      "Losgistic Regression(    8100/10000): loss= 3518.53543280592\n",
      "Losgistic Regression(    8200/10000): loss= 3518.15951577026\n",
      "Losgistic Regression(    8300/10000): loss= 3517.8784583826\n",
      "Losgistic Regression(    8400/10000): loss= 3517.6698144666\n",
      "Losgistic Regression(    8500/10000): loss= 3517.48611795173\n",
      "Losgistic Regression(    8600/10000): loss= 3517.27812124225\n",
      "Losgistic Regression(    8700/10000): loss= 3517.02169046692\n",
      "Losgistic Regression(    8800/10000): loss= 3516.75749296208\n",
      "Losgistic Regression(    8900/10000): loss= 3516.52416623547\n",
      "Losgistic Regression(    9000/10000): loss= 3516.33260301923\n",
      "Losgistic Regression(    9100/10000): loss= 3516.19109644156\n",
      "Losgistic Regression(    9200/10000): loss= 3516.10853339583\n",
      "Losgistic Regression(    9300/10000): loss= 3516.0592710376\n",
      "Losgistic Regression(    9400/10000): loss= 3516.04505954321\n",
      "Totoal number of iterations =  9400\n",
      "Loss                        =  3516.04505954\n",
      "Time for  2th cross validation = 296.374s\n",
      "Training Accuracy         = 0.8469\n",
      "Cross Validation Accuracy = 0.814356\n",
      "Losgistic Regression(       0/10000): loss= 6822.87879470309\n",
      "Losgistic Regression(     100/10000): loss= 4536.21573776033\n",
      "Losgistic Regression(     200/10000): loss= 4276.75800159108\n",
      "Losgistic Regression(     300/10000): loss= 4135.41202988982\n",
      "Losgistic Regression(     400/10000): loss= 4046.56034254423\n",
      "Losgistic Regression(     500/10000): loss= 3981.01823964973\n",
      "Losgistic Regression(     600/10000): loss= 3926.77493141299\n",
      "Losgistic Regression(     700/10000): loss= 3881.85997274755\n",
      "Losgistic Regression(     800/10000): loss= 3851.64006375499\n",
      "Losgistic Regression(     900/10000): loss= 3826.83263988254\n",
      "Losgistic Regression(    1000/10000): loss= 3809.05870704307\n",
      "Losgistic Regression(    1100/10000): loss= 3794.07919404159\n",
      "Losgistic Regression(    1200/10000): loss= 3782.29078068065\n",
      "Losgistic Regression(    1300/10000): loss= 3770.42093102867\n",
      "Losgistic Regression(    1400/10000): loss= 3758.23859677475\n",
      "Losgistic Regression(    1500/10000): loss= 3746.88354065016\n",
      "Losgistic Regression(    1600/10000): loss= 3737.04122057362\n",
      "Losgistic Regression(    1700/10000): loss= 3727.74605482993\n",
      "Losgistic Regression(    1800/10000): loss= 3718.81079083259\n",
      "Losgistic Regression(    1900/10000): loss= 3710.70648367946\n",
      "Losgistic Regression(    2000/10000): loss= 3703.24693250154\n",
      "Losgistic Regression(    2100/10000): loss= 3695.98660681175\n",
      "Losgistic Regression(    2200/10000): loss= 3688.6347245841\n",
      "Losgistic Regression(    2300/10000): loss= 3681.27210234362\n",
      "Losgistic Regression(    2400/10000): loss= 3674.18288332862\n",
      "Losgistic Regression(    2500/10000): loss= 3667.45794202597\n",
      "Losgistic Regression(    2600/10000): loss= 3660.92081038034\n",
      "Losgistic Regression(    2700/10000): loss= 3654.39629396646\n",
      "Losgistic Regression(    2800/10000): loss= 3647.95353590946\n",
      "Losgistic Regression(    2900/10000): loss= 3641.76524909056\n",
      "Losgistic Regression(    3000/10000): loss= 3635.9103945746\n",
      "Losgistic Regression(    3100/10000): loss= 3630.44933087365\n",
      "Losgistic Regression(    3200/10000): loss= 3625.34044565259\n",
      "Losgistic Regression(    3300/10000): loss= 3620.59470723546\n",
      "Losgistic Regression(    3400/10000): loss= 3616.19490781888\n",
      "Losgistic Regression(    3500/10000): loss= 3612.00743637796\n",
      "Losgistic Regression(    3600/10000): loss= 3608.03401814932\n",
      "Losgistic Regression(    3700/10000): loss= 3604.37967520112\n",
      "Losgistic Regression(    3800/10000): loss= 3601.0446698444\n",
      "Losgistic Regression(    3900/10000): loss= 3597.98588540031\n",
      "Losgistic Regression(    4000/10000): loss= 3595.08327277298\n",
      "Losgistic Regression(    4100/10000): loss= 3592.25574305927\n",
      "Losgistic Regression(    4200/10000): loss= 3589.44404117059\n",
      "Losgistic Regression(    4300/10000): loss= 3586.68130104467\n",
      "Losgistic Regression(    4400/10000): loss= 3584.01301086592\n",
      "Losgistic Regression(    4500/10000): loss= 3581.52038342633\n",
      "Losgistic Regression(    4600/10000): loss= 3579.19484563616\n",
      "Losgistic Regression(    4700/10000): loss= 3576.92410158351\n",
      "Losgistic Regression(    4800/10000): loss= 3574.649515238\n",
      "Losgistic Regression(    4900/10000): loss= 3572.43139550568\n",
      "Losgistic Regression(    5000/10000): loss= 3570.31654477343\n",
      "Losgistic Regression(    5100/10000): loss= 3568.3213607293\n",
      "Losgistic Regression(    5200/10000): loss= 3566.42284653435\n",
      "Losgistic Regression(    5300/10000): loss= 3564.58917352956\n",
      "Losgistic Regression(    5400/10000): loss= 3562.83742902949\n",
      "Losgistic Regression(    5500/10000): loss= 3561.17693798975\n",
      "Losgistic Regression(    5600/10000): loss= 3559.61522074092\n",
      "Losgistic Regression(    5700/10000): loss= 3558.1607042199\n",
      "Losgistic Regression(    5800/10000): loss= 3556.81522809381\n",
      "Losgistic Regression(    5900/10000): loss= 3555.57609138705\n",
      "Losgistic Regression(    6000/10000): loss= 3554.42470833689\n",
      "Losgistic Regression(    6100/10000): loss= 3553.33394677866\n",
      "Losgistic Regression(    6200/10000): loss= 3552.27520180151\n",
      "Losgistic Regression(    6300/10000): loss= 3551.24239968148\n",
      "Losgistic Regression(    6400/10000): loss= 3550.24105226524\n",
      "Losgistic Regression(    6500/10000): loss= 3549.26532042406\n",
      "Losgistic Regression(    6600/10000): loss= 3548.32015019003\n",
      "Losgistic Regression(    6700/10000): loss= 3547.39033907193\n",
      "Losgistic Regression(    6800/10000): loss= 3546.48002088744\n",
      "Losgistic Regression(    6900/10000): loss= 3545.59914040371\n",
      "Losgistic Regression(    7000/10000): loss= 3544.68620960462\n",
      "Losgistic Regression(    7100/10000): loss= 3543.71209067003\n",
      "Losgistic Regression(    7200/10000): loss= 3542.72291475405\n",
      "Losgistic Regression(    7300/10000): loss= 3541.7106747631\n",
      "Losgistic Regression(    7400/10000): loss= 3540.69756847751\n",
      "Losgistic Regression(    7500/10000): loss= 3539.71051059809\n",
      "Losgistic Regression(    7600/10000): loss= 3538.75705280957\n",
      "Losgistic Regression(    7700/10000): loss= 3537.81736620786\n",
      "Losgistic Regression(    7800/10000): loss= 3536.87678371564\n",
      "Losgistic Regression(    7900/10000): loss= 3535.95573579321\n",
      "Losgistic Regression(    8000/10000): loss= 3535.05765202224\n",
      "Losgistic Regression(    8100/10000): loss= 3534.13032809717\n",
      "Losgistic Regression(    8200/10000): loss= 3533.18849180878\n",
      "Losgistic Regression(    8300/10000): loss= 3532.2661139289\n",
      "Losgistic Regression(    8400/10000): loss= 3531.39161004654\n",
      "Losgistic Regression(    8500/10000): loss= 3530.569255798\n",
      "Losgistic Regression(    8600/10000): loss= 3529.80708373888\n",
      "Losgistic Regression(    8700/10000): loss= 3529.09921420191\n",
      "Losgistic Regression(    8800/10000): loss= 3528.43779625312\n",
      "Losgistic Regression(    8900/10000): loss= 3527.82895604759\n",
      "Losgistic Regression(    9000/10000): loss= 3527.28278497702\n",
      "Losgistic Regression(    9100/10000): loss= 3526.7831092694\n",
      "Losgistic Regression(    9200/10000): loss= 3526.31212622863\n",
      "Losgistic Regression(    9300/10000): loss= 3525.87293760819\n",
      "Losgistic Regression(    9400/10000): loss= 3525.46582486917\n",
      "Losgistic Regression(    9500/10000): loss= 3525.08950492385\n",
      "Losgistic Regression(    9600/10000): loss= 3524.7411012861\n",
      "Losgistic Regression(    9700/10000): loss= 3524.42716980013\n",
      "Losgistic Regression(    9800/10000): loss= 3524.13222041681\n",
      "Losgistic Regression(    9900/10000): loss= 3523.85744949444\n",
      "Time for  3th cross validation = 314.902s\n",
      "Training Accuracy         = 0.8451\n",
      "Cross Validation Accuracy = 0.814524\n",
      "Losgistic Regression(       0/10000): loss= 6820.81851328814\n",
      "Losgistic Regression(     100/10000): loss= 4549.72419558616\n",
      "Losgistic Regression(     200/10000): loss= 4305.22159019334\n",
      "Losgistic Regression(     300/10000): loss= 4188.62662424448\n",
      "Losgistic Regression(     400/10000): loss= 4115.8102899981\n",
      "Losgistic Regression(     500/10000): loss= 4062.21856468941\n",
      "Losgistic Regression(     600/10000): loss= 4016.05912125216\n",
      "Losgistic Regression(     700/10000): loss= 3977.07419966934\n",
      "Losgistic Regression(     800/10000): loss= 3943.44074460786\n",
      "Losgistic Regression(     900/10000): loss= 3915.06015278496\n",
      "Losgistic Regression(    1000/10000): loss= 3892.11899832645\n",
      "Losgistic Regression(    1100/10000): loss= 3871.92493247575\n",
      "Losgistic Regression(    1200/10000): loss= 3854.38327507597\n",
      "Losgistic Regression(    1300/10000): loss= 3839.65634917048\n",
      "Losgistic Regression(    1400/10000): loss= 3828.6465951455\n",
      "Losgistic Regression(    1500/10000): loss= 3818.40607529825\n",
      "Losgistic Regression(    1600/10000): loss= 3809.82459336349\n",
      "Losgistic Regression(    1700/10000): loss= 3800.08100900925\n",
      "Losgistic Regression(    1800/10000): loss= 3791.14380437002\n",
      "Losgistic Regression(    1900/10000): loss= 3783.22601343862\n",
      "Losgistic Regression(    2000/10000): loss= 3776.53335321701\n",
      "Losgistic Regression(    2100/10000): loss= 3770.37372158257\n",
      "Losgistic Regression(    2200/10000): loss= 3764.08025700599\n",
      "Losgistic Regression(    2300/10000): loss= 3758.37714184235\n",
      "Losgistic Regression(    2400/10000): loss= 3752.49666197449\n",
      "Losgistic Regression(    2500/10000): loss= 3745.44752266746\n",
      "Losgistic Regression(    2600/10000): loss= 3738.19475949608\n",
      "Losgistic Regression(    2700/10000): loss= 3731.09171682208\n",
      "Losgistic Regression(    2800/10000): loss= 3724.55890036097\n",
      "Losgistic Regression(    2900/10000): loss= 3717.83859909727\n",
      "Losgistic Regression(    3000/10000): loss= 3711.07977256468\n",
      "Losgistic Regression(    3100/10000): loss= 3704.17381532208\n",
      "Losgistic Regression(    3200/10000): loss= 3696.53884945684\n",
      "Losgistic Regression(    3300/10000): loss= 3688.74458395711\n",
      "Losgistic Regression(    3400/10000): loss= 3680.8520680725\n",
      "Losgistic Regression(    3500/10000): loss= 3673.65865530997\n",
      "Losgistic Regression(    3600/10000): loss= 3666.56874157313\n",
      "Losgistic Regression(    3700/10000): loss= 3660.17837818988\n",
      "Losgistic Regression(    3800/10000): loss= 3654.34285280248\n",
      "Losgistic Regression(    3900/10000): loss= 3649.31661452835\n",
      "Losgistic Regression(    4000/10000): loss= 3644.78904381323\n",
      "Losgistic Regression(    4100/10000): loss= 3640.61148543595\n",
      "Losgistic Regression(    4200/10000): loss= 3636.68844352054\n",
      "Losgistic Regression(    4300/10000): loss= 3632.9260001811\n",
      "Losgistic Regression(    4400/10000): loss= 3629.26000487741\n",
      "Losgistic Regression(    4500/10000): loss= 3625.67350406699\n",
      "Losgistic Regression(    4600/10000): loss= 3622.27166356721\n",
      "Losgistic Regression(    4700/10000): loss= 3618.87769108789\n",
      "Losgistic Regression(    4800/10000): loss= 3615.602728907\n",
      "Losgistic Regression(    4900/10000): loss= 3612.48419054154\n",
      "Losgistic Regression(    5000/10000): loss= 3609.41999356521\n",
      "Losgistic Regression(    5100/10000): loss= 3606.1788120342\n",
      "Losgistic Regression(    5200/10000): loss= 3603.07041150912\n",
      "Losgistic Regression(    5300/10000): loss= 3599.91994166588\n",
      "Losgistic Regression(    5400/10000): loss= 3596.90909642467\n",
      "Losgistic Regression(    5500/10000): loss= 3594.05604364013\n",
      "Losgistic Regression(    5600/10000): loss= 3591.38427521852\n",
      "Losgistic Regression(    5700/10000): loss= 3588.8495151123\n",
      "Losgistic Regression(    5800/10000): loss= 3586.3619770439\n",
      "Losgistic Regression(    5900/10000): loss= 3583.93989342366\n",
      "Losgistic Regression(    6000/10000): loss= 3581.86945831954\n",
      "Losgistic Regression(    6100/10000): loss= 3580.06381599105\n",
      "Losgistic Regression(    6200/10000): loss= 3578.48391965109\n",
      "Losgistic Regression(    6300/10000): loss= 3577.12163457026\n",
      "Losgistic Regression(    6400/10000): loss= 3575.98073273547\n",
      "Losgistic Regression(    6500/10000): loss= 3574.91386583542\n",
      "Losgistic Regression(    6600/10000): loss= 3574.03918886232\n",
      "Losgistic Regression(    6700/10000): loss= 3573.36479618753\n",
      "Losgistic Regression(    6800/10000): loss= 3572.72635358321\n",
      "Losgistic Regression(    6900/10000): loss= 3572.17269912993\n",
      "Losgistic Regression(    7000/10000): loss= 3571.77044663277\n",
      "Losgistic Regression(    7100/10000): loss= 3571.43925763086\n",
      "Losgistic Regression(    7200/10000): loss= 3571.15687129347\n",
      "Losgistic Regression(    7300/10000): loss= 3570.90197255033\n",
      "Losgistic Regression(    7400/10000): loss= 3570.71239194332\n",
      "Losgistic Regression(    7500/10000): loss= 3570.53087605796\n",
      "Losgistic Regression(    7600/10000): loss= 3570.19599835232\n",
      "Losgistic Regression(    7700/10000): loss= 3569.83137169983\n",
      "Losgistic Regression(    7800/10000): loss= 3569.41948588794\n",
      "Losgistic Regression(    7900/10000): loss= 3568.9961863199\n",
      "Losgistic Regression(    8000/10000): loss= 3568.59390190856\n",
      "Losgistic Regression(    8100/10000): loss= 3568.13872759246\n",
      "Losgistic Regression(    8200/10000): loss= 3567.62932269274\n",
      "Losgistic Regression(    8300/10000): loss= 3567.00631707579\n",
      "Losgistic Regression(    8400/10000): loss= 3566.36689889153\n",
      "Losgistic Regression(    8500/10000): loss= 3565.61030829619\n",
      "Losgistic Regression(    8600/10000): loss= 3564.69333503553\n",
      "Losgistic Regression(    8700/10000): loss= 3563.76429902412\n",
      "Losgistic Regression(    8800/10000): loss= 3562.82099361418\n",
      "Losgistic Regression(    8900/10000): loss= 3561.86463236118\n",
      "Losgistic Regression(    9000/10000): loss= 3560.91330522943\n",
      "Losgistic Regression(    9100/10000): loss= 3559.94464867408\n",
      "Losgistic Regression(    9200/10000): loss= 3558.9414918975\n",
      "Losgistic Regression(    9300/10000): loss= 3557.82933423656\n",
      "Losgistic Regression(    9400/10000): loss= 3556.62997431957\n",
      "Losgistic Regression(    9500/10000): loss= 3555.41835399786\n",
      "Losgistic Regression(    9600/10000): loss= 3554.13592987804\n",
      "Losgistic Regression(    9700/10000): loss= 3552.87231304727\n",
      "Losgistic Regression(    9800/10000): loss= 3551.62920278574\n",
      "Losgistic Regression(    9900/10000): loss= 3550.42858448201\n",
      "Time for  4th cross validation = 323.383s\n",
      "Training Accuracy         = 0.8355\n",
      "Cross Validation Accuracy = 0.816112\n",
      "*************** ([0.84050000000000002, 0.8417, 0.84689999999999999, 0.84509999999999996, 0.83550000000000002], [0.81368399999999996, 0.81589999999999996, 0.81435599999999997, 0.81452400000000003, 0.81611199999999995])\n",
      "Losgistic Regression(       0/10000): loss= 6821.12390701744\n",
      "Losgistic Regression(     100/10000): loss= 4539.40434818244\n",
      "Losgistic Regression(     200/10000): loss= 4261.39437880599\n",
      "Losgistic Regression(     300/10000): loss= 4122.9101373504\n",
      "Losgistic Regression(     400/10000): loss= 4033.47735794233\n",
      "Losgistic Regression(     500/10000): loss= 3968.50483336188\n",
      "Losgistic Regression(     600/10000): loss= 3920.62473052878\n",
      "Losgistic Regression(     700/10000): loss= 3884.61839020908\n",
      "Losgistic Regression(     800/10000): loss= 3855.56201623562\n",
      "Losgistic Regression(     900/10000): loss= 3831.78222566842\n",
      "Losgistic Regression(    1000/10000): loss= 3811.54811075492\n",
      "Losgistic Regression(    1100/10000): loss= 3794.01403644776\n",
      "Losgistic Regression(    1200/10000): loss= 3778.5196843037\n",
      "Losgistic Regression(    1300/10000): loss= 3765.07820916068\n",
      "Losgistic Regression(    1400/10000): loss= 3753.66957267993\n",
      "Losgistic Regression(    1500/10000): loss= 3744.85614696765\n",
      "Losgistic Regression(    1600/10000): loss= 3737.13254725842\n",
      "Losgistic Regression(    1700/10000): loss= 3729.32163738255\n",
      "Losgistic Regression(    1800/10000): loss= 3723.51476287897\n",
      "Losgistic Regression(    1900/10000): loss= 3717.73425750115\n",
      "Losgistic Regression(    2000/10000): loss= 3713.75815775948\n",
      "Losgistic Regression(    2100/10000): loss= 3709.32376280191\n",
      "Losgistic Regression(    2200/10000): loss= 3705.05061218333\n",
      "Losgistic Regression(    2300/10000): loss= 3700.69641003877\n",
      "Losgistic Regression(    2400/10000): loss= 3696.48691059737\n",
      "Losgistic Regression(    2500/10000): loss= 3693.22086762785\n",
      "Losgistic Regression(    2600/10000): loss= 3689.97106393283\n",
      "Losgistic Regression(    2700/10000): loss= 3687.24844103026\n",
      "Losgistic Regression(    2800/10000): loss= 3684.75900944712\n",
      "Losgistic Regression(    2900/10000): loss= 3682.47515976195\n",
      "Losgistic Regression(    3000/10000): loss= 3680.51128740333\n",
      "Losgistic Regression(    3100/10000): loss= 3678.94846053441\n",
      "Losgistic Regression(    3200/10000): loss= 3677.3652156598\n",
      "Losgistic Regression(    3300/10000): loss= 3675.74426670422\n",
      "Losgistic Regression(    3400/10000): loss= 3674.10572173903\n",
      "Losgistic Regression(    3500/10000): loss= 3672.75657906036\n",
      "Losgistic Regression(    3600/10000): loss= 3671.49844123958\n",
      "Losgistic Regression(    3700/10000): loss= 3670.42746999667\n",
      "Losgistic Regression(    3800/10000): loss= 3669.28878097342\n",
      "Losgistic Regression(    3900/10000): loss= 3668.2954384576\n",
      "Losgistic Regression(    4000/10000): loss= 3667.16128104936\n",
      "Losgistic Regression(    4100/10000): loss= 3666.1750815125\n",
      "Losgistic Regression(    4200/10000): loss= 3665.25772557602\n",
      "Losgistic Regression(    4300/10000): loss= 3664.41345625772\n",
      "Losgistic Regression(    4400/10000): loss= 3663.63818477553\n",
      "Losgistic Regression(    4500/10000): loss= 3663.03929112314\n",
      "Losgistic Regression(    4600/10000): loss= 3662.60069479442\n",
      "Losgistic Regression(    4700/10000): loss= 3662.20644633691\n",
      "Losgistic Regression(    4800/10000): loss= 3661.79174069188\n",
      "Losgistic Regression(    4900/10000): loss= 3661.44546540057\n",
      "Losgistic Regression(    5000/10000): loss= 3661.11634743278\n",
      "Losgistic Regression(    5100/10000): loss= 3660.84872008028\n",
      "Losgistic Regression(    5200/10000): loss= 3660.624157939\n",
      "Losgistic Regression(    5300/10000): loss= 3660.29867431565\n",
      "Losgistic Regression(    5400/10000): loss= 3659.94680856064\n",
      "Losgistic Regression(    5500/10000): loss= 3659.60455197742\n",
      "Losgistic Regression(    5600/10000): loss= 3659.29418449892\n",
      "Losgistic Regression(    5700/10000): loss= 3659.00908682148\n",
      "Losgistic Regression(    5800/10000): loss= 3658.83164550446\n",
      "Losgistic Regression(    5900/10000): loss= 3658.74527427713\n",
      "Losgistic Regression(    6000/10000): loss= 3658.74512738899\n",
      "Totoal number of iterations =  6000\n",
      "Loss                        =  3658.74512739\n",
      "Time for  0th cross validation = 192.729s\n",
      "Training Accuracy         = 0.8395\n",
      "Cross Validation Accuracy = 0.813976\n",
      "Losgistic Regression(       0/10000): loss= 6827.57789539823\n",
      "Losgistic Regression(     100/10000): loss= 4489.03067516677\n",
      "Losgistic Regression(     200/10000): loss= 4215.56743716573\n",
      "Losgistic Regression(     300/10000): loss= 4072.96147311587\n",
      "Losgistic Regression(     400/10000): loss= 3977.25105360246\n",
      "Losgistic Regression(     500/10000): loss= 3908.7029109439\n",
      "Losgistic Regression(     600/10000): loss= 3858.97872835935\n",
      "Losgistic Regression(     700/10000): loss= 3821.4154885278\n",
      "Losgistic Regression(     800/10000): loss= 3791.86333021649\n",
      "Losgistic Regression(     900/10000): loss= 3768.40016039389\n",
      "Losgistic Regression(    1000/10000): loss= 3750.0554786232\n",
      "Losgistic Regression(    1100/10000): loss= 3735.65783335196\n",
      "Losgistic Regression(    1200/10000): loss= 3724.08945807058\n",
      "Losgistic Regression(    1300/10000): loss= 3714.42772000454\n",
      "Losgistic Regression(    1400/10000): loss= 3706.06242947876\n",
      "Losgistic Regression(    1500/10000): loss= 3698.97456467067\n",
      "Losgistic Regression(    1600/10000): loss= 3692.59872105043\n",
      "Losgistic Regression(    1700/10000): loss= 3686.72691794822\n",
      "Losgistic Regression(    1800/10000): loss= 3681.30394071065\n",
      "Losgistic Regression(    1900/10000): loss= 3676.3118604022\n",
      "Losgistic Regression(    2000/10000): loss= 3671.7842252384\n",
      "Losgistic Regression(    2100/10000): loss= 3667.59575285673\n",
      "Losgistic Regression(    2200/10000): loss= 3663.75487475357\n",
      "Losgistic Regression(    2300/10000): loss= 3660.25579805878\n",
      "Losgistic Regression(    2400/10000): loss= 3657.09236581591\n",
      "Losgistic Regression(    2500/10000): loss= 3654.17485389577\n",
      "Losgistic Regression(    2600/10000): loss= 3651.46668001697\n",
      "Losgistic Regression(    2700/10000): loss= 3648.94231258779\n",
      "Losgistic Regression(    2800/10000): loss= 3646.48420087201\n",
      "Losgistic Regression(    2900/10000): loss= 3644.0044591071\n",
      "Losgistic Regression(    3000/10000): loss= 3641.6766900497\n",
      "Losgistic Regression(    3100/10000): loss= 3639.44018650985\n",
      "Losgistic Regression(    3200/10000): loss= 3637.22561351303\n",
      "Losgistic Regression(    3300/10000): loss= 3633.3165601776\n",
      "Losgistic Regression(    3400/10000): loss= 3629.09462481208\n",
      "Losgistic Regression(    3500/10000): loss= 3624.91954490989\n",
      "Losgistic Regression(    3600/10000): loss= 3620.94364517648\n",
      "Losgistic Regression(    3700/10000): loss= 3617.10516780363\n",
      "Losgistic Regression(    3800/10000): loss= 3613.48218207101\n",
      "Losgistic Regression(    3900/10000): loss= 3610.02626975693\n",
      "Losgistic Regression(    4000/10000): loss= 3606.7698214593\n",
      "Losgistic Regression(    4100/10000): loss= 3603.59897460089\n",
      "Losgistic Regression(    4200/10000): loss= 3600.47389884927\n",
      "Losgistic Regression(    4300/10000): loss= 3597.32134848417\n",
      "Losgistic Regression(    4400/10000): loss= 3594.19147349222\n",
      "Losgistic Regression(    4500/10000): loss= 3591.13021982451\n",
      "Losgistic Regression(    4600/10000): loss= 3588.17633103829\n",
      "Losgistic Regression(    4700/10000): loss= 3585.33657922843\n",
      "Losgistic Regression(    4800/10000): loss= 3582.60636121233\n",
      "Losgistic Regression(    4900/10000): loss= 3580.03798885976\n",
      "Losgistic Regression(    5000/10000): loss= 3577.54395971521\n",
      "Losgistic Regression(    5100/10000): loss= 3575.12734908131\n",
      "Losgistic Regression(    5200/10000): loss= 3572.74192067228\n",
      "Losgistic Regression(    5300/10000): loss= 3570.47321722582\n",
      "Losgistic Regression(    5400/10000): loss= 3568.33235930755\n",
      "Losgistic Regression(    5500/10000): loss= 3566.36755345559\n",
      "Losgistic Regression(    5600/10000): loss= 3564.5842586748\n",
      "Losgistic Regression(    5700/10000): loss= 3563.08587731192\n",
      "Losgistic Regression(    5800/10000): loss= 3561.79766065684\n",
      "Losgistic Regression(    5900/10000): loss= 3560.6946498662\n",
      "Losgistic Regression(    6000/10000): loss= 3559.70910796327\n",
      "Losgistic Regression(    6100/10000): loss= 3558.78443796495\n",
      "Losgistic Regression(    6200/10000): loss= 3557.82991081564\n",
      "Losgistic Regression(    6300/10000): loss= 3556.78847377081\n",
      "Losgistic Regression(    6400/10000): loss= 3556.18765196814\n",
      "Losgistic Regression(    6500/10000): loss= 3555.74951458511\n",
      "Losgistic Regression(    6600/10000): loss= 3555.46339566589\n",
      "Losgistic Regression(    6700/10000): loss= 3555.33917118289\n",
      "Losgistic Regression(    6800/10000): loss= 3555.34426427375\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  3555.34426427\n",
      "Time for  1th cross validation = 216.197s\n",
      "Training Accuracy         = 0.8416\n",
      "Cross Validation Accuracy = 0.816628\n",
      "Losgistic Regression(       0/10000): loss= 6825.31471982272\n",
      "Losgistic Regression(     100/10000): loss= 4455.350954496\n",
      "Losgistic Regression(     200/10000): loss= 4198.18983185897\n",
      "Losgistic Regression(     300/10000): loss= 4060.969783813\n",
      "Losgistic Regression(     400/10000): loss= 3967.17453464469\n",
      "Losgistic Regression(     500/10000): loss= 3898.71624247546\n",
      "Losgistic Regression(     600/10000): loss= 3844.25395885569\n",
      "Losgistic Regression(     700/10000): loss= 3800.65152842151\n",
      "Losgistic Regression(     800/10000): loss= 3766.27190419612\n",
      "Losgistic Regression(     900/10000): loss= 3737.87337849323\n",
      "Losgistic Regression(    1000/10000): loss= 3718.36243369096\n",
      "Losgistic Regression(    1100/10000): loss= 3703.88348389155\n",
      "Losgistic Regression(    1200/10000): loss= 3693.17448923374\n",
      "Losgistic Regression(    1300/10000): loss= 3685.63195018787\n",
      "Losgistic Regression(    1400/10000): loss= 3680.28861986441\n",
      "Losgistic Regression(    1500/10000): loss= 3676.38899482963\n",
      "Losgistic Regression(    1600/10000): loss= 3673.79107968759\n",
      "Losgistic Regression(    1700/10000): loss= 3672.27564434049\n",
      "Losgistic Regression(    1800/10000): loss= 3670.09879593479\n",
      "Losgistic Regression(    1900/10000): loss= 3668.18516438016\n",
      "Losgistic Regression(    2000/10000): loss= 3666.35152243903\n",
      "Losgistic Regression(    2100/10000): loss= 3665.01245530389\n",
      "Losgistic Regression(    2200/10000): loss= 3662.71790564344\n",
      "Losgistic Regression(    2300/10000): loss= 3657.97939042699\n",
      "Losgistic Regression(    2400/10000): loss= 3653.70267536304\n",
      "Losgistic Regression(    2500/10000): loss= 3649.67788016759\n",
      "Losgistic Regression(    2600/10000): loss= 3645.72853307407\n",
      "Losgistic Regression(    2700/10000): loss= 3641.79753880098\n",
      "Losgistic Regression(    2800/10000): loss= 3638.17240316526\n",
      "Losgistic Regression(    2900/10000): loss= 3634.77311084913\n",
      "Losgistic Regression(    3000/10000): loss= 3631.52480229812\n",
      "Losgistic Regression(    3100/10000): loss= 3628.38077678281\n",
      "Losgistic Regression(    3200/10000): loss= 3625.53217375787\n",
      "Losgistic Regression(    3300/10000): loss= 3622.95700583485\n",
      "Losgistic Regression(    3400/10000): loss= 3620.57636427839\n",
      "Losgistic Regression(    3500/10000): loss= 3618.45582606295\n",
      "Losgistic Regression(    3600/10000): loss= 3616.54913255838\n",
      "Losgistic Regression(    3700/10000): loss= 3614.81056607598\n",
      "Losgistic Regression(    3800/10000): loss= 3613.00151027836\n",
      "Losgistic Regression(    3900/10000): loss= 3611.22997359583\n",
      "Losgistic Regression(    4000/10000): loss= 3609.49299613324\n",
      "Losgistic Regression(    4100/10000): loss= 3607.96643859131\n",
      "Losgistic Regression(    4200/10000): loss= 3606.56535952207\n",
      "Losgistic Regression(    4300/10000): loss= 3605.28818252537\n",
      "Losgistic Regression(    4400/10000): loss= 3602.69893026809\n",
      "Losgistic Regression(    4500/10000): loss= 3600.29012327237\n",
      "Losgistic Regression(    4600/10000): loss= 3598.08639706195\n",
      "Losgistic Regression(    4700/10000): loss= 3596.12237264552\n",
      "Losgistic Regression(    4800/10000): loss= 3594.47997726066\n",
      "Losgistic Regression(    4900/10000): loss= 3593.03265837266\n",
      "Losgistic Regression(    5000/10000): loss= 3591.78198236653\n",
      "Losgistic Regression(    5100/10000): loss= 3590.55478028253\n",
      "Losgistic Regression(    5200/10000): loss= 3589.37665242794\n",
      "Losgistic Regression(    5300/10000): loss= 3588.28688680559\n",
      "Losgistic Regression(    5400/10000): loss= 3587.23755904622\n",
      "Losgistic Regression(    5500/10000): loss= 3586.18207848653\n",
      "Losgistic Regression(    5600/10000): loss= 3585.21619772312\n",
      "Losgistic Regression(    5700/10000): loss= 3584.29577361111\n",
      "Losgistic Regression(    5800/10000): loss= 3583.46004095508\n",
      "Losgistic Regression(    5900/10000): loss= 3582.63394420276\n",
      "Losgistic Regression(    6000/10000): loss= 3581.80669182313\n",
      "Losgistic Regression(    6100/10000): loss= 3580.925468226\n",
      "Losgistic Regression(    6200/10000): loss= 3580.06732678284\n",
      "Losgistic Regression(    6300/10000): loss= 3579.25218299725\n",
      "Losgistic Regression(    6400/10000): loss= 3578.50951276638\n",
      "Losgistic Regression(    6500/10000): loss= 3577.86941824712\n",
      "Losgistic Regression(    6600/10000): loss= 3577.26217741513\n",
      "Losgistic Regression(    6700/10000): loss= 3576.6138967605\n",
      "Losgistic Regression(    6800/10000): loss= 3575.99214795494\n",
      "Losgistic Regression(    6900/10000): loss= 3575.50473014118\n",
      "Losgistic Regression(    7000/10000): loss= 3575.12701785561\n",
      "Losgistic Regression(    7100/10000): loss= 3574.76880620824\n",
      "Losgistic Regression(    7200/10000): loss= 3574.44874644761\n",
      "Losgistic Regression(    7300/10000): loss= 3574.18433647415\n",
      "Losgistic Regression(    7400/10000): loss= 3573.93233500349\n",
      "Losgistic Regression(    7500/10000): loss= 3573.69064711135\n",
      "Losgistic Regression(    7600/10000): loss= 3573.4365941539\n",
      "Losgistic Regression(    7700/10000): loss= 3573.17009091994\n",
      "Losgistic Regression(    7800/10000): loss= 3572.96344236034\n",
      "Losgistic Regression(    7900/10000): loss= 3572.73934709054\n",
      "Losgistic Regression(    8000/10000): loss= 3572.50232903227\n",
      "Losgistic Regression(    8100/10000): loss= 3572.34319983233\n",
      "Losgistic Regression(    8200/10000): loss= 3572.22215181955\n",
      "Losgistic Regression(    8300/10000): loss= 3572.19525201211\n",
      "Totoal number of iterations =  8300\n",
      "Loss                        =  3572.19525201\n",
      "Time for  2th cross validation = 262.224s\n",
      "Training Accuracy         = 0.8469\n",
      "Cross Validation Accuracy = 0.81508\n",
      "Losgistic Regression(       0/10000): loss= 6822.89891149946\n",
      "Losgistic Regression(     100/10000): loss= 4538.44070029444\n",
      "Losgistic Regression(     200/10000): loss= 4280.68661486095\n",
      "Losgistic Regression(     300/10000): loss= 4141.02931631007\n",
      "Losgistic Regression(     400/10000): loss= 4053.59009875736\n",
      "Losgistic Regression(     500/10000): loss= 3989.42668671112\n",
      "Losgistic Regression(     600/10000): loss= 3936.56628571643\n",
      "Losgistic Regression(     700/10000): loss= 3892.84422640086\n",
      "Losgistic Regression(     800/10000): loss= 3863.64368011025\n",
      "Losgistic Regression(     900/10000): loss= 3840.12839896731\n",
      "Losgistic Regression(    1000/10000): loss= 3823.14754706151\n",
      "Losgistic Regression(    1100/10000): loss= 3809.18172531068\n",
      "Losgistic Regression(    1200/10000): loss= 3798.1620169894\n",
      "Losgistic Regression(    1300/10000): loss= 3787.614458764\n",
      "Losgistic Regression(    1400/10000): loss= 3776.26130304444\n",
      "Losgistic Regression(    1500/10000): loss= 3765.71191031071\n",
      "Losgistic Regression(    1600/10000): loss= 3756.61472863941\n",
      "Losgistic Regression(    1700/10000): loss= 3748.0929838081\n",
      "Losgistic Regression(    1800/10000): loss= 3739.87983198303\n",
      "Losgistic Regression(    1900/10000): loss= 3732.49579939693\n",
      "Losgistic Regression(    2000/10000): loss= 3725.71551509489\n",
      "Losgistic Regression(    2100/10000): loss= 3719.1232890698\n",
      "Losgistic Regression(    2200/10000): loss= 3712.44261675411\n",
      "Losgistic Regression(    2300/10000): loss= 3705.76736880674\n",
      "Losgistic Regression(    2400/10000): loss= 3699.43176160714\n",
      "Losgistic Regression(    2500/10000): loss= 3693.50660865585\n",
      "Losgistic Regression(    2600/10000): loss= 3687.78728772755\n",
      "Losgistic Regression(    2700/10000): loss= 3681.99474898398\n",
      "Losgistic Regression(    2800/10000): loss= 3676.30197437472\n",
      "Losgistic Regression(    2900/10000): loss= 3670.8435166397\n",
      "Losgistic Regression(    3000/10000): loss= 3665.72923406277\n",
      "Losgistic Regression(    3100/10000): loss= 3660.94633098396\n",
      "Losgistic Regression(    3200/10000): loss= 3656.45675266272\n",
      "Losgistic Regression(    3300/10000): loss= 3652.29368409763\n",
      "Losgistic Regression(    3400/10000): loss= 3648.47628125071\n",
      "Losgistic Regression(    3500/10000): loss= 3644.88175460777\n",
      "Losgistic Regression(    3600/10000): loss= 3641.45973020425\n",
      "Losgistic Regression(    3700/10000): loss= 3638.24734312599\n",
      "Losgistic Regression(    3800/10000): loss= 3635.34479816964\n",
      "Losgistic Regression(    3900/10000): loss= 3632.68173918766\n",
      "Losgistic Regression(    4000/10000): loss= 3630.20559767706\n",
      "Losgistic Regression(    4100/10000): loss= 3627.79590646506\n",
      "Losgistic Regression(    4200/10000): loss= 3625.40291911727\n",
      "Losgistic Regression(    4300/10000): loss= 3623.06470500056\n",
      "Losgistic Regression(    4400/10000): loss= 3620.82285603699\n",
      "Losgistic Regression(    4500/10000): loss= 3618.71807754792\n",
      "Losgistic Regression(    4600/10000): loss= 3616.76544826681\n",
      "Losgistic Regression(    4700/10000): loss= 3614.87232009002\n",
      "Losgistic Regression(    4800/10000): loss= 3613.00649355076\n",
      "Losgistic Regression(    4900/10000): loss= 3611.22007281689\n",
      "Losgistic Regression(    5000/10000): loss= 3609.50518722705\n",
      "Losgistic Regression(    5100/10000): loss= 3607.88490207694\n",
      "Losgistic Regression(    5200/10000): loss= 3606.35176557861\n",
      "Losgistic Regression(    5300/10000): loss= 3604.88105235854\n",
      "Losgistic Regression(    5400/10000): loss= 3603.44200537198\n",
      "Losgistic Regression(    5500/10000): loss= 3602.11270275191\n",
      "Losgistic Regression(    5600/10000): loss= 3600.88208601519\n",
      "Losgistic Regression(    5700/10000): loss= 3599.77453756353\n",
      "Losgistic Regression(    5800/10000): loss= 3598.8079857554\n",
      "Losgistic Regression(    5900/10000): loss= 3597.90107365136\n",
      "Losgistic Regression(    6000/10000): loss= 3597.09966970016\n",
      "Losgistic Regression(    6100/10000): loss= 3596.3735565994\n",
      "Losgistic Regression(    6200/10000): loss= 3595.63147417315\n",
      "Losgistic Regression(    6300/10000): loss= 3594.91835231296\n",
      "Losgistic Regression(    6400/10000): loss= 3594.28585067934\n",
      "Losgistic Regression(    6500/10000): loss= 3593.67942409763\n",
      "Losgistic Regression(    6600/10000): loss= 3593.09116905395\n",
      "Losgistic Regression(    6700/10000): loss= 3592.46392683777\n",
      "Losgistic Regression(    6800/10000): loss= 3591.80633838311\n",
      "Losgistic Regression(    6900/10000): loss= 3591.18448667649\n",
      "Losgistic Regression(    7000/10000): loss= 3590.58327275931\n",
      "Losgistic Regression(    7100/10000): loss= 3590.01936142162\n",
      "Losgistic Regression(    7200/10000): loss= 3589.49146990494\n",
      "Losgistic Regression(    7300/10000): loss= 3588.9657824807\n",
      "Losgistic Regression(    7400/10000): loss= 3588.42619573839\n",
      "Losgistic Regression(    7500/10000): loss= 3587.91277403369\n",
      "Losgistic Regression(    7600/10000): loss= 3587.38452960643\n",
      "Losgistic Regression(    7700/10000): loss= 3586.83487631288\n",
      "Losgistic Regression(    7800/10000): loss= 3586.24896677335\n",
      "Losgistic Regression(    7900/10000): loss= 3585.71517821332\n",
      "Losgistic Regression(    8000/10000): loss= 3585.22834868615\n",
      "Losgistic Regression(    8100/10000): loss= 3584.81017424025\n",
      "Losgistic Regression(    8200/10000): loss= 3584.37991936657\n",
      "Losgistic Regression(    8300/10000): loss= 3583.92187006998\n",
      "Losgistic Regression(    8400/10000): loss= 3583.46316060042\n",
      "Losgistic Regression(    8500/10000): loss= 3583.03103400501\n",
      "Losgistic Regression(    8600/10000): loss= 3582.6415803567\n",
      "Losgistic Regression(    8700/10000): loss= 3582.31494461965\n",
      "Losgistic Regression(    8800/10000): loss= 3582.02819853592\n",
      "Losgistic Regression(    8900/10000): loss= 3581.69684064073\n",
      "Losgistic Regression(    9000/10000): loss= 3581.38315648675\n",
      "Losgistic Regression(    9100/10000): loss= 3581.05377758846\n",
      "Losgistic Regression(    9200/10000): loss= 3580.76477020924\n",
      "Losgistic Regression(    9300/10000): loss= 3580.45253624063\n",
      "Losgistic Regression(    9400/10000): loss= 3580.14165053674\n",
      "Losgistic Regression(    9500/10000): loss= 3579.86272850281\n",
      "Losgistic Regression(    9600/10000): loss= 3579.58818190091\n",
      "Losgistic Regression(    9700/10000): loss= 3579.32382158728\n",
      "Losgistic Regression(    9800/10000): loss= 3579.10554529884\n",
      "Losgistic Regression(    9900/10000): loss= 3578.86596775926\n",
      "Time for  3th cross validation = 316.125s\n",
      "Training Accuracy         = 0.8447\n",
      "Cross Validation Accuracy = 0.815308\n",
      "Losgistic Regression(       0/10000): loss= 6820.83920412068\n",
      "Losgistic Regression(     100/10000): loss= 4551.93702133672\n",
      "Losgistic Regression(     200/10000): loss= 4309.18320776818\n",
      "Losgistic Regression(     300/10000): loss= 4194.06089306936\n",
      "Losgistic Regression(     400/10000): loss= 4122.54482053096\n",
      "Losgistic Regression(     500/10000): loss= 4070.26015868395\n",
      "Losgistic Regression(     600/10000): loss= 4025.93610175644\n",
      "Losgistic Regression(     700/10000): loss= 3988.27991890188\n",
      "Losgistic Regression(     800/10000): loss= 3955.86396565954\n",
      "Losgistic Regression(     900/10000): loss= 3928.61476641128\n",
      "Losgistic Regression(    1000/10000): loss= 3906.62011818016\n",
      "Losgistic Regression(    1100/10000): loss= 3887.52458179105\n",
      "Losgistic Regression(    1200/10000): loss= 3870.73295482379\n",
      "Losgistic Regression(    1300/10000): loss= 3856.82669555067\n",
      "Losgistic Regression(    1400/10000): loss= 3846.70578564764\n",
      "Losgistic Regression(    1500/10000): loss= 3837.16568858799\n",
      "Losgistic Regression(    1600/10000): loss= 3829.29187112903\n",
      "Losgistic Regression(    1700/10000): loss= 3821.37909026902\n",
      "Losgistic Regression(    1800/10000): loss= 3814.50627182758\n",
      "Losgistic Regression(    1900/10000): loss= 3807.68228203902\n",
      "Losgistic Regression(    2000/10000): loss= 3801.75748893634\n",
      "Losgistic Regression(    2100/10000): loss= 3795.88382505914\n",
      "Losgistic Regression(    2200/10000): loss= 3789.23656567495\n",
      "Losgistic Regression(    2300/10000): loss= 3783.22569661688\n",
      "Losgistic Regression(    2400/10000): loss= 3777.47771191121\n",
      "Losgistic Regression(    2500/10000): loss= 3771.35530962177\n",
      "Losgistic Regression(    2600/10000): loss= 3765.09505493932\n",
      "Losgistic Regression(    2700/10000): loss= 3759.40690614458\n",
      "Losgistic Regression(    2800/10000): loss= 3753.82292543695\n",
      "Losgistic Regression(    2900/10000): loss= 3747.97776473957\n",
      "Losgistic Regression(    3000/10000): loss= 3742.20264377501\n",
      "Losgistic Regression(    3100/10000): loss= 3736.38830989611\n",
      "Losgistic Regression(    3200/10000): loss= 3730.38654095942\n",
      "Losgistic Regression(    3300/10000): loss= 3724.44629290486\n",
      "Losgistic Regression(    3400/10000): loss= 3718.46528501342\n",
      "Losgistic Regression(    3500/10000): loss= 3712.84447764721\n",
      "Losgistic Regression(    3600/10000): loss= 3707.50812990584\n",
      "Losgistic Regression(    3700/10000): loss= 3702.5446837738\n",
      "Losgistic Regression(    3800/10000): loss= 3698.15138681832\n",
      "Losgistic Regression(    3900/10000): loss= 3694.44449534334\n",
      "Losgistic Regression(    4000/10000): loss= 3691.02816214406\n",
      "Losgistic Regression(    4100/10000): loss= 3688.05993934114\n",
      "Losgistic Regression(    4200/10000): loss= 3685.14840998056\n",
      "Losgistic Regression(    4300/10000): loss= 3682.38586712128\n",
      "Losgistic Regression(    4400/10000): loss= 3679.59000073732\n",
      "Losgistic Regression(    4500/10000): loss= 3677.10011477217\n",
      "Losgistic Regression(    4600/10000): loss= 3674.60381039956\n",
      "Losgistic Regression(    4700/10000): loss= 3672.00223862866\n",
      "Losgistic Regression(    4800/10000): loss= 3669.42254582967\n",
      "Losgistic Regression(    4900/10000): loss= 3666.90797838906\n",
      "Losgistic Regression(    5000/10000): loss= 3664.31135028421\n",
      "Losgistic Regression(    5100/10000): loss= 3661.79058778048\n",
      "Losgistic Regression(    5200/10000): loss= 3659.05278283092\n",
      "Losgistic Regression(    5300/10000): loss= 3655.97957796117\n",
      "Losgistic Regression(    5400/10000): loss= 3652.67424918128\n",
      "Losgistic Regression(    5500/10000): loss= 3649.63140549899\n",
      "Losgistic Regression(    5600/10000): loss= 3646.85404729559\n",
      "Losgistic Regression(    5700/10000): loss= 3644.56533646791\n",
      "Losgistic Regression(    5800/10000): loss= 3642.72268540058\n",
      "Losgistic Regression(    5900/10000): loss= 3641.50559926859\n",
      "Losgistic Regression(    6000/10000): loss= 3641.02788181574\n",
      "Losgistic Regression(    6100/10000): loss= 3640.97727032541\n",
      "Losgistic Regression(    6200/10000): loss= 3640.98292727901\n",
      "Totoal number of iterations =  6200\n",
      "Loss                        =  3640.98292728\n",
      "Time for  4th cross validation = 196.907s\n",
      "Training Accuracy         = 0.8348\n",
      "Cross Validation Accuracy = 0.816352\n",
      "*************** ([0.83950000000000002, 0.84160000000000001, 0.84689999999999999, 0.84470000000000001, 0.83479999999999999], [0.81397600000000003, 0.81662800000000002, 0.81508000000000003, 0.81530800000000003, 0.81635199999999997])\n",
      "Losgistic Regression(       0/10000): loss= 6821.19601185404\n",
      "Losgistic Regression(     100/10000): loss= 4547.27767250635\n",
      "Losgistic Regression(     200/10000): loss= 4275.70784547716\n",
      "Losgistic Regression(     300/10000): loss= 4143.11554561968\n",
      "Losgistic Regression(     400/10000): loss= 4058.77990659877\n",
      "Losgistic Regression(     500/10000): loss= 3998.51833307041\n",
      "Losgistic Regression(     600/10000): loss= 3954.73693057267\n",
      "Losgistic Regression(     700/10000): loss= 3922.38684815212\n",
      "Losgistic Regression(     800/10000): loss= 3896.99378842093\n",
      "Losgistic Regression(     900/10000): loss= 3876.83274253617\n",
      "Losgistic Regression(    1000/10000): loss= 3860.058530871\n",
      "Losgistic Regression(    1100/10000): loss= 3845.36863657506\n",
      "Losgistic Regression(    1200/10000): loss= 3832.72148509227\n",
      "Losgistic Regression(    1300/10000): loss= 3821.80165116201\n",
      "Losgistic Regression(    1400/10000): loss= 3812.53437089857\n",
      "Losgistic Regression(    1500/10000): loss= 3805.28399785965\n",
      "Losgistic Regression(    1600/10000): loss= 3799.64879801657\n",
      "Losgistic Regression(    1700/10000): loss= 3794.14337022261\n",
      "Losgistic Regression(    1800/10000): loss= 3789.90322155263\n",
      "Losgistic Regression(    1900/10000): loss= 3785.51216050405\n",
      "Losgistic Regression(    2000/10000): loss= 3782.15859216737\n",
      "Losgistic Regression(    2100/10000): loss= 3778.80022673189\n",
      "Losgistic Regression(    2200/10000): loss= 3775.13897871308\n",
      "Losgistic Regression(    2300/10000): loss= 3771.8990013737\n",
      "Losgistic Regression(    2400/10000): loss= 3768.9178207216\n",
      "Losgistic Regression(    2500/10000): loss= 3767.2286482323\n",
      "Losgistic Regression(    2600/10000): loss= 3765.33089283631\n",
      "Losgistic Regression(    2700/10000): loss= 3763.70958174039\n",
      "Losgistic Regression(    2800/10000): loss= 3761.91040351399\n",
      "Losgistic Regression(    2900/10000): loss= 3760.1790257565\n",
      "Losgistic Regression(    3000/10000): loss= 3759.07948545236\n",
      "Losgistic Regression(    3100/10000): loss= 3758.19859865199\n",
      "Losgistic Regression(    3200/10000): loss= 3757.27268016391\n",
      "Losgistic Regression(    3300/10000): loss= 3756.9729801343\n",
      "Losgistic Regression(    3400/10000): loss= 3756.68963803937\n",
      "Losgistic Regression(    3500/10000): loss= 3756.12006100886\n",
      "Losgistic Regression(    3600/10000): loss= 3755.60577844019\n",
      "Losgistic Regression(    3700/10000): loss= 3754.84118540386\n",
      "Losgistic Regression(    3800/10000): loss= 3754.04055593161\n",
      "Losgistic Regression(    3900/10000): loss= 3753.56306427069\n",
      "Losgistic Regression(    4000/10000): loss= 3753.16467217924\n",
      "Losgistic Regression(    4100/10000): loss= 3752.87172149847\n",
      "Losgistic Regression(    4200/10000): loss= 3752.70255112611\n",
      "Losgistic Regression(    4300/10000): loss= 3752.52521833311\n",
      "Losgistic Regression(    4400/10000): loss= 3752.10823704896\n",
      "Losgistic Regression(    4500/10000): loss= 3751.84672484568\n",
      "Losgistic Regression(    4600/10000): loss= 3751.6513878687\n",
      "Losgistic Regression(    4700/10000): loss= 3751.46945465587\n",
      "Losgistic Regression(    4800/10000): loss= 3751.47115384526\n",
      "Totoal number of iterations =  4800\n",
      "Loss                        =  3751.47115385\n",
      "Time for  0th cross validation = 152.269s\n",
      "Training Accuracy         =  0.838\n",
      "Cross Validation Accuracy = 0.814188\n",
      "Losgistic Regression(       0/10000): loss= 6827.64574552893\n",
      "Losgistic Regression(     100/10000): loss= 4496.74717744117\n",
      "Losgistic Regression(     200/10000): loss= 4229.43024447799\n",
      "Losgistic Regression(     300/10000): loss= 4092.43613725544\n",
      "Losgistic Regression(     400/10000): loss= 4001.90257944396\n",
      "Losgistic Regression(     500/10000): loss= 3938.35052846538\n",
      "Losgistic Regression(     600/10000): loss= 3892.82524177014\n",
      "Losgistic Regression(     700/10000): loss= 3858.8640523506\n",
      "Losgistic Regression(     800/10000): loss= 3832.8653872002\n",
      "Losgistic Regression(     900/10000): loss= 3812.89839300294\n",
      "Losgistic Regression(    1000/10000): loss= 3797.8861446984\n",
      "Losgistic Regression(    1100/10000): loss= 3786.48668711512\n",
      "Losgistic Regression(    1200/10000): loss= 3777.56954216463\n",
      "Losgistic Regression(    1300/10000): loss= 3770.26514143495\n",
      "Losgistic Regression(    1400/10000): loss= 3763.95753651214\n",
      "Losgistic Regression(    1500/10000): loss= 3758.51140469299\n",
      "Losgistic Regression(    1600/10000): loss= 3753.80684918732\n",
      "Losgistic Regression(    1700/10000): loss= 3749.58966956652\n",
      "Losgistic Regression(    1800/10000): loss= 3745.81902389617\n",
      "Losgistic Regression(    1900/10000): loss= 3742.37441185931\n",
      "Losgistic Regression(    2000/10000): loss= 3739.25824293513\n",
      "Losgistic Regression(    2100/10000): loss= 3736.42482576289\n",
      "Losgistic Regression(    2200/10000): loss= 3733.84425433274\n",
      "Losgistic Regression(    2300/10000): loss= 3731.54592157394\n",
      "Losgistic Regression(    2400/10000): loss= 3729.5413623109\n",
      "Losgistic Regression(    2500/10000): loss= 3727.65937711026\n",
      "Losgistic Regression(    2600/10000): loss= 3725.9834928281\n",
      "Losgistic Regression(    2700/10000): loss= 3724.4718498099\n",
      "Losgistic Regression(    2800/10000): loss= 3723.02146053503\n",
      "Losgistic Regression(    2900/10000): loss= 3721.635897142\n",
      "Losgistic Regression(    3000/10000): loss= 3720.31707459575\n",
      "Losgistic Regression(    3100/10000): loss= 3719.0548163392\n",
      "Losgistic Regression(    3200/10000): loss= 3717.57440769686\n",
      "Losgistic Regression(    3300/10000): loss= 3714.88500842554\n",
      "Losgistic Regression(    3400/10000): loss= 3712.11455728058\n",
      "Losgistic Regression(    3500/10000): loss= 3709.525124127\n",
      "Losgistic Regression(    3600/10000): loss= 3707.04346087901\n",
      "Losgistic Regression(    3700/10000): loss= 3704.66766338564\n",
      "Losgistic Regression(    3800/10000): loss= 3702.26235117427\n",
      "Losgistic Regression(    3900/10000): loss= 3700.01077310567\n",
      "Losgistic Regression(    4000/10000): loss= 3698.39388265564\n",
      "Losgistic Regression(    4100/10000): loss= 3697.4141974836\n",
      "Losgistic Regression(    4200/10000): loss= 3697.07289237904\n",
      "Losgistic Regression(    4300/10000): loss= 3697.07714586026\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  3697.07714586\n",
      "Time for  1th cross validation = 138.56s\n",
      "Training Accuracy         = 0.8386\n",
      "Cross Validation Accuracy = 0.817816\n",
      "Losgistic Regression(       0/10000): loss= 6825.38569611239\n",
      "Losgistic Regression(     100/10000): loss= 4463.30096730272\n",
      "Losgistic Regression(     200/10000): loss= 4212.20522478998\n",
      "Losgistic Regression(     300/10000): loss= 4080.45341419572\n",
      "Losgistic Regression(     400/10000): loss= 3991.83833588258\n",
      "Losgistic Regression(     500/10000): loss= 3928.30188685378\n",
      "Losgistic Regression(     600/10000): loss= 3878.45776056371\n",
      "Losgistic Regression(     700/10000): loss= 3839.31378507815\n",
      "Losgistic Regression(     800/10000): loss= 3809.1766834991\n",
      "Losgistic Regression(     900/10000): loss= 3784.70209422253\n",
      "Losgistic Regression(    1000/10000): loss= 3768.16720999086\n",
      "Losgistic Regression(    1100/10000): loss= 3756.50748233301\n",
      "Losgistic Regression(    1200/10000): loss= 3748.39721846564\n",
      "Losgistic Regression(    1300/10000): loss= 3742.72215628147\n",
      "Losgistic Regression(    1400/10000): loss= 3738.93415633896\n",
      "Losgistic Regression(    1500/10000): loss= 3736.90925103914\n",
      "Losgistic Regression(    1600/10000): loss= 3735.9822937897\n",
      "Losgistic Regression(    1700/10000): loss= 3735.91156958176\n",
      "Losgistic Regression(    1800/10000): loss= 3735.7210114798\n",
      "Losgistic Regression(    1900/10000): loss= 3735.38363658339\n",
      "Losgistic Regression(    2000/10000): loss= 3735.02647823128\n",
      "Losgistic Regression(    2100/10000): loss= 3734.67181031909\n",
      "Losgistic Regression(    2200/10000): loss= 3734.37855172299\n",
      "Losgistic Regression(    2300/10000): loss= 3733.94127933032\n",
      "Losgistic Regression(    2400/10000): loss= 3733.6063846825\n",
      "Losgistic Regression(    2500/10000): loss= 3733.34508823905\n",
      "Losgistic Regression(    2600/10000): loss= 3733.24956335122\n",
      "Losgistic Regression(    2700/10000): loss= 3733.22371318422\n",
      "Totoal number of iterations =  2700\n",
      "Loss                        =  3733.22371318\n",
      "Time for  2th cross validation = 88.8427s\n",
      "Training Accuracy         = 0.8415\n",
      "Cross Validation Accuracy = 0.814108\n",
      "Losgistic Regression(       0/10000): loss= 6822.96896225311\n",
      "Losgistic Regression(     100/10000): loss= 4546.12283109772\n",
      "Losgistic Regression(     200/10000): loss= 4294.18589566902\n",
      "Losgistic Regression(     300/10000): loss= 4160.22286663216\n",
      "Losgistic Regression(     400/10000): loss= 4077.45774065672\n",
      "Losgistic Regression(     500/10000): loss= 4017.86280550195\n",
      "Losgistic Regression(     600/10000): loss= 3969.48956401046\n",
      "Losgistic Regression(     700/10000): loss= 3929.54105695448\n",
      "Losgistic Regression(     800/10000): loss= 3903.48541432756\n",
      "Losgistic Regression(     900/10000): loss= 3883.8769146636\n",
      "Losgistic Regression(    1000/10000): loss= 3869.33665660877\n",
      "Losgistic Regression(    1100/10000): loss= 3858.04993692861\n",
      "Losgistic Regression(    1200/10000): loss= 3849.60840548319\n",
      "Losgistic Regression(    1300/10000): loss= 3842.608214811\n",
      "Losgistic Regression(    1400/10000): loss= 3834.34077846141\n",
      "Losgistic Regression(    1500/10000): loss= 3826.05746802808\n",
      "Losgistic Regression(    1600/10000): loss= 3818.98365906454\n",
      "Losgistic Regression(    1700/10000): loss= 3812.56327486369\n",
      "Losgistic Regression(    1800/10000): loss= 3806.39266700697\n",
      "Losgistic Regression(    1900/10000): loss= 3800.95531607713\n",
      "Losgistic Regression(    2000/10000): loss= 3795.89932181656\n",
      "Losgistic Regression(    2100/10000): loss= 3791.22927942886\n",
      "Losgistic Regression(    2200/10000): loss= 3786.71518077218\n",
      "Losgistic Regression(    2300/10000): loss= 3782.047438967\n",
      "Losgistic Regression(    2400/10000): loss= 3777.77845310858\n",
      "Losgistic Regression(    2500/10000): loss= 3773.66049650978\n",
      "Losgistic Regression(    2600/10000): loss= 3769.60422019757\n",
      "Losgistic Regression(    2700/10000): loss= 3765.58241087033\n",
      "Losgistic Regression(    2800/10000): loss= 3761.73451255747\n",
      "Losgistic Regression(    2900/10000): loss= 3758.21337598668\n",
      "Losgistic Regression(    3000/10000): loss= 3755.02123474238\n",
      "Losgistic Regression(    3100/10000): loss= 3752.03671227754\n",
      "Losgistic Regression(    3200/10000): loss= 3749.05279687312\n",
      "Losgistic Regression(    3300/10000): loss= 3746.22123021311\n",
      "Losgistic Regression(    3400/10000): loss= 3743.58240611896\n",
      "Losgistic Regression(    3500/10000): loss= 3741.04198573664\n",
      "Losgistic Regression(    3600/10000): loss= 3738.76020404834\n",
      "Losgistic Regression(    3700/10000): loss= 3736.69561029991\n",
      "Losgistic Regression(    3800/10000): loss= 3734.95670027256\n",
      "Losgistic Regression(    3900/10000): loss= 3733.41792036762\n",
      "Losgistic Regression(    4000/10000): loss= 3731.87917449673\n",
      "Losgistic Regression(    4100/10000): loss= 3730.45722122203\n",
      "Losgistic Regression(    4200/10000): loss= 3728.86133715315\n",
      "Losgistic Regression(    4300/10000): loss= 3727.31080467655\n",
      "Losgistic Regression(    4400/10000): loss= 3726.09092189378\n",
      "Losgistic Regression(    4500/10000): loss= 3724.94881543595\n",
      "Losgistic Regression(    4600/10000): loss= 3723.6808224514\n",
      "Losgistic Regression(    4700/10000): loss= 3722.29442497797\n",
      "Losgistic Regression(    4800/10000): loss= 3720.83361096541\n",
      "Losgistic Regression(    4900/10000): loss= 3719.37553723942\n",
      "Losgistic Regression(    5000/10000): loss= 3718.15874982647\n",
      "Losgistic Regression(    5100/10000): loss= 3717.07409482989\n",
      "Losgistic Regression(    5200/10000): loss= 3716.1545073818\n",
      "Losgistic Regression(    5300/10000): loss= 3715.28473330159\n",
      "Losgistic Regression(    5400/10000): loss= 3714.52443142141\n",
      "Losgistic Regression(    5500/10000): loss= 3713.64337347154\n",
      "Losgistic Regression(    5600/10000): loss= 3712.73288922214\n",
      "Losgistic Regression(    5700/10000): loss= 3711.77444410727\n",
      "Losgistic Regression(    5800/10000): loss= 3711.16628150655\n",
      "Losgistic Regression(    5900/10000): loss= 3710.81040985372\n",
      "Losgistic Regression(    6000/10000): loss= 3710.43043959604\n",
      "Losgistic Regression(    6100/10000): loss= 3709.96745404925\n",
      "Losgistic Regression(    6200/10000): loss= 3709.81351445771\n",
      "Losgistic Regression(    6300/10000): loss= 3709.55663827198\n",
      "Losgistic Regression(    6400/10000): loss= 3709.30006402134\n",
      "Losgistic Regression(    6500/10000): loss= 3709.13467011116\n",
      "Losgistic Regression(    6600/10000): loss= 3708.95891124053\n",
      "Losgistic Regression(    6700/10000): loss= 3708.94827518763\n",
      "Totoal number of iterations =  6700\n",
      "Loss                        =  3708.94827519\n",
      "Time for  3th cross validation = 219.63s\n",
      "Training Accuracy         = 0.8413\n",
      "Cross Validation Accuracy = 0.816776\n",
      "Losgistic Regression(       0/10000): loss= 6820.91125378439\n",
      "Losgistic Regression(     100/10000): loss= 4559.58614407347\n",
      "Losgistic Regression(     200/10000): loss= 4322.78036329876\n",
      "Losgistic Regression(     300/10000): loss= 4212.58827903687\n",
      "Losgistic Regression(     400/10000): loss= 4145.39541767585\n",
      "Losgistic Regression(     500/10000): loss= 4097.39360331468\n",
      "Losgistic Regression(     600/10000): loss= 4058.93413064483\n",
      "Losgistic Regression(     700/10000): loss= 4025.42509164483\n",
      "Losgistic Regression(     800/10000): loss= 3996.91322650454\n",
      "Losgistic Regression(     900/10000): loss= 3972.83543999672\n",
      "Losgistic Regression(    1000/10000): loss= 3953.45205528581\n",
      "Losgistic Regression(    1100/10000): loss= 3937.09038059216\n",
      "Losgistic Regression(    1200/10000): loss= 3923.59557950607\n",
      "Losgistic Regression(    1300/10000): loss= 3912.52969736445\n",
      "Losgistic Regression(    1400/10000): loss= 3904.00510259237\n",
      "Losgistic Regression(    1500/10000): loss= 3896.46860659959\n",
      "Losgistic Regression(    1600/10000): loss= 3890.26336489818\n",
      "Losgistic Regression(    1700/10000): loss= 3884.7246238823\n",
      "Losgistic Regression(    1800/10000): loss= 3879.88649867786\n",
      "Losgistic Regression(    1900/10000): loss= 3874.84676826114\n",
      "Losgistic Regression(    2000/10000): loss= 3870.96925911921\n",
      "Losgistic Regression(    2100/10000): loss= 3866.8076943115\n",
      "Losgistic Regression(    2200/10000): loss= 3862.12384419375\n",
      "Losgistic Regression(    2300/10000): loss= 3858.51169736109\n",
      "Losgistic Regression(    2400/10000): loss= 3855.24594486773\n",
      "Losgistic Regression(    2500/10000): loss= 3851.95436389553\n",
      "Losgistic Regression(    2600/10000): loss= 3848.43091156079\n",
      "Losgistic Regression(    2700/10000): loss= 3845.38642492046\n",
      "Losgistic Regression(    2800/10000): loss= 3842.84800514538\n",
      "Losgistic Regression(    2900/10000): loss= 3840.2690419078\n",
      "Losgistic Regression(    3000/10000): loss= 3837.68222387158\n",
      "Losgistic Regression(    3100/10000): loss= 3835.64898872552\n",
      "Losgistic Regression(    3200/10000): loss= 3833.35807971309\n",
      "Losgistic Regression(    3300/10000): loss= 3830.43645616306\n",
      "Losgistic Regression(    3400/10000): loss= 3827.37019078906\n",
      "Losgistic Regression(    3500/10000): loss= 3824.06568574358\n",
      "Losgistic Regression(    3600/10000): loss= 3820.30481843232\n",
      "Losgistic Regression(    3700/10000): loss= 3816.77960144624\n",
      "Losgistic Regression(    3800/10000): loss= 3814.15927301094\n",
      "Losgistic Regression(    3900/10000): loss= 3812.78181213882\n",
      "Losgistic Regression(    4000/10000): loss= 3811.52132244719\n",
      "Losgistic Regression(    4100/10000): loss= 3810.65366029621\n",
      "Losgistic Regression(    4200/10000): loss= 3809.1497708419\n",
      "Losgistic Regression(    4300/10000): loss= 3808.19984361065\n",
      "Losgistic Regression(    4400/10000): loss= 3808.15906643229\n",
      "Losgistic Regression(    4500/10000): loss= 3808.16755869348\n",
      "Totoal number of iterations =  4500\n",
      "Loss                        =  3808.16755869\n",
      "Time for  4th cross validation = 144.278s\n",
      "Training Accuracy         = 0.8337\n",
      "Cross Validation Accuracy = 0.816508\n",
      "*************** ([0.83799999999999997, 0.83860000000000001, 0.84150000000000003, 0.84130000000000005, 0.8337], [0.81418800000000002, 0.81781599999999999, 0.81410800000000005, 0.81677599999999995, 0.81650800000000001])\n",
      "Losgistic Regression(       0/10000): loss= 6821.44709547855\n",
      "Losgistic Regression(     100/10000): loss= 4573.96580663368\n",
      "Losgistic Regression(     200/10000): loss= 4323.25409774488\n",
      "Losgistic Regression(     300/10000): loss= 4208.88135446421\n",
      "Losgistic Regression(     400/10000): loss= 4139.15739985591\n",
      "Losgistic Regression(     500/10000): loss= 4092.17190973676\n",
      "Losgistic Regression(     600/10000): loss= 4058.4455616405\n",
      "Losgistic Regression(     700/10000): loss= 4035.12533804903\n",
      "Losgistic Regression(     800/10000): loss= 4017.80865567601\n",
      "Losgistic Regression(     900/10000): loss= 4005.59028968458\n",
      "Losgistic Regression(    1000/10000): loss= 3995.6269910383\n",
      "Losgistic Regression(    1100/10000): loss= 3986.81066155132\n",
      "Losgistic Regression(    1200/10000): loss= 3979.01721352283\n",
      "Losgistic Regression(    1300/10000): loss= 3972.27010645776\n",
      "Losgistic Regression(    1400/10000): loss= 3966.56671629153\n",
      "Losgistic Regression(    1500/10000): loss= 3961.49941751261\n",
      "Losgistic Regression(    1600/10000): loss= 3957.9179572071\n",
      "Losgistic Regression(    1700/10000): loss= 3955.17192151825\n",
      "Losgistic Regression(    1800/10000): loss= 3952.81527420916\n",
      "Losgistic Regression(    1900/10000): loss= 3949.97727898706\n",
      "Losgistic Regression(    2000/10000): loss= 3947.43397839232\n",
      "Losgistic Regression(    2100/10000): loss= 3944.17707112557\n",
      "Losgistic Regression(    2200/10000): loss= 3942.36004585886\n",
      "Losgistic Regression(    2300/10000): loss= 3941.65646431599\n",
      "Losgistic Regression(    2400/10000): loss= 3940.88659711468\n",
      "Losgistic Regression(    2500/10000): loss= 3939.68355239464\n",
      "Losgistic Regression(    2600/10000): loss= 3939.62133892823\n",
      "Losgistic Regression(    2700/10000): loss= 3939.62296535131\n",
      "Totoal number of iterations =  2700\n",
      "Loss                        =  3939.62296535\n",
      "Time for  0th cross validation = 86.6655s\n",
      "Training Accuracy         = 0.8312\n",
      "Cross Validation Accuracy = 0.813672\n",
      "Losgistic Regression(       0/10000): loss= 6827.88201340692\n",
      "Losgistic Regression(     100/10000): loss= 4522.93641228144\n",
      "Losgistic Regression(     200/10000): loss= 4275.48503597931\n",
      "Losgistic Regression(     300/10000): loss= 4155.99867746796\n",
      "Losgistic Regression(     400/10000): loss= 4080.44997036909\n",
      "Losgistic Regression(     500/10000): loss= 4030.49631328745\n",
      "Losgistic Regression(     600/10000): loss= 3995.97865353272\n",
      "Losgistic Regression(     700/10000): loss= 3971.48700845288\n",
      "Losgistic Regression(     800/10000): loss= 3954.09874912893\n",
      "Losgistic Regression(     900/10000): loss= 3941.3230182347\n",
      "Losgistic Regression(    1000/10000): loss= 3932.65352623668\n",
      "Losgistic Regression(    1100/10000): loss= 3926.1613748029\n",
      "Losgistic Regression(    1200/10000): loss= 3920.93929157532\n",
      "Losgistic Regression(    1300/10000): loss= 3916.83471293914\n",
      "Losgistic Regression(    1400/10000): loss= 3913.40555773104\n",
      "Losgistic Regression(    1500/10000): loss= 3910.24187607108\n",
      "Losgistic Regression(    1600/10000): loss= 3907.32655001728\n",
      "Losgistic Regression(    1700/10000): loss= 3904.28159951787\n",
      "Losgistic Regression(    1800/10000): loss= 3901.88132205939\n",
      "Losgistic Regression(    1900/10000): loss= 3899.9423381412\n",
      "Losgistic Regression(    2000/10000): loss= 3898.605557845\n",
      "Losgistic Regression(    2100/10000): loss= 3897.2263429327\n",
      "Losgistic Regression(    2200/10000): loss= 3896.20940830023\n",
      "Losgistic Regression(    2300/10000): loss= 3894.83040524775\n",
      "Losgistic Regression(    2400/10000): loss= 3893.65609177922\n",
      "Losgistic Regression(    2500/10000): loss= 3892.70691051706\n",
      "Losgistic Regression(    2600/10000): loss= 3891.57676424149\n",
      "Losgistic Regression(    2700/10000): loss= 3890.52632933275\n",
      "Losgistic Regression(    2800/10000): loss= 3890.19627343042\n",
      "Losgistic Regression(    2900/10000): loss= 3889.67320389976\n",
      "Losgistic Regression(    3000/10000): loss= 3889.23407076343\n",
      "Losgistic Regression(    3100/10000): loss= 3888.89630300627\n",
      "Losgistic Regression(    3200/10000): loss= 3888.39589092077\n",
      "Losgistic Regression(    3300/10000): loss= 3887.89914760036\n",
      "Losgistic Regression(    3400/10000): loss= 3887.46477581751\n",
      "Losgistic Regression(    3500/10000): loss= 3887.01558450503\n",
      "Losgistic Regression(    3600/10000): loss= 3886.62040269234\n",
      "Losgistic Regression(    3700/10000): loss= 3886.3069282487\n",
      "Losgistic Regression(    3800/10000): loss= 3885.88412522691\n",
      "Losgistic Regression(    3900/10000): loss= 3885.60824805139\n",
      "Losgistic Regression(    4000/10000): loss= 3885.48237880664\n",
      "Losgistic Regression(    4100/10000): loss= 3885.38225607665\n",
      "Losgistic Regression(    4200/10000): loss= 3885.28432418764\n",
      "Losgistic Regression(    4300/10000): loss= 3885.17973810262\n",
      "Losgistic Regression(    4400/10000): loss= 3885.06069127808\n",
      "Losgistic Regression(    4500/10000): loss= 3885.04480013143\n",
      "Totoal number of iterations =  4500\n",
      "Loss                        =  3885.04480013\n",
      "Time for  1th cross validation = 142.804s\n",
      "Training Accuracy         = 0.8347\n",
      "Cross Validation Accuracy = 0.818452\n",
      "Losgistic Regression(       0/10000): loss= 6825.63284990822\n",
      "Losgistic Regression(     100/10000): loss= 4490.27242022065\n",
      "Losgistic Regression(     200/10000): loss= 4258.73131104639\n",
      "Losgistic Regression(     300/10000): loss= 4143.88803205751\n",
      "Losgistic Regression(     400/10000): loss= 4070.6383671931\n",
      "Losgistic Regression(     500/10000): loss= 4020.366226236\n",
      "Losgistic Regression(     600/10000): loss= 3982.31067488434\n",
      "Losgistic Regression(     700/10000): loss= 3954.57845765958\n",
      "Losgistic Regression(     800/10000): loss= 3934.81084934352\n",
      "Losgistic Regression(     900/10000): loss= 3918.52899144519\n",
      "Losgistic Regression(    1000/10000): loss= 3907.59020775087\n",
      "Losgistic Regression(    1100/10000): loss= 3900.73127179208\n",
      "Losgistic Regression(    1200/10000): loss= 3896.49783453559\n",
      "Losgistic Regression(    1300/10000): loss= 3893.48594295076\n",
      "Losgistic Regression(    1400/10000): loss= 3890.54743819048\n",
      "Losgistic Regression(    1500/10000): loss= 3888.42114956024\n",
      "Losgistic Regression(    1600/10000): loss= 3886.75241179823\n",
      "Losgistic Regression(    1700/10000): loss= 3884.84598751816\n",
      "Losgistic Regression(    1800/10000): loss= 3882.69715647884\n",
      "Losgistic Regression(    1900/10000): loss= 3880.95719898872\n",
      "Losgistic Regression(    2000/10000): loss= 3879.58561438923\n",
      "Losgistic Regression(    2100/10000): loss= 3878.68006812545\n",
      "Losgistic Regression(    2200/10000): loss= 3877.55521772635\n",
      "Losgistic Regression(    2300/10000): loss= 3876.44136004708\n",
      "Losgistic Regression(    2400/10000): loss= 3875.5884360031\n",
      "Losgistic Regression(    2500/10000): loss= 3875.37933637713\n",
      "Losgistic Regression(    2600/10000): loss= 3875.16350487644\n",
      "Losgistic Regression(    2700/10000): loss= 3875.02078631558\n",
      "Losgistic Regression(    2800/10000): loss= 3874.94533273142\n",
      "Losgistic Regression(    2900/10000): loss= 3874.85258177798\n",
      "Losgistic Regression(    3000/10000): loss= 3874.75864104803\n",
      "Losgistic Regression(    3100/10000): loss= 3874.65570759387\n",
      "Losgistic Regression(    3200/10000): loss= 3874.52609672767\n",
      "Losgistic Regression(    3300/10000): loss= 3874.37967701788\n",
      "Losgistic Regression(    3400/10000): loss= 3874.20771248106\n",
      "Losgistic Regression(    3500/10000): loss= 3874.02246022367\n",
      "Losgistic Regression(    3600/10000): loss= 3873.86851166224\n",
      "Losgistic Regression(    3700/10000): loss= 3873.63481583187\n",
      "Losgistic Regression(    3800/10000): loss= 3873.453944747\n",
      "Losgistic Regression(    3900/10000): loss= 3873.27040088579\n",
      "Losgistic Regression(    4000/10000): loss= 3873.06480775581\n",
      "Losgistic Regression(    4100/10000): loss= 3872.83918002478\n",
      "Losgistic Regression(    4200/10000): loss= 3872.57322291142\n",
      "Losgistic Regression(    4300/10000): loss= 3872.36558018553\n",
      "Losgistic Regression(    4400/10000): loss= 3872.18862071807\n",
      "Losgistic Regression(    4500/10000): loss= 3871.96574150152\n",
      "Losgistic Regression(    4600/10000): loss= 3871.68629864322\n",
      "Losgistic Regression(    4700/10000): loss= 3871.44611973453\n",
      "Losgistic Regression(    4800/10000): loss= 3871.23662090677\n",
      "Losgistic Regression(    4900/10000): loss= 3871.05387961523\n",
      "Losgistic Regression(    5000/10000): loss= 3870.78780984398\n",
      "Losgistic Regression(    5100/10000): loss= 3870.53378310865\n",
      "Losgistic Regression(    5200/10000): loss= 3870.27898959203\n",
      "Losgistic Regression(    5300/10000): loss= 3870.14068141313\n",
      "Losgistic Regression(    5400/10000): loss= 3869.91772260805\n",
      "Losgistic Regression(    5500/10000): loss= 3869.66520388749\n",
      "Losgistic Regression(    5600/10000): loss= 3869.41579577136\n",
      "Losgistic Regression(    5700/10000): loss= 3869.39239715689\n",
      "Totoal number of iterations =  5700\n",
      "Loss                        =  3869.39239716\n",
      "Time for  2th cross validation = 179.756s\n",
      "Training Accuracy         = 0.8398\n",
      "Cross Validation Accuracy = 0.815976\n",
      "Losgistic Regression(       0/10000): loss= 6823.21289314529\n",
      "Losgistic Regression(     100/10000): loss= 4572.11257979059\n",
      "Losgistic Regression(     200/10000): loss= 4338.98000703059\n",
      "Losgistic Regression(     300/10000): loss= 4222.52864956585\n",
      "Losgistic Regression(     400/10000): loss= 4153.56199138003\n",
      "Losgistic Regression(     500/10000): loss= 4106.79219624772\n",
      "Losgistic Regression(     600/10000): loss= 4069.94718632773\n",
      "Losgistic Regression(     700/10000): loss= 4038.50319261785\n",
      "Losgistic Regression(     800/10000): loss= 4019.27228091873\n",
      "Losgistic Regression(     900/10000): loss= 4006.48519679986\n",
      "Losgistic Regression(    1000/10000): loss= 3997.65785938332\n",
      "Losgistic Regression(    1100/10000): loss= 3990.49857967702\n",
      "Losgistic Regression(    1200/10000): loss= 3985.70860930194\n",
      "Losgistic Regression(    1300/10000): loss= 3982.72295118608\n",
      "Losgistic Regression(    1400/10000): loss= 3980.75918507006\n",
      "Losgistic Regression(    1500/10000): loss= 3978.59502877097\n",
      "Losgistic Regression(    1600/10000): loss= 3976.58230257175\n",
      "Losgistic Regression(    1700/10000): loss= 3974.22999630962\n",
      "Losgistic Regression(    1800/10000): loss= 3971.73171166184\n",
      "Losgistic Regression(    1900/10000): loss= 3969.591247001\n",
      "Losgistic Regression(    2000/10000): loss= 3967.58422246234\n",
      "Losgistic Regression(    2100/10000): loss= 3966.07720701759\n",
      "Losgistic Regression(    2200/10000): loss= 3965.28440975821\n",
      "Losgistic Regression(    2300/10000): loss= 3964.02040523555\n",
      "Losgistic Regression(    2400/10000): loss= 3963.29015333276\n",
      "Losgistic Regression(    2500/10000): loss= 3961.83928178559\n",
      "Losgistic Regression(    2600/10000): loss= 3961.25061570216\n",
      "Losgistic Regression(    2700/10000): loss= 3960.66283123614\n",
      "Losgistic Regression(    2800/10000): loss= 3960.62243904108\n",
      "Losgistic Regression(    2900/10000): loss= 3960.61562824058\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  3960.61562824\n",
      "Time for  3th cross validation = 92.1413s\n",
      "Training Accuracy         = 0.8351\n",
      "Cross Validation Accuracy = 0.816056\n",
      "Losgistic Regression(       0/10000): loss= 6821.16214528569\n",
      "Losgistic Regression(     100/10000): loss= 4585.54504358286\n",
      "Losgistic Regression(     200/10000): loss= 4367.66014471588\n",
      "Losgistic Regression(     300/10000): loss= 4272.28524937707\n",
      "Losgistic Regression(     400/10000): loss= 4217.8018539104\n",
      "Losgistic Regression(     500/10000): loss= 4181.24944485811\n",
      "Losgistic Regression(     600/10000): loss= 4154.13265052421\n",
      "Losgistic Regression(     700/10000): loss= 4131.96282243003\n",
      "Losgistic Regression(     800/10000): loss= 4112.90241847431\n",
      "Losgistic Regression(     900/10000): loss= 4097.04898424702\n",
      "Losgistic Regression(    1000/10000): loss= 4083.4959641867\n",
      "Losgistic Regression(    1100/10000): loss= 4072.27639165412\n",
      "Losgistic Regression(    1200/10000): loss= 4063.00382665799\n",
      "Losgistic Regression(    1300/10000): loss= 4055.8212745669\n",
      "Losgistic Regression(    1400/10000): loss= 4051.01357492056\n",
      "Losgistic Regression(    1500/10000): loss= 4047.7296126623\n",
      "Losgistic Regression(    1600/10000): loss= 4044.6876686168\n",
      "Losgistic Regression(    1700/10000): loss= 4042.4380115336\n",
      "Losgistic Regression(    1800/10000): loss= 4040.60366788832\n",
      "Losgistic Regression(    1900/10000): loss= 4038.92944383189\n",
      "Losgistic Regression(    2000/10000): loss= 4037.42625080367\n",
      "Losgistic Regression(    2100/10000): loss= 4036.05286746361\n",
      "Losgistic Regression(    2200/10000): loss= 4034.58543468739\n",
      "Losgistic Regression(    2300/10000): loss= 4033.32072388513\n",
      "Losgistic Regression(    2400/10000): loss= 4032.32873987172\n",
      "Losgistic Regression(    2500/10000): loss= 4031.23799492487\n",
      "Losgistic Regression(    2600/10000): loss= 4030.26804408339\n",
      "Losgistic Regression(    2700/10000): loss= 4029.42879819842\n",
      "Losgistic Regression(    2800/10000): loss= 4028.59818720035\n",
      "Losgistic Regression(    2900/10000): loss= 4027.71742359962\n",
      "Losgistic Regression(    3000/10000): loss= 4026.88311413484\n",
      "Losgistic Regression(    3100/10000): loss= 4026.26484305289\n",
      "Losgistic Regression(    3200/10000): loss= 4025.17739694776\n",
      "Losgistic Regression(    3300/10000): loss= 4024.36717756995\n",
      "Losgistic Regression(    3400/10000): loss= 4023.99641558941\n",
      "Losgistic Regression(    3500/10000): loss= 4023.37800713988\n",
      "Losgistic Regression(    3600/10000): loss= 4022.96987194275\n",
      "Losgistic Regression(    3700/10000): loss= 4022.56342885829\n",
      "Losgistic Regression(    3800/10000): loss= 4022.16389630712\n",
      "Losgistic Regression(    3900/10000): loss= 4021.83322822475\n",
      "Losgistic Regression(    4000/10000): loss= 4021.43150664276\n",
      "Losgistic Regression(    4100/10000): loss= 4021.07012227506\n",
      "Losgistic Regression(    4200/10000): loss= 4020.7333322312\n",
      "Losgistic Regression(    4300/10000): loss= 4020.69638574055\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  4020.69638574\n",
      "Time for  4th cross validation = 136.345s\n",
      "Training Accuracy         = 0.8286\n",
      "Cross Validation Accuracy = 0.816032\n",
      "*************** ([0.83120000000000005, 0.8347, 0.83979999999999999, 0.83509999999999995, 0.8286], [0.81367199999999995, 0.81845199999999996, 0.81597600000000003, 0.816056, 0.81603199999999998])\n",
      "Losgistic Regression(       0/10000): loss= 6822.32141944154\n",
      "Losgistic Regression(     100/10000): loss= 4658.14651552443\n",
      "Losgistic Regression(     200/10000): loss= 4462.78669946749\n",
      "Losgistic Regression(     300/10000): loss= 4387.07828882383\n",
      "Losgistic Regression(     400/10000): loss= 4342.87670808852\n",
      "Losgistic Regression(     500/10000): loss= 4315.29621165487\n",
      "Losgistic Regression(     600/10000): loss= 4295.82231932856\n",
      "Losgistic Regression(     700/10000): loss= 4285.17993077222\n",
      "Losgistic Regression(     800/10000): loss= 4277.87565345289\n",
      "Losgistic Regression(     900/10000): loss= 4274.42331869931\n",
      "Losgistic Regression(    1000/10000): loss= 4271.95226413865\n",
      "Losgistic Regression(    1100/10000): loss= 4269.21908906051\n",
      "Losgistic Regression(    1200/10000): loss= 4267.67895752786\n",
      "Losgistic Regression(    1300/10000): loss= 4266.30631107955\n",
      "Losgistic Regression(    1400/10000): loss= 4266.13952294425\n",
      "Losgistic Regression(    1500/10000): loss= 4266.13130808456\n",
      "Totoal number of iterations =  1500\n",
      "Loss                        =  4266.13130808\n",
      "Time for  0th cross validation = 47.9852s\n",
      "Training Accuracy         = 0.8198\n",
      "Cross Validation Accuracy = 0.809852\n",
      "Losgistic Regression(       0/10000): loss= 6828.70474594403\n",
      "Losgistic Regression(     100/10000): loss= 4606.16256561438\n",
      "Losgistic Regression(     200/10000): loss= 4411.10670785557\n",
      "Losgistic Regression(     300/10000): loss= 4329.5481671434\n",
      "Losgistic Regression(     400/10000): loss= 4280.11357213024\n",
      "Losgistic Regression(     500/10000): loss= 4251.03511415833\n",
      "Losgistic Regression(     600/10000): loss= 4233.63661824286\n",
      "Losgistic Regression(     700/10000): loss= 4221.9039116508\n",
      "Losgistic Regression(     800/10000): loss= 4212.58043602563\n",
      "Losgistic Regression(     900/10000): loss= 4206.4440591356\n",
      "Losgistic Regression(    1000/10000): loss= 4202.34759705678\n",
      "Losgistic Regression(    1100/10000): loss= 4198.31772526547\n",
      "Losgistic Regression(    1200/10000): loss= 4194.97460170249\n",
      "Losgistic Regression(    1300/10000): loss= 4192.03122307332\n",
      "Losgistic Regression(    1400/10000): loss= 4189.09796364856\n",
      "Losgistic Regression(    1500/10000): loss= 4186.44169412284\n",
      "Losgistic Regression(    1600/10000): loss= 4184.61117371937\n",
      "Losgistic Regression(    1700/10000): loss= 4184.2616553546\n",
      "Losgistic Regression(    1800/10000): loss= 4183.77861750168\n",
      "Losgistic Regression(    1900/10000): loss= 4183.49609892192\n",
      "Losgistic Regression(    2000/10000): loss= 4183.18866841986\n",
      "Losgistic Regression(    2100/10000): loss= 4182.84498782538\n",
      "Losgistic Regression(    2200/10000): loss= 4182.45661519815\n",
      "Losgistic Regression(    2300/10000): loss= 4182.02717972744\n",
      "Losgistic Regression(    2400/10000): loss= 4181.56033110852\n",
      "Losgistic Regression(    2500/10000): loss= 4181.06195614258\n",
      "Losgistic Regression(    2600/10000): loss= 4180.53388559013\n",
      "Losgistic Regression(    2700/10000): loss= 4179.97761597432\n",
      "Losgistic Regression(    2800/10000): loss= 4179.66299337943\n",
      "Losgistic Regression(    2900/10000): loss= 4179.66064639863\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  4179.6606464\n",
      "Time for  1th cross validation = 92.1645s\n",
      "Training Accuracy         = 0.8239\n",
      "Cross Validation Accuracy = 0.813132\n",
      "Losgistic Regression(       0/10000): loss= 6826.49348941293\n",
      "Losgistic Regression(     100/10000): loss= 4575.40191199879\n",
      "Losgistic Regression(     200/10000): loss= 4395.02383986609\n",
      "Losgistic Regression(     300/10000): loss= 4317.90124872744\n",
      "Losgistic Regression(     400/10000): loss= 4270.57297137895\n",
      "Losgistic Regression(     500/10000): loss= 4237.16998883101\n",
      "Losgistic Regression(     600/10000): loss= 4218.77129602252\n",
      "Losgistic Regression(     700/10000): loss= 4205.45789045256\n",
      "Losgistic Regression(     800/10000): loss= 4197.1116396885\n",
      "Losgistic Regression(     900/10000): loss= 4193.64053308406\n",
      "Losgistic Regression(    1000/10000): loss= 4190.67834786559\n",
      "Losgistic Regression(    1100/10000): loss= 4187.72638619271\n",
      "Losgistic Regression(    1200/10000): loss= 4186.4469197209\n",
      "Losgistic Regression(    1300/10000): loss= 4186.26833309143\n",
      "Losgistic Regression(    1400/10000): loss= 4186.2554329748\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  4186.25543297\n",
      "Time for  2th cross validation = 44.9504s\n",
      "Training Accuracy         = 0.8277\n",
      "Cross Validation Accuracy = 0.809456\n",
      "Losgistic Regression(       0/10000): loss= 6824.06230984767\n",
      "Losgistic Regression(     100/10000): loss= 4653.88587231966\n",
      "Losgistic Regression(     200/10000): loss= 4469.90404487886\n",
      "Losgistic Regression(     300/10000): loss= 4392.02591104022\n",
      "Losgistic Regression(     400/10000): loss= 4348.12113625595\n",
      "Losgistic Regression(     500/10000): loss= 4321.10744779862\n",
      "Losgistic Regression(     600/10000): loss= 4299.26792708951\n",
      "Losgistic Regression(     700/10000): loss= 4281.92802369654\n",
      "Losgistic Regression(     800/10000): loss= 4267.89865030509\n",
      "Losgistic Regression(     900/10000): loss= 4259.46450642324\n",
      "Losgistic Regression(    1000/10000): loss= 4253.78031457365\n",
      "Losgistic Regression(    1100/10000): loss= 4250.14338266834\n",
      "Losgistic Regression(    1200/10000): loss= 4247.73798045885\n",
      "Losgistic Regression(    1300/10000): loss= 4245.48730594249\n",
      "Losgistic Regression(    1400/10000): loss= 4243.46063189376\n",
      "Losgistic Regression(    1500/10000): loss= 4242.11164864088\n",
      "Losgistic Regression(    1600/10000): loss= 4240.38926063494\n",
      "Losgistic Regression(    1700/10000): loss= 4239.47070129951\n",
      "Losgistic Regression(    1800/10000): loss= 4238.6316964756\n",
      "Losgistic Regression(    1900/10000): loss= 4237.50065495632\n",
      "Losgistic Regression(    2000/10000): loss= 4237.27881871422\n",
      "Losgistic Regression(    2100/10000): loss= 4237.25694096051\n",
      "Totoal number of iterations =  2100\n",
      "Loss                        =  4237.25694096\n",
      "Time for  3th cross validation = 66.8339s\n",
      "Training Accuracy         = 0.8241\n",
      "Cross Validation Accuracy = 0.81164\n",
      "Losgistic Regression(       0/10000): loss= 6822.03580023684\n",
      "Losgistic Regression(     100/10000): loss= 4667.9156416593\n",
      "Losgistic Regression(     200/10000): loss= 4497.31266583698\n",
      "Losgistic Regression(     300/10000): loss= 4432.52299520895\n",
      "Losgistic Regression(     400/10000): loss= 4398.52700915552\n",
      "Losgistic Regression(     500/10000): loss= 4378.17686340663\n",
      "Losgistic Regression(     600/10000): loss= 4365.07823178627\n",
      "Losgistic Regression(     700/10000): loss= 4355.26240502475\n",
      "Losgistic Regression(     800/10000): loss= 4347.8668820935\n",
      "Losgistic Regression(     900/10000): loss= 4342.42016762999\n",
      "Losgistic Regression(    1000/10000): loss= 4337.67453781376\n",
      "Losgistic Regression(    1100/10000): loss= 4334.52857660295\n",
      "Losgistic Regression(    1200/10000): loss= 4331.92938840988\n",
      "Losgistic Regression(    1300/10000): loss= 4329.61470924317\n",
      "Losgistic Regression(    1400/10000): loss= 4327.84275607148\n",
      "Losgistic Regression(    1500/10000): loss= 4326.19496734859\n",
      "Losgistic Regression(    1600/10000): loss= 4324.74981976372\n",
      "Losgistic Regression(    1700/10000): loss= 4323.53315654154\n",
      "Losgistic Regression(    1800/10000): loss= 4322.30924450157\n",
      "Losgistic Regression(    1900/10000): loss= 4322.04887323687\n",
      "Losgistic Regression(    2000/10000): loss= 4321.898427704\n",
      "Losgistic Regression(    2100/10000): loss= 4321.75757121563\n",
      "Losgistic Regression(    2200/10000): loss= 4321.73622708903\n",
      "Totoal number of iterations =  2200\n",
      "Loss                        =  4321.73622709\n",
      "Time for  4th cross validation = 70.1543s\n",
      "Training Accuracy         = 0.8148\n",
      "Cross Validation Accuracy = 0.808876\n",
      "*************** ([0.81979999999999997, 0.82389999999999997, 0.82769999999999999, 0.82410000000000005, 0.81479999999999997], [0.80985200000000002, 0.81313199999999997, 0.80945599999999995, 0.81164000000000003, 0.80887600000000004])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.84109999999999996,\n",
       "   0.84240000000000004,\n",
       "   0.84740000000000004,\n",
       "   0.84499999999999997,\n",
       "   0.83530000000000004],\n",
       "  [0.81363200000000002,\n",
       "   0.81584000000000001,\n",
       "   0.81413999999999997,\n",
       "   0.81413999999999997,\n",
       "   0.81575200000000003]),\n",
       " ([0.84050000000000002,\n",
       "   0.8417,\n",
       "   0.84689999999999999,\n",
       "   0.84509999999999996,\n",
       "   0.83550000000000002],\n",
       "  [0.81368399999999996,\n",
       "   0.81589999999999996,\n",
       "   0.81435599999999997,\n",
       "   0.81452400000000003,\n",
       "   0.81611199999999995]),\n",
       " ([0.83950000000000002,\n",
       "   0.84160000000000001,\n",
       "   0.84689999999999999,\n",
       "   0.84470000000000001,\n",
       "   0.83479999999999999],\n",
       "  [0.81397600000000003,\n",
       "   0.81662800000000002,\n",
       "   0.81508000000000003,\n",
       "   0.81530800000000003,\n",
       "   0.81635199999999997]),\n",
       " ([0.83799999999999997,\n",
       "   0.83860000000000001,\n",
       "   0.84150000000000003,\n",
       "   0.84130000000000005,\n",
       "   0.8337],\n",
       "  [0.81418800000000002,\n",
       "   0.81781599999999999,\n",
       "   0.81410800000000005,\n",
       "   0.81677599999999995,\n",
       "   0.81650800000000001]),\n",
       " ([0.83120000000000005,\n",
       "   0.8347,\n",
       "   0.83979999999999999,\n",
       "   0.83509999999999995,\n",
       "   0.8286],\n",
       "  [0.81367199999999995,\n",
       "   0.81845199999999996,\n",
       "   0.81597600000000003,\n",
       "   0.816056,\n",
       "   0.81603199999999998]),\n",
       " ([0.81979999999999997,\n",
       "   0.82389999999999997,\n",
       "   0.82769999999999999,\n",
       "   0.82410000000000005,\n",
       "   0.81479999999999997],\n",
       "  [0.80985200000000002,\n",
       "   0.81313199999999997,\n",
       "   0.80945599999999995,\n",
       "   0.81164000000000003,\n",
       "   0.80887600000000004])]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_10000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(10000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_10000.append(tmp)\n",
    "\n",
    "accu_10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 3436.25369024268\n",
      "Losgistic Regression(     100/10000): loss= 2335.38408481608\n",
      "Losgistic Regression(     200/10000): loss= 2120.8244973092\n",
      "Losgistic Regression(     300/10000): loss= 2038.86023994235\n",
      "Losgistic Regression(     400/10000): loss= 1982.2365087908\n",
      "Losgistic Regression(     500/10000): loss= 1939.79190706255\n",
      "Losgistic Regression(     600/10000): loss= 1906.11170764866\n",
      "Losgistic Regression(     700/10000): loss= 1878.43608128396\n",
      "Losgistic Regression(     800/10000): loss= 1855.55657337835\n",
      "Losgistic Regression(     900/10000): loss= 1836.53791910732\n",
      "Losgistic Regression(    1000/10000): loss= 1820.35362213278\n",
      "Losgistic Regression(    1100/10000): loss= 1806.22017349487\n",
      "Losgistic Regression(    1200/10000): loss= 1793.66089585403\n",
      "Losgistic Regression(    1300/10000): loss= 1782.3396391158\n",
      "Losgistic Regression(    1400/10000): loss= 1772.28250224786\n",
      "Losgistic Regression(    1500/10000): loss= 1763.84201200582\n",
      "Losgistic Regression(    1600/10000): loss= 1756.14797405185\n",
      "Losgistic Regression(    1700/10000): loss= 1749.37774649725\n",
      "Losgistic Regression(    1800/10000): loss= 1743.38438097274\n",
      "Losgistic Regression(    1900/10000): loss= 1738.05332793769\n",
      "Losgistic Regression(    2000/10000): loss= 1733.30288252629\n",
      "Losgistic Regression(    2100/10000): loss= 1728.6669491873\n",
      "Losgistic Regression(    2200/10000): loss= 1724.71313486582\n",
      "Losgistic Regression(    2300/10000): loss= 1721.05883423618\n",
      "Losgistic Regression(    2400/10000): loss= 1717.46015870837\n",
      "Losgistic Regression(    2500/10000): loss= 1714.23393028509\n",
      "Losgistic Regression(    2600/10000): loss= 1710.85555287582\n",
      "Losgistic Regression(    2700/10000): loss= 1707.50737055236\n",
      "Losgistic Regression(    2800/10000): loss= 1704.44245565712\n",
      "Losgistic Regression(    2900/10000): loss= 1701.62099528256\n",
      "Losgistic Regression(    3000/10000): loss= 1698.94288307236\n",
      "Losgistic Regression(    3100/10000): loss= 1696.45479683125\n",
      "Losgistic Regression(    3200/10000): loss= 1694.03236116277\n",
      "Losgistic Regression(    3300/10000): loss= 1691.43441720798\n",
      "Losgistic Regression(    3400/10000): loss= 1688.77953872545\n",
      "Losgistic Regression(    3500/10000): loss= 1686.1727618056\n",
      "Losgistic Regression(    3600/10000): loss= 1683.71793622599\n",
      "Losgistic Regression(    3700/10000): loss= 1681.32244148538\n",
      "Losgistic Regression(    3800/10000): loss= 1678.74311790536\n",
      "Losgistic Regression(    3900/10000): loss= 1676.19505939456\n",
      "Losgistic Regression(    4000/10000): loss= 1673.65502000397\n",
      "Losgistic Regression(    4100/10000): loss= 1671.10698704082\n",
      "Losgistic Regression(    4200/10000): loss= 1668.55375639221\n",
      "Losgistic Regression(    4300/10000): loss= 1665.99071154689\n",
      "Losgistic Regression(    4400/10000): loss= 1663.45235648379\n",
      "Losgistic Regression(    4500/10000): loss= 1660.83935622069\n",
      "Losgistic Regression(    4600/10000): loss= 1658.02413424539\n",
      "Losgistic Regression(    4700/10000): loss= 1655.29260314267\n",
      "Losgistic Regression(    4800/10000): loss= 1652.68686157434\n",
      "Losgistic Regression(    4900/10000): loss= 1650.13133297363\n",
      "Losgistic Regression(    5000/10000): loss= 1647.61038421736\n",
      "Losgistic Regression(    5100/10000): loss= 1645.219824974\n",
      "Losgistic Regression(    5200/10000): loss= 1642.95360038188\n",
      "Losgistic Regression(    5300/10000): loss= 1640.80019243772\n",
      "Losgistic Regression(    5400/10000): loss= 1638.79672848377\n",
      "Losgistic Regression(    5500/10000): loss= 1636.90769960708\n",
      "Losgistic Regression(    5600/10000): loss= 1635.08066234283\n",
      "Losgistic Regression(    5700/10000): loss= 1633.32352242648\n",
      "Losgistic Regression(    5800/10000): loss= 1631.66756143926\n",
      "Losgistic Regression(    5900/10000): loss= 1630.11293168877\n",
      "Losgistic Regression(    6000/10000): loss= 1628.63562669533\n",
      "Losgistic Regression(    6100/10000): loss= 1626.97826547838\n",
      "Losgistic Regression(    6200/10000): loss= 1625.30031732047\n",
      "Losgistic Regression(    6300/10000): loss= 1623.74476216898\n",
      "Losgistic Regression(    6400/10000): loss= 1622.31841611641\n",
      "Losgistic Regression(    6500/10000): loss= 1620.99345711954\n",
      "Losgistic Regression(    6600/10000): loss= 1619.76420955088\n",
      "Losgistic Regression(    6700/10000): loss= 1618.6382589615\n",
      "Losgistic Regression(    6800/10000): loss= 1617.59108336269\n",
      "Losgistic Regression(    6900/10000): loss= 1616.6166074819\n",
      "Losgistic Regression(    7000/10000): loss= 1615.72035483453\n",
      "Losgistic Regression(    7100/10000): loss= 1614.87936867919\n",
      "Losgistic Regression(    7200/10000): loss= 1614.05606145282\n",
      "Losgistic Regression(    7300/10000): loss= 1613.24206791797\n",
      "Losgistic Regression(    7400/10000): loss= 1612.43660479524\n",
      "Losgistic Regression(    7500/10000): loss= 1611.64780354486\n",
      "Losgistic Regression(    7600/10000): loss= 1610.87080098175\n",
      "Losgistic Regression(    7700/10000): loss= 1610.10096938839\n",
      "Losgistic Regression(    7800/10000): loss= 1609.34844813343\n",
      "Losgistic Regression(    7900/10000): loss= 1608.57977475058\n",
      "Losgistic Regression(    8000/10000): loss= 1607.84025937449\n",
      "Losgistic Regression(    8100/10000): loss= 1607.1381016912\n",
      "Losgistic Regression(    8200/10000): loss= 1606.42927052921\n",
      "Losgistic Regression(    8300/10000): loss= 1605.74967416198\n",
      "Losgistic Regression(    8400/10000): loss= 1605.1175636036\n",
      "Losgistic Regression(    8500/10000): loss= 1604.5166218184\n",
      "Losgistic Regression(    8600/10000): loss= 1603.96545509868\n",
      "Losgistic Regression(    8700/10000): loss= 1603.48984665349\n",
      "Losgistic Regression(    8800/10000): loss= 1603.07331106498\n",
      "Losgistic Regression(    8900/10000): loss= 1602.6763294143\n",
      "Losgistic Regression(    9000/10000): loss= 1602.28838946694\n",
      "Losgistic Regression(    9100/10000): loss= 1601.93004422541\n",
      "Losgistic Regression(    9200/10000): loss= 1601.60299875908\n",
      "Losgistic Regression(    9300/10000): loss= 1601.29425135481\n",
      "Losgistic Regression(    9400/10000): loss= 1600.99559635413\n",
      "Losgistic Regression(    9500/10000): loss= 1600.67974700196\n",
      "Losgistic Regression(    9600/10000): loss= 1600.33069193013\n",
      "Losgistic Regression(    9700/10000): loss= 1599.96083422651\n",
      "Losgistic Regression(    9800/10000): loss= 1599.58224788871\n",
      "Losgistic Regression(    9900/10000): loss= 1599.1843110742\n",
      "Time for  0th cross validation = 165.499s\n",
      "Training Accuracy         =  0.859\n",
      "Cross Validation Accuracy = 0.805328\n",
      "Losgistic Regression(       0/10000): loss= 3438.20162231723\n",
      "Losgistic Regression(     100/10000): loss= 2368.58528188815\n",
      "Losgistic Regression(     200/10000): loss= 2164.74372176491\n",
      "Losgistic Regression(     300/10000): loss= 2073.60896950236\n",
      "Losgistic Regression(     400/10000): loss= 2015.82753076316\n",
      "Losgistic Regression(     500/10000): loss= 1972.01656844111\n",
      "Losgistic Regression(     600/10000): loss= 1935.76590789647\n",
      "Losgistic Regression(     700/10000): loss= 1906.62435938728\n",
      "Losgistic Regression(     800/10000): loss= 1880.85765430676\n",
      "Losgistic Regression(     900/10000): loss= 1858.27395243026\n",
      "Losgistic Regression(    1000/10000): loss= 1839.21455857314\n",
      "Losgistic Regression(    1100/10000): loss= 1823.02880658849\n",
      "Losgistic Regression(    1200/10000): loss= 1809.35474653064\n",
      "Losgistic Regression(    1300/10000): loss= 1797.53051749349\n",
      "Losgistic Regression(    1400/10000): loss= 1786.99054473857\n",
      "Losgistic Regression(    1500/10000): loss= 1777.60141462437\n",
      "Losgistic Regression(    1600/10000): loss= 1769.11835874655\n",
      "Losgistic Regression(    1700/10000): loss= 1761.36936313888\n",
      "Losgistic Regression(    1800/10000): loss= 1754.22562008187\n",
      "Losgistic Regression(    1900/10000): loss= 1747.68607661495\n",
      "Losgistic Regression(    2000/10000): loss= 1741.77243701924\n",
      "Losgistic Regression(    2100/10000): loss= 1736.41206927975\n",
      "Losgistic Regression(    2200/10000): loss= 1731.50786310019\n",
      "Losgistic Regression(    2300/10000): loss= 1727.01803223903\n",
      "Losgistic Regression(    2400/10000): loss= 1722.90743767241\n",
      "Losgistic Regression(    2500/10000): loss= 1719.11046455995\n",
      "Losgistic Regression(    2600/10000): loss= 1715.63535697168\n",
      "Losgistic Regression(    2700/10000): loss= 1712.47464807965\n",
      "Losgistic Regression(    2800/10000): loss= 1709.59614488648\n",
      "Losgistic Regression(    2900/10000): loss= 1706.96956332587\n",
      "Losgistic Regression(    3000/10000): loss= 1704.56439296061\n",
      "Losgistic Regression(    3100/10000): loss= 1702.31715594132\n",
      "Losgistic Regression(    3200/10000): loss= 1700.17977354203\n",
      "Losgistic Regression(    3300/10000): loss= 1698.15668007792\n",
      "Losgistic Regression(    3400/10000): loss= 1696.22931522308\n",
      "Losgistic Regression(    3500/10000): loss= 1694.3538354599\n",
      "Losgistic Regression(    3600/10000): loss= 1692.52409983294\n",
      "Losgistic Regression(    3700/10000): loss= 1690.75016641947\n",
      "Losgistic Regression(    3800/10000): loss= 1689.02229169686\n",
      "Losgistic Regression(    3900/10000): loss= 1687.28746799622\n",
      "Losgistic Regression(    4000/10000): loss= 1685.08018884668\n",
      "Losgistic Regression(    4100/10000): loss= 1682.54887153442\n",
      "Losgistic Regression(    4200/10000): loss= 1680.10585327617\n",
      "Losgistic Regression(    4300/10000): loss= 1677.74577045173\n",
      "Losgistic Regression(    4400/10000): loss= 1675.4692383963\n",
      "Losgistic Regression(    4500/10000): loss= 1673.28198497356\n",
      "Losgistic Regression(    4600/10000): loss= 1671.18268175914\n",
      "Losgistic Regression(    4700/10000): loss= 1669.16957228836\n",
      "Losgistic Regression(    4800/10000): loss= 1667.23911872816\n",
      "Losgistic Regression(    4900/10000): loss= 1665.38168759295\n",
      "Losgistic Regression(    5000/10000): loss= 1663.57326397155\n",
      "Losgistic Regression(    5100/10000): loss= 1661.75749268742\n",
      "Losgistic Regression(    5200/10000): loss= 1659.92137613105\n",
      "Losgistic Regression(    5300/10000): loss= 1658.11357907925\n",
      "Losgistic Regression(    5400/10000): loss= 1656.31972084087\n",
      "Losgistic Regression(    5500/10000): loss= 1654.52943335953\n",
      "Losgistic Regression(    5600/10000): loss= 1653.01629995564\n",
      "Losgistic Regression(    5700/10000): loss= 1651.89485893122\n",
      "Losgistic Regression(    5800/10000): loss= 1650.89531283267\n",
      "Losgistic Regression(    5900/10000): loss= 1649.61840069149\n",
      "Losgistic Regression(    6000/10000): loss= 1648.27512751248\n",
      "Losgistic Regression(    6100/10000): loss= 1647.10064081043\n",
      "Losgistic Regression(    6200/10000): loss= 1646.04799773933\n",
      "Losgistic Regression(    6300/10000): loss= 1645.0899047982\n",
      "Losgistic Regression(    6400/10000): loss= 1644.15655745671\n",
      "Losgistic Regression(    6500/10000): loss= 1643.37052790831\n",
      "Losgistic Regression(    6600/10000): loss= 1642.42015189747\n",
      "Losgistic Regression(    6700/10000): loss= 1641.61924846094\n",
      "Losgistic Regression(    6800/10000): loss= 1641.01288538831\n",
      "Losgistic Regression(    6900/10000): loss= 1640.45104537817\n",
      "Losgistic Regression(    7000/10000): loss= 1639.80637206489\n",
      "Losgistic Regression(    7100/10000): loss= 1639.24466280351\n",
      "Losgistic Regression(    7200/10000): loss= 1638.96348592661\n",
      "Losgistic Regression(    7300/10000): loss= 1638.87047553047\n",
      "Losgistic Regression(    7400/10000): loss= 1638.6621505768\n",
      "Losgistic Regression(    7500/10000): loss= 1638.37312355901\n",
      "Losgistic Regression(    7600/10000): loss= 1638.1857792475\n",
      "Losgistic Regression(    7700/10000): loss= 1637.9918398239\n",
      "Losgistic Regression(    7800/10000): loss= 1637.71635729861\n",
      "Losgistic Regression(    7900/10000): loss= 1637.38088580457\n",
      "Losgistic Regression(    8000/10000): loss= 1636.98332000494\n",
      "Losgistic Regression(    8100/10000): loss= 1636.72487654895\n",
      "Losgistic Regression(    8200/10000): loss= 1636.67619016226\n",
      "Losgistic Regression(    8300/10000): loss= 1636.67710643615\n",
      "Totoal number of iterations =  8300\n",
      "Loss                        =  1636.67710644\n",
      "Time for  1th cross validation = 136.193s\n",
      "Training Accuracy         = 0.8504\n",
      "Cross Validation Accuracy = 0.794028\n",
      "Losgistic Regression(       0/10000): loss= 3438.5671189856\n",
      "Losgistic Regression(     100/10000): loss= 2321.42623682973\n",
      "Losgistic Regression(     200/10000): loss= 2116.11296811218\n",
      "Losgistic Regression(     300/10000): loss= 2025.32160119401\n",
      "Losgistic Regression(     400/10000): loss= 1963.89415167178\n",
      "Losgistic Regression(     500/10000): loss= 1918.80401817087\n",
      "Losgistic Regression(     600/10000): loss= 1884.34006419215\n",
      "Losgistic Regression(     700/10000): loss= 1856.91242802465\n",
      "Losgistic Regression(     800/10000): loss= 1834.53866899363\n",
      "Losgistic Regression(     900/10000): loss= 1816.14046400515\n",
      "Losgistic Regression(    1000/10000): loss= 1800.8037905313\n",
      "Losgistic Regression(    1100/10000): loss= 1787.82976115422\n",
      "Losgistic Regression(    1200/10000): loss= 1775.30614739616\n",
      "Losgistic Regression(    1300/10000): loss= 1763.41608566995\n",
      "Losgistic Regression(    1400/10000): loss= 1752.7811813921\n",
      "Losgistic Regression(    1500/10000): loss= 1743.17986949519\n",
      "Losgistic Regression(    1600/10000): loss= 1734.44570324537\n",
      "Losgistic Regression(    1700/10000): loss= 1725.96870030846\n",
      "Losgistic Regression(    1800/10000): loss= 1718.06883724722\n",
      "Losgistic Regression(    1900/10000): loss= 1710.79779095635\n",
      "Losgistic Regression(    2000/10000): loss= 1704.04527320417\n",
      "Losgistic Regression(    2100/10000): loss= 1697.72330272646\n",
      "Losgistic Regression(    2200/10000): loss= 1691.77019650065\n",
      "Losgistic Regression(    2300/10000): loss= 1686.14135830559\n",
      "Losgistic Regression(    2400/10000): loss= 1680.04135800873\n",
      "Losgistic Regression(    2500/10000): loss= 1674.10630566887\n",
      "Losgistic Regression(    2600/10000): loss= 1668.43749949377\n",
      "Losgistic Regression(    2700/10000): loss= 1663.35760245051\n",
      "Losgistic Regression(    2800/10000): loss= 1658.29562372782\n",
      "Losgistic Regression(    2900/10000): loss= 1652.84433429432\n",
      "Losgistic Regression(    3000/10000): loss= 1647.6692041957\n",
      "Losgistic Regression(    3100/10000): loss= 1642.60707493996\n",
      "Losgistic Regression(    3200/10000): loss= 1637.29857404437\n",
      "Losgistic Regression(    3300/10000): loss= 1632.37465890759\n",
      "Losgistic Regression(    3400/10000): loss= 1627.7424635778\n",
      "Losgistic Regression(    3500/10000): loss= 1622.78543080524\n",
      "Losgistic Regression(    3600/10000): loss= 1617.92801569966\n",
      "Losgistic Regression(    3700/10000): loss= 1613.04864224201\n",
      "Losgistic Regression(    3800/10000): loss= 1607.82949095021\n",
      "Losgistic Regression(    3900/10000): loss= 1602.65740373859\n",
      "Losgistic Regression(    4000/10000): loss= 1597.56214543596\n",
      "Losgistic Regression(    4100/10000): loss= 1592.41323400268\n",
      "Losgistic Regression(    4200/10000): loss= 1587.39806479966\n",
      "Losgistic Regression(    4300/10000): loss= 1582.591886309\n",
      "Losgistic Regression(    4400/10000): loss= 1577.76057095712\n",
      "Losgistic Regression(    4500/10000): loss= 1572.9419642585\n",
      "Losgistic Regression(    4600/10000): loss= 1568.16594859583\n",
      "Losgistic Regression(    4700/10000): loss= 1563.18709922229\n",
      "Losgistic Regression(    4800/10000): loss= 1558.01501669423\n",
      "Losgistic Regression(    4900/10000): loss= 1552.92054496777\n",
      "Losgistic Regression(    5000/10000): loss= 1547.83525548711\n",
      "Losgistic Regression(    5100/10000): loss= 1542.6628161257\n",
      "Losgistic Regression(    5200/10000): loss= 1537.40714750125\n",
      "Losgistic Regression(    5300/10000): loss= 1532.10155604455\n",
      "Losgistic Regression(    5400/10000): loss= 1526.73696402422\n",
      "Losgistic Regression(    5500/10000): loss= 1521.20225720934\n",
      "Losgistic Regression(    5600/10000): loss= 1515.54324882685\n",
      "Losgistic Regression(    5700/10000): loss= 1509.92242738186\n",
      "Losgistic Regression(    5800/10000): loss= 1504.27679330881\n",
      "Losgistic Regression(    5900/10000): loss= 1498.50054665893\n",
      "Losgistic Regression(    6000/10000): loss= 1492.67709208187\n",
      "Losgistic Regression(    6100/10000): loss= 1486.87710278394\n",
      "Losgistic Regression(    6200/10000): loss= 1481.03438975966\n",
      "Losgistic Regression(    6300/10000): loss= 1475.10358840692\n",
      "Losgistic Regression(    6400/10000): loss= 1469.15539125857\n",
      "Losgistic Regression(    6500/10000): loss= 1463.2452489606\n",
      "Losgistic Regression(    6600/10000): loss= 1457.32826188421\n",
      "Losgistic Regression(    6700/10000): loss= 1451.39596584417\n",
      "Losgistic Regression(    6800/10000): loss= 1445.52397558467\n",
      "Losgistic Regression(    6900/10000): loss= 1439.71228211422\n",
      "Losgistic Regression(    7000/10000): loss= 1433.85713095119\n",
      "Losgistic Regression(    7100/10000): loss= 1427.98413844219\n",
      "Losgistic Regression(    7200/10000): loss= 1422.15328076399\n",
      "Losgistic Regression(    7300/10000): loss= 1416.32772975411\n",
      "Losgistic Regression(    7400/10000): loss= 1410.51414866942\n",
      "Losgistic Regression(    7500/10000): loss= 1404.76313139098\n",
      "Losgistic Regression(    7600/10000): loss= 1399.09187594771\n",
      "Losgistic Regression(    7700/10000): loss= 1393.45218213325\n",
      "Losgistic Regression(    7800/10000): loss= 1387.84032535804\n",
      "Losgistic Regression(    7900/10000): loss= 1382.29608105021\n",
      "Losgistic Regression(    8000/10000): loss= 1376.79109833637\n",
      "Losgistic Regression(    8100/10000): loss= 1371.30021525541\n",
      "Losgistic Regression(    8200/10000): loss= 1365.84828538222\n",
      "Losgistic Regression(    8300/10000): loss= 1360.46307720325\n",
      "Losgistic Regression(    8400/10000): loss= 1355.13269507566\n",
      "Losgistic Regression(    8500/10000): loss= 1349.84887288633\n",
      "Losgistic Regression(    8600/10000): loss= 1344.64857985853\n",
      "Losgistic Regression(    8700/10000): loss= 1339.55326307925\n",
      "Losgistic Regression(    8800/10000): loss= 1334.53389137172\n",
      "Losgistic Regression(    8900/10000): loss= 1329.59541897115\n",
      "Losgistic Regression(    9000/10000): loss= 1324.77718092465\n",
      "Losgistic Regression(    9100/10000): loss= 1320.06212530122\n",
      "Losgistic Regression(    9200/10000): loss= 1315.41128639641\n",
      "Losgistic Regression(    9300/10000): loss= 1310.83217881163\n",
      "Losgistic Regression(    9400/10000): loss= 1306.33970665892\n",
      "Losgistic Regression(    9500/10000): loss= 1301.91684568409\n",
      "Losgistic Regression(    9600/10000): loss= 1297.57311541336\n",
      "Losgistic Regression(    9700/10000): loss= 1293.32974610669\n",
      "Losgistic Regression(    9800/10000): loss= 1289.16648951204\n",
      "Losgistic Regression(    9900/10000): loss= 1285.07391652835\n",
      "Time for  2th cross validation = 164.273s\n",
      "Training Accuracy         = 0.8564\n",
      "Cross Validation Accuracy = 0.803584\n",
      "Losgistic Regression(       0/10000): loss= 3439.00528690578\n",
      "Losgistic Regression(     100/10000): loss= 2328.79488663219\n",
      "Losgistic Regression(     200/10000): loss= 2134.83791880897\n",
      "Losgistic Regression(     300/10000): loss= 2056.15492651242\n",
      "Losgistic Regression(     400/10000): loss= 2002.3375900623\n",
      "Losgistic Regression(     500/10000): loss= 1960.48019530323\n",
      "Losgistic Regression(     600/10000): loss= 1927.11519641434\n",
      "Losgistic Regression(     700/10000): loss= 1899.8995910392\n",
      "Losgistic Regression(     800/10000): loss= 1877.34095073868\n",
      "Losgistic Regression(     900/10000): loss= 1858.21333569949\n",
      "Losgistic Regression(    1000/10000): loss= 1841.62456294567\n",
      "Losgistic Regression(    1100/10000): loss= 1827.01552996407\n",
      "Losgistic Regression(    1200/10000): loss= 1813.41234259787\n",
      "Losgistic Regression(    1300/10000): loss= 1800.59989014435\n",
      "Losgistic Regression(    1400/10000): loss= 1789.27147701506\n",
      "Losgistic Regression(    1500/10000): loss= 1779.1144616045\n",
      "Losgistic Regression(    1600/10000): loss= 1769.77613424221\n",
      "Losgistic Regression(    1700/10000): loss= 1760.21836806269\n",
      "Losgistic Regression(    1800/10000): loss= 1751.47005986354\n",
      "Losgistic Regression(    1900/10000): loss= 1743.39776699156\n",
      "Losgistic Regression(    2000/10000): loss= 1735.97315824014\n",
      "Losgistic Regression(    2100/10000): loss= 1729.12281318908\n",
      "Losgistic Regression(    2200/10000): loss= 1722.8118760268\n",
      "Losgistic Regression(    2300/10000): loss= 1717.12401275328\n",
      "Losgistic Regression(    2400/10000): loss= 1712.07245333968\n",
      "Losgistic Regression(    2500/10000): loss= 1707.17641584497\n",
      "Losgistic Regression(    2600/10000): loss= 1702.71191740794\n",
      "Losgistic Regression(    2700/10000): loss= 1698.52239792609\n",
      "Losgistic Regression(    2800/10000): loss= 1694.51923742025\n",
      "Losgistic Regression(    2900/10000): loss= 1690.68521558929\n",
      "Losgistic Regression(    3000/10000): loss= 1687.03782045771\n",
      "Losgistic Regression(    3100/10000): loss= 1683.57223098584\n",
      "Losgistic Regression(    3200/10000): loss= 1680.21894426969\n",
      "Losgistic Regression(    3300/10000): loss= 1676.91171781211\n",
      "Losgistic Regression(    3400/10000): loss= 1673.65660664547\n",
      "Losgistic Regression(    3500/10000): loss= 1670.47717589862\n",
      "Losgistic Regression(    3600/10000): loss= 1667.40001255396\n",
      "Losgistic Regression(    3700/10000): loss= 1664.39879111987\n",
      "Losgistic Regression(    3800/10000): loss= 1661.39907559465\n",
      "Losgistic Regression(    3900/10000): loss= 1658.35272293618\n",
      "Losgistic Regression(    4000/10000): loss= 1655.26166318948\n",
      "Losgistic Regression(    4100/10000): loss= 1652.16695762223\n",
      "Losgistic Regression(    4200/10000): loss= 1649.0997920463\n",
      "Losgistic Regression(    4300/10000): loss= 1646.11684200565\n",
      "Losgistic Regression(    4400/10000): loss= 1643.24478911419\n",
      "Losgistic Regression(    4500/10000): loss= 1640.05012629495\n",
      "Losgistic Regression(    4600/10000): loss= 1637.16042069919\n",
      "Losgistic Regression(    4700/10000): loss= 1634.64068821454\n",
      "Losgistic Regression(    4800/10000): loss= 1631.82698602058\n",
      "Losgistic Regression(    4900/10000): loss= 1628.8929545992\n",
      "Losgistic Regression(    5000/10000): loss= 1626.25790198256\n",
      "Losgistic Regression(    5100/10000): loss= 1623.75834961872\n",
      "Losgistic Regression(    5200/10000): loss= 1620.83884331507\n",
      "Losgistic Regression(    5300/10000): loss= 1617.77758090965\n",
      "Losgistic Regression(    5400/10000): loss= 1614.7358296316\n",
      "Losgistic Regression(    5500/10000): loss= 1611.64864343722\n",
      "Losgistic Regression(    5600/10000): loss= 1608.14358319844\n",
      "Losgistic Regression(    5700/10000): loss= 1605.09606041397\n",
      "Losgistic Regression(    5800/10000): loss= 1602.73823990388\n",
      "Losgistic Regression(    5900/10000): loss= 1600.57808494445\n",
      "Losgistic Regression(    6000/10000): loss= 1598.38241222244\n",
      "Losgistic Regression(    6100/10000): loss= 1596.0684281714\n",
      "Losgistic Regression(    6200/10000): loss= 1593.48895484757\n",
      "Losgistic Regression(    6300/10000): loss= 1591.01166522968\n",
      "Losgistic Regression(    6400/10000): loss= 1588.7756359836\n",
      "Losgistic Regression(    6500/10000): loss= 1586.37124776157\n",
      "Losgistic Regression(    6600/10000): loss= 1583.92357556209\n",
      "Losgistic Regression(    6700/10000): loss= 1581.9741340755\n",
      "Losgistic Regression(    6800/10000): loss= 1580.32517723907\n",
      "Losgistic Regression(    6900/10000): loss= 1578.68085931451\n",
      "Losgistic Regression(    7000/10000): loss= 1576.85471427022\n",
      "Losgistic Regression(    7100/10000): loss= 1575.02624556886\n",
      "Losgistic Regression(    7200/10000): loss= 1573.15751167642\n",
      "Losgistic Regression(    7300/10000): loss= 1570.93326176416\n",
      "Losgistic Regression(    7400/10000): loss= 1568.76540718703\n",
      "Losgistic Regression(    7500/10000): loss= 1566.70076973592\n",
      "Losgistic Regression(    7600/10000): loss= 1564.590142311\n",
      "Losgistic Regression(    7700/10000): loss= 1562.79728661375\n",
      "Losgistic Regression(    7800/10000): loss= 1561.47557664877\n",
      "Losgistic Regression(    7900/10000): loss= 1560.38380467511\n",
      "Losgistic Regression(    8000/10000): loss= 1559.42323784201\n",
      "Losgistic Regression(    8100/10000): loss= 1558.65761334526\n",
      "Losgistic Regression(    8200/10000): loss= 1558.16933916356\n",
      "Losgistic Regression(    8300/10000): loss= 1557.77463254534\n",
      "Losgistic Regression(    8400/10000): loss= 1557.20797238477\n",
      "Losgistic Regression(    8500/10000): loss= 1556.43810449063\n",
      "Losgistic Regression(    8600/10000): loss= 1555.62346135706\n",
      "Losgistic Regression(    8700/10000): loss= 1555.02197719129\n",
      "Losgistic Regression(    8800/10000): loss= 1554.60813514468\n",
      "Losgistic Regression(    8900/10000): loss= 1554.36321079999\n",
      "Losgistic Regression(    9000/10000): loss= 1554.33950646614\n",
      "Losgistic Regression(    9100/10000): loss= 1554.34101456991\n",
      "Totoal number of iterations =  9100\n",
      "Loss                        =  1554.34101457\n",
      "Time for  3th cross validation = 150.926s\n",
      "Training Accuracy         = 0.8536\n",
      "Cross Validation Accuracy = 0.805448\n",
      "Losgistic Regression(       0/10000): loss= 3439.90417004716\n",
      "Losgistic Regression(     100/10000): loss= 2309.4106845367\n",
      "Losgistic Regression(     200/10000): loss= 2119.9554498385\n",
      "Losgistic Regression(     300/10000): loss= 2036.53170665932\n",
      "Losgistic Regression(     400/10000): loss= 1967.83224815826\n",
      "Losgistic Regression(     500/10000): loss= 1910.99633104121\n",
      "Losgistic Regression(     600/10000): loss= 1865.19065834839\n",
      "Losgistic Regression(     700/10000): loss= 1826.41836410906\n",
      "Losgistic Regression(     800/10000): loss= 1791.90585990756\n",
      "Losgistic Regression(     900/10000): loss= 1765.16853211037\n",
      "Losgistic Regression(    1000/10000): loss= 1743.91711534188\n",
      "Losgistic Regression(    1100/10000): loss= 1724.43052195393\n",
      "Losgistic Regression(    1200/10000): loss= 1709.13833290662\n",
      "Losgistic Regression(    1300/10000): loss= 1696.45394380393\n",
      "Losgistic Regression(    1400/10000): loss= 1685.1503601362\n",
      "Losgistic Regression(    1500/10000): loss= 1676.63195455905\n",
      "Losgistic Regression(    1600/10000): loss= 1670.44264393253\n",
      "Losgistic Regression(    1700/10000): loss= 1665.29396401083\n",
      "Losgistic Regression(    1800/10000): loss= 1661.28331054897\n",
      "Losgistic Regression(    1900/10000): loss= 1658.11596751886\n",
      "Losgistic Regression(    2000/10000): loss= 1654.43093624709\n",
      "Losgistic Regression(    2100/10000): loss= 1651.06223422191\n",
      "Losgistic Regression(    2200/10000): loss= 1648.33407817813\n",
      "Losgistic Regression(    2300/10000): loss= 1646.0570781432\n",
      "Losgistic Regression(    2400/10000): loss= 1644.20454394571\n",
      "Losgistic Regression(    2500/10000): loss= 1642.74085048374\n",
      "Losgistic Regression(    2600/10000): loss= 1641.36022774254\n",
      "Losgistic Regression(    2700/10000): loss= 1640.13042141143\n",
      "Losgistic Regression(    2800/10000): loss= 1639.11640498458\n",
      "Losgistic Regression(    2900/10000): loss= 1638.18001301199\n",
      "Losgistic Regression(    3000/10000): loss= 1637.32284849347\n",
      "Losgistic Regression(    3100/10000): loss= 1636.64130125888\n",
      "Losgistic Regression(    3200/10000): loss= 1635.9710374858\n",
      "Losgistic Regression(    3300/10000): loss= 1635.22662470988\n",
      "Losgistic Regression(    3400/10000): loss= 1634.42677057737\n",
      "Losgistic Regression(    3500/10000): loss= 1633.64813619322\n",
      "Losgistic Regression(    3600/10000): loss= 1632.84745839736\n",
      "Losgistic Regression(    3700/10000): loss= 1631.97933925738\n",
      "Losgistic Regression(    3800/10000): loss= 1631.05828776349\n",
      "Losgistic Regression(    3900/10000): loss= 1630.1552093748\n",
      "Losgistic Regression(    4000/10000): loss= 1629.24515175974\n",
      "Losgistic Regression(    4100/10000): loss= 1628.31733920084\n",
      "Losgistic Regression(    4200/10000): loss= 1627.43804679999\n",
      "Losgistic Regression(    4300/10000): loss= 1626.55744408434\n",
      "Losgistic Regression(    4400/10000): loss= 1625.63945193809\n",
      "Losgistic Regression(    4500/10000): loss= 1624.70331234986\n",
      "Losgistic Regression(    4600/10000): loss= 1623.75204915803\n",
      "Losgistic Regression(    4700/10000): loss= 1622.81460652108\n",
      "Losgistic Regression(    4800/10000): loss= 1621.92522557378\n",
      "Losgistic Regression(    4900/10000): loss= 1621.07603894497\n",
      "Losgistic Regression(    5000/10000): loss= 1620.24870608116\n",
      "Losgistic Regression(    5100/10000): loss= 1619.43592579019\n",
      "Losgistic Regression(    5200/10000): loss= 1618.62479678645\n",
      "Losgistic Regression(    5300/10000): loss= 1617.70064148697\n",
      "Losgistic Regression(    5400/10000): loss= 1616.7827626426\n",
      "Losgistic Regression(    5500/10000): loss= 1615.9042968322\n",
      "Losgistic Regression(    5600/10000): loss= 1615.05901294711\n",
      "Losgistic Regression(    5700/10000): loss= 1614.26708132382\n",
      "Losgistic Regression(    5800/10000): loss= 1613.54226281013\n",
      "Losgistic Regression(    5900/10000): loss= 1612.87738746566\n",
      "Losgistic Regression(    6000/10000): loss= 1612.26711102091\n",
      "Losgistic Regression(    6100/10000): loss= 1611.72849157714\n",
      "Losgistic Regression(    6200/10000): loss= 1611.26659568057\n",
      "Losgistic Regression(    6300/10000): loss= 1610.88213093624\n",
      "Losgistic Regression(    6400/10000): loss= 1610.57940785165\n",
      "Losgistic Regression(    6500/10000): loss= 1610.3540115578\n",
      "Losgistic Regression(    6600/10000): loss= 1610.2032302512\n",
      "Losgistic Regression(    6700/10000): loss= 1610.12381176973\n",
      "Losgistic Regression(    6800/10000): loss= 1610.11103211613\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  1610.11103212\n",
      "Time for  4th cross validation = 111.459s\n",
      "Training Accuracy         = 0.8542\n",
      "Cross Validation Accuracy = 0.805876\n",
      "*************** ([0.85899999999999999, 0.85040000000000004, 0.85640000000000005, 0.85360000000000003, 0.85419999999999996], [0.80532800000000004, 0.79402799999999996, 0.80358399999999996, 0.80544800000000005, 0.80587600000000004])\n",
      "Losgistic Regression(       0/10000): loss= 3436.25677405945\n",
      "Losgistic Regression(     100/10000): loss= 2335.86980196649\n",
      "Losgistic Regression(     200/10000): loss= 2121.80042804355\n",
      "Losgistic Regression(     300/10000): loss= 2040.23171350559\n",
      "Losgistic Regression(     400/10000): loss= 1984.00662050533\n",
      "Losgistic Regression(     500/10000): loss= 1941.95608750388\n",
      "Losgistic Regression(     600/10000): loss= 1908.66773481697\n",
      "Losgistic Regression(     700/10000): loss= 1881.37729513754\n",
      "Losgistic Regression(     800/10000): loss= 1858.85887471543\n",
      "Losgistic Regression(     900/10000): loss= 1840.17800325565\n",
      "Losgistic Regression(    1000/10000): loss= 1824.31236919769\n",
      "Losgistic Regression(    1100/10000): loss= 1810.48163889281\n",
      "Losgistic Regression(    1200/10000): loss= 1798.221789176\n",
      "Losgistic Regression(    1300/10000): loss= 1787.18793842039\n",
      "Losgistic Regression(    1400/10000): loss= 1777.39961121762\n",
      "Losgistic Regression(    1500/10000): loss= 1769.20695523461\n",
      "Losgistic Regression(    1600/10000): loss= 1761.74485837634\n",
      "Losgistic Regression(    1700/10000): loss= 1755.19190381914\n",
      "Losgistic Regression(    1800/10000): loss= 1749.40206834062\n",
      "Losgistic Regression(    1900/10000): loss= 1744.26100617087\n",
      "Losgistic Regression(    2000/10000): loss= 1739.68558049042\n",
      "Losgistic Regression(    2100/10000): loss= 1735.56010598771\n",
      "Losgistic Regression(    2200/10000): loss= 1731.77318562011\n",
      "Losgistic Regression(    2300/10000): loss= 1728.27439875978\n",
      "Losgistic Regression(    2400/10000): loss= 1724.83220212954\n",
      "Losgistic Regression(    2500/10000): loss= 1721.75074455585\n",
      "Losgistic Regression(    2600/10000): loss= 1718.89890369087\n",
      "Losgistic Regression(    2700/10000): loss= 1715.72021925446\n",
      "Losgistic Regression(    2800/10000): loss= 1712.80756308456\n",
      "Losgistic Regression(    2900/10000): loss= 1710.13340137883\n",
      "Losgistic Regression(    3000/10000): loss= 1707.62448826364\n",
      "Losgistic Regression(    3100/10000): loss= 1705.31691883618\n",
      "Losgistic Regression(    3200/10000): loss= 1703.21156040569\n",
      "Losgistic Regression(    3300/10000): loss= 1701.13386489059\n",
      "Losgistic Regression(    3400/10000): loss= 1698.73701365915\n",
      "Losgistic Regression(    3500/10000): loss= 1696.35347050221\n",
      "Losgistic Regression(    3600/10000): loss= 1694.04706969958\n",
      "Losgistic Regression(    3700/10000): loss= 1691.65874567493\n",
      "Losgistic Regression(    3800/10000): loss= 1689.28799833783\n",
      "Losgistic Regression(    3900/10000): loss= 1686.96754300216\n",
      "Losgistic Regression(    4000/10000): loss= 1684.6467338772\n",
      "Losgistic Regression(    4100/10000): loss= 1682.31898873522\n",
      "Losgistic Regression(    4200/10000): loss= 1679.99398233179\n",
      "Losgistic Regression(    4300/10000): loss= 1677.67903424363\n",
      "Losgistic Regression(    4400/10000): loss= 1675.39239181833\n",
      "Losgistic Regression(    4500/10000): loss= 1673.14952961517\n",
      "Losgistic Regression(    4600/10000): loss= 1670.94470312156\n",
      "Losgistic Regression(    4700/10000): loss= 1668.78688920491\n",
      "Losgistic Regression(    4800/10000): loss= 1666.4763413327\n",
      "Losgistic Regression(    4900/10000): loss= 1664.21406516244\n",
      "Losgistic Regression(    5000/10000): loss= 1661.98809809092\n",
      "Losgistic Regression(    5100/10000): loss= 1659.88843043356\n",
      "Losgistic Regression(    5200/10000): loss= 1657.88605141405\n",
      "Losgistic Regression(    5300/10000): loss= 1656.02326624131\n",
      "Losgistic Regression(    5400/10000): loss= 1654.33288531413\n",
      "Losgistic Regression(    5500/10000): loss= 1652.76118529306\n",
      "Losgistic Regression(    5600/10000): loss= 1651.24692586676\n",
      "Losgistic Regression(    5700/10000): loss= 1649.79406707945\n",
      "Losgistic Regression(    5800/10000): loss= 1648.44012669257\n",
      "Losgistic Regression(    5900/10000): loss= 1647.17691164055\n",
      "Losgistic Regression(    6000/10000): loss= 1645.9562912252\n",
      "Losgistic Regression(    6100/10000): loss= 1644.76142984364\n",
      "Losgistic Regression(    6200/10000): loss= 1643.60984896567\n",
      "Losgistic Regression(    6300/10000): loss= 1642.55656217674\n",
      "Losgistic Regression(    6400/10000): loss= 1641.3855636109\n",
      "Losgistic Regression(    6500/10000): loss= 1640.25367100707\n",
      "Losgistic Regression(    6600/10000): loss= 1639.20817870427\n",
      "Losgistic Regression(    6700/10000): loss= 1638.23895740241\n",
      "Losgistic Regression(    6800/10000): loss= 1637.33220216111\n",
      "Losgistic Regression(    6900/10000): loss= 1636.4989810444\n",
      "Losgistic Regression(    7000/10000): loss= 1635.73310621339\n",
      "Losgistic Regression(    7100/10000): loss= 1635.00625166989\n",
      "Losgistic Regression(    7200/10000): loss= 1634.28312651377\n",
      "Losgistic Regression(    7300/10000): loss= 1633.57174158869\n",
      "Losgistic Regression(    7400/10000): loss= 1632.89953866676\n",
      "Losgistic Regression(    7500/10000): loss= 1632.25469956931\n",
      "Losgistic Regression(    7600/10000): loss= 1631.64461395293\n",
      "Losgistic Regression(    7700/10000): loss= 1631.05275368144\n",
      "Losgistic Regression(    7800/10000): loss= 1630.42750304601\n",
      "Losgistic Regression(    7900/10000): loss= 1629.84587856403\n",
      "Losgistic Regression(    8000/10000): loss= 1629.31451757153\n",
      "Losgistic Regression(    8100/10000): loss= 1628.85183903196\n",
      "Losgistic Regression(    8200/10000): loss= 1628.41803797457\n",
      "Losgistic Regression(    8300/10000): loss= 1628.01695796498\n",
      "Losgistic Regression(    8400/10000): loss= 1627.63229254191\n",
      "Losgistic Regression(    8500/10000): loss= 1627.23841024447\n",
      "Losgistic Regression(    8600/10000): loss= 1626.9186794582\n",
      "Losgistic Regression(    8700/10000): loss= 1626.66981572356\n",
      "Losgistic Regression(    8800/10000): loss= 1626.4630315708\n",
      "Losgistic Regression(    8900/10000): loss= 1626.27517178574\n",
      "Losgistic Regression(    9000/10000): loss= 1626.09341700962\n",
      "Losgistic Regression(    9100/10000): loss= 1625.91973534606\n",
      "Losgistic Regression(    9200/10000): loss= 1625.79044833181\n",
      "Losgistic Regression(    9300/10000): loss= 1625.66250573245\n",
      "Losgistic Regression(    9400/10000): loss= 1625.50616954028\n",
      "Losgistic Regression(    9500/10000): loss= 1625.36115503191\n",
      "Losgistic Regression(    9600/10000): loss= 1625.21831705335\n",
      "Losgistic Regression(    9700/10000): loss= 1625.07325198847\n",
      "Losgistic Regression(    9800/10000): loss= 1624.94536353975\n",
      "Losgistic Regression(    9900/10000): loss= 1624.80086214702\n",
      "Time for  0th cross validation = 163.531s\n",
      "Training Accuracy         = 0.8584\n",
      "Cross Validation Accuracy = 0.805716\n",
      "Losgistic Regression(       0/10000): loss= 3438.20461692028\n",
      "Losgistic Regression(     100/10000): loss= 2369.10240163811\n",
      "Losgistic Regression(     200/10000): loss= 2165.75408897889\n",
      "Losgistic Regression(     300/10000): loss= 2075.09826115574\n",
      "Losgistic Regression(     400/10000): loss= 2017.71951411503\n",
      "Losgistic Regression(     500/10000): loss= 1974.32738494594\n",
      "Losgistic Regression(     600/10000): loss= 1938.49099446413\n",
      "Losgistic Regression(     700/10000): loss= 1909.75877037112\n",
      "Losgistic Regression(     800/10000): loss= 1884.43328787432\n",
      "Losgistic Regression(     900/10000): loss= 1862.245503604\n",
      "Losgistic Regression(    1000/10000): loss= 1843.56697801453\n",
      "Losgistic Regression(    1100/10000): loss= 1827.75083138911\n",
      "Losgistic Regression(    1200/10000): loss= 1814.4364878671\n",
      "Losgistic Regression(    1300/10000): loss= 1802.94863606913\n",
      "Losgistic Regression(    1400/10000): loss= 1792.73068745835\n",
      "Losgistic Regression(    1500/10000): loss= 1783.64808584376\n",
      "Losgistic Regression(    1600/10000): loss= 1775.46323095904\n",
      "Losgistic Regression(    1700/10000): loss= 1768.01234931218\n",
      "Losgistic Regression(    1800/10000): loss= 1761.15434017505\n",
      "Losgistic Regression(    1900/10000): loss= 1754.90612867594\n",
      "Losgistic Regression(    2000/10000): loss= 1749.28699331786\n",
      "Losgistic Regression(    2100/10000): loss= 1744.20454826549\n",
      "Losgistic Regression(    2200/10000): loss= 1739.55674123883\n",
      "Losgistic Regression(    2300/10000): loss= 1735.31823986773\n",
      "Losgistic Regression(    2400/10000): loss= 1731.43691314594\n",
      "Losgistic Regression(    2500/10000): loss= 1727.84871214146\n",
      "Losgistic Regression(    2600/10000): loss= 1724.56541652757\n",
      "Losgistic Regression(    2700/10000): loss= 1721.58235816886\n",
      "Losgistic Regression(    2800/10000): loss= 1718.86166778866\n",
      "Losgistic Regression(    2900/10000): loss= 1716.38185102627\n",
      "Losgistic Regression(    3000/10000): loss= 1714.11686545998\n",
      "Losgistic Regression(    3100/10000): loss= 1712.00784216631\n",
      "Losgistic Regression(    3200/10000): loss= 1710.00732694955\n",
      "Losgistic Regression(    3300/10000): loss= 1708.11312760268\n",
      "Losgistic Regression(    3400/10000): loss= 1706.30956322188\n",
      "Losgistic Regression(    3500/10000): loss= 1704.54578605668\n",
      "Losgistic Regression(    3600/10000): loss= 1702.83304060095\n",
      "Losgistic Regression(    3700/10000): loss= 1701.18149678249\n",
      "Losgistic Regression(    3800/10000): loss= 1699.57714659854\n",
      "Losgistic Regression(    3900/10000): loss= 1698.0198602073\n",
      "Losgistic Regression(    4000/10000): loss= 1696.50662801712\n",
      "Losgistic Regression(    4100/10000): loss= 1694.93789555558\n",
      "Losgistic Regression(    4200/10000): loss= 1692.72750166909\n",
      "Losgistic Regression(    4300/10000): loss= 1690.50171349774\n",
      "Losgistic Regression(    4400/10000): loss= 1688.36609697662\n",
      "Losgistic Regression(    4500/10000): loss= 1686.34802117035\n",
      "Losgistic Regression(    4600/10000): loss= 1684.41949619112\n",
      "Losgistic Regression(    4700/10000): loss= 1682.55608482756\n",
      "Losgistic Regression(    4800/10000): loss= 1680.78911941895\n",
      "Losgistic Regression(    4900/10000): loss= 1679.14334804781\n",
      "Losgistic Regression(    5000/10000): loss= 1677.59001905163\n",
      "Losgistic Regression(    5100/10000): loss= 1676.09611702748\n",
      "Losgistic Regression(    5200/10000): loss= 1674.57017994031\n",
      "Losgistic Regression(    5300/10000): loss= 1673.04104617239\n",
      "Losgistic Regression(    5400/10000): loss= 1671.52497867699\n",
      "Losgistic Regression(    5500/10000): loss= 1669.98504125882\n",
      "Losgistic Regression(    5600/10000): loss= 1668.77193223079\n",
      "Losgistic Regression(    5700/10000): loss= 1667.93896049891\n",
      "Losgistic Regression(    5800/10000): loss= 1667.19216926031\n",
      "Losgistic Regression(    5900/10000): loss= 1666.16716729588\n",
      "Losgistic Regression(    6000/10000): loss= 1665.109395439\n",
      "Losgistic Regression(    6100/10000): loss= 1664.18351500628\n",
      "Losgistic Regression(    6200/10000): loss= 1663.35751310715\n",
      "Losgistic Regression(    6300/10000): loss= 1662.59466534645\n",
      "Losgistic Regression(    6400/10000): loss= 1661.87261649635\n",
      "Losgistic Regression(    6500/10000): loss= 1661.27834100281\n",
      "Losgistic Regression(    6600/10000): loss= 1660.56053223174\n",
      "Losgistic Regression(    6700/10000): loss= 1659.99122441812\n",
      "Losgistic Regression(    6800/10000): loss= 1659.57439612135\n",
      "Losgistic Regression(    6900/10000): loss= 1659.1551211978\n",
      "Losgistic Regression(    7000/10000): loss= 1658.64419381797\n",
      "Losgistic Regression(    7100/10000): loss= 1658.24785490398\n",
      "Losgistic Regression(    7200/10000): loss= 1658.14096729252\n",
      "Losgistic Regression(    7300/10000): loss= 1658.14235005809\n",
      "Totoal number of iterations =  7300\n",
      "Loss                        =  1658.14235006\n",
      "Time for  1th cross validation = 119.649s\n",
      "Training Accuracy         = 0.8492\n",
      "Cross Validation Accuracy = 0.794768\n",
      "Losgistic Regression(       0/10000): loss= 3438.57003306499\n",
      "Losgistic Regression(     100/10000): loss= 2321.9325393893\n",
      "Losgistic Regression(     200/10000): loss= 2117.08580333844\n",
      "Losgistic Regression(     300/10000): loss= 2026.71425373974\n",
      "Losgistic Regression(     400/10000): loss= 1965.69545935213\n",
      "Losgistic Regression(     500/10000): loss= 1920.99839778236\n",
      "Losgistic Regression(     600/10000): loss= 1886.90874307486\n",
      "Losgistic Regression(     700/10000): loss= 1859.82954648904\n",
      "Losgistic Regression(     800/10000): loss= 1837.7862932511\n",
      "Losgistic Regression(     900/10000): loss= 1819.71808306069\n",
      "Losgistic Regression(    1000/10000): loss= 1804.70047136261\n",
      "Losgistic Regression(    1100/10000): loss= 1792.03412143283\n",
      "Losgistic Regression(    1200/10000): loss= 1779.82454708483\n",
      "Losgistic Regression(    1300/10000): loss= 1768.23802634108\n",
      "Losgistic Regression(    1400/10000): loss= 1757.91507158973\n",
      "Losgistic Regression(    1500/10000): loss= 1748.66523493717\n",
      "Losgistic Regression(    1600/10000): loss= 1740.29538967969\n",
      "Losgistic Regression(    1700/10000): loss= 1732.34890890956\n",
      "Losgistic Regression(    1800/10000): loss= 1724.84229396815\n",
      "Losgistic Regression(    1900/10000): loss= 1717.96683519455\n",
      "Losgistic Regression(    2000/10000): loss= 1711.6149155481\n",
      "Losgistic Regression(    2100/10000): loss= 1705.69411013213\n",
      "Losgistic Regression(    2200/10000): loss= 1700.14557127645\n",
      "Losgistic Regression(    2300/10000): loss= 1694.92691739829\n",
      "Losgistic Regression(    2400/10000): loss= 1689.78873670895\n",
      "Losgistic Regression(    2500/10000): loss= 1684.46845672906\n",
      "Losgistic Regression(    2600/10000): loss= 1679.39226644483\n",
      "Losgistic Regression(    2700/10000): loss= 1674.80651747311\n",
      "Losgistic Regression(    2800/10000): loss= 1670.19795218693\n",
      "Losgistic Regression(    2900/10000): loss= 1665.23698146726\n",
      "Losgistic Regression(    3000/10000): loss= 1660.60927778751\n",
      "Losgistic Regression(    3100/10000): loss= 1656.00750851585\n",
      "Losgistic Regression(    3200/10000): loss= 1651.23034876555\n",
      "Losgistic Regression(    3300/10000): loss= 1646.87908573046\n",
      "Losgistic Regression(    3400/10000): loss= 1642.71128536844\n",
      "Losgistic Regression(    3500/10000): loss= 1638.24713594737\n",
      "Losgistic Regression(    3600/10000): loss= 1633.90338749287\n",
      "Losgistic Regression(    3700/10000): loss= 1629.45863693281\n",
      "Losgistic Regression(    3800/10000): loss= 1624.69929240448\n",
      "Losgistic Regression(    3900/10000): loss= 1619.98178610782\n",
      "Losgistic Regression(    4000/10000): loss= 1615.36362360688\n",
      "Losgistic Regression(    4100/10000): loss= 1610.68811500775\n",
      "Losgistic Regression(    4200/10000): loss= 1606.13888277486\n",
      "Losgistic Regression(    4300/10000): loss= 1601.75039488196\n",
      "Losgistic Regression(    4400/10000): loss= 1597.40536794894\n",
      "Losgistic Regression(    4500/10000): loss= 1593.00407583391\n",
      "Losgistic Regression(    4600/10000): loss= 1588.56061520212\n",
      "Losgistic Regression(    4700/10000): loss= 1584.0839761891\n",
      "Losgistic Regression(    4800/10000): loss= 1579.58156558987\n",
      "Losgistic Regression(    4900/10000): loss= 1574.89016712996\n",
      "Losgistic Regression(    5000/10000): loss= 1570.17988941911\n",
      "Losgistic Regression(    5100/10000): loss= 1565.44904006939\n",
      "Losgistic Regression(    5200/10000): loss= 1560.62691750104\n",
      "Losgistic Regression(    5300/10000): loss= 1555.78252943817\n",
      "Losgistic Regression(    5400/10000): loss= 1550.90371879168\n",
      "Losgistic Regression(    5500/10000): loss= 1545.85612363749\n",
      "Losgistic Regression(    5600/10000): loss= 1540.73363111031\n",
      "Losgistic Regression(    5700/10000): loss= 1535.67506776233\n",
      "Losgistic Regression(    5800/10000): loss= 1530.54667843945\n",
      "Losgistic Regression(    5900/10000): loss= 1525.27728621454\n",
      "Losgistic Regression(    6000/10000): loss= 1520.00369232475\n",
      "Losgistic Regression(    6100/10000): loss= 1514.74588342375\n",
      "Losgistic Regression(    6200/10000): loss= 1509.40978793809\n",
      "Losgistic Regression(    6300/10000): loss= 1504.00303441639\n",
      "Losgistic Regression(    6400/10000): loss= 1498.61829691527\n",
      "Losgistic Regression(    6500/10000): loss= 1493.25473075319\n",
      "Losgistic Regression(    6600/10000): loss= 1487.82385605029\n",
      "Losgistic Regression(    6700/10000): loss= 1482.3963301437\n",
      "Losgistic Regression(    6800/10000): loss= 1477.02736596239\n",
      "Losgistic Regression(    6900/10000): loss= 1471.65481250419\n",
      "Losgistic Regression(    7000/10000): loss= 1466.22989403218\n",
      "Losgistic Regression(    7100/10000): loss= 1460.80665666794\n",
      "Losgistic Regression(    7200/10000): loss= 1455.3871928663\n",
      "Losgistic Regression(    7300/10000): loss= 1449.93291726762\n",
      "Losgistic Regression(    7400/10000): loss= 1444.51669879306\n",
      "Losgistic Regression(    7500/10000): loss= 1439.15824808687\n",
      "Losgistic Regression(    7600/10000): loss= 1433.84640557236\n",
      "Losgistic Regression(    7700/10000): loss= 1428.56030348499\n",
      "Losgistic Regression(    7800/10000): loss= 1423.30535952637\n",
      "Losgistic Regression(    7900/10000): loss= 1418.10808040588\n",
      "Losgistic Regression(    8000/10000): loss= 1412.90818447764\n",
      "Losgistic Regression(    8100/10000): loss= 1407.72180528211\n",
      "Losgistic Regression(    8200/10000): loss= 1402.59208450402\n",
      "Losgistic Regression(    8300/10000): loss= 1397.50937175095\n",
      "Losgistic Regression(    8400/10000): loss= 1392.44698416876\n",
      "Losgistic Regression(    8500/10000): loss= 1387.41760263588\n",
      "Losgistic Regression(    8600/10000): loss= 1382.48356426352\n",
      "Losgistic Regression(    8700/10000): loss= 1377.62679967\n",
      "Losgistic Regression(    8800/10000): loss= 1372.84156442324\n",
      "Losgistic Regression(    8900/10000): loss= 1368.15143098485\n",
      "Losgistic Regression(    9000/10000): loss= 1363.57234419351\n",
      "Losgistic Regression(    9100/10000): loss= 1359.08139712783\n",
      "Losgistic Regression(    9200/10000): loss= 1354.65702984486\n",
      "Losgistic Regression(    9300/10000): loss= 1350.32387030114\n",
      "Losgistic Regression(    9400/10000): loss= 1346.09577851685\n",
      "Losgistic Regression(    9500/10000): loss= 1341.9488302014\n",
      "Losgistic Regression(    9600/10000): loss= 1337.89832891546\n",
      "Losgistic Regression(    9700/10000): loss= 1333.98468213105\n",
      "Losgistic Regression(    9800/10000): loss= 1330.20219492108\n",
      "Losgistic Regression(    9900/10000): loss= 1326.52920589225\n",
      "Time for  2th cross validation = 163.619s\n",
      "Training Accuracy         = 0.8562\n",
      "Cross Validation Accuracy = 0.804136\n",
      "Losgistic Regression(       0/10000): loss= 3439.00821969135\n",
      "Losgistic Regression(     100/10000): loss= 2329.29758573702\n",
      "Losgistic Regression(     200/10000): loss= 2135.78046361515\n",
      "Losgistic Regression(     300/10000): loss= 2057.45646452031\n",
      "Losgistic Regression(     400/10000): loss= 2003.97925525691\n",
      "Losgistic Regression(     500/10000): loss= 1962.45759271285\n",
      "Losgistic Regression(     600/10000): loss= 1929.4244879401\n",
      "Losgistic Regression(     700/10000): loss= 1902.53943486734\n",
      "Losgistic Regression(     800/10000): loss= 1880.30672482938\n",
      "Losgistic Regression(     900/10000): loss= 1861.49058420775\n",
      "Losgistic Regression(    1000/10000): loss= 1845.21073963081\n",
      "Losgistic Regression(    1100/10000): loss= 1830.92104984872\n",
      "Losgistic Regression(    1200/10000): loss= 1817.65581131719\n",
      "Losgistic Regression(    1300/10000): loss= 1805.15962629651\n",
      "Losgistic Regression(    1400/10000): loss= 1794.15679097796\n",
      "Losgistic Regression(    1500/10000): loss= 1784.34263315446\n",
      "Losgistic Regression(    1600/10000): loss= 1775.49437989831\n",
      "Losgistic Regression(    1700/10000): loss= 1766.39081283313\n",
      "Losgistic Regression(    1800/10000): loss= 1757.99740594018\n",
      "Losgistic Regression(    1900/10000): loss= 1750.26070778678\n",
      "Losgistic Regression(    2000/10000): loss= 1743.15834594247\n",
      "Losgistic Regression(    2100/10000): loss= 1736.60585090714\n",
      "Losgistic Regression(    2200/10000): loss= 1730.56380414446\n",
      "Losgistic Regression(    2300/10000): loss= 1725.16846868092\n",
      "Losgistic Regression(    2400/10000): loss= 1720.42106507963\n",
      "Losgistic Regression(    2500/10000): loss= 1716.12504139908\n",
      "Losgistic Regression(    2600/10000): loss= 1711.97895999584\n",
      "Losgistic Regression(    2700/10000): loss= 1708.10147465179\n",
      "Losgistic Regression(    2800/10000): loss= 1704.37586766379\n",
      "Losgistic Regression(    2900/10000): loss= 1700.8123556411\n",
      "Losgistic Regression(    3000/10000): loss= 1697.42379062033\n",
      "Losgistic Regression(    3100/10000): loss= 1694.20085951526\n",
      "Losgistic Regression(    3200/10000): loss= 1691.06194727568\n",
      "Losgistic Regression(    3300/10000): loss= 1687.95397883493\n",
      "Losgistic Regression(    3400/10000): loss= 1684.8951132836\n",
      "Losgistic Regression(    3500/10000): loss= 1681.91861498871\n",
      "Losgistic Regression(    3600/10000): loss= 1679.0529797122\n",
      "Losgistic Regression(    3700/10000): loss= 1676.27294928684\n",
      "Losgistic Regression(    3800/10000): loss= 1673.47762114219\n",
      "Losgistic Regression(    3900/10000): loss= 1670.61384124106\n",
      "Losgistic Regression(    4000/10000): loss= 1667.71137819159\n",
      "Losgistic Regression(    4100/10000): loss= 1664.8356028878\n",
      "Losgistic Regression(    4200/10000): loss= 1662.007482203\n",
      "Losgistic Regression(    4300/10000): loss= 1659.32515676189\n",
      "Losgistic Regression(    4400/10000): loss= 1656.71877187353\n",
      "Losgistic Regression(    4500/10000): loss= 1654.07306114743\n",
      "Losgistic Regression(    4600/10000): loss= 1651.85485034523\n",
      "Losgistic Regression(    4700/10000): loss= 1649.85977595954\n",
      "Losgistic Regression(    4800/10000): loss= 1647.5542203731\n",
      "Losgistic Regression(    4900/10000): loss= 1645.13799370036\n",
      "Losgistic Regression(    5000/10000): loss= 1643.01286699169\n",
      "Losgistic Regression(    5100/10000): loss= 1640.8743753121\n",
      "Losgistic Regression(    5200/10000): loss= 1638.28439881869\n",
      "Losgistic Regression(    5300/10000): loss= 1635.65133100961\n",
      "Losgistic Regression(    5400/10000): loss= 1633.44657878141\n",
      "Losgistic Regression(    5500/10000): loss= 1631.36931345363\n",
      "Losgistic Regression(    5600/10000): loss= 1629.1280157145\n",
      "Losgistic Regression(    5700/10000): loss= 1627.20923155233\n",
      "Losgistic Regression(    5800/10000): loss= 1625.3928377992\n",
      "Losgistic Regression(    5900/10000): loss= 1623.65548670725\n",
      "Losgistic Regression(    6000/10000): loss= 1621.85047080185\n",
      "Losgistic Regression(    6100/10000): loss= 1619.79884955384\n",
      "Losgistic Regression(    6200/10000): loss= 1617.52793159462\n",
      "Losgistic Regression(    6300/10000): loss= 1615.48252675286\n",
      "Losgistic Regression(    6400/10000): loss= 1613.66244572497\n",
      "Losgistic Regression(    6500/10000): loss= 1611.82483497397\n",
      "Losgistic Regression(    6600/10000): loss= 1610.09266160776\n",
      "Losgistic Regression(    6700/10000): loss= 1608.69701249245\n",
      "Losgistic Regression(    6800/10000): loss= 1607.5286783341\n",
      "Losgistic Regression(    6900/10000): loss= 1606.28998321218\n",
      "Losgistic Regression(    7000/10000): loss= 1604.71231990626\n",
      "Losgistic Regression(    7100/10000): loss= 1603.24150545648\n",
      "Losgistic Regression(    7200/10000): loss= 1601.75990065172\n",
      "Losgistic Regression(    7300/10000): loss= 1600.03605813079\n",
      "Losgistic Regression(    7400/10000): loss= 1598.44825473082\n",
      "Losgistic Regression(    7500/10000): loss= 1596.96127220995\n",
      "Losgistic Regression(    7600/10000): loss= 1595.55085599714\n",
      "Losgistic Regression(    7700/10000): loss= 1594.47712155468\n",
      "Losgistic Regression(    7800/10000): loss= 1593.74548997875\n",
      "Losgistic Regression(    7900/10000): loss= 1593.21132573515\n",
      "Losgistic Regression(    8000/10000): loss= 1592.79496829149\n",
      "Losgistic Regression(    8100/10000): loss= 1592.53559320405\n",
      "Losgistic Regression(    8200/10000): loss= 1592.50712248332\n",
      "Losgistic Regression(    8300/10000): loss= 1592.50923349577\n",
      "Totoal number of iterations =  8300\n",
      "Loss                        =  1592.5092335\n",
      "Time for  3th cross validation = 136.526s\n",
      "Training Accuracy         = 0.8522\n",
      "Cross Validation Accuracy = 0.806132\n",
      "Losgistic Regression(       0/10000): loss= 3439.90708551067\n",
      "Losgistic Regression(     100/10000): loss= 2309.95104822929\n",
      "Losgistic Regression(     200/10000): loss= 2120.93563297214\n",
      "Losgistic Regression(     300/10000): loss= 2037.93697981962\n",
      "Losgistic Regression(     400/10000): loss= 1969.67457196636\n",
      "Losgistic Regression(     500/10000): loss= 1913.30372613139\n",
      "Losgistic Regression(     600/10000): loss= 1867.96695649531\n",
      "Losgistic Regression(     700/10000): loss= 1829.69827513709\n",
      "Losgistic Regression(     800/10000): loss= 1795.65365182776\n",
      "Losgistic Regression(     900/10000): loss= 1769.35390113315\n",
      "Losgistic Regression(    1000/10000): loss= 1748.54267132111\n",
      "Losgistic Regression(    1100/10000): loss= 1729.4652395887\n",
      "Losgistic Regression(    1200/10000): loss= 1714.55158369588\n",
      "Losgistic Regression(    1300/10000): loss= 1702.22236455208\n",
      "Losgistic Regression(    1400/10000): loss= 1691.24964038682\n",
      "Losgistic Regression(    1500/10000): loss= 1683.04955344894\n",
      "Losgistic Regression(    1600/10000): loss= 1677.17260776669\n",
      "Losgistic Regression(    1700/10000): loss= 1672.31254635567\n",
      "Losgistic Regression(    1800/10000): loss= 1668.52623628611\n",
      "Losgistic Regression(    1900/10000): loss= 1665.5526217232\n",
      "Losgistic Regression(    2000/10000): loss= 1662.10933448615\n",
      "Losgistic Regression(    2100/10000): loss= 1658.91516684462\n",
      "Losgistic Regression(    2200/10000): loss= 1656.35501266318\n",
      "Losgistic Regression(    2300/10000): loss= 1654.24195302266\n",
      "Losgistic Regression(    2400/10000): loss= 1652.5231772661\n",
      "Losgistic Regression(    2500/10000): loss= 1651.18887648292\n",
      "Losgistic Regression(    2600/10000): loss= 1649.94402777217\n",
      "Losgistic Regression(    2700/10000): loss= 1648.843060734\n",
      "Losgistic Regression(    2800/10000): loss= 1647.95252066922\n",
      "Losgistic Regression(    2900/10000): loss= 1647.12441897271\n",
      "Losgistic Regression(    3000/10000): loss= 1646.38329895769\n",
      "Losgistic Regression(    3100/10000): loss= 1645.82798583243\n",
      "Losgistic Regression(    3200/10000): loss= 1645.27817194751\n",
      "Losgistic Regression(    3300/10000): loss= 1644.64258500615\n",
      "Losgistic Regression(    3400/10000): loss= 1643.96526939089\n",
      "Losgistic Regression(    3500/10000): loss= 1643.31280701519\n",
      "Losgistic Regression(    3600/10000): loss= 1642.61483281612\n",
      "Losgistic Regression(    3700/10000): loss= 1641.83575593225\n",
      "Losgistic Regression(    3800/10000): loss= 1640.99890740445\n",
      "Losgistic Regression(    3900/10000): loss= 1640.17217969568\n",
      "Losgistic Regression(    4000/10000): loss= 1639.33237660003\n",
      "Losgistic Regression(    4100/10000): loss= 1638.48777884861\n",
      "Losgistic Regression(    4200/10000): loss= 1637.70793621861\n",
      "Losgistic Regression(    4300/10000): loss= 1636.91924864549\n",
      "Losgistic Regression(    4400/10000): loss= 1636.09667729033\n",
      "Losgistic Regression(    4500/10000): loss= 1635.26973430884\n",
      "Losgistic Regression(    4600/10000): loss= 1634.43468499506\n",
      "Losgistic Regression(    4700/10000): loss= 1633.62695431787\n",
      "Losgistic Regression(    4800/10000): loss= 1632.88335845178\n",
      "Losgistic Regression(    4900/10000): loss= 1632.17802078268\n",
      "Losgistic Regression(    5000/10000): loss= 1631.49471719145\n",
      "Losgistic Regression(    5100/10000): loss= 1630.84031862479\n",
      "Losgistic Regression(    5200/10000): loss= 1630.18745429657\n",
      "Losgistic Regression(    5300/10000): loss= 1629.53687278384\n",
      "Losgistic Regression(    5400/10000): loss= 1628.92349039961\n",
      "Losgistic Regression(    5500/10000): loss= 1628.32743617832\n",
      "Losgistic Regression(    5600/10000): loss= 1627.74827020751\n",
      "Losgistic Regression(    5700/10000): loss= 1627.22933179252\n",
      "Losgistic Regression(    5800/10000): loss= 1626.79314752793\n",
      "Losgistic Regression(    5900/10000): loss= 1626.41608602516\n",
      "Losgistic Regression(    6000/10000): loss= 1626.09426818402\n",
      "Losgistic Regression(    6100/10000): loss= 1625.74693481307\n",
      "Losgistic Regression(    6200/10000): loss= 1625.46418677314\n",
      "Losgistic Regression(    6300/10000): loss= 1625.25249249129\n",
      "Losgistic Regression(    6400/10000): loss= 1625.10194784387\n",
      "Losgistic Regression(    6500/10000): loss= 1625.01944499603\n",
      "Losgistic Regression(    6600/10000): loss= 1625.00425467614\n",
      "Totoal number of iterations =  6600\n",
      "Loss                        =  1625.00425468\n",
      "Time for  4th cross validation = 110.797s\n",
      "Training Accuracy         = 0.8536\n",
      "Cross Validation Accuracy = 0.806352\n",
      "*************** ([0.85840000000000005, 0.84919999999999995, 0.85619999999999996, 0.85219999999999996, 0.85360000000000003], [0.80571599999999999, 0.79476800000000003, 0.80413599999999996, 0.80613199999999996, 0.80635199999999996])\n",
      "Losgistic Regression(       0/10000): loss= 3436.26751253314\n",
      "Losgistic Regression(     100/10000): loss= 2337.5558073351\n",
      "Losgistic Regression(     200/10000): loss= 2125.17913082784\n",
      "Losgistic Regression(     300/10000): loss= 2044.97074319175\n",
      "Losgistic Regression(     400/10000): loss= 1990.11631406029\n",
      "Losgistic Regression(     500/10000): loss= 1949.41211671717\n",
      "Losgistic Regression(     600/10000): loss= 1917.44771847646\n",
      "Losgistic Regression(     700/10000): loss= 1891.45479592646\n",
      "Losgistic Regression(     800/10000): loss= 1870.14586854592\n",
      "Losgistic Regression(     900/10000): loss= 1852.59143163127\n",
      "Losgistic Regression(    1000/10000): loss= 1837.7796408099\n",
      "Losgistic Regression(    1100/10000): loss= 1824.95485551434\n",
      "Losgistic Regression(    1200/10000): loss= 1813.69045856918\n",
      "Losgistic Regression(    1300/10000): loss= 1803.59800255552\n",
      "Losgistic Regression(    1400/10000): loss= 1794.67611719477\n",
      "Losgistic Regression(    1500/10000): loss= 1787.28027137509\n",
      "Losgistic Regression(    1600/10000): loss= 1780.56356309085\n",
      "Losgistic Regression(    1700/10000): loss= 1774.70174587548\n",
      "Losgistic Regression(    1800/10000): loss= 1769.55908179607\n",
      "Losgistic Regression(    1900/10000): loss= 1765.01521146766\n",
      "Losgistic Regression(    2000/10000): loss= 1760.98820183547\n",
      "Losgistic Regression(    2100/10000): loss= 1757.3822880719\n",
      "Losgistic Regression(    2200/10000): loss= 1754.1069285495\n",
      "Losgistic Regression(    2300/10000): loss= 1751.1192004145\n",
      "Losgistic Regression(    2400/10000): loss= 1748.3929848104\n",
      "Losgistic Regression(    2500/10000): loss= 1745.89495983265\n",
      "Losgistic Regression(    2600/10000): loss= 1743.57913845054\n",
      "Losgistic Regression(    2700/10000): loss= 1741.42812346689\n",
      "Losgistic Regression(    2800/10000): loss= 1739.4137290452\n",
      "Losgistic Regression(    2900/10000): loss= 1737.5105249713\n",
      "Losgistic Regression(    3000/10000): loss= 1735.72725329705\n",
      "Losgistic Regression(    3100/10000): loss= 1734.03377184859\n",
      "Losgistic Regression(    3200/10000): loss= 1732.32921563178\n",
      "Losgistic Regression(    3300/10000): loss= 1730.5343788837\n",
      "Losgistic Regression(    3400/10000): loss= 1728.79346513998\n",
      "Losgistic Regression(    3500/10000): loss= 1727.08700757062\n",
      "Losgistic Regression(    3600/10000): loss= 1725.39180328939\n",
      "Losgistic Regression(    3700/10000): loss= 1723.75262546489\n",
      "Losgistic Regression(    3800/10000): loss= 1722.11066018862\n",
      "Losgistic Regression(    3900/10000): loss= 1720.53186023233\n",
      "Losgistic Regression(    4000/10000): loss= 1719.00791294485\n",
      "Losgistic Regression(    4100/10000): loss= 1717.49787522363\n",
      "Losgistic Regression(    4200/10000): loss= 1715.96064405979\n",
      "Losgistic Regression(    4300/10000): loss= 1714.3555226398\n",
      "Losgistic Regression(    4400/10000): loss= 1712.57863559735\n",
      "Losgistic Regression(    4500/10000): loss= 1710.88218081118\n",
      "Losgistic Regression(    4600/10000): loss= 1709.30351942117\n",
      "Losgistic Regression(    4700/10000): loss= 1707.77620071408\n",
      "Losgistic Regression(    4800/10000): loss= 1706.34430620891\n",
      "Losgistic Regression(    4900/10000): loss= 1704.96367072435\n",
      "Losgistic Regression(    5000/10000): loss= 1703.7121191936\n",
      "Losgistic Regression(    5100/10000): loss= 1702.62459246527\n",
      "Losgistic Regression(    5200/10000): loss= 1701.74505552549\n",
      "Losgistic Regression(    5300/10000): loss= 1700.92962271244\n",
      "Losgistic Regression(    5400/10000): loss= 1700.14443681267\n",
      "Losgistic Regression(    5500/10000): loss= 1699.42650951106\n",
      "Losgistic Regression(    5600/10000): loss= 1698.73453776682\n",
      "Losgistic Regression(    5700/10000): loss= 1698.15396624485\n",
      "Losgistic Regression(    5800/10000): loss= 1697.62115007732\n",
      "Losgistic Regression(    5900/10000): loss= 1697.07875067546\n",
      "Losgistic Regression(    6000/10000): loss= 1696.5549464312\n",
      "Losgistic Regression(    6100/10000): loss= 1695.90122189305\n",
      "Losgistic Regression(    6200/10000): loss= 1695.15145277051\n",
      "Losgistic Regression(    6300/10000): loss= 1694.45481180725\n",
      "Losgistic Regression(    6400/10000): loss= 1693.82104012752\n",
      "Losgistic Regression(    6500/10000): loss= 1693.14785820498\n",
      "Losgistic Regression(    6600/10000): loss= 1692.3723740132\n",
      "Losgistic Regression(    6700/10000): loss= 1691.75605627488\n",
      "Losgistic Regression(    6800/10000): loss= 1691.19497861859\n",
      "Losgistic Regression(    6900/10000): loss= 1690.6585893496\n",
      "Losgistic Regression(    7000/10000): loss= 1690.26589021276\n",
      "Losgistic Regression(    7100/10000): loss= 1689.87789735216\n",
      "Losgistic Regression(    7200/10000): loss= 1689.63008272053\n",
      "Losgistic Regression(    7300/10000): loss= 1689.37724240887\n",
      "Losgistic Regression(    7400/10000): loss= 1689.10587460061\n",
      "Losgistic Regression(    7500/10000): loss= 1688.91067521711\n",
      "Losgistic Regression(    7600/10000): loss= 1688.64190386385\n",
      "Losgistic Regression(    7700/10000): loss= 1688.36022292434\n",
      "Losgistic Regression(    7800/10000): loss= 1688.1177347987\n",
      "Losgistic Regression(    7900/10000): loss= 1687.86706248697\n",
      "Losgistic Regression(    8000/10000): loss= 1687.60482618144\n",
      "Losgistic Regression(    8100/10000): loss= 1687.36414830918\n",
      "Losgistic Regression(    8200/10000): loss= 1687.11443815302\n",
      "Losgistic Regression(    8300/10000): loss= 1686.80847474498\n",
      "Losgistic Regression(    8400/10000): loss= 1686.54832863766\n",
      "Losgistic Regression(    8500/10000): loss= 1686.27387272952\n",
      "Losgistic Regression(    8600/10000): loss= 1686.06220884635\n",
      "Losgistic Regression(    8700/10000): loss= 1685.79235956292\n",
      "Losgistic Regression(    8800/10000): loss= 1685.53044819971\n",
      "Losgistic Regression(    8900/10000): loss= 1685.28264975921\n",
      "Losgistic Regression(    9000/10000): loss= 1685.10573609743\n",
      "Losgistic Regression(    9100/10000): loss= 1684.98395291735\n",
      "Losgistic Regression(    9200/10000): loss= 1684.916228562\n",
      "Losgistic Regression(    9300/10000): loss= 1684.88836503624\n",
      "Losgistic Regression(    9400/10000): loss= 1684.8887160946\n",
      "Totoal number of iterations =  9400\n",
      "Loss                        =  1684.88871609\n",
      "Time for  0th cross validation = 155.615s\n",
      "Training Accuracy         = 0.8584\n",
      "Cross Validation Accuracy = 0.807548\n",
      "Losgistic Regression(       0/10000): loss= 3438.21504473377\n",
      "Losgistic Regression(     100/10000): loss= 2370.89738775485\n",
      "Losgistic Regression(     200/10000): loss= 2169.25544617623\n",
      "Losgistic Regression(     300/10000): loss= 2080.2466440885\n",
      "Losgistic Regression(     400/10000): loss= 2024.24321136758\n",
      "Losgistic Regression(     500/10000): loss= 1982.2871922674\n",
      "Losgistic Regression(     600/10000): loss= 1947.86318057244\n",
      "Losgistic Regression(     700/10000): loss= 1920.51327440353\n",
      "Losgistic Regression(     800/10000): loss= 1896.68043842181\n",
      "Losgistic Regression(     900/10000): loss= 1875.80615087689\n",
      "Losgistic Regression(    1000/10000): loss= 1858.40026790619\n",
      "Losgistic Regression(    1100/10000): loss= 1843.81373205108\n",
      "Losgistic Regression(    1200/10000): loss= 1831.6683871348\n",
      "Losgistic Regression(    1300/10000): loss= 1821.30738955143\n",
      "Losgistic Regression(    1400/10000): loss= 1812.13185094988\n",
      "Losgistic Regression(    1500/10000): loss= 1804.03788703442\n",
      "Losgistic Regression(    1600/10000): loss= 1796.84230363358\n",
      "Losgistic Regression(    1700/10000): loss= 1790.37836443436\n",
      "Losgistic Regression(    1800/10000): loss= 1784.44395231988\n",
      "Losgistic Regression(    1900/10000): loss= 1779.21645255498\n",
      "Losgistic Regression(    2000/10000): loss= 1774.53820595052\n",
      "Losgistic Regression(    2100/10000): loss= 1770.24744730811\n",
      "Losgistic Regression(    2200/10000): loss= 1766.3176688662\n",
      "Losgistic Regression(    2300/10000): loss= 1762.74187508388\n",
      "Losgistic Regression(    2400/10000): loss= 1759.4707490632\n",
      "Losgistic Regression(    2500/10000): loss= 1756.46878027757\n",
      "Losgistic Regression(    2600/10000): loss= 1753.74029443282\n",
      "Losgistic Regression(    2700/10000): loss= 1751.29227569549\n",
      "Losgistic Regression(    2800/10000): loss= 1749.02517007261\n",
      "Losgistic Regression(    2900/10000): loss= 1746.96476385942\n",
      "Losgistic Regression(    3000/10000): loss= 1745.11750250984\n",
      "Losgistic Regression(    3100/10000): loss= 1743.3975016497\n",
      "Losgistic Regression(    3200/10000): loss= 1741.71976329033\n",
      "Losgistic Regression(    3300/10000): loss= 1740.07128457342\n",
      "Losgistic Regression(    3400/10000): loss= 1738.38014054656\n",
      "Losgistic Regression(    3500/10000): loss= 1736.67267606563\n",
      "Losgistic Regression(    3600/10000): loss= 1735.0974126357\n",
      "Losgistic Regression(    3700/10000): loss= 1733.6631145744\n",
      "Losgistic Regression(    3800/10000): loss= 1732.35806971838\n",
      "Losgistic Regression(    3900/10000): loss= 1731.17184612799\n",
      "Losgistic Regression(    4000/10000): loss= 1730.09308854359\n",
      "Losgistic Regression(    4100/10000): loss= 1729.08048512409\n",
      "Losgistic Regression(    4200/10000): loss= 1728.12350801894\n",
      "Losgistic Regression(    4300/10000): loss= 1727.23760039126\n",
      "Losgistic Regression(    4400/10000): loss= 1726.41293151152\n",
      "Losgistic Regression(    4500/10000): loss= 1725.55912120177\n",
      "Losgistic Regression(    4600/10000): loss= 1724.53007132926\n",
      "Losgistic Regression(    4700/10000): loss= 1723.57854461498\n",
      "Losgistic Regression(    4800/10000): loss= 1722.73300305728\n",
      "Losgistic Regression(    4900/10000): loss= 1721.96697493281\n",
      "Losgistic Regression(    5000/10000): loss= 1721.14176361822\n",
      "Losgistic Regression(    5100/10000): loss= 1720.44117691015\n",
      "Losgistic Regression(    5200/10000): loss= 1719.85793848054\n",
      "Losgistic Regression(    5300/10000): loss= 1719.32844803124\n",
      "Losgistic Regression(    5400/10000): loss= 1718.81309121656\n",
      "Losgistic Regression(    5500/10000): loss= 1718.20437206812\n",
      "Losgistic Regression(    5600/10000): loss= 1717.76287832721\n",
      "Losgistic Regression(    5700/10000): loss= 1717.58342050635\n",
      "Losgistic Regression(    5800/10000): loss= 1717.50884318799\n",
      "Losgistic Regression(    5900/10000): loss= 1717.3874218192\n",
      "Losgistic Regression(    6000/10000): loss= 1717.13134960423\n",
      "Losgistic Regression(    6100/10000): loss= 1716.87615241543\n",
      "Losgistic Regression(    6200/10000): loss= 1716.75321920519\n",
      "Losgistic Regression(    6300/10000): loss= 1716.75503473593\n",
      "Totoal number of iterations =  6300\n",
      "Loss                        =  1716.75503474\n",
      "Time for  1th cross validation = 103.855s\n",
      "Training Accuracy         =   0.85\n",
      "Cross Validation Accuracy = 0.795696\n",
      "Losgistic Regression(       0/10000): loss= 3438.58018047882\n",
      "Losgistic Regression(     100/10000): loss= 2323.69058155292\n",
      "Losgistic Regression(     200/10000): loss= 2120.4550664169\n",
      "Losgistic Regression(     300/10000): loss= 2031.52830139024\n",
      "Losgistic Regression(     400/10000): loss= 1971.90669447069\n",
      "Losgistic Regression(     500/10000): loss= 1928.54995967354\n",
      "Losgistic Regression(     600/10000): loss= 1895.73175129222\n",
      "Losgistic Regression(     700/10000): loss= 1869.82307535958\n",
      "Losgistic Regression(     800/10000): loss= 1848.89026573364\n",
      "Losgistic Regression(     900/10000): loss= 1831.92126945559\n",
      "Losgistic Regression(    1000/10000): loss= 1817.95415258871\n",
      "Losgistic Regression(    1100/10000): loss= 1806.30524249018\n",
      "Losgistic Regression(    1200/10000): loss= 1795.23632599235\n",
      "Losgistic Regression(    1300/10000): loss= 1784.95232269677\n",
      "Losgistic Regression(    1400/10000): loss= 1775.99651972077\n",
      "Losgistic Regression(    1500/10000): loss= 1768.12445810737\n",
      "Losgistic Regression(    1600/10000): loss= 1761.14189362362\n",
      "Losgistic Regression(    1700/10000): loss= 1754.91375767468\n",
      "Losgistic Regression(    1800/10000): loss= 1749.01244119887\n",
      "Losgistic Regression(    1900/10000): loss= 1743.5226710385\n",
      "Losgistic Regression(    2000/10000): loss= 1738.52680619622\n",
      "Losgistic Regression(    2100/10000): loss= 1733.94981510068\n",
      "Losgistic Regression(    2200/10000): loss= 1729.7423940163\n",
      "Losgistic Regression(    2300/10000): loss= 1725.87233577428\n",
      "Losgistic Regression(    2400/10000): loss= 1722.25470933728\n",
      "Losgistic Regression(    2500/10000): loss= 1718.70241133161\n",
      "Losgistic Regression(    2600/10000): loss= 1715.27075609975\n",
      "Losgistic Regression(    2700/10000): loss= 1711.9633969617\n",
      "Losgistic Regression(    2800/10000): loss= 1708.50403879303\n",
      "Losgistic Regression(    2900/10000): loss= 1704.84563517532\n",
      "Losgistic Regression(    3000/10000): loss= 1701.54399406763\n",
      "Losgistic Regression(    3100/10000): loss= 1698.05448712036\n",
      "Losgistic Regression(    3200/10000): loss= 1694.82679794905\n",
      "Losgistic Regression(    3300/10000): loss= 1692.00857893542\n",
      "Losgistic Regression(    3400/10000): loss= 1689.00298364298\n",
      "Losgistic Regression(    3500/10000): loss= 1685.85353991128\n",
      "Losgistic Regression(    3600/10000): loss= 1682.78252272137\n",
      "Losgistic Regression(    3700/10000): loss= 1679.4206420608\n",
      "Losgistic Regression(    3800/10000): loss= 1675.95374321222\n",
      "Losgistic Regression(    3900/10000): loss= 1672.66524501126\n",
      "Losgistic Regression(    4000/10000): loss= 1669.47438015532\n",
      "Losgistic Regression(    4100/10000): loss= 1666.32805641128\n",
      "Losgistic Regression(    4200/10000): loss= 1663.30373834832\n",
      "Losgistic Regression(    4300/10000): loss= 1660.40524881191\n",
      "Losgistic Regression(    4400/10000): loss= 1657.47471454997\n",
      "Losgistic Regression(    4500/10000): loss= 1654.52155659985\n",
      "Losgistic Regression(    4600/10000): loss= 1651.6234448367\n",
      "Losgistic Regression(    4700/10000): loss= 1648.74275283234\n",
      "Losgistic Regression(    4800/10000): loss= 1645.80307876936\n",
      "Losgistic Regression(    4900/10000): loss= 1642.78825641147\n",
      "Losgistic Regression(    5000/10000): loss= 1639.83219307191\n",
      "Losgistic Regression(    5100/10000): loss= 1636.76292755039\n",
      "Losgistic Regression(    5200/10000): loss= 1633.46650553966\n",
      "Losgistic Regression(    5300/10000): loss= 1630.16780646397\n",
      "Losgistic Regression(    5400/10000): loss= 1626.86266566451\n",
      "Losgistic Regression(    5500/10000): loss= 1623.37694754925\n",
      "Losgistic Regression(    5600/10000): loss= 1619.70615767359\n",
      "Losgistic Regression(    5700/10000): loss= 1616.03468804021\n",
      "Losgistic Regression(    5800/10000): loss= 1612.25266995015\n",
      "Losgistic Regression(    5900/10000): loss= 1608.42432542993\n",
      "Losgistic Regression(    6000/10000): loss= 1604.63888291534\n",
      "Losgistic Regression(    6100/10000): loss= 1600.80138588937\n",
      "Losgistic Regression(    6200/10000): loss= 1596.87003608393\n",
      "Losgistic Regression(    6300/10000): loss= 1592.8452201049\n",
      "Losgistic Regression(    6400/10000): loss= 1588.9448702017\n",
      "Losgistic Regression(    6500/10000): loss= 1585.12286458343\n",
      "Losgistic Regression(    6600/10000): loss= 1581.29745115384\n",
      "Losgistic Regression(    6700/10000): loss= 1577.51572633312\n",
      "Losgistic Regression(    6800/10000): loss= 1573.75082071214\n",
      "Losgistic Regression(    6900/10000): loss= 1569.98816455299\n",
      "Losgistic Regression(    7000/10000): loss= 1566.18782652882\n",
      "Losgistic Regression(    7100/10000): loss= 1562.35780981528\n",
      "Losgistic Regression(    7200/10000): loss= 1558.64790382953\n",
      "Losgistic Regression(    7300/10000): loss= 1555.01162853727\n",
      "Losgistic Regression(    7400/10000): loss= 1551.40001392031\n",
      "Losgistic Regression(    7500/10000): loss= 1547.84619879141\n",
      "Losgistic Regression(    7600/10000): loss= 1544.31206507166\n",
      "Losgistic Regression(    7700/10000): loss= 1540.77889608474\n",
      "Losgistic Regression(    7800/10000): loss= 1537.2535956434\n",
      "Losgistic Regression(    7900/10000): loss= 1533.81149266062\n",
      "Losgistic Regression(    8000/10000): loss= 1530.41253867603\n",
      "Losgistic Regression(    8100/10000): loss= 1526.9267781226\n",
      "Losgistic Regression(    8200/10000): loss= 1523.47378462874\n",
      "Losgistic Regression(    8300/10000): loss= 1520.11696523286\n",
      "Losgistic Regression(    8400/10000): loss= 1516.77250014945\n",
      "Losgistic Regression(    8500/10000): loss= 1513.33628672074\n",
      "Losgistic Regression(    8600/10000): loss= 1509.85723238804\n",
      "Losgistic Regression(    8700/10000): loss= 1506.36303448312\n",
      "Losgistic Regression(    8800/10000): loss= 1502.88874659772\n",
      "Losgistic Regression(    8900/10000): loss= 1499.54454346596\n",
      "Losgistic Regression(    9000/10000): loss= 1496.340119162\n",
      "Losgistic Regression(    9100/10000): loss= 1493.19423960122\n",
      "Losgistic Regression(    9200/10000): loss= 1490.15872308473\n",
      "Losgistic Regression(    9300/10000): loss= 1487.22135792322\n",
      "Losgistic Regression(    9400/10000): loss= 1484.27098042966\n",
      "Losgistic Regression(    9500/10000): loss= 1481.4262243419\n",
      "Losgistic Regression(    9600/10000): loss= 1478.69485766537\n",
      "Losgistic Regression(    9700/10000): loss= 1476.11275859688\n",
      "Losgistic Regression(    9800/10000): loss= 1473.62650824524\n",
      "Losgistic Regression(    9900/10000): loss= 1471.23785175816\n",
      "Time for  2th cross validation = 163.206s\n",
      "Training Accuracy         = 0.8568\n",
      "Cross Validation Accuracy = 0.805732\n",
      "Losgistic Regression(       0/10000): loss= 3439.01843224387\n",
      "Losgistic Regression(     100/10000): loss= 2331.04340470093\n",
      "Losgistic Regression(     200/10000): loss= 2139.0445652405\n",
      "Losgistic Regression(     300/10000): loss= 2061.95399137107\n",
      "Losgistic Regression(     400/10000): loss= 2009.64104413037\n",
      "Losgistic Regression(     500/10000): loss= 1969.261768235\n",
      "Losgistic Regression(     600/10000): loss= 1937.35419860505\n",
      "Losgistic Regression(     700/10000): loss= 1911.58750456686\n",
      "Losgistic Regression(     800/10000): loss= 1890.44077962489\n",
      "Losgistic Regression(     900/10000): loss= 1872.66161014098\n",
      "Losgistic Regression(    1000/10000): loss= 1857.41729858973\n",
      "Losgistic Regression(    1100/10000): loss= 1844.18425578115\n",
      "Losgistic Regression(    1200/10000): loss= 1831.98777939159\n",
      "Losgistic Regression(    1300/10000): loss= 1820.54798007017\n",
      "Losgistic Regression(    1400/10000): loss= 1810.57911858735\n",
      "Losgistic Regression(    1500/10000): loss= 1801.8221548521\n",
      "Losgistic Regression(    1600/10000): loss= 1793.98512471739\n",
      "Losgistic Regression(    1700/10000): loss= 1786.77883927736\n",
      "Losgistic Regression(    1800/10000): loss= 1779.61449936325\n",
      "Losgistic Regression(    1900/10000): loss= 1773.06149977336\n",
      "Losgistic Regression(    2000/10000): loss= 1767.05805151213\n",
      "Losgistic Regression(    2100/10000): loss= 1761.53691903786\n",
      "Losgistic Regression(    2200/10000): loss= 1756.37148259028\n",
      "Losgistic Regression(    2300/10000): loss= 1751.71590115282\n",
      "Losgistic Regression(    2400/10000): loss= 1747.68312362128\n",
      "Losgistic Regression(    2500/10000): loss= 1744.25209515663\n",
      "Losgistic Regression(    2600/10000): loss= 1741.16695546344\n",
      "Losgistic Regression(    2700/10000): loss= 1738.18187062503\n",
      "Losgistic Regression(    2800/10000): loss= 1735.24839050546\n",
      "Losgistic Regression(    2900/10000): loss= 1732.17901949166\n",
      "Losgistic Regression(    3000/10000): loss= 1729.42507355813\n",
      "Losgistic Regression(    3100/10000): loss= 1726.94469501385\n",
      "Losgistic Regression(    3200/10000): loss= 1724.60205495318\n",
      "Losgistic Regression(    3300/10000): loss= 1722.28028520649\n",
      "Losgistic Regression(    3400/10000): loss= 1719.76719493751\n",
      "Losgistic Regression(    3500/10000): loss= 1717.42489657272\n",
      "Losgistic Regression(    3600/10000): loss= 1715.33791366154\n",
      "Losgistic Regression(    3700/10000): loss= 1713.34090564369\n",
      "Losgistic Regression(    3800/10000): loss= 1711.4047138624\n",
      "Losgistic Regression(    3900/10000): loss= 1709.31186717745\n",
      "Losgistic Regression(    4000/10000): loss= 1707.37643162967\n",
      "Losgistic Regression(    4100/10000): loss= 1705.54835040999\n",
      "Losgistic Regression(    4200/10000): loss= 1703.74059115922\n",
      "Losgistic Regression(    4300/10000): loss= 1702.08686553545\n",
      "Losgistic Regression(    4400/10000): loss= 1700.38508407867\n",
      "Losgistic Regression(    4500/10000): loss= 1698.7116161644\n",
      "Losgistic Regression(    4600/10000): loss= 1697.25436832721\n",
      "Losgistic Regression(    4700/10000): loss= 1695.74666846076\n",
      "Losgistic Regression(    4800/10000): loss= 1693.98149420872\n",
      "Losgistic Regression(    4900/10000): loss= 1692.27957523867\n",
      "Losgistic Regression(    5000/10000): loss= 1690.73216694639\n",
      "Losgistic Regression(    5100/10000): loss= 1689.10853589172\n",
      "Losgistic Regression(    5200/10000): loss= 1687.3754004212\n",
      "Losgistic Regression(    5300/10000): loss= 1685.72493729007\n",
      "Losgistic Regression(    5400/10000): loss= 1684.51091538279\n",
      "Losgistic Regression(    5500/10000): loss= 1683.49342891002\n",
      "Losgistic Regression(    5600/10000): loss= 1682.45281522271\n",
      "Losgistic Regression(    5700/10000): loss= 1681.61729102327\n",
      "Losgistic Regression(    5800/10000): loss= 1680.80849581751\n",
      "Losgistic Regression(    5900/10000): loss= 1679.94346627142\n",
      "Losgistic Regression(    6000/10000): loss= 1678.91966378777\n",
      "Losgistic Regression(    6100/10000): loss= 1677.72500675919\n",
      "Losgistic Regression(    6200/10000): loss= 1676.45644306978\n",
      "Losgistic Regression(    6300/10000): loss= 1675.22227952798\n",
      "Losgistic Regression(    6400/10000): loss= 1674.00508124127\n",
      "Losgistic Regression(    6500/10000): loss= 1672.85523525301\n",
      "Losgistic Regression(    6600/10000): loss= 1672.03403916065\n",
      "Losgistic Regression(    6700/10000): loss= 1671.21728801557\n",
      "Losgistic Regression(    6800/10000): loss= 1670.33535055779\n",
      "Losgistic Regression(    6900/10000): loss= 1669.34562325334\n",
      "Losgistic Regression(    7000/10000): loss= 1668.34985207243\n",
      "Losgistic Regression(    7100/10000): loss= 1667.38277051015\n",
      "Losgistic Regression(    7200/10000): loss= 1666.43107253843\n",
      "Losgistic Regression(    7300/10000): loss= 1665.14987789132\n",
      "Losgistic Regression(    7400/10000): loss= 1663.81323452154\n",
      "Losgistic Regression(    7500/10000): loss= 1662.94003794071\n",
      "Losgistic Regression(    7600/10000): loss= 1662.34314615922\n",
      "Losgistic Regression(    7700/10000): loss= 1661.86540390625\n",
      "Losgistic Regression(    7800/10000): loss= 1661.35273475702\n",
      "Losgistic Regression(    7900/10000): loss= 1661.01533390588\n",
      "Losgistic Regression(    8000/10000): loss= 1660.46500583188\n",
      "Losgistic Regression(    8100/10000): loss= 1659.79611150886\n",
      "Losgistic Regression(    8200/10000): loss= 1659.16671623153\n",
      "Losgistic Regression(    8300/10000): loss= 1658.56309371644\n",
      "Losgistic Regression(    8400/10000): loss= 1657.68821180662\n",
      "Losgistic Regression(    8500/10000): loss= 1656.88972446629\n",
      "Losgistic Regression(    8600/10000): loss= 1656.72114008787\n",
      "Losgistic Regression(    8700/10000): loss= 1656.72678047881\n",
      "Totoal number of iterations =  8700\n",
      "Loss                        =  1656.72678048\n",
      "Time for  3th cross validation = 142.903s\n",
      "Training Accuracy         = 0.8502\n",
      "Cross Validation Accuracy = 0.807272\n",
      "Losgistic Regression(       0/10000): loss= 3439.91723774427\n",
      "Losgistic Regression(     100/10000): loss= 2311.82745019293\n",
      "Losgistic Regression(     200/10000): loss= 2124.32947555038\n",
      "Losgistic Regression(     300/10000): loss= 2042.78847331054\n",
      "Losgistic Regression(     400/10000): loss= 1976.0265896052\n",
      "Losgistic Regression(     500/10000): loss= 1921.25394397609\n",
      "Losgistic Regression(     600/10000): loss= 1877.53843469183\n",
      "Losgistic Regression(     700/10000): loss= 1840.97509168119\n",
      "Losgistic Regression(     800/10000): loss= 1808.5011880889\n",
      "Losgistic Regression(     900/10000): loss= 1783.65917552101\n",
      "Losgistic Regression(    1000/10000): loss= 1764.29779866731\n",
      "Losgistic Regression(    1100/10000): loss= 1746.56486028317\n",
      "Losgistic Regression(    1200/10000): loss= 1732.9074070411\n",
      "Losgistic Regression(    1300/10000): loss= 1721.71801773251\n",
      "Losgistic Regression(    1400/10000): loss= 1711.7976042246\n",
      "Losgistic Regression(    1500/10000): loss= 1704.61669404389\n",
      "Losgistic Regression(    1600/10000): loss= 1699.69568032949\n",
      "Losgistic Regression(    1700/10000): loss= 1695.73922912764\n",
      "Losgistic Regression(    1800/10000): loss= 1692.72035014585\n",
      "Losgistic Regression(    1900/10000): loss= 1690.37378991373\n",
      "Losgistic Regression(    2000/10000): loss= 1687.76262153003\n",
      "Losgistic Regression(    2100/10000): loss= 1685.10223398279\n",
      "Losgistic Regression(    2200/10000): loss= 1683.01509384222\n",
      "Losgistic Regression(    2300/10000): loss= 1681.36009388316\n",
      "Losgistic Regression(    2400/10000): loss= 1680.08953969693\n",
      "Losgistic Regression(    2500/10000): loss= 1679.14508374428\n",
      "Losgistic Regression(    2600/10000): loss= 1678.27714733594\n",
      "Losgistic Regression(    2700/10000): loss= 1677.55512754801\n",
      "Losgistic Regression(    2800/10000): loss= 1677.00679828353\n",
      "Losgistic Regression(    2900/10000): loss= 1676.45955684482\n",
      "Losgistic Regression(    3000/10000): loss= 1676.03414779486\n",
      "Losgistic Regression(    3100/10000): loss= 1675.79182362242\n",
      "Losgistic Regression(    3200/10000): loss= 1675.48594108662\n",
      "Losgistic Regression(    3300/10000): loss= 1675.03674346215\n",
      "Losgistic Regression(    3400/10000): loss= 1674.52156843676\n",
      "Losgistic Regression(    3500/10000): loss= 1674.05992410279\n",
      "Losgistic Regression(    3600/10000): loss= 1673.56989221966\n",
      "Losgistic Regression(    3700/10000): loss= 1673.00428631351\n",
      "Losgistic Regression(    3800/10000): loss= 1672.3470499146\n",
      "Losgistic Regression(    3900/10000): loss= 1671.67467788048\n",
      "Losgistic Regression(    4000/10000): loss= 1670.98334121933\n",
      "Losgistic Regression(    4100/10000): loss= 1670.30624762119\n",
      "Losgistic Regression(    4200/10000): loss= 1669.75468471726\n",
      "Losgistic Regression(    4300/10000): loss= 1669.23332899469\n",
      "Losgistic Regression(    4400/10000): loss= 1668.68161303728\n",
      "Losgistic Regression(    4500/10000): loss= 1668.11967004152\n",
      "Losgistic Regression(    4600/10000): loss= 1667.58523937164\n",
      "Losgistic Regression(    4700/10000): loss= 1667.13162143604\n",
      "Losgistic Regression(    4800/10000): loss= 1666.7966825476\n",
      "Losgistic Regression(    4900/10000): loss= 1666.55362085597\n",
      "Losgistic Regression(    5000/10000): loss= 1666.29718940154\n",
      "Losgistic Regression(    5100/10000): loss= 1665.90767923865\n",
      "Losgistic Regression(    5200/10000): loss= 1665.48962465236\n",
      "Losgistic Regression(    5300/10000): loss= 1665.05378867005\n",
      "Losgistic Regression(    5400/10000): loss= 1664.66964034845\n",
      "Losgistic Regression(    5500/10000): loss= 1664.40136503511\n",
      "Losgistic Regression(    5600/10000): loss= 1664.19255719262\n",
      "Losgistic Regression(    5700/10000): loss= 1663.96684898778\n",
      "Losgistic Regression(    5800/10000): loss= 1663.82258715681\n",
      "Losgistic Regression(    5900/10000): loss= 1663.77044419674\n",
      "Losgistic Regression(    6000/10000): loss= 1663.77094935612\n",
      "Totoal number of iterations =  6000\n",
      "Loss                        =  1663.77094936\n",
      "Time for  4th cross validation = 98.7274s\n",
      "Training Accuracy         =  0.854\n",
      "Cross Validation Accuracy = 0.80766\n",
      "*************** ([0.85840000000000005, 0.84999999999999998, 0.85680000000000001, 0.85019999999999996, 0.85399999999999998], [0.80754800000000004, 0.79569599999999996, 0.805732, 0.80727199999999999, 0.80766000000000004])\n",
      "Losgistic Regression(       0/10000): loss= 3436.30490607044\n",
      "Losgistic Regression(     100/10000): loss= 2343.36656329851\n",
      "Losgistic Regression(     200/10000): loss= 2136.71508411647\n",
      "Losgistic Regression(     300/10000): loss= 2061.04392208202\n",
      "Losgistic Regression(     400/10000): loss= 2010.73164015807\n",
      "Losgistic Regression(     500/10000): loss= 1974.36960627158\n",
      "Losgistic Regression(     600/10000): loss= 1946.57187369872\n",
      "Losgistic Regression(     700/10000): loss= 1924.59653508861\n",
      "Losgistic Regression(     800/10000): loss= 1906.93127883111\n",
      "Losgistic Regression(     900/10000): loss= 1892.69220561865\n",
      "Losgistic Regression(    1000/10000): loss= 1881.00180997959\n",
      "Losgistic Regression(    1100/10000): loss= 1871.15441365405\n",
      "Losgistic Regression(    1200/10000): loss= 1862.67158427513\n",
      "Losgistic Regression(    1300/10000): loss= 1855.17433590828\n",
      "Losgistic Regression(    1400/10000): loss= 1848.53830303148\n",
      "Losgistic Regression(    1500/10000): loss= 1843.1835430857\n",
      "Losgistic Regression(    1600/10000): loss= 1838.41442522231\n",
      "Losgistic Regression(    1700/10000): loss= 1834.27624762362\n",
      "Losgistic Regression(    1800/10000): loss= 1830.62925790154\n",
      "Losgistic Regression(    1900/10000): loss= 1827.37444999318\n",
      "Losgistic Regression(    2000/10000): loss= 1824.46247673304\n",
      "Losgistic Regression(    2100/10000): loss= 1821.93125583143\n",
      "Losgistic Regression(    2200/10000): loss= 1819.60255270282\n",
      "Losgistic Regression(    2300/10000): loss= 1817.42994784465\n",
      "Losgistic Regression(    2400/10000): loss= 1815.51061555527\n",
      "Losgistic Regression(    2500/10000): loss= 1813.79875263657\n",
      "Losgistic Regression(    2600/10000): loss= 1812.18524970349\n",
      "Losgistic Regression(    2700/10000): loss= 1810.77841964046\n",
      "Losgistic Regression(    2800/10000): loss= 1809.530888673\n",
      "Losgistic Regression(    2900/10000): loss= 1808.3260538781\n",
      "Losgistic Regression(    3000/10000): loss= 1807.19966303708\n",
      "Losgistic Regression(    3100/10000): loss= 1806.14173099329\n",
      "Losgistic Regression(    3200/10000): loss= 1805.11732217344\n",
      "Losgistic Regression(    3300/10000): loss= 1804.21865257027\n",
      "Losgistic Regression(    3400/10000): loss= 1803.35067656188\n",
      "Losgistic Regression(    3500/10000): loss= 1802.48684186882\n",
      "Losgistic Regression(    3600/10000): loss= 1801.70427336516\n",
      "Losgistic Regression(    3700/10000): loss= 1800.96983085451\n",
      "Losgistic Regression(    3800/10000): loss= 1800.34968383533\n",
      "Losgistic Regression(    3900/10000): loss= 1799.83865875615\n",
      "Losgistic Regression(    4000/10000): loss= 1799.4081020841\n",
      "Losgistic Regression(    4100/10000): loss= 1798.96685333391\n",
      "Losgistic Regression(    4200/10000): loss= 1798.54097476468\n",
      "Losgistic Regression(    4300/10000): loss= 1798.02800448245\n",
      "Losgistic Regression(    4400/10000): loss= 1797.51559719341\n",
      "Losgistic Regression(    4500/10000): loss= 1797.03900596312\n",
      "Losgistic Regression(    4600/10000): loss= 1796.589215666\n",
      "Losgistic Regression(    4700/10000): loss= 1796.17625914529\n",
      "Losgistic Regression(    4800/10000): loss= 1795.77378704936\n",
      "Losgistic Regression(    4900/10000): loss= 1795.3506475589\n",
      "Losgistic Regression(    5000/10000): loss= 1794.94422863891\n",
      "Losgistic Regression(    5100/10000): loss= 1794.58392281144\n",
      "Losgistic Regression(    5200/10000): loss= 1794.20458137566\n",
      "Losgistic Regression(    5300/10000): loss= 1793.79971420418\n",
      "Losgistic Regression(    5400/10000): loss= 1793.40895144185\n",
      "Losgistic Regression(    5500/10000): loss= 1793.07230433995\n",
      "Losgistic Regression(    5600/10000): loss= 1792.76874512417\n",
      "Losgistic Regression(    5700/10000): loss= 1792.47876170185\n",
      "Losgistic Regression(    5800/10000): loss= 1792.19023831612\n",
      "Losgistic Regression(    5900/10000): loss= 1791.93866289205\n",
      "Losgistic Regression(    6000/10000): loss= 1791.68917625009\n",
      "Losgistic Regression(    6100/10000): loss= 1791.42708497241\n",
      "Losgistic Regression(    6200/10000): loss= 1791.15980167233\n",
      "Losgistic Regression(    6300/10000): loss= 1790.93995889569\n",
      "Losgistic Regression(    6400/10000): loss= 1790.7331858907\n",
      "Losgistic Regression(    6500/10000): loss= 1790.52240394451\n",
      "Losgistic Regression(    6600/10000): loss= 1790.31193230451\n",
      "Losgistic Regression(    6700/10000): loss= 1790.11628347984\n",
      "Losgistic Regression(    6800/10000): loss= 1789.91745760173\n",
      "Losgistic Regression(    6900/10000): loss= 1789.73409777061\n",
      "Losgistic Regression(    7000/10000): loss= 1789.53290155464\n",
      "Losgistic Regression(    7100/10000): loss= 1789.35556772219\n",
      "Losgistic Regression(    7200/10000): loss= 1789.20472639632\n",
      "Losgistic Regression(    7300/10000): loss= 1789.04618504757\n",
      "Losgistic Regression(    7400/10000): loss= 1788.8855506829\n",
      "Losgistic Regression(    7500/10000): loss= 1788.74839245281\n",
      "Losgistic Regression(    7600/10000): loss= 1788.61740905561\n",
      "Losgistic Regression(    7700/10000): loss= 1788.4816853747\n",
      "Losgistic Regression(    7800/10000): loss= 1788.35252832564\n",
      "Losgistic Regression(    7900/10000): loss= 1788.2216466093\n",
      "Losgistic Regression(    8000/10000): loss= 1788.09121321441\n",
      "Losgistic Regression(    8100/10000): loss= 1787.99676486138\n",
      "Losgistic Regression(    8200/10000): loss= 1787.89099167972\n",
      "Losgistic Regression(    8300/10000): loss= 1787.77099078584\n",
      "Losgistic Regression(    8400/10000): loss= 1787.67340507066\n",
      "Losgistic Regression(    8500/10000): loss= 1787.57077747789\n",
      "Losgistic Regression(    8600/10000): loss= 1787.4768011037\n",
      "Losgistic Regression(    8700/10000): loss= 1787.37396535664\n",
      "Losgistic Regression(    8800/10000): loss= 1787.28407635748\n",
      "Losgistic Regression(    8900/10000): loss= 1787.19803419303\n",
      "Losgistic Regression(    9000/10000): loss= 1787.1206128449\n",
      "Losgistic Regression(    9100/10000): loss= 1787.03692944181\n",
      "Losgistic Regression(    9200/10000): loss= 1786.95826398527\n",
      "Losgistic Regression(    9300/10000): loss= 1786.88334255766\n",
      "Losgistic Regression(    9400/10000): loss= 1786.81119378906\n",
      "Losgistic Regression(    9500/10000): loss= 1786.74882133117\n",
      "Losgistic Regression(    9600/10000): loss= 1786.67900531385\n",
      "Losgistic Regression(    9700/10000): loss= 1786.61536241809\n",
      "Losgistic Regression(    9800/10000): loss= 1786.55402227803\n",
      "Losgistic Regression(    9900/10000): loss= 1786.49507964094\n",
      "Time for  0th cross validation = 163.355s\n",
      "Training Accuracy         = 0.8546\n",
      "Cross Validation Accuracy = 0.8098\n",
      "Losgistic Regression(       0/10000): loss= 3438.2513564894\n",
      "Losgistic Regression(     100/10000): loss= 2377.0809299234\n",
      "Losgistic Regression(     200/10000): loss= 2181.24805766411\n",
      "Losgistic Regression(     300/10000): loss= 2097.71275571168\n",
      "Losgistic Regression(     400/10000): loss= 2046.16979431116\n",
      "Losgistic Regression(     500/10000): loss= 2008.95873228756\n",
      "Losgistic Regression(     600/10000): loss= 1979.03616553809\n",
      "Losgistic Regression(     700/10000): loss= 1956.03121173819\n",
      "Losgistic Regression(     800/10000): loss= 1936.88354448471\n",
      "Losgistic Regression(     900/10000): loss= 1919.98716890783\n",
      "Losgistic Regression(    1000/10000): loss= 1906.57801773924\n",
      "Losgistic Regression(    1100/10000): loss= 1895.64753773141\n",
      "Losgistic Regression(    1200/10000): loss= 1886.71930944593\n",
      "Losgistic Regression(    1300/10000): loss= 1879.37931402317\n",
      "Losgistic Regression(    1400/10000): loss= 1873.26812358631\n",
      "Losgistic Regression(    1500/10000): loss= 1868.01224931872\n",
      "Losgistic Regression(    1600/10000): loss= 1863.36523870328\n",
      "Losgistic Regression(    1700/10000): loss= 1858.84797002074\n",
      "Losgistic Regression(    1800/10000): loss= 1855.0668513525\n",
      "Losgistic Regression(    1900/10000): loss= 1851.72283878223\n",
      "Losgistic Regression(    2000/10000): loss= 1848.65616841872\n",
      "Losgistic Regression(    2100/10000): loss= 1845.76912793888\n",
      "Losgistic Regression(    2200/10000): loss= 1843.28892713395\n",
      "Losgistic Regression(    2300/10000): loss= 1841.07566207868\n",
      "Losgistic Regression(    2400/10000): loss= 1839.11619094849\n",
      "Losgistic Regression(    2500/10000): loss= 1837.37770877658\n",
      "Losgistic Regression(    2600/10000): loss= 1835.63954270735\n",
      "Losgistic Regression(    2700/10000): loss= 1834.19089789656\n",
      "Losgistic Regression(    2800/10000): loss= 1832.93609320974\n",
      "Losgistic Regression(    2900/10000): loss= 1831.50517449845\n",
      "Losgistic Regression(    3000/10000): loss= 1830.45308989045\n",
      "Losgistic Regression(    3100/10000): loss= 1829.69479077071\n",
      "Losgistic Regression(    3200/10000): loss= 1828.43993744278\n",
      "Losgistic Regression(    3300/10000): loss= 1827.56647796801\n",
      "Losgistic Regression(    3400/10000): loss= 1826.61543867666\n",
      "Losgistic Regression(    3500/10000): loss= 1825.92180038743\n",
      "Losgistic Regression(    3600/10000): loss= 1825.29753726796\n",
      "Losgistic Regression(    3700/10000): loss= 1825.23208925391\n",
      "Losgistic Regression(    3800/10000): loss= 1825.23522924852\n",
      "Totoal number of iterations =  3800\n",
      "Loss                        =  1825.23522925\n",
      "Time for  1th cross validation = 63.101s\n",
      "Training Accuracy         = 0.8418\n",
      "Cross Validation Accuracy = 0.797284\n",
      "Losgistic Regression(       0/10000): loss= 3438.61551582613\n",
      "Losgistic Regression(     100/10000): loss= 2329.75171846073\n",
      "Losgistic Regression(     200/10000): loss= 2131.97340361772\n",
      "Losgistic Regression(     300/10000): loss= 2047.87232953783\n",
      "Losgistic Regression(     400/10000): loss= 1992.8006041957\n",
      "Losgistic Regression(     500/10000): loss= 1953.75082143966\n",
      "Losgistic Regression(     600/10000): loss= 1924.94166844712\n",
      "Losgistic Regression(     700/10000): loss= 1902.6119017795\n",
      "Losgistic Regression(     800/10000): loss= 1885.01323672733\n",
      "Losgistic Regression(     900/10000): loss= 1871.28225283251\n",
      "Losgistic Regression(    1000/10000): loss= 1860.36439570067\n",
      "Losgistic Regression(    1100/10000): loss= 1851.61268381473\n",
      "Losgistic Regression(    1200/10000): loss= 1844.48918968289\n",
      "Losgistic Regression(    1300/10000): loss= 1838.58410199102\n",
      "Losgistic Regression(    1400/10000): loss= 1833.75840301355\n",
      "Losgistic Regression(    1500/10000): loss= 1829.85672091538\n",
      "Losgistic Regression(    1600/10000): loss= 1826.55769460252\n",
      "Losgistic Regression(    1700/10000): loss= 1823.74587310045\n",
      "Losgistic Regression(    1800/10000): loss= 1821.33143638612\n",
      "Losgistic Regression(    1900/10000): loss= 1819.21469848125\n",
      "Losgistic Regression(    2000/10000): loss= 1817.32789415595\n",
      "Losgistic Regression(    2100/10000): loss= 1815.60068198085\n",
      "Losgistic Regression(    2200/10000): loss= 1814.03692191394\n",
      "Losgistic Regression(    2300/10000): loss= 1812.61542441575\n",
      "Losgistic Regression(    2400/10000): loss= 1811.34898167303\n",
      "Losgistic Regression(    2500/10000): loss= 1810.1166897971\n",
      "Losgistic Regression(    2600/10000): loss= 1809.1015882712\n",
      "Losgistic Regression(    2700/10000): loss= 1808.0231831725\n",
      "Losgistic Regression(    2800/10000): loss= 1806.98314269186\n",
      "Losgistic Regression(    2900/10000): loss= 1805.82972308561\n",
      "Losgistic Regression(    3000/10000): loss= 1804.81763354799\n",
      "Losgistic Regression(    3100/10000): loss= 1804.00207775815\n",
      "Losgistic Regression(    3200/10000): loss= 1803.16662565133\n",
      "Losgistic Regression(    3300/10000): loss= 1802.49154865559\n",
      "Losgistic Regression(    3400/10000): loss= 1801.86769420557\n",
      "Losgistic Regression(    3500/10000): loss= 1801.27703815983\n",
      "Losgistic Regression(    3600/10000): loss= 1800.72354891793\n",
      "Losgistic Regression(    3700/10000): loss= 1800.2199620927\n",
      "Losgistic Regression(    3800/10000): loss= 1799.74016047886\n",
      "Losgistic Regression(    3900/10000): loss= 1799.20495959706\n",
      "Losgistic Regression(    4000/10000): loss= 1798.79059420482\n",
      "Losgistic Regression(    4100/10000): loss= 1798.2992625127\n",
      "Losgistic Regression(    4200/10000): loss= 1797.79610431044\n",
      "Losgistic Regression(    4300/10000): loss= 1797.22637120255\n",
      "Losgistic Regression(    4400/10000): loss= 1796.74203655387\n",
      "Losgistic Regression(    4500/10000): loss= 1796.35123515978\n",
      "Losgistic Regression(    4600/10000): loss= 1796.01087256679\n",
      "Losgistic Regression(    4700/10000): loss= 1795.64727164658\n",
      "Losgistic Regression(    4800/10000): loss= 1795.29765111698\n",
      "Losgistic Regression(    4900/10000): loss= 1795.05543878078\n",
      "Losgistic Regression(    5000/10000): loss= 1794.88056479865\n",
      "Losgistic Regression(    5100/10000): loss= 1794.72325236059\n",
      "Losgistic Regression(    5200/10000): loss= 1794.66606854573\n",
      "Losgistic Regression(    5300/10000): loss= 1794.50426717487\n",
      "Losgistic Regression(    5400/10000): loss= 1794.3133010673\n",
      "Losgistic Regression(    5500/10000): loss= 1794.07056699924\n",
      "Losgistic Regression(    5600/10000): loss= 1793.82273293637\n",
      "Losgistic Regression(    5700/10000): loss= 1793.62412680783\n",
      "Losgistic Regression(    5800/10000): loss= 1793.45687528736\n",
      "Losgistic Regression(    5900/10000): loss= 1793.15933406362\n",
      "Losgistic Regression(    6000/10000): loss= 1792.70009850996\n",
      "Losgistic Regression(    6100/10000): loss= 1792.36162498727\n",
      "Losgistic Regression(    6200/10000): loss= 1792.07906082977\n",
      "Losgistic Regression(    6300/10000): loss= 1791.84398119818\n",
      "Losgistic Regression(    6400/10000): loss= 1791.549252272\n",
      "Losgistic Regression(    6500/10000): loss= 1791.29894146673\n",
      "Losgistic Regression(    6600/10000): loss= 1791.00638384632\n",
      "Losgistic Regression(    6700/10000): loss= 1790.6722215017\n",
      "Losgistic Regression(    6800/10000): loss= 1790.23382194288\n",
      "Losgistic Regression(    6900/10000): loss= 1789.82570984612\n",
      "Losgistic Regression(    7000/10000): loss= 1789.48875802317\n",
      "Losgistic Regression(    7100/10000): loss= 1789.23867510488\n",
      "Losgistic Regression(    7200/10000): loss= 1789.04805574112\n",
      "Losgistic Regression(    7300/10000): loss= 1788.99803497911\n",
      "Losgistic Regression(    7400/10000): loss= 1788.95941637437\n",
      "Losgistic Regression(    7500/10000): loss= 1788.91083174616\n",
      "Losgistic Regression(    7600/10000): loss= 1788.81530374156\n",
      "Losgistic Regression(    7700/10000): loss= 1788.68726208902\n",
      "Losgistic Regression(    7800/10000): loss= 1788.44807015376\n",
      "Losgistic Regression(    7900/10000): loss= 1788.1053852851\n",
      "Losgistic Regression(    8000/10000): loss= 1787.68084772122\n",
      "Losgistic Regression(    8100/10000): loss= 1787.15454441489\n",
      "Losgistic Regression(    8200/10000): loss= 1786.64452079136\n",
      "Losgistic Regression(    8300/10000): loss= 1786.20535641892\n",
      "Losgistic Regression(    8400/10000): loss= 1785.74611140656\n",
      "Losgistic Regression(    8500/10000): loss= 1785.37176761671\n",
      "Losgistic Regression(    8600/10000): loss= 1785.04653425402\n",
      "Losgistic Regression(    8700/10000): loss= 1784.79085950168\n",
      "Losgistic Regression(    8800/10000): loss= 1784.58027117955\n",
      "Losgistic Regression(    8900/10000): loss= 1784.3593467893\n",
      "Losgistic Regression(    9000/10000): loss= 1784.16431713675\n",
      "Losgistic Regression(    9100/10000): loss= 1783.99147407136\n",
      "Losgistic Regression(    9200/10000): loss= 1783.84672663823\n",
      "Losgistic Regression(    9300/10000): loss= 1783.64032255005\n",
      "Losgistic Regression(    9400/10000): loss= 1783.44191401479\n",
      "Losgistic Regression(    9500/10000): loss= 1783.19479964137\n",
      "Losgistic Regression(    9600/10000): loss= 1782.98796246925\n",
      "Losgistic Regression(    9700/10000): loss= 1782.77642496867\n",
      "Losgistic Regression(    9800/10000): loss= 1782.56829451037\n",
      "Losgistic Regression(    9900/10000): loss= 1782.34750639312\n",
      "Time for  2th cross validation = 164.301s\n",
      "Training Accuracy         =  0.853\n",
      "Cross Validation Accuracy = 0.810392\n",
      "Losgistic Regression(       0/10000): loss= 3439.05399441729\n",
      "Losgistic Regression(     100/10000): loss= 2337.06120035486\n",
      "Losgistic Regression(     200/10000): loss= 2150.1893045916\n",
      "Losgistic Regression(     300/10000): loss= 2077.21449726043\n",
      "Losgistic Regression(     400/10000): loss= 2028.70998090459\n",
      "Losgistic Regression(     500/10000): loss= 1992.00027605154\n",
      "Losgistic Regression(     600/10000): loss= 1963.63039733518\n",
      "Losgistic Regression(     700/10000): loss= 1941.31963692848\n",
      "Losgistic Regression(     800/10000): loss= 1923.43831087322\n",
      "Losgistic Regression(     900/10000): loss= 1908.74580792227\n",
      "Losgistic Regression(    1000/10000): loss= 1896.51731314102\n",
      "Losgistic Regression(    1100/10000): loss= 1886.2978068372\n",
      "Losgistic Regression(    1200/10000): loss= 1877.5975426437\n",
      "Losgistic Regression(    1300/10000): loss= 1869.43385867307\n",
      "Losgistic Regression(    1400/10000): loss= 1862.34837778493\n",
      "Losgistic Regression(    1500/10000): loss= 1856.39198756759\n",
      "Losgistic Regression(    1600/10000): loss= 1851.12645215451\n",
      "Losgistic Regression(    1700/10000): loss= 1846.70672209336\n",
      "Losgistic Regression(    1800/10000): loss= 1842.66668705707\n",
      "Losgistic Regression(    1900/10000): loss= 1838.91763784747\n",
      "Losgistic Regression(    2000/10000): loss= 1835.96017833818\n",
      "Losgistic Regression(    2100/10000): loss= 1833.09379002808\n",
      "Losgistic Regression(    2200/10000): loss= 1830.40554738046\n",
      "Losgistic Regression(    2300/10000): loss= 1828.07577143095\n",
      "Losgistic Regression(    2400/10000): loss= 1826.12410937264\n",
      "Losgistic Regression(    2500/10000): loss= 1824.36944059055\n",
      "Losgistic Regression(    2600/10000): loss= 1822.66195228622\n",
      "Losgistic Regression(    2700/10000): loss= 1820.93809750673\n",
      "Losgistic Regression(    2800/10000): loss= 1819.32152592366\n",
      "Losgistic Regression(    2900/10000): loss= 1817.86094473589\n",
      "Losgistic Regression(    3000/10000): loss= 1816.3490125397\n",
      "Losgistic Regression(    3100/10000): loss= 1815.06584813575\n",
      "Losgistic Regression(    3200/10000): loss= 1813.81827262817\n",
      "Losgistic Regression(    3300/10000): loss= 1813.05012271019\n",
      "Losgistic Regression(    3400/10000): loss= 1812.17355804641\n",
      "Losgistic Regression(    3500/10000): loss= 1811.18500601806\n",
      "Losgistic Regression(    3600/10000): loss= 1810.41465643005\n",
      "Losgistic Regression(    3700/10000): loss= 1809.93214890906\n",
      "Losgistic Regression(    3800/10000): loss= 1809.4443392992\n",
      "Losgistic Regression(    3900/10000): loss= 1809.04830058564\n",
      "Losgistic Regression(    4000/10000): loss= 1808.51255721925\n",
      "Losgistic Regression(    4100/10000): loss= 1808.0574836276\n",
      "Losgistic Regression(    4200/10000): loss= 1807.68259004626\n",
      "Losgistic Regression(    4300/10000): loss= 1807.28145593622\n",
      "Losgistic Regression(    4400/10000): loss= 1806.84829446426\n",
      "Losgistic Regression(    4500/10000): loss= 1806.6136900719\n",
      "Losgistic Regression(    4600/10000): loss= 1806.34447763256\n",
      "Losgistic Regression(    4700/10000): loss= 1806.05640210522\n",
      "Losgistic Regression(    4800/10000): loss= 1806.00661104388\n",
      "Losgistic Regression(    4900/10000): loss= 1806.0095200129\n",
      "Totoal number of iterations =  4900\n",
      "Loss                        =  1806.00952001\n",
      "Time for  3th cross validation = 80.7491s\n",
      "Training Accuracy         = 0.8414\n",
      "Cross Validation Accuracy = 0.80814\n",
      "Losgistic Regression(       0/10000): loss= 3439.95258987496\n",
      "Losgistic Regression(     100/10000): loss= 2318.29884897182\n",
      "Losgistic Regression(     200/10000): loss= 2135.92035482531\n",
      "Losgistic Regression(     300/10000): loss= 2059.25610153006\n",
      "Losgistic Regression(     400/10000): loss= 1997.43610036848\n",
      "Losgistic Regression(     500/10000): loss= 1947.9485440179\n",
      "Losgistic Regression(     600/10000): loss= 1909.48535671109\n",
      "Losgistic Regression(     700/10000): loss= 1878.02855299703\n",
      "Losgistic Regression(     800/10000): loss= 1850.61905204763\n",
      "Losgistic Regression(     900/10000): loss= 1830.00959366074\n",
      "Losgistic Regression(    1000/10000): loss= 1814.82278855666\n",
      "Losgistic Regression(    1100/10000): loss= 1801.13668238729\n",
      "Losgistic Regression(    1200/10000): loss= 1790.74621660223\n",
      "Losgistic Regression(    1300/10000): loss= 1782.50053605677\n",
      "Losgistic Regression(    1400/10000): loss= 1775.54432628978\n",
      "Losgistic Regression(    1500/10000): loss= 1770.92527933797\n",
      "Losgistic Regression(    1600/10000): loss= 1768.37598046413\n",
      "Losgistic Regression(    1700/10000): loss= 1767.0127677321\n",
      "Losgistic Regression(    1800/10000): loss= 1765.70475416449\n",
      "Losgistic Regression(    1900/10000): loss= 1764.3857525346\n",
      "Losgistic Regression(    2000/10000): loss= 1763.44033620972\n",
      "Losgistic Regression(    2100/10000): loss= 1762.23147988218\n",
      "Losgistic Regression(    2200/10000): loss= 1760.91935653753\n",
      "Losgistic Regression(    2300/10000): loss= 1759.7694070661\n",
      "Losgistic Regression(    2400/10000): loss= 1758.94857121818\n",
      "Losgistic Regression(    2500/10000): loss= 1758.67263566681\n",
      "Losgistic Regression(    2600/10000): loss= 1758.32880054046\n",
      "Losgistic Regression(    2700/10000): loss= 1757.64070914639\n",
      "Losgistic Regression(    2800/10000): loss= 1756.91789511391\n",
      "Losgistic Regression(    2900/10000): loss= 1756.2943512759\n",
      "Losgistic Regression(    3000/10000): loss= 1755.51280461446\n",
      "Losgistic Regression(    3100/10000): loss= 1754.87168049539\n",
      "Losgistic Regression(    3200/10000): loss= 1754.49013606203\n",
      "Losgistic Regression(    3300/10000): loss= 1754.28745604161\n",
      "Losgistic Regression(    3400/10000): loss= 1753.80150047835\n",
      "Losgistic Regression(    3500/10000): loss= 1753.55053024248\n",
      "Losgistic Regression(    3600/10000): loss= 1753.12087573655\n",
      "Losgistic Regression(    3700/10000): loss= 1752.75638542855\n",
      "Losgistic Regression(    3800/10000): loss= 1752.51073014493\n",
      "Losgistic Regression(    3900/10000): loss= 1752.13246725615\n",
      "Losgistic Regression(    4000/10000): loss= 1751.77499171986\n",
      "Losgistic Regression(    4100/10000): loss= 1751.43502404173\n",
      "Losgistic Regression(    4200/10000): loss= 1750.99444264276\n",
      "Losgistic Regression(    4300/10000): loss= 1750.7365135523\n",
      "Losgistic Regression(    4400/10000): loss= 1749.80834571827\n",
      "Losgistic Regression(    4500/10000): loss= 1748.90292995276\n",
      "Losgistic Regression(    4600/10000): loss= 1748.47068267344\n",
      "Losgistic Regression(    4700/10000): loss= 1747.98483110977\n",
      "Losgistic Regression(    4800/10000): loss= 1747.81602037728\n",
      "Losgistic Regression(    4900/10000): loss= 1747.78589025037\n",
      "Losgistic Regression(    5000/10000): loss= 1747.78374638555\n",
      "Totoal number of iterations =  5000\n",
      "Loss                        =  1747.78374639\n",
      "Time for  4th cross validation = 82.996s\n",
      "Training Accuracy         =  0.849\n",
      "Cross Validation Accuracy = 0.809496\n",
      "*************** ([0.85460000000000003, 0.84179999999999999, 0.85299999999999998, 0.84140000000000004, 0.84899999999999998], [0.80979999999999996, 0.79728399999999999, 0.810392, 0.80813999999999997, 0.80949599999999999])\n",
      "Losgistic Regression(       0/10000): loss= 3436.43511793025\n",
      "Losgistic Regression(     100/10000): loss= 2362.83667692508\n",
      "Losgistic Regression(     200/10000): loss= 2174.2994631098\n",
      "Losgistic Regression(     300/10000): loss= 2112.16284705386\n",
      "Losgistic Regression(     400/10000): loss= 2074.38485128516\n",
      "Losgistic Regression(     500/10000): loss= 2048.81435631214\n",
      "Losgistic Regression(     600/10000): loss= 2030.79855288988\n",
      "Losgistic Regression(     700/10000): loss= 2017.37469801788\n",
      "Losgistic Regression(     800/10000): loss= 2007.14196542147\n",
      "Losgistic Regression(     900/10000): loss= 1999.51657498611\n",
      "Losgistic Regression(    1000/10000): loss= 1993.50810526049\n",
      "Losgistic Regression(    1100/10000): loss= 1988.46792710068\n",
      "Losgistic Regression(    1200/10000): loss= 1984.04714612785\n",
      "Losgistic Regression(    1300/10000): loss= 1980.66219549255\n",
      "Losgistic Regression(    1400/10000): loss= 1977.29474548119\n",
      "Losgistic Regression(    1500/10000): loss= 1974.24418387469\n",
      "Losgistic Regression(    1600/10000): loss= 1971.49059166504\n",
      "Losgistic Regression(    1700/10000): loss= 1969.34030107035\n",
      "Losgistic Regression(    1800/10000): loss= 1967.30520162792\n",
      "Losgistic Regression(    1900/10000): loss= 1966.00540460073\n",
      "Losgistic Regression(    2000/10000): loss= 1964.84567217985\n",
      "Losgistic Regression(    2100/10000): loss= 1963.6802050595\n",
      "Losgistic Regression(    2200/10000): loss= 1962.59008480312\n",
      "Losgistic Regression(    2300/10000): loss= 1961.59167182893\n",
      "Losgistic Regression(    2400/10000): loss= 1960.80054166616\n",
      "Losgistic Regression(    2500/10000): loss= 1959.9692554426\n",
      "Losgistic Regression(    2600/10000): loss= 1959.24640620886\n",
      "Losgistic Regression(    2700/10000): loss= 1958.64558848988\n",
      "Losgistic Regression(    2800/10000): loss= 1957.91786329848\n",
      "Losgistic Regression(    2900/10000): loss= 1957.42549362699\n",
      "Losgistic Regression(    3000/10000): loss= 1957.04421911662\n",
      "Losgistic Regression(    3100/10000): loss= 1956.70468248074\n",
      "Losgistic Regression(    3200/10000): loss= 1956.39545100225\n",
      "Losgistic Regression(    3300/10000): loss= 1956.07123297436\n",
      "Losgistic Regression(    3400/10000): loss= 1955.74200716491\n",
      "Losgistic Regression(    3500/10000): loss= 1955.41371406084\n",
      "Losgistic Regression(    3600/10000): loss= 1955.14706745593\n",
      "Losgistic Regression(    3700/10000): loss= 1954.84114912996\n",
      "Losgistic Regression(    3800/10000): loss= 1954.56801337342\n",
      "Losgistic Regression(    3900/10000): loss= 1954.21606763074\n",
      "Losgistic Regression(    4000/10000): loss= 1953.95109003167\n",
      "Losgistic Regression(    4100/10000): loss= 1953.72353636319\n",
      "Losgistic Regression(    4200/10000): loss= 1953.53707933511\n",
      "Losgistic Regression(    4300/10000): loss= 1953.48159025657\n",
      "Losgistic Regression(    4400/10000): loss= 1953.42683170903\n",
      "Losgistic Regression(    4500/10000): loss= 1953.3738916982\n",
      "Losgistic Regression(    4600/10000): loss= 1953.33770098703\n",
      "Losgistic Regression(    4700/10000): loss= 1953.30799791674\n",
      "Losgistic Regression(    4800/10000): loss= 1953.27938456856\n",
      "Losgistic Regression(    4900/10000): loss= 1953.2503834122\n",
      "Losgistic Regression(    5000/10000): loss= 1953.22077652387\n",
      "Losgistic Regression(    5100/10000): loss= 1953.19021453041\n",
      "Losgistic Regression(    5200/10000): loss= 1953.1582305067\n",
      "Losgistic Regression(    5300/10000): loss= 1953.12445720876\n",
      "Losgistic Regression(    5400/10000): loss= 1953.08912290957\n",
      "Losgistic Regression(    5500/10000): loss= 1953.05205978847\n",
      "Losgistic Regression(    5600/10000): loss= 1953.01338878581\n",
      "Losgistic Regression(    5700/10000): loss= 1952.97334710608\n",
      "Losgistic Regression(    5800/10000): loss= 1952.93199637304\n",
      "Losgistic Regression(    5900/10000): loss= 1952.88964997996\n",
      "Losgistic Regression(    6000/10000): loss= 1952.84610098887\n",
      "Losgistic Regression(    6100/10000): loss= 1952.80120102948\n",
      "Losgistic Regression(    6200/10000): loss= 1952.75547735683\n",
      "Losgistic Regression(    6300/10000): loss= 1952.7087143843\n",
      "Losgistic Regression(    6400/10000): loss= 1952.66108494592\n",
      "Losgistic Regression(    6500/10000): loss= 1952.61293213202\n",
      "Losgistic Regression(    6600/10000): loss= 1952.5641341076\n",
      "Losgistic Regression(    6700/10000): loss= 1952.51447580569\n",
      "Losgistic Regression(    6800/10000): loss= 1952.46419899781\n",
      "Losgistic Regression(    6900/10000): loss= 1952.41354084526\n",
      "Losgistic Regression(    7000/10000): loss= 1952.36274964654\n",
      "Losgistic Regression(    7100/10000): loss= 1952.31210158705\n",
      "Losgistic Regression(    7200/10000): loss= 1952.2613210961\n",
      "Losgistic Regression(    7300/10000): loss= 1952.21055126126\n",
      "Losgistic Regression(    7400/10000): loss= 1952.15980154781\n",
      "Losgistic Regression(    7500/10000): loss= 1952.109322616\n",
      "Losgistic Regression(    7600/10000): loss= 1952.0589856877\n",
      "Losgistic Regression(    7700/10000): loss= 1952.00896681587\n",
      "Losgistic Regression(    7800/10000): loss= 1951.97440075219\n",
      "Losgistic Regression(    7900/10000): loss= 1951.97435476533\n",
      "Totoal number of iterations =  7900\n",
      "Loss                        =  1951.97435477\n",
      "Time for  0th cross validation = 129.552s\n",
      "Training Accuracy         = 0.8396\n",
      "Cross Validation Accuracy = 0.810008\n",
      "Losgistic Regression(       0/10000): loss= 3438.37780136668\n",
      "Losgistic Regression(     100/10000): loss= 2397.81971038168\n",
      "Losgistic Regression(     200/10000): loss= 2220.4970632507\n",
      "Losgistic Regression(     300/10000): loss= 2152.81102629899\n",
      "Losgistic Regression(     400/10000): loss= 2113.48314703769\n",
      "Losgistic Regression(     500/10000): loss= 2088.65018616236\n",
      "Losgistic Regression(     600/10000): loss= 2070.31160248535\n",
      "Losgistic Regression(     700/10000): loss= 2056.90499481874\n",
      "Losgistic Regression(     800/10000): loss= 2046.70117257272\n",
      "Losgistic Regression(     900/10000): loss= 2039.14843658777\n",
      "Losgistic Regression(    1000/10000): loss= 2032.57530736451\n",
      "Losgistic Regression(    1100/10000): loss= 2026.46288348107\n",
      "Losgistic Regression(    1200/10000): loss= 2021.4669153089\n",
      "Losgistic Regression(    1300/10000): loss= 2018.52487683628\n",
      "Losgistic Regression(    1400/10000): loss= 2016.58639985624\n",
      "Losgistic Regression(    1500/10000): loss= 2014.70187012116\n",
      "Losgistic Regression(    1600/10000): loss= 2012.18930361829\n",
      "Losgistic Regression(    1700/10000): loss= 2009.61098313289\n",
      "Losgistic Regression(    1800/10000): loss= 2008.58238876135\n",
      "Losgistic Regression(    1900/10000): loss= 2007.47304757689\n",
      "Losgistic Regression(    2000/10000): loss= 2006.47355259225\n",
      "Losgistic Regression(    2100/10000): loss= 2005.15523665955\n",
      "Losgistic Regression(    2200/10000): loss= 2003.78195714739\n",
      "Losgistic Regression(    2300/10000): loss= 2002.59091403718\n",
      "Losgistic Regression(    2400/10000): loss= 2001.95329314656\n",
      "Losgistic Regression(    2500/10000): loss= 2001.2759744699\n",
      "Losgistic Regression(    2600/10000): loss= 2001.26001175595\n",
      "Totoal number of iterations =  2600\n",
      "Loss                        =  2001.26001176\n",
      "Time for  1th cross validation = 42.8929s\n",
      "Training Accuracy         =  0.837\n",
      "Cross Validation Accuracy = 0.800488\n",
      "Losgistic Regression(       0/10000): loss= 3438.73856065215\n",
      "Losgistic Regression(     100/10000): loss= 2350.15922698008\n",
      "Losgistic Regression(     200/10000): loss= 2169.4084966787\n",
      "Losgistic Regression(     300/10000): loss= 2099.69201463657\n",
      "Losgistic Regression(     400/10000): loss= 2057.19839322505\n",
      "Losgistic Regression(     500/10000): loss= 2029.38411193551\n",
      "Losgistic Regression(     600/10000): loss= 2009.58630461138\n",
      "Losgistic Regression(     700/10000): loss= 1994.88234079673\n",
      "Losgistic Regression(     800/10000): loss= 1983.84506111853\n",
      "Losgistic Regression(     900/10000): loss= 1975.97812298713\n",
      "Losgistic Regression(    1000/10000): loss= 1970.14716299072\n",
      "Losgistic Regression(    1100/10000): loss= 1966.16173773936\n",
      "Losgistic Regression(    1200/10000): loss= 1963.21872053289\n",
      "Losgistic Regression(    1300/10000): loss= 1961.09776882781\n",
      "Losgistic Regression(    1400/10000): loss= 1959.15359805941\n",
      "Losgistic Regression(    1500/10000): loss= 1957.3923421089\n",
      "Losgistic Regression(    1600/10000): loss= 1955.78272522087\n",
      "Losgistic Regression(    1700/10000): loss= 1954.48167956268\n",
      "Losgistic Regression(    1800/10000): loss= 1953.10062997601\n",
      "Losgistic Regression(    1900/10000): loss= 1951.84751297366\n",
      "Losgistic Regression(    2000/10000): loss= 1951.0780162671\n",
      "Losgistic Regression(    2100/10000): loss= 1950.23791087031\n",
      "Losgistic Regression(    2200/10000): loss= 1949.43958335258\n",
      "Losgistic Regression(    2300/10000): loss= 1948.64352025985\n",
      "Losgistic Regression(    2400/10000): loss= 1947.93885415214\n",
      "Losgistic Regression(    2500/10000): loss= 1947.33527660275\n",
      "Losgistic Regression(    2600/10000): loss= 1946.67906245176\n",
      "Losgistic Regression(    2700/10000): loss= 1946.11808023026\n",
      "Losgistic Regression(    2800/10000): loss= 1945.81570142314\n",
      "Losgistic Regression(    2900/10000): loss= 1945.46360811754\n",
      "Losgistic Regression(    3000/10000): loss= 1945.20243104731\n",
      "Losgistic Regression(    3100/10000): loss= 1944.82427778991\n",
      "Losgistic Regression(    3200/10000): loss= 1944.41527022891\n",
      "Losgistic Regression(    3300/10000): loss= 1944.04482731806\n",
      "Losgistic Regression(    3400/10000): loss= 1943.6792311852\n",
      "Losgistic Regression(    3500/10000): loss= 1943.48875255651\n",
      "Losgistic Regression(    3600/10000): loss= 1943.29610239038\n",
      "Losgistic Regression(    3700/10000): loss= 1942.9910979134\n",
      "Losgistic Regression(    3800/10000): loss= 1942.63902737206\n",
      "Losgistic Regression(    3900/10000): loss= 1942.54052773695\n",
      "Losgistic Regression(    4000/10000): loss= 1942.47353726778\n",
      "Losgistic Regression(    4100/10000): loss= 1942.41732795827\n",
      "Losgistic Regression(    4200/10000): loss= 1942.36187764968\n",
      "Losgistic Regression(    4300/10000): loss= 1942.36093908947\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  1942.36093909\n",
      "Time for  2th cross validation = 70.796s\n",
      "Training Accuracy         = 0.8428\n",
      "Cross Validation Accuracy = 0.811464\n",
      "Losgistic Regression(       0/10000): loss= 3439.17782909768\n",
      "Losgistic Regression(     100/10000): loss= 2357.31064623942\n",
      "Losgistic Regression(     200/10000): loss= 2186.42850925518\n",
      "Losgistic Regression(     300/10000): loss= 2125.57660902118\n",
      "Losgistic Regression(     400/10000): loss= 2087.41508141153\n",
      "Losgistic Regression(     500/10000): loss= 2060.13869149171\n",
      "Losgistic Regression(     600/10000): loss= 2040.24646075519\n",
      "Losgistic Regression(     700/10000): loss= 2025.53620073917\n",
      "Losgistic Regression(     800/10000): loss= 2014.2691871439\n",
      "Losgistic Regression(     900/10000): loss= 2005.71135214943\n",
      "Losgistic Regression(    1000/10000): loss= 1999.28479009885\n",
      "Losgistic Regression(    1100/10000): loss= 1993.9979823554\n",
      "Losgistic Regression(    1200/10000): loss= 1989.64823904795\n",
      "Losgistic Regression(    1300/10000): loss= 1985.93029469219\n",
      "Losgistic Regression(    1400/10000): loss= 1982.77495834827\n",
      "Losgistic Regression(    1500/10000): loss= 1980.20224894657\n",
      "Losgistic Regression(    1600/10000): loss= 1978.31849497828\n",
      "Losgistic Regression(    1700/10000): loss= 1976.48485873842\n",
      "Losgistic Regression(    1800/10000): loss= 1974.95636312221\n",
      "Losgistic Regression(    1900/10000): loss= 1973.59945056792\n",
      "Losgistic Regression(    2000/10000): loss= 1972.47277672637\n",
      "Losgistic Regression(    2100/10000): loss= 1971.69900062245\n",
      "Losgistic Regression(    2200/10000): loss= 1971.0292367723\n",
      "Losgistic Regression(    2300/10000): loss= 1970.41888556731\n",
      "Losgistic Regression(    2400/10000): loss= 1969.70292556967\n",
      "Losgistic Regression(    2500/10000): loss= 1969.10474088896\n",
      "Losgistic Regression(    2600/10000): loss= 1968.50678606693\n",
      "Losgistic Regression(    2700/10000): loss= 1967.85787064973\n",
      "Losgistic Regression(    2800/10000): loss= 1967.21938168241\n",
      "Losgistic Regression(    2900/10000): loss= 1966.59375627689\n",
      "Losgistic Regression(    3000/10000): loss= 1966.12710601863\n",
      "Losgistic Regression(    3100/10000): loss= 1965.92202012561\n",
      "Losgistic Regression(    3200/10000): loss= 1965.77334624168\n",
      "Losgistic Regression(    3300/10000): loss= 1965.66772768287\n",
      "Losgistic Regression(    3400/10000): loss= 1965.57063122214\n",
      "Losgistic Regression(    3500/10000): loss= 1965.46898893856\n",
      "Losgistic Regression(    3600/10000): loss= 1965.35072240076\n",
      "Losgistic Regression(    3700/10000): loss= 1965.21356061424\n",
      "Losgistic Regression(    3800/10000): loss= 1965.14085472616\n",
      "Losgistic Regression(    3900/10000): loss= 1965.07803269186\n",
      "Losgistic Regression(    4000/10000): loss= 1964.98408943267\n",
      "Losgistic Regression(    4100/10000): loss= 1964.90960337618\n",
      "Losgistic Regression(    4200/10000): loss= 1964.83591063741\n",
      "Losgistic Regression(    4300/10000): loss= 1964.76015109391\n",
      "Losgistic Regression(    4400/10000): loss= 1964.68000070507\n",
      "Losgistic Regression(    4500/10000): loss= 1964.6010433887\n",
      "Losgistic Regression(    4600/10000): loss= 1964.51717986172\n",
      "Losgistic Regression(    4700/10000): loss= 1964.43767835649\n",
      "Losgistic Regression(    4800/10000): loss= 1964.35676822214\n",
      "Losgistic Regression(    4900/10000): loss= 1964.27355862403\n",
      "Losgistic Regression(    5000/10000): loss= 1964.19367999212\n",
      "Losgistic Regression(    5100/10000): loss= 1964.11232771171\n",
      "Losgistic Regression(    5200/10000): loss= 1964.02865795787\n",
      "Losgistic Regression(    5300/10000): loss= 1963.94454816859\n",
      "Losgistic Regression(    5400/10000): loss= 1963.859382753\n",
      "Losgistic Regression(    5500/10000): loss= 1963.78657640886\n",
      "Losgistic Regression(    5600/10000): loss= 1963.7861468627\n",
      "Totoal number of iterations =  5600\n",
      "Loss                        =  1963.78614686\n",
      "Time for  3th cross validation = 92.1409s\n",
      "Training Accuracy         = 0.8346\n",
      "Cross Validation Accuracy = 0.807668\n",
      "Losgistic Regression(       0/10000): loss= 3440.07569314414\n",
      "Losgistic Regression(     100/10000): loss= 2340.04584639612\n",
      "Losgistic Regression(     200/10000): loss= 2173.6630512212\n",
      "Losgistic Regression(     300/10000): loss= 2111.40159922639\n",
      "Losgistic Regression(     400/10000): loss= 2063.88571721465\n",
      "Losgistic Regression(     500/10000): loss= 2028.99454405234\n",
      "Losgistic Regression(     600/10000): loss= 2003.24994569784\n",
      "Losgistic Regression(     700/10000): loss= 1982.95356134161\n",
      "Losgistic Regression(     800/10000): loss= 1966.74795206592\n",
      "Losgistic Regression(     900/10000): loss= 1954.11334584739\n",
      "Losgistic Regression(    1000/10000): loss= 1946.01856067693\n",
      "Losgistic Regression(    1100/10000): loss= 1939.39712471985\n",
      "Losgistic Regression(    1200/10000): loss= 1933.01209210788\n",
      "Losgistic Regression(    1300/10000): loss= 1930.25626881486\n",
      "Losgistic Regression(    1400/10000): loss= 1928.02844175446\n",
      "Losgistic Regression(    1500/10000): loss= 1926.29416035217\n",
      "Losgistic Regression(    1600/10000): loss= 1925.8095520203\n",
      "Losgistic Regression(    1700/10000): loss= 1925.80596189201\n",
      "Totoal number of iterations =  1700\n",
      "Loss                        =  1925.80596189\n",
      "Time for  4th cross validation = 28.1989s\n",
      "Training Accuracy         = 0.8388\n",
      "Cross Validation Accuracy = 0.805544\n",
      "*************** ([0.83960000000000001, 0.83699999999999997, 0.84279999999999999, 0.83460000000000001, 0.83879999999999999], [0.81000799999999995, 0.80048799999999998, 0.81146399999999996, 0.80766800000000005, 0.80554400000000004])\n",
      "Losgistic Regression(       0/10000): loss= 3436.88854196188\n",
      "Losgistic Regression(     100/10000): loss= 2422.07502547281\n",
      "Losgistic Regression(     200/10000): loss= 2276.92427738244\n",
      "Losgistic Regression(     300/10000): loss= 2240.15253955927\n",
      "Losgistic Regression(     400/10000): loss= 2219.89722417542\n",
      "Losgistic Regression(     500/10000): loss= 2207.23536571765\n",
      "Losgistic Regression(     600/10000): loss= 2198.44291979959\n",
      "Losgistic Regression(     700/10000): loss= 2192.80095458832\n",
      "Losgistic Regression(     800/10000): loss= 2188.28019142184\n",
      "Losgistic Regression(     900/10000): loss= 2185.13644614844\n",
      "Losgistic Regression(    1000/10000): loss= 2182.43311329151\n",
      "Losgistic Regression(    1100/10000): loss= 2180.1077817887\n",
      "Losgistic Regression(    1200/10000): loss= 2178.21815489864\n",
      "Losgistic Regression(    1300/10000): loss= 2176.8520406411\n",
      "Losgistic Regression(    1400/10000): loss= 2175.58355205266\n",
      "Losgistic Regression(    1500/10000): loss= 2175.01052864813\n",
      "Losgistic Regression(    1600/10000): loss= 2174.79613645634\n",
      "Losgistic Regression(    1700/10000): loss= 2174.5943976486\n",
      "Losgistic Regression(    1800/10000): loss= 2174.48800237438\n",
      "Losgistic Regression(    1900/10000): loss= 2174.4844778533\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  2174.48447785\n",
      "Time for  0th cross validation = 31.3791s\n",
      "Training Accuracy         = 0.8216\n",
      "Cross Validation Accuracy = 0.802548\n",
      "Losgistic Regression(       0/10000): loss= 3438.81810800326\n",
      "Losgistic Regression(     100/10000): loss= 2461.14048511831\n",
      "Losgistic Regression(     200/10000): loss= 2329.2226596707\n",
      "Losgistic Regression(     300/10000): loss= 2290.2917151482\n",
      "Losgistic Regression(     400/10000): loss= 2269.25337219101\n",
      "Losgistic Regression(     500/10000): loss= 2256.05916332991\n",
      "Losgistic Regression(     600/10000): loss= 2247.58441059543\n",
      "Losgistic Regression(     700/10000): loss= 2242.00429615003\n",
      "Losgistic Regression(     800/10000): loss= 2238.17272390211\n",
      "Losgistic Regression(     900/10000): loss= 2235.40044380255\n",
      "Losgistic Regression(    1000/10000): loss= 2233.29905490685\n",
      "Losgistic Regression(    1100/10000): loss= 2232.56140104189\n",
      "Losgistic Regression(    1200/10000): loss= 2232.21822242884\n",
      "Losgistic Regression(    1300/10000): loss= 2231.94762310427\n",
      "Losgistic Regression(    1400/10000): loss= 2231.64889901358\n",
      "Losgistic Regression(    1500/10000): loss= 2231.27857636443\n",
      "Losgistic Regression(    1600/10000): loss= 2230.86712859216\n",
      "Losgistic Regression(    1700/10000): loss= 2230.44331280567\n",
      "Losgistic Regression(    1800/10000): loss= 2230.02637730451\n",
      "Losgistic Regression(    1900/10000): loss= 2229.59197805255\n",
      "Losgistic Regression(    2000/10000): loss= 2229.14927372669\n",
      "Losgistic Regression(    2100/10000): loss= 2228.7021664696\n",
      "Losgistic Regression(    2200/10000): loss= 2228.28312902109\n",
      "Losgistic Regression(    2300/10000): loss= 2227.87387457787\n",
      "Losgistic Regression(    2400/10000): loss= 2227.67991899212\n",
      "Losgistic Regression(    2500/10000): loss= 2227.67855641063\n",
      "Totoal number of iterations =  2500\n",
      "Loss                        =  2227.67855641\n",
      "Time for  1th cross validation = 41.0332s\n",
      "Training Accuracy         = 0.8178\n",
      "Cross Validation Accuracy = 0.797564\n",
      "Losgistic Regression(       0/10000): loss= 3439.16702762254\n",
      "Losgistic Regression(     100/10000): loss= 2412.7847749669\n",
      "Losgistic Regression(     200/10000): loss= 2273.21703185044\n",
      "Losgistic Regression(     300/10000): loss= 2231.56818032164\n",
      "Losgistic Regression(     400/10000): loss= 2209.72198623999\n",
      "Losgistic Regression(     500/10000): loss= 2194.7499355595\n",
      "Losgistic Regression(     600/10000): loss= 2183.87559348125\n",
      "Losgistic Regression(     700/10000): loss= 2176.07678910064\n",
      "Losgistic Regression(     800/10000): loss= 2171.23785499966\n",
      "Losgistic Regression(     900/10000): loss= 2167.92798615989\n",
      "Losgistic Regression(    1000/10000): loss= 2165.29665138148\n",
      "Losgistic Regression(    1100/10000): loss= 2163.49740313451\n",
      "Losgistic Regression(    1200/10000): loss= 2162.18065542125\n",
      "Losgistic Regression(    1300/10000): loss= 2161.83115950643\n",
      "Losgistic Regression(    1400/10000): loss= 2161.62846017097\n",
      "Losgistic Regression(    1500/10000): loss= 2161.40379560288\n",
      "Losgistic Regression(    1600/10000): loss= 2161.13969240353\n",
      "Losgistic Regression(    1700/10000): loss= 2160.83674495137\n",
      "Losgistic Regression(    1800/10000): loss= 2160.49735684051\n",
      "Losgistic Regression(    1900/10000): loss= 2160.12391406136\n",
      "Losgistic Regression(    2000/10000): loss= 2159.7208427634\n",
      "Losgistic Regression(    2100/10000): loss= 2159.30467062906\n",
      "Losgistic Regression(    2200/10000): loss= 2158.86981874956\n",
      "Losgistic Regression(    2300/10000): loss= 2158.41978082437\n",
      "Losgistic Regression(    2400/10000): loss= 2157.97322734457\n",
      "Losgistic Regression(    2500/10000): loss= 2157.55279966166\n",
      "Losgistic Regression(    2600/10000): loss= 2157.20192888207\n",
      "Losgistic Regression(    2700/10000): loss= 2157.19801166341\n",
      "Totoal number of iterations =  2700\n",
      "Loss                        =  2157.19801166\n",
      "Time for  2th cross validation = 44.6733s\n",
      "Training Accuracy         = 0.8218\n",
      "Cross Validation Accuracy = 0.803776\n",
      "Losgistic Regression(       0/10000): loss= 3439.60904650075\n",
      "Losgistic Regression(     100/10000): loss= 2419.29973869335\n",
      "Losgistic Regression(     200/10000): loss= 2285.98584155347\n",
      "Losgistic Regression(     300/10000): loss= 2249.70592358447\n",
      "Losgistic Regression(     400/10000): loss= 2228.31006707728\n",
      "Losgistic Regression(     500/10000): loss= 2213.05452610922\n",
      "Losgistic Regression(     600/10000): loss= 2203.68022946573\n",
      "Losgistic Regression(     700/10000): loss= 2196.73934379506\n",
      "Losgistic Regression(     800/10000): loss= 2192.22819796773\n",
      "Losgistic Regression(     900/10000): loss= 2189.1992513601\n",
      "Losgistic Regression(    1000/10000): loss= 2186.6212477953\n",
      "Losgistic Regression(    1100/10000): loss= 2184.35256203238\n",
      "Losgistic Regression(    1200/10000): loss= 2182.5870375483\n",
      "Losgistic Regression(    1300/10000): loss= 2181.11125298023\n",
      "Losgistic Regression(    1400/10000): loss= 2179.55999488206\n",
      "Losgistic Regression(    1500/10000): loss= 2178.40285971298\n",
      "Losgistic Regression(    1600/10000): loss= 2177.28072447775\n",
      "Losgistic Regression(    1700/10000): loss= 2176.05835065014\n",
      "Losgistic Regression(    1800/10000): loss= 2175.93807163478\n",
      "Losgistic Regression(    1900/10000): loss= 2175.93262110481\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  2175.9326211\n",
      "Time for  3th cross validation = 30.656s\n",
      "Training Accuracy         = 0.8216\n",
      "Cross Validation Accuracy = 0.803396\n",
      "Losgistic Regression(       0/10000): loss= 3440.50436362544\n",
      "Losgistic Regression(     100/10000): loss= 2406.51789928352\n",
      "Losgistic Regression(     200/10000): loss= 2277.60360730416\n",
      "Losgistic Regression(     300/10000): loss= 2242.20297676209\n",
      "Losgistic Regression(     400/10000): loss= 2219.83676774876\n",
      "Losgistic Regression(     500/10000): loss= 2203.38789389431\n",
      "Losgistic Regression(     600/10000): loss= 2192.3956300541\n",
      "Losgistic Regression(     700/10000): loss= 2185.80013400588\n",
      "Losgistic Regression(     800/10000): loss= 2180.33255372055\n",
      "Losgistic Regression(     900/10000): loss= 2174.86161835612\n",
      "Losgistic Regression(    1000/10000): loss= 2170.32367302994\n",
      "Losgistic Regression(    1100/10000): loss= 2166.66270667815\n",
      "Losgistic Regression(    1200/10000): loss= 2164.84608882104\n",
      "Losgistic Regression(    1300/10000): loss= 2163.39548136397\n",
      "Losgistic Regression(    1400/10000): loss= 2161.55143104348\n",
      "Losgistic Regression(    1500/10000): loss= 2160.37509144261\n",
      "Losgistic Regression(    1600/10000): loss= 2159.67272556972\n",
      "Losgistic Regression(    1700/10000): loss= 2159.67774996368\n",
      "Totoal number of iterations =  1700\n",
      "Loss                        =  2159.67774996\n",
      "Time for  4th cross validation = 27.5177s\n",
      "Training Accuracy         =  0.819\n",
      "Cross Validation Accuracy = 0.798812\n",
      "*************** ([0.8216, 0.81779999999999997, 0.82179999999999997, 0.8216, 0.81899999999999995], [0.80254800000000004, 0.79756400000000005, 0.80377600000000005, 0.803396, 0.79881199999999997])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.85899999999999999,\n",
       "   0.85040000000000004,\n",
       "   0.85640000000000005,\n",
       "   0.85360000000000003,\n",
       "   0.85419999999999996],\n",
       "  [0.80532800000000004,\n",
       "   0.79402799999999996,\n",
       "   0.80358399999999996,\n",
       "   0.80544800000000005,\n",
       "   0.80587600000000004]),\n",
       " ([0.85840000000000005,\n",
       "   0.84919999999999995,\n",
       "   0.85619999999999996,\n",
       "   0.85219999999999996,\n",
       "   0.85360000000000003],\n",
       "  [0.80571599999999999,\n",
       "   0.79476800000000003,\n",
       "   0.80413599999999996,\n",
       "   0.80613199999999996,\n",
       "   0.80635199999999996]),\n",
       " ([0.85840000000000005,\n",
       "   0.84999999999999998,\n",
       "   0.85680000000000001,\n",
       "   0.85019999999999996,\n",
       "   0.85399999999999998],\n",
       "  [0.80754800000000004,\n",
       "   0.79569599999999996,\n",
       "   0.805732,\n",
       "   0.80727199999999999,\n",
       "   0.80766000000000004]),\n",
       " ([0.85460000000000003,\n",
       "   0.84179999999999999,\n",
       "   0.85299999999999998,\n",
       "   0.84140000000000004,\n",
       "   0.84899999999999998],\n",
       "  [0.80979999999999996,\n",
       "   0.79728399999999999,\n",
       "   0.810392,\n",
       "   0.80813999999999997,\n",
       "   0.80949599999999999]),\n",
       " ([0.83960000000000001,\n",
       "   0.83699999999999997,\n",
       "   0.84279999999999999,\n",
       "   0.83460000000000001,\n",
       "   0.83879999999999999],\n",
       "  [0.81000799999999995,\n",
       "   0.80048799999999998,\n",
       "   0.81146399999999996,\n",
       "   0.80766800000000005,\n",
       "   0.80554400000000004]),\n",
       " ([0.8216,\n",
       "   0.81779999999999997,\n",
       "   0.82179999999999997,\n",
       "   0.8216,\n",
       "   0.81899999999999995],\n",
       "  [0.80254800000000004,\n",
       "   0.79756400000000005,\n",
       "   0.80377600000000005,\n",
       "   0.803396,\n",
       "   0.79881199999999997])]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_5000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(5000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_5000.append(tmp)\n",
    "\n",
    "accu_5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 1725.77375301506\n",
      "Losgistic Regression(     100/10000): loss= 1237.43155470564\n",
      "Losgistic Regression(     200/10000): loss= 1074.24030546807\n",
      "Losgistic Regression(     300/10000): loss= 1014.06618478128\n",
      "Losgistic Regression(     400/10000): loss= 978.027651243102\n",
      "Losgistic Regression(     500/10000): loss= 951.520269440068\n",
      "Losgistic Regression(     600/10000): loss= 930.424732596964\n",
      "Losgistic Regression(     700/10000): loss= 913.044139632886\n",
      "Losgistic Regression(     800/10000): loss= 897.968405972309\n",
      "Losgistic Regression(     900/10000): loss= 884.696716677702\n",
      "Losgistic Regression(    1000/10000): loss= 872.801885068041\n",
      "Losgistic Regression(    1100/10000): loss= 862.062953044757\n",
      "Losgistic Regression(    1200/10000): loss= 852.332872905332\n",
      "Losgistic Regression(    1300/10000): loss= 843.503209681073\n",
      "Losgistic Regression(    1400/10000): loss= 835.484573541811\n",
      "Losgistic Regression(    1500/10000): loss= 828.184729862094\n",
      "Losgistic Regression(    1600/10000): loss= 821.520489627183\n",
      "Losgistic Regression(    1700/10000): loss= 815.418109721283\n",
      "Losgistic Regression(    1800/10000): loss= 809.813549915975\n",
      "Losgistic Regression(    1900/10000): loss= 804.650222536568\n",
      "Losgistic Regression(    2000/10000): loss= 799.876595212611\n",
      "Losgistic Regression(    2100/10000): loss= 795.447664922853\n",
      "Losgistic Regression(    2200/10000): loss= 791.325175454427\n",
      "Losgistic Regression(    2300/10000): loss= 787.477573446029\n",
      "Losgistic Regression(    2400/10000): loss= 783.877770719885\n",
      "Losgistic Regression(    2500/10000): loss= 780.500490183363\n",
      "Losgistic Regression(    2600/10000): loss= 776.882791543116\n",
      "Losgistic Regression(    2700/10000): loss= 773.379428739551\n",
      "Losgistic Regression(    2800/10000): loss= 770.084397471544\n",
      "Losgistic Regression(    2900/10000): loss= 766.969566296655\n",
      "Losgistic Regression(    3000/10000): loss= 764.006662054039\n",
      "Losgistic Regression(    3100/10000): loss= 761.175156675755\n",
      "Losgistic Regression(    3200/10000): loss= 758.138573883386\n",
      "Losgistic Regression(    3300/10000): loss= 755.175267423992\n",
      "Losgistic Regression(    3400/10000): loss= 752.319615063159\n",
      "Losgistic Regression(    3500/10000): loss= 749.365087319731\n",
      "Losgistic Regression(    3600/10000): loss= 746.44336914406\n",
      "Losgistic Regression(    3700/10000): loss= 743.626879045995\n",
      "Losgistic Regression(    3800/10000): loss= 740.914104109728\n",
      "Losgistic Regression(    3900/10000): loss= 738.293929862056\n",
      "Losgistic Regression(    4000/10000): loss= 735.752506704935\n",
      "Losgistic Regression(    4100/10000): loss= 733.286619205147\n",
      "Losgistic Regression(    4200/10000): loss= 730.896850099576\n",
      "Losgistic Regression(    4300/10000): loss= 728.253524582533\n",
      "Losgistic Regression(    4400/10000): loss= 725.561596610137\n",
      "Losgistic Regression(    4500/10000): loss= 722.94852457766\n",
      "Losgistic Regression(    4600/10000): loss= 720.417118394179\n",
      "Losgistic Regression(    4700/10000): loss= 717.946482397499\n",
      "Losgistic Regression(    4800/10000): loss= 715.52950501773\n",
      "Losgistic Regression(    4900/10000): loss= 713.165691650749\n",
      "Losgistic Regression(    5000/10000): loss= 710.849843577383\n",
      "Losgistic Regression(    5100/10000): loss= 708.589209857254\n",
      "Losgistic Regression(    5200/10000): loss= 706.391011509459\n",
      "Losgistic Regression(    5300/10000): loss= 704.25751740066\n",
      "Losgistic Regression(    5400/10000): loss= 702.181769094512\n",
      "Losgistic Regression(    5500/10000): loss= 700.041696502665\n",
      "Losgistic Regression(    5600/10000): loss= 697.9094935969\n",
      "Losgistic Regression(    5700/10000): loss= 695.699824989111\n",
      "Losgistic Regression(    5800/10000): loss= 693.312628092548\n",
      "Losgistic Regression(    5900/10000): loss= 690.893272013875\n",
      "Losgistic Regression(    6000/10000): loss= 688.512197054916\n",
      "Losgistic Regression(    6100/10000): loss= 686.174491690843\n",
      "Losgistic Regression(    6200/10000): loss= 683.884796819837\n",
      "Losgistic Regression(    6300/10000): loss= 681.643077101081\n",
      "Losgistic Regression(    6400/10000): loss= 679.367761049828\n",
      "Losgistic Regression(    6500/10000): loss= 677.052377271445\n",
      "Losgistic Regression(    6600/10000): loss= 674.77780115829\n",
      "Losgistic Regression(    6700/10000): loss= 672.536897428978\n",
      "Losgistic Regression(    6800/10000): loss= 670.323362031155\n",
      "Losgistic Regression(    6900/10000): loss= 668.051025178809\n",
      "Losgistic Regression(    7000/10000): loss= 665.52554159041\n",
      "Losgistic Regression(    7100/10000): loss= 663.030429634338\n",
      "Losgistic Regression(    7200/10000): loss= 660.513070598005\n",
      "Losgistic Regression(    7300/10000): loss= 657.981269947697\n",
      "Losgistic Regression(    7400/10000): loss= 655.47730986618\n",
      "Losgistic Regression(    7500/10000): loss= 653.009404233245\n",
      "Losgistic Regression(    7600/10000): loss= 650.581825179534\n",
      "Losgistic Regression(    7700/10000): loss= 648.196209446445\n",
      "Losgistic Regression(    7800/10000): loss= 645.855818694734\n",
      "Losgistic Regression(    7900/10000): loss= 643.561894194413\n",
      "Losgistic Regression(    8000/10000): loss= 641.312000211746\n",
      "Losgistic Regression(    8100/10000): loss= 639.109534397726\n",
      "Losgistic Regression(    8200/10000): loss= 636.961683980917\n",
      "Losgistic Regression(    8300/10000): loss= 634.799290217609\n",
      "Losgistic Regression(    8400/10000): loss= 632.672937023156\n",
      "Losgistic Regression(    8500/10000): loss= 630.590091978443\n",
      "Losgistic Regression(    8600/10000): loss= 628.554168527897\n",
      "Losgistic Regression(    8700/10000): loss= 626.540138029335\n",
      "Losgistic Regression(    8800/10000): loss= 624.511967038077\n",
      "Losgistic Regression(    8900/10000): loss= 622.547146795716\n",
      "Losgistic Regression(    9000/10000): loss= 620.63701477058\n",
      "Losgistic Regression(    9100/10000): loss= 618.776701910284\n",
      "Losgistic Regression(    9200/10000): loss= 616.958496596126\n",
      "Losgistic Regression(    9300/10000): loss= 615.175616828867\n",
      "Losgistic Regression(    9400/10000): loss= 613.431504257603\n",
      "Losgistic Regression(    9500/10000): loss= 611.729954509764\n",
      "Losgistic Regression(    9600/10000): loss= 610.067269806176\n",
      "Losgistic Regression(    9700/10000): loss= 608.438213869229\n",
      "Losgistic Regression(    9800/10000): loss= 606.788785872125\n",
      "Losgistic Regression(    9900/10000): loss= 604.991952078666\n",
      "Time for  0th cross validation = 83.6812s\n",
      "Training Accuracy         = 0.8832\n",
      "Cross Validation Accuracy = 0.784008\n",
      "Losgistic Regression(       0/10000): loss= 1724.814857444\n",
      "Losgistic Regression(     100/10000): loss= 1222.98495004634\n",
      "Losgistic Regression(     200/10000): loss= 1061.3014782467\n",
      "Losgistic Regression(     300/10000): loss= 995.832498248115\n",
      "Losgistic Regression(     400/10000): loss= 957.500008870819\n",
      "Losgistic Regression(     500/10000): loss= 929.475588296635\n",
      "Losgistic Regression(     600/10000): loss= 907.092668403834\n",
      "Losgistic Regression(     700/10000): loss= 888.401725215759\n",
      "Losgistic Regression(     800/10000): loss= 872.003075358262\n",
      "Losgistic Regression(     900/10000): loss= 857.030257059228\n",
      "Losgistic Regression(    1000/10000): loss= 844.107573150108\n",
      "Losgistic Regression(    1100/10000): loss= 832.95617782461\n",
      "Losgistic Regression(    1200/10000): loss= 823.256515875825\n",
      "Losgistic Regression(    1300/10000): loss= 814.814958886843\n",
      "Losgistic Regression(    1400/10000): loss= 807.472951370465\n",
      "Losgistic Regression(    1500/10000): loss= 801.053124403236\n",
      "Losgistic Regression(    1600/10000): loss= 795.318164701772\n",
      "Losgistic Regression(    1700/10000): loss= 789.591597241278\n",
      "Losgistic Regression(    1800/10000): loss= 784.466658329429\n",
      "Losgistic Regression(    1900/10000): loss= 779.8535264438\n",
      "Losgistic Regression(    2000/10000): loss= 775.443700705197\n",
      "Losgistic Regression(    2100/10000): loss= 770.868423871056\n",
      "Losgistic Regression(    2200/10000): loss= 765.573891120385\n",
      "Losgistic Regression(    2300/10000): loss= 760.385648586441\n",
      "Losgistic Regression(    2400/10000): loss= 755.348842907565\n",
      "Losgistic Regression(    2500/10000): loss= 750.080095691769\n",
      "Losgistic Regression(    2600/10000): loss= 744.980182868051\n",
      "Losgistic Regression(    2700/10000): loss= 740.024923643562\n",
      "Losgistic Regression(    2800/10000): loss= 735.054359041118\n",
      "Losgistic Regression(    2900/10000): loss= 729.558994367887\n",
      "Losgistic Regression(    3000/10000): loss= 723.953472345845\n",
      "Losgistic Regression(    3100/10000): loss= 718.419251222667\n",
      "Losgistic Regression(    3200/10000): loss= 712.954975240717\n",
      "Losgistic Regression(    3300/10000): loss= 707.304650701405\n",
      "Losgistic Regression(    3400/10000): loss= 700.874556256969\n",
      "Losgistic Regression(    3500/10000): loss= 694.561349652008\n",
      "Losgistic Regression(    3600/10000): loss= 688.400376762565\n",
      "Losgistic Regression(    3700/10000): loss= 682.387240479151\n",
      "Losgistic Regression(    3800/10000): loss= 676.535024877311\n",
      "Losgistic Regression(    3900/10000): loss= 670.846812576379\n",
      "Losgistic Regression(    4000/10000): loss= 664.979154304175\n",
      "Losgistic Regression(    4100/10000): loss= 659.209811850983\n",
      "Losgistic Regression(    4200/10000): loss= 653.59597166036\n",
      "Losgistic Regression(    4300/10000): loss= 648.135088005125\n",
      "Losgistic Regression(    4400/10000): loss= 642.749350589757\n",
      "Losgistic Regression(    4500/10000): loss= 637.056588573402\n",
      "Losgistic Regression(    4600/10000): loss= 631.572459938828\n",
      "Losgistic Regression(    4700/10000): loss= 626.287463220662\n",
      "Losgistic Regression(    4800/10000): loss= 621.15719527941\n",
      "Losgistic Regression(    4900/10000): loss= 616.030213382713\n",
      "Losgistic Regression(    5000/10000): loss= 611.078347667441\n",
      "Losgistic Regression(    5100/10000): loss= 606.335190530468\n",
      "Losgistic Regression(    5200/10000): loss= 601.751281613006\n",
      "Losgistic Regression(    5300/10000): loss= 597.195891600591\n",
      "Losgistic Regression(    5400/10000): loss= 592.516441923868\n",
      "Losgistic Regression(    5500/10000): loss= 587.846702656705\n",
      "Losgistic Regression(    5600/10000): loss= 583.305059983862\n",
      "Losgistic Regression(    5700/10000): loss= 578.834417794333\n",
      "Losgistic Regression(    5800/10000): loss= 574.295683635531\n",
      "Losgistic Regression(    5900/10000): loss= 569.818943316322\n",
      "Losgistic Regression(    6000/10000): loss= 565.428230757643\n",
      "Losgistic Regression(    6100/10000): loss= 561.108056228409\n",
      "Losgistic Regression(    6200/10000): loss= 556.832768474383\n",
      "Losgistic Regression(    6300/10000): loss= 552.588778852667\n",
      "Losgistic Regression(    6400/10000): loss= 548.384891212929\n",
      "Losgistic Regression(    6500/10000): loss= 544.222968589352\n",
      "Losgistic Regression(    6600/10000): loss= 540.078257000474\n",
      "Losgistic Regression(    6700/10000): loss= 535.930315022805\n",
      "Losgistic Regression(    6800/10000): loss= 531.77179235835\n",
      "Losgistic Regression(    6900/10000): loss= 527.459437004537\n",
      "Losgistic Regression(    7000/10000): loss= 522.899960740485\n",
      "Losgistic Regression(    7100/10000): loss= 518.344690911204\n",
      "Losgistic Regression(    7200/10000): loss= 513.786045304396\n",
      "Losgistic Regression(    7300/10000): loss= 509.223738773628\n",
      "Losgistic Regression(    7400/10000): loss= 504.678093720595\n",
      "Losgistic Regression(    7500/10000): loss= 500.143637205879\n",
      "Losgistic Regression(    7600/10000): loss= 495.401950067069\n",
      "Losgistic Regression(    7700/10000): loss= 490.653275971835\n",
      "Losgistic Regression(    7800/10000): loss= 485.807551064417\n",
      "Losgistic Regression(    7900/10000): loss= 480.889902698203\n",
      "Losgistic Regression(    8000/10000): loss= 475.962834711058\n",
      "Losgistic Regression(    8100/10000): loss= 471.027588041421\n",
      "Losgistic Regression(    8200/10000): loss= 466.119638132812\n",
      "Losgistic Regression(    8300/10000): loss= 461.19537385493\n",
      "Losgistic Regression(    8400/10000): loss= 456.135543809476\n",
      "Losgistic Regression(    8500/10000): loss= 451.032259087595\n",
      "Losgistic Regression(    8600/10000): loss= 445.751552096507\n",
      "Losgistic Regression(    8700/10000): loss= 440.481865224388\n",
      "Losgistic Regression(    8800/10000): loss= 435.239819792648\n",
      "Losgistic Regression(    8900/10000): loss= 430.039296767203\n",
      "Losgistic Regression(    9000/10000): loss= 424.869067442884\n",
      "Losgistic Regression(    9100/10000): loss= 419.714280624596\n",
      "Losgistic Regression(    9200/10000): loss= 414.579045752933\n",
      "Losgistic Regression(    9300/10000): loss= 409.445309911269\n",
      "Losgistic Regression(    9400/10000): loss= 404.291528134814\n",
      "Losgistic Regression(    9500/10000): loss= 399.108641244243\n",
      "Losgistic Regression(    9600/10000): loss= 393.880742247812\n",
      "Losgistic Regression(    9700/10000): loss= 388.615332175891\n",
      "Losgistic Regression(    9800/10000): loss= 383.326030047509\n",
      "Losgistic Regression(    9900/10000): loss= 378.017292020486\n",
      "Time for  1th cross validation = 83.9296s\n",
      "Training Accuracy         = 0.8864\n",
      "Cross Validation Accuracy = 0.77524\n",
      "Losgistic Regression(       0/10000): loss= 1726.0657242736\n",
      "Losgistic Regression(     100/10000): loss= 1223.73950531529\n",
      "Losgistic Regression(     200/10000): loss= 1066.33070269407\n",
      "Losgistic Regression(     300/10000): loss= 1004.28207083148\n",
      "Losgistic Regression(     400/10000): loss= 964.047178934336\n",
      "Losgistic Regression(     500/10000): loss= 934.845327726354\n",
      "Losgistic Regression(     600/10000): loss= 912.259966041411\n",
      "Losgistic Regression(     700/10000): loss= 893.948881608084\n",
      "Losgistic Regression(     800/10000): loss= 878.604438063928\n",
      "Losgistic Regression(     900/10000): loss= 865.513317836607\n",
      "Losgistic Regression(    1000/10000): loss= 852.706721912075\n",
      "Losgistic Regression(    1100/10000): loss= 840.830457153392\n",
      "Losgistic Regression(    1200/10000): loss= 830.131906936437\n",
      "Losgistic Regression(    1300/10000): loss= 820.498432970123\n",
      "Losgistic Regression(    1400/10000): loss= 811.796445227663\n",
      "Losgistic Regression(    1500/10000): loss= 803.903679337103\n",
      "Losgistic Regression(    1600/10000): loss= 796.721658929592\n",
      "Losgistic Regression(    1700/10000): loss= 790.152829003292\n",
      "Losgistic Regression(    1800/10000): loss= 783.416218456375\n",
      "Losgistic Regression(    1900/10000): loss= 776.14274602673\n",
      "Losgistic Regression(    2000/10000): loss= 768.90507844094\n",
      "Losgistic Regression(    2100/10000): loss= 762.162396115243\n",
      "Losgistic Regression(    2200/10000): loss= 755.825441229267\n",
      "Losgistic Regression(    2300/10000): loss= 749.267243041033\n",
      "Losgistic Regression(    2400/10000): loss= 742.469730812116\n",
      "Losgistic Regression(    2500/10000): loss= 736.037964128169\n",
      "Losgistic Regression(    2600/10000): loss= 729.949191118883\n",
      "Losgistic Regression(    2700/10000): loss= 724.183728707354\n",
      "Losgistic Regression(    2800/10000): loss= 718.697398037917\n",
      "Losgistic Regression(    2900/10000): loss= 713.261907950068\n",
      "Losgistic Regression(    3000/10000): loss= 708.146919496125\n",
      "Losgistic Regression(    3100/10000): loss= 703.323966708978\n",
      "Losgistic Regression(    3200/10000): loss= 698.787912935346\n",
      "Losgistic Regression(    3300/10000): loss= 694.537969213512\n",
      "Losgistic Regression(    3400/10000): loss= 690.571347209487\n",
      "Losgistic Regression(    3500/10000): loss= 686.887398859487\n",
      "Losgistic Regression(    3600/10000): loss= 683.457865070139\n",
      "Losgistic Regression(    3700/10000): loss= 680.240080045844\n",
      "Losgistic Regression(    3800/10000): loss= 677.201132239424\n",
      "Losgistic Regression(    3900/10000): loss= 674.321836296713\n",
      "Losgistic Regression(    4000/10000): loss= 671.586424503399\n",
      "Losgistic Regression(    4100/10000): loss= 668.873699658915\n",
      "Losgistic Regression(    4200/10000): loss= 666.210140466743\n",
      "Losgistic Regression(    4300/10000): loss= 663.673635378751\n",
      "Losgistic Regression(    4400/10000): loss= 661.261275623352\n",
      "Losgistic Regression(    4500/10000): loss= 658.965449518948\n",
      "Losgistic Regression(    4600/10000): loss= 656.768861002683\n",
      "Losgistic Regression(    4700/10000): loss= 654.654306854772\n",
      "Losgistic Regression(    4800/10000): loss= 652.610757378876\n",
      "Losgistic Regression(    4900/10000): loss= 650.640948011163\n",
      "Losgistic Regression(    5000/10000): loss= 648.73650072762\n",
      "Losgistic Regression(    5100/10000): loss= 646.902270468455\n",
      "Losgistic Regression(    5200/10000): loss= 645.156534357518\n",
      "Losgistic Regression(    5300/10000): loss= 643.437758679442\n",
      "Losgistic Regression(    5400/10000): loss= 641.735454703755\n",
      "Losgistic Regression(    5500/10000): loss= 640.099153028292\n",
      "Losgistic Regression(    5600/10000): loss= 638.51480913222\n",
      "Losgistic Regression(    5700/10000): loss= 636.982316316371\n",
      "Losgistic Regression(    5800/10000): loss= 635.501390832766\n",
      "Losgistic Regression(    5900/10000): loss= 634.056971043687\n",
      "Losgistic Regression(    6000/10000): loss= 632.649502786175\n",
      "Losgistic Regression(    6100/10000): loss= 631.276139399031\n",
      "Losgistic Regression(    6200/10000): loss= 629.93310497766\n",
      "Losgistic Regression(    6300/10000): loss= 628.618788167835\n",
      "Losgistic Regression(    6400/10000): loss= 627.346212907812\n",
      "Losgistic Regression(    6500/10000): loss= 626.116473622508\n",
      "Losgistic Regression(    6600/10000): loss= 624.921971178796\n",
      "Losgistic Regression(    6700/10000): loss= 623.754369467213\n",
      "Losgistic Regression(    6800/10000): loss= 622.618982553011\n",
      "Losgistic Regression(    6900/10000): loss= 621.516792391457\n",
      "Losgistic Regression(    7000/10000): loss= 620.448192771629\n",
      "Losgistic Regression(    7100/10000): loss= 619.235393719311\n",
      "Losgistic Regression(    7200/10000): loss= 618.037322955008\n",
      "Losgistic Regression(    7300/10000): loss= 616.871611897913\n",
      "Losgistic Regression(    7400/10000): loss= 615.734434150066\n",
      "Losgistic Regression(    7500/10000): loss= 614.625260722922\n",
      "Losgistic Regression(    7600/10000): loss= 613.518588072561\n",
      "Losgistic Regression(    7700/10000): loss= 612.393334163346\n",
      "Losgistic Regression(    7800/10000): loss= 611.288189480565\n",
      "Losgistic Regression(    7900/10000): loss= 610.198762162843\n",
      "Losgistic Regression(    8000/10000): loss= 609.13057147693\n",
      "Losgistic Regression(    8100/10000): loss= 608.080526427522\n",
      "Losgistic Regression(    8200/10000): loss= 607.03929385651\n",
      "Losgistic Regression(    8300/10000): loss= 606.022315463991\n",
      "Losgistic Regression(    8400/10000): loss= 605.028156300611\n",
      "Losgistic Regression(    8500/10000): loss= 604.061043823138\n",
      "Losgistic Regression(    8600/10000): loss= 603.124245234683\n",
      "Losgistic Regression(    8700/10000): loss= 602.222979449997\n",
      "Losgistic Regression(    8800/10000): loss= 601.356529072053\n",
      "Losgistic Regression(    8900/10000): loss= 600.512237012553\n",
      "Losgistic Regression(    9000/10000): loss= 599.689082144395\n",
      "Losgistic Regression(    9100/10000): loss= 598.90238446385\n",
      "Losgistic Regression(    9200/10000): loss= 598.155084661028\n",
      "Losgistic Regression(    9300/10000): loss= 597.440939537806\n",
      "Losgistic Regression(    9400/10000): loss= 596.758357442787\n",
      "Losgistic Regression(    9500/10000): loss= 596.104637115267\n",
      "Losgistic Regression(    9600/10000): loss= 595.479042574099\n",
      "Losgistic Regression(    9700/10000): loss= 594.882756769566\n",
      "Losgistic Regression(    9800/10000): loss= 594.148478208819\n",
      "Losgistic Regression(    9900/10000): loss= 593.416288059089\n",
      "Time for  2th cross validation = 83.0282s\n",
      "Training Accuracy         =   0.88\n",
      "Cross Validation Accuracy = 0.769156\n",
      "Losgistic Regression(       0/10000): loss= 1725.43977258251\n",
      "Losgistic Regression(     100/10000): loss= 1249.44492223939\n",
      "Losgistic Regression(     200/10000): loss= 1109.97797742257\n",
      "Losgistic Regression(     300/10000): loss= 1048.96579032818\n",
      "Losgistic Regression(     400/10000): loss= 1008.30115009525\n",
      "Losgistic Regression(     500/10000): loss= 977.778117713839\n",
      "Losgistic Regression(     600/10000): loss= 953.90534880792\n",
      "Losgistic Regression(     700/10000): loss= 934.370150729569\n",
      "Losgistic Regression(     800/10000): loss= 914.370322546053\n",
      "Losgistic Regression(     900/10000): loss= 895.60573394289\n",
      "Losgistic Regression(    1000/10000): loss= 874.586676076435\n",
      "Losgistic Regression(    1100/10000): loss= 855.840169113347\n",
      "Losgistic Regression(    1200/10000): loss= 839.414731586339\n",
      "Losgistic Regression(    1300/10000): loss= 825.068345647464\n",
      "Losgistic Regression(    1400/10000): loss= 812.671995287228\n",
      "Losgistic Regression(    1500/10000): loss= 802.000126644201\n",
      "Losgistic Regression(    1600/10000): loss= 792.657246927466\n",
      "Losgistic Regression(    1700/10000): loss= 783.913168152112\n",
      "Losgistic Regression(    1800/10000): loss= 776.049076938202\n",
      "Losgistic Regression(    1900/10000): loss= 768.925996823395\n",
      "Losgistic Regression(    2000/10000): loss= 762.410554855213\n",
      "Losgistic Regression(    2100/10000): loss= 756.363249342327\n",
      "Losgistic Regression(    2200/10000): loss= 750.686321268076\n",
      "Losgistic Regression(    2300/10000): loss= 745.052726660917\n",
      "Losgistic Regression(    2400/10000): loss= 739.643139754849\n",
      "Losgistic Regression(    2500/10000): loss= 734.598430123359\n",
      "Losgistic Regression(    2600/10000): loss= 729.849514745378\n",
      "Losgistic Regression(    2700/10000): loss= 725.343769203752\n",
      "Losgistic Regression(    2800/10000): loss= 721.084754194799\n",
      "Losgistic Regression(    2900/10000): loss= 717.098378551176\n",
      "Losgistic Regression(    3000/10000): loss= 713.38576481519\n",
      "Losgistic Regression(    3100/10000): loss= 709.923517452849\n",
      "Losgistic Regression(    3200/10000): loss= 706.698540380458\n",
      "Losgistic Regression(    3300/10000): loss= 703.70410777254\n",
      "Losgistic Regression(    3400/10000): loss= 700.914363107316\n",
      "Losgistic Regression(    3500/10000): loss= 697.98076129573\n",
      "Losgistic Regression(    3600/10000): loss= 695.068856475273\n",
      "Losgistic Regression(    3700/10000): loss= 692.283341707344\n",
      "Losgistic Regression(    3800/10000): loss= 689.481309467266\n",
      "Losgistic Regression(    3900/10000): loss= 686.552306158779\n",
      "Losgistic Regression(    4000/10000): loss= 683.471845663905\n",
      "Losgistic Regression(    4100/10000): loss= 680.479516715718\n",
      "Losgistic Regression(    4200/10000): loss= 677.562684404134\n",
      "Losgistic Regression(    4300/10000): loss= 674.726920298921\n",
      "Losgistic Regression(    4400/10000): loss= 671.975098082174\n",
      "Losgistic Regression(    4500/10000): loss= 669.02184905725\n",
      "Losgistic Regression(    4600/10000): loss= 666.043302868272\n",
      "Losgistic Regression(    4700/10000): loss= 663.161005121292\n",
      "Losgistic Regression(    4800/10000): loss= 660.398621562062\n",
      "Losgistic Regression(    4900/10000): loss= 657.759832483856\n",
      "Losgistic Regression(    5000/10000): loss= 655.229104046108\n",
      "Losgistic Regression(    5100/10000): loss= 652.81776296261\n",
      "Losgistic Regression(    5200/10000): loss= 650.562288838893\n",
      "Losgistic Regression(    5300/10000): loss= 648.474564685224\n",
      "Losgistic Regression(    5400/10000): loss= 646.53525111976\n",
      "Losgistic Regression(    5500/10000): loss= 644.737958719747\n",
      "Losgistic Regression(    5600/10000): loss= 642.970532298552\n",
      "Losgistic Regression(    5700/10000): loss= 641.209850442101\n",
      "Losgistic Regression(    5800/10000): loss= 639.560163633605\n",
      "Losgistic Regression(    5900/10000): loss= 638.022024355789\n",
      "Losgistic Regression(    6000/10000): loss= 636.590521226558\n",
      "Losgistic Regression(    6100/10000): loss= 635.267254953552\n",
      "Losgistic Regression(    6200/10000): loss= 634.047631385683\n",
      "Losgistic Regression(    6300/10000): loss= 632.92692058053\n",
      "Losgistic Regression(    6400/10000): loss= 631.903396090547\n",
      "Losgistic Regression(    6500/10000): loss= 630.839557018502\n",
      "Losgistic Regression(    6600/10000): loss= 629.818284024017\n",
      "Losgistic Regression(    6700/10000): loss= 628.86147723534\n",
      "Losgistic Regression(    6800/10000): loss= 627.952801256819\n",
      "Losgistic Regression(    6900/10000): loss= 626.97605516279\n",
      "Losgistic Regression(    7000/10000): loss= 626.084956160543\n",
      "Losgistic Regression(    7100/10000): loss= 625.274144779175\n",
      "Losgistic Regression(    7200/10000): loss= 624.528337597234\n",
      "Losgistic Regression(    7300/10000): loss= 623.821441728598\n",
      "Losgistic Regression(    7400/10000): loss= 622.992306014599\n",
      "Losgistic Regression(    7500/10000): loss= 622.243632763374\n",
      "Losgistic Regression(    7600/10000): loss= 621.575425283489\n",
      "Losgistic Regression(    7700/10000): loss= 620.982551661067\n",
      "Losgistic Regression(    7800/10000): loss= 620.474860512094\n",
      "Losgistic Regression(    7900/10000): loss= 619.663179614826\n",
      "Losgistic Regression(    8000/10000): loss= 618.658850076437\n",
      "Losgistic Regression(    8100/10000): loss= 617.690731551498\n",
      "Losgistic Regression(    8200/10000): loss= 616.753257578426\n",
      "Losgistic Regression(    8300/10000): loss= 615.847236302373\n",
      "Losgistic Regression(    8400/10000): loss= 614.964353018956\n",
      "Losgistic Regression(    8500/10000): loss= 614.103779611669\n",
      "Losgistic Regression(    8600/10000): loss= 613.267521769267\n",
      "Losgistic Regression(    8700/10000): loss= 612.456333825669\n",
      "Losgistic Regression(    8800/10000): loss= 611.685479918843\n",
      "Losgistic Regression(    8900/10000): loss= 610.957107676977\n",
      "Losgistic Regression(    9000/10000): loss= 610.275552240386\n",
      "Losgistic Regression(    9100/10000): loss= 609.646599471584\n",
      "Losgistic Regression(    9200/10000): loss= 608.96534119153\n",
      "Losgistic Regression(    9300/10000): loss= 608.148181284579\n",
      "Losgistic Regression(    9400/10000): loss= 607.220431162699\n",
      "Losgistic Regression(    9500/10000): loss= 606.321874176936\n",
      "Losgistic Regression(    9600/10000): loss= 605.44968833129\n",
      "Losgistic Regression(    9700/10000): loss= 604.606098836588\n",
      "Losgistic Regression(    9800/10000): loss= 603.795181437835\n",
      "Losgistic Regression(    9900/10000): loss= 602.961330598426\n",
      "Time for  3th cross validation = 81.7174s\n",
      "Training Accuracy         = 0.8744\n",
      "Cross Validation Accuracy = 0.78068\n",
      "Losgistic Regression(       0/10000): loss= 1726.0408963008\n",
      "Losgistic Regression(     100/10000): loss= 1230.33901101375\n",
      "Losgistic Regression(     200/10000): loss= 1074.0319133613\n",
      "Losgistic Regression(     300/10000): loss= 1011.596930871\n",
      "Losgistic Regression(     400/10000): loss= 969.884363047395\n",
      "Losgistic Regression(     500/10000): loss= 938.379834861964\n",
      "Losgistic Regression(     600/10000): loss= 913.336786099266\n",
      "Losgistic Regression(     700/10000): loss= 892.837942937383\n",
      "Losgistic Regression(     800/10000): loss= 874.345943007701\n",
      "Losgistic Regression(     900/10000): loss= 855.758212851357\n",
      "Losgistic Regression(    1000/10000): loss= 839.23489827231\n",
      "Losgistic Regression(    1100/10000): loss= 824.114010984937\n",
      "Losgistic Regression(    1200/10000): loss= 810.47612911422\n",
      "Losgistic Regression(    1300/10000): loss= 798.234027308609\n",
      "Losgistic Regression(    1400/10000): loss= 787.16478540486\n",
      "Losgistic Regression(    1500/10000): loss= 775.25680812325\n",
      "Losgistic Regression(    1600/10000): loss= 764.237235215388\n",
      "Losgistic Regression(    1700/10000): loss= 754.040187972589\n",
      "Losgistic Regression(    1800/10000): loss= 744.10360974512\n",
      "Losgistic Regression(    1900/10000): loss= 734.546771624468\n",
      "Losgistic Regression(    2000/10000): loss= 725.593408167114\n",
      "Losgistic Regression(    2100/10000): loss= 717.206709472922\n",
      "Losgistic Regression(    2200/10000): loss= 709.25944453633\n",
      "Losgistic Regression(    2300/10000): loss= 701.644266599502\n",
      "Losgistic Regression(    2400/10000): loss= 694.352537642211\n",
      "Losgistic Regression(    2500/10000): loss= 687.417541291983\n",
      "Losgistic Regression(    2600/10000): loss= 680.825999633391\n",
      "Losgistic Regression(    2700/10000): loss= 674.38393618256\n",
      "Losgistic Regression(    2800/10000): loss= 667.906738640157\n",
      "Losgistic Regression(    2900/10000): loss= 661.592998959678\n",
      "Losgistic Regression(    3000/10000): loss= 655.431397919825\n",
      "Losgistic Regression(    3100/10000): loss= 649.405906062104\n",
      "Losgistic Regression(    3200/10000): loss= 643.271104917367\n",
      "Losgistic Regression(    3300/10000): loss= 637.088166628393\n",
      "Losgistic Regression(    3400/10000): loss= 630.909493558772\n",
      "Losgistic Regression(    3500/10000): loss= 624.696171791977\n",
      "Losgistic Regression(    3600/10000): loss= 618.432607545915\n",
      "Losgistic Regression(    3700/10000): loss= 612.110222058633\n",
      "Losgistic Regression(    3800/10000): loss= 605.714232874529\n",
      "Losgistic Regression(    3900/10000): loss= 599.231247887745\n",
      "Losgistic Regression(    4000/10000): loss= 592.667063446323\n",
      "Losgistic Regression(    4100/10000): loss= 586.024678937057\n",
      "Losgistic Regression(    4200/10000): loss= 579.189437482439\n",
      "Losgistic Regression(    4300/10000): loss= 572.161379349717\n",
      "Losgistic Regression(    4400/10000): loss= 565.05411901277\n",
      "Losgistic Regression(    4500/10000): loss= 557.610084391489\n",
      "Losgistic Regression(    4600/10000): loss= 550.075288444323\n",
      "Losgistic Regression(    4700/10000): loss= 542.35721125553\n",
      "Losgistic Regression(    4800/10000): loss= 534.417565785714\n",
      "Losgistic Regression(    4900/10000): loss= 526.446244857001\n",
      "Losgistic Regression(    5000/10000): loss= 518.470795445707\n",
      "Losgistic Regression(    5100/10000): loss= 510.496915308593\n",
      "Losgistic Regression(    5200/10000): loss= 502.526703900502\n",
      "Losgistic Regression(    5300/10000): loss= 494.560742029215\n",
      "Losgistic Regression(    5400/10000): loss= 486.609663320972\n",
      "Losgistic Regression(    5500/10000): loss= 478.689349730453\n",
      "Losgistic Regression(    5600/10000): loss= 470.673536853183\n",
      "Losgistic Regression(    5700/10000): loss= 462.498527011489\n",
      "Losgistic Regression(    5800/10000): loss= 454.282358124964\n",
      "Losgistic Regression(    5900/10000): loss= 445.973590781988\n",
      "Losgistic Regression(    6000/10000): loss= 437.672909941106\n",
      "Losgistic Regression(    6100/10000): loss= 429.366759359469\n",
      "Losgistic Regression(    6200/10000): loss= 421.046430235565\n",
      "Losgistic Regression(    6300/10000): loss= 412.731104182337\n",
      "Losgistic Regression(    6400/10000): loss= 404.432549330851\n",
      "Losgistic Regression(    6500/10000): loss= 396.145856871434\n",
      "Losgistic Regression(    6600/10000): loss= 387.874072730037\n",
      "Losgistic Regression(    6700/10000): loss= 379.616241033672\n",
      "Losgistic Regression(    6800/10000): loss= 371.373530680586\n",
      "Losgistic Regression(    6900/10000): loss= 363.163700661794\n",
      "Losgistic Regression(    7000/10000): loss= 354.994303691297\n",
      "Losgistic Regression(    7100/10000): loss= 346.861469803592\n",
      "Losgistic Regression(    7200/10000): loss= 338.764243424952\n",
      "Losgistic Regression(    7300/10000): loss= 330.694261129473\n",
      "Losgistic Regression(    7400/10000): loss= 322.657420325291\n",
      "Losgistic Regression(    7500/10000): loss= 314.653367436429\n",
      "Losgistic Regression(    7600/10000): loss= 306.683387462005\n",
      "Losgistic Regression(    7700/10000): loss= 298.725865245984\n",
      "Losgistic Regression(    7800/10000): loss= 290.709969202403\n",
      "Losgistic Regression(    7900/10000): loss= 282.711518543134\n",
      "Losgistic Regression(    8000/10000): loss= 274.73396309278\n",
      "Losgistic Regression(    8100/10000): loss= 266.781358434512\n",
      "Losgistic Regression(    8200/10000): loss= 258.863987519745\n",
      "Losgistic Regression(    8300/10000): loss= 250.987386794525\n",
      "Losgistic Regression(    8400/10000): loss= 243.160339512384\n",
      "Losgistic Regression(    8500/10000): loss= 235.38409776165\n",
      "Losgistic Regression(    8600/10000): loss= 227.666472553602\n",
      "Losgistic Regression(    8700/10000): loss= 220.004173171218\n",
      "Losgistic Regression(    8800/10000): loss= 212.385510800714\n",
      "Losgistic Regression(    8900/10000): loss= 204.738335357283\n",
      "Losgistic Regression(    9000/10000): loss= 196.988670758664\n",
      "Losgistic Regression(    9100/10000): loss= 189.233736530162\n",
      "Losgistic Regression(    9200/10000): loss= 181.562324866253\n",
      "Losgistic Regression(    9300/10000): loss= 174.061792636552\n",
      "Losgistic Regression(    9400/10000): loss= 166.616634806296\n",
      "Losgistic Regression(    9500/10000): loss= 159.051456357966\n",
      "Losgistic Regression(    9600/10000): loss= 151.456714621324\n",
      "Losgistic Regression(    9700/10000): loss= 144.028580835982\n",
      "Losgistic Regression(    9800/10000): loss= 136.857840198807\n",
      "Losgistic Regression(    9900/10000): loss= 129.790033332561\n",
      "Time for  4th cross validation = 79.3895s\n",
      "Training Accuracy         = 0.8868\n",
      "Cross Validation Accuracy = 0.778544\n",
      "*************** ([0.88319999999999999, 0.88639999999999997, 0.88, 0.87439999999999996, 0.88680000000000003], [0.78400800000000004, 0.77524000000000004, 0.76915599999999995, 0.78068000000000004, 0.77854400000000001])\n",
      "Losgistic Regression(       0/10000): loss= 1725.775272995\n",
      "Losgistic Regression(     100/10000): loss= 1237.79948562259\n",
      "Losgistic Regression(     200/10000): loss= 1075.05724797123\n",
      "Losgistic Regression(     300/10000): loss= 1015.2565691403\n",
      "Losgistic Regression(     400/10000): loss= 979.548164693301\n",
      "Losgistic Regression(     500/10000): loss= 953.373333485631\n",
      "Losgistic Regression(     600/10000): loss= 932.614697706418\n",
      "Losgistic Regression(     700/10000): loss= 915.5596552726\n",
      "Losgistic Regression(     800/10000): loss= 900.807786943038\n",
      "Losgistic Regression(     900/10000): loss= 887.857317407539\n",
      "Losgistic Regression(    1000/10000): loss= 876.280439299983\n",
      "Losgistic Regression(    1100/10000): loss= 865.86320069131\n",
      "Losgistic Regression(    1200/10000): loss= 856.458083774774\n",
      "Losgistic Regression(    1300/10000): loss= 847.952740919722\n",
      "Losgistic Regression(    1400/10000): loss= 840.254227674166\n",
      "Losgistic Regression(    1500/10000): loss= 833.269934369251\n",
      "Losgistic Regression(    1600/10000): loss= 826.915415047483\n",
      "Losgistic Regression(    1700/10000): loss= 821.114281784174\n",
      "Losgistic Regression(    1800/10000): loss= 815.800049914877\n",
      "Losgistic Regression(    1900/10000): loss= 810.916229475823\n",
      "Losgistic Regression(    2000/10000): loss= 806.412488005446\n",
      "Losgistic Regression(    2100/10000): loss= 802.246232193724\n",
      "Losgistic Regression(    2200/10000): loss= 798.377733539878\n",
      "Losgistic Regression(    2300/10000): loss= 794.781961542085\n",
      "Losgistic Regression(    2400/10000): loss= 791.428439141923\n",
      "Losgistic Regression(    2500/10000): loss= 788.290775749674\n",
      "Losgistic Regression(    2600/10000): loss= 785.348404603222\n",
      "Losgistic Regression(    2700/10000): loss= 782.121404944497\n",
      "Losgistic Regression(    2800/10000): loss= 779.094818370452\n",
      "Losgistic Regression(    2900/10000): loss= 776.243665097846\n",
      "Losgistic Regression(    3000/10000): loss= 773.536750590214\n",
      "Losgistic Regression(    3100/10000): loss= 770.958314334253\n",
      "Losgistic Regression(    3200/10000): loss= 768.490794700278\n",
      "Losgistic Regression(    3300/10000): loss= 766.075349029657\n",
      "Losgistic Regression(    3400/10000): loss= 763.485762498497\n",
      "Losgistic Regression(    3500/10000): loss= 760.989955162867\n",
      "Losgistic Regression(    3600/10000): loss= 758.583418013244\n",
      "Losgistic Regression(    3700/10000): loss= 756.069934880171\n",
      "Losgistic Regression(    3800/10000): loss= 753.627217032174\n",
      "Losgistic Regression(    3900/10000): loss= 751.277035616033\n",
      "Losgistic Regression(    4000/10000): loss= 749.000775069257\n",
      "Losgistic Regression(    4100/10000): loss= 746.794725425464\n",
      "Losgistic Regression(    4200/10000): loss= 744.676803167943\n",
      "Losgistic Regression(    4300/10000): loss= 742.639337927195\n",
      "Losgistic Regression(    4400/10000): loss= 740.673458367755\n",
      "Losgistic Regression(    4500/10000): loss= 738.581982844216\n",
      "Losgistic Regression(    4600/10000): loss= 736.372515437094\n",
      "Losgistic Regression(    4700/10000): loss= 734.220170166942\n",
      "Losgistic Regression(    4800/10000): loss= 732.123640251934\n",
      "Losgistic Regression(    4900/10000): loss= 730.067541172257\n",
      "Losgistic Regression(    5000/10000): loss= 728.049573331682\n",
      "Losgistic Regression(    5100/10000): loss= 726.07122400243\n",
      "Losgistic Regression(    5200/10000): loss= 724.157880028075\n",
      "Losgistic Regression(    5300/10000): loss= 722.315895979479\n",
      "Losgistic Regression(    5400/10000): loss= 720.534987744757\n",
      "Losgistic Regression(    5500/10000): loss= 718.797626739371\n",
      "Losgistic Regression(    5600/10000): loss= 717.091572329415\n",
      "Losgistic Regression(    5700/10000): loss= 715.407249947481\n",
      "Losgistic Regression(    5800/10000): loss= 713.751000602667\n",
      "Losgistic Regression(    5900/10000): loss= 712.134748703083\n",
      "Losgistic Regression(    6000/10000): loss= 710.564252288692\n",
      "Losgistic Regression(    6100/10000): loss= 708.961040713797\n",
      "Losgistic Regression(    6200/10000): loss= 707.155056009159\n",
      "Losgistic Regression(    6300/10000): loss= 705.409972496126\n",
      "Losgistic Regression(    6400/10000): loss= 703.674840475555\n",
      "Losgistic Regression(    6500/10000): loss= 701.835911653376\n",
      "Losgistic Regression(    6600/10000): loss= 699.990894870768\n",
      "Losgistic Regression(    6700/10000): loss= 698.091656756074\n",
      "Losgistic Regression(    6800/10000): loss= 695.982976166274\n",
      "Losgistic Regression(    6900/10000): loss= 693.906624579672\n",
      "Losgistic Regression(    7000/10000): loss= 691.743211110867\n",
      "Losgistic Regression(    7100/10000): loss= 689.539235900624\n",
      "Losgistic Regression(    7200/10000): loss= 687.354080690134\n",
      "Losgistic Regression(    7300/10000): loss= 685.225657544792\n",
      "Losgistic Regression(    7400/10000): loss= 683.165290399843\n",
      "Losgistic Regression(    7500/10000): loss= 681.167975667077\n",
      "Losgistic Regression(    7600/10000): loss= 679.210060029592\n",
      "Losgistic Regression(    7700/10000): loss= 677.265890157107\n",
      "Losgistic Regression(    7800/10000): loss= 675.345265663279\n",
      "Losgistic Regression(    7900/10000): loss= 673.491040953056\n",
      "Losgistic Regression(    8000/10000): loss= 671.701522445987\n",
      "Losgistic Regression(    8100/10000): loss= 669.976724955393\n",
      "Losgistic Regression(    8200/10000): loss= 668.29907219667\n",
      "Losgistic Regression(    8300/10000): loss= 666.660726483117\n",
      "Losgistic Regression(    8400/10000): loss= 665.052546317726\n",
      "Losgistic Regression(    8500/10000): loss= 663.49826070833\n",
      "Losgistic Regression(    8600/10000): loss= 662.009588401041\n",
      "Losgistic Regression(    8700/10000): loss= 660.594440913781\n",
      "Losgistic Regression(    8800/10000): loss= 659.247320558413\n",
      "Losgistic Regression(    8900/10000): loss= 657.95638063281\n",
      "Losgistic Regression(    9000/10000): loss= 656.697230673609\n",
      "Losgistic Regression(    9100/10000): loss= 655.454071258994\n",
      "Losgistic Regression(    9200/10000): loss= 654.245672139868\n",
      "Losgistic Regression(    9300/10000): loss= 653.08551472539\n",
      "Losgistic Regression(    9400/10000): loss= 651.966369852846\n",
      "Losgistic Regression(    9500/10000): loss= 650.880145531839\n",
      "Losgistic Regression(    9600/10000): loss= 649.827007389165\n",
      "Losgistic Regression(    9700/10000): loss= 648.779165269464\n",
      "Losgistic Regression(    9800/10000): loss= 647.727366526696\n",
      "Losgistic Regression(    9900/10000): loss= 646.656085591702\n",
      "Time for  0th cross validation = 79.6116s\n",
      "Training Accuracy         = 0.8832\n",
      "Cross Validation Accuracy = 0.78554\n",
      "Losgistic Regression(       0/10000): loss= 1724.81651699789\n",
      "Losgistic Regression(     100/10000): loss= 1223.34935891003\n",
      "Losgistic Regression(     200/10000): loss= 1062.09277313622\n",
      "Losgistic Regression(     300/10000): loss= 997.004107667449\n",
      "Losgistic Regression(     400/10000): loss= 959.035719610665\n",
      "Losgistic Regression(     500/10000): loss= 931.371133671158\n",
      "Losgistic Regression(     600/10000): loss= 909.331623432289\n",
      "Losgistic Regression(     700/10000): loss= 890.966691815915\n",
      "Losgistic Regression(     800/10000): loss= 875.055921227302\n",
      "Losgistic Regression(     900/10000): loss= 860.407667405488\n",
      "Losgistic Regression(    1000/10000): loss= 847.806341012235\n",
      "Losgistic Regression(    1100/10000): loss= 836.972807777147\n",
      "Losgistic Regression(    1200/10000): loss= 827.580397190652\n",
      "Losgistic Regression(    1300/10000): loss= 819.444865197958\n",
      "Losgistic Regression(    1400/10000): loss= 812.402509877449\n",
      "Losgistic Regression(    1500/10000): loss= 806.277520100243\n",
      "Losgistic Regression(    1600/10000): loss= 800.907837921917\n",
      "Losgistic Regression(    1700/10000): loss= 795.69498210429\n",
      "Losgistic Regression(    1800/10000): loss= 790.890872199192\n",
      "Losgistic Regression(    1900/10000): loss= 786.591417251727\n",
      "Losgistic Regression(    2000/10000): loss= 782.160881161218\n",
      "Losgistic Regression(    2100/10000): loss= 777.929239859167\n",
      "Losgistic Regression(    2200/10000): loss= 774.010796980638\n",
      "Losgistic Regression(    2300/10000): loss= 769.682516375074\n",
      "Losgistic Regression(    2400/10000): loss= 765.239307513781\n",
      "Losgistic Regression(    2500/10000): loss= 760.754662380663\n",
      "Losgistic Regression(    2600/10000): loss= 756.159751119076\n",
      "Losgistic Regression(    2700/10000): loss= 751.715991794467\n",
      "Losgistic Regression(    2800/10000): loss= 747.436377848723\n",
      "Losgistic Regression(    2900/10000): loss= 743.293619986648\n",
      "Losgistic Regression(    3000/10000): loss= 738.642692866542\n",
      "Losgistic Regression(    3100/10000): loss= 733.695808843672\n",
      "Losgistic Regression(    3200/10000): loss= 728.757685271786\n",
      "Losgistic Regression(    3300/10000): loss= 723.963014364574\n",
      "Losgistic Regression(    3400/10000): loss= 719.232323542653\n",
      "Losgistic Regression(    3500/10000): loss= 714.223646198797\n",
      "Losgistic Regression(    3600/10000): loss= 708.790441090452\n",
      "Losgistic Regression(    3700/10000): loss= 703.387868820886\n",
      "Losgistic Regression(    3800/10000): loss= 698.218341860227\n",
      "Losgistic Regression(    3900/10000): loss= 693.245725085981\n",
      "Losgistic Regression(    4000/10000): loss= 688.439968925217\n",
      "Losgistic Regression(    4100/10000): loss= 683.809207364844\n",
      "Losgistic Regression(    4200/10000): loss= 679.353009049905\n",
      "Losgistic Regression(    4300/10000): loss= 674.787302207875\n",
      "Losgistic Regression(    4400/10000): loss= 670.282666364083\n",
      "Losgistic Regression(    4500/10000): loss= 665.903516579453\n",
      "Losgistic Regression(    4600/10000): loss= 661.647570869803\n",
      "Losgistic Regression(    4700/10000): loss= 657.511133360748\n",
      "Losgistic Regression(    4800/10000): loss= 653.430119197149\n",
      "Losgistic Regression(    4900/10000): loss= 649.142688793653\n",
      "Losgistic Regression(    5000/10000): loss= 645.051433285895\n",
      "Losgistic Regression(    5100/10000): loss= 641.135431193979\n",
      "Losgistic Regression(    5200/10000): loss= 637.335681748969\n",
      "Losgistic Regression(    5300/10000): loss= 633.572305439717\n",
      "Losgistic Regression(    5400/10000): loss= 629.741938323488\n",
      "Losgistic Regression(    5500/10000): loss= 626.004361371342\n",
      "Losgistic Regression(    5600/10000): loss= 622.218813416447\n",
      "Losgistic Regression(    5700/10000): loss= 618.453078892375\n",
      "Losgistic Regression(    5800/10000): loss= 614.747676015672\n",
      "Losgistic Regression(    5900/10000): loss= 611.077957576086\n",
      "Losgistic Regression(    6000/10000): loss= 607.425930162404\n",
      "Losgistic Regression(    6100/10000): loss= 603.803620534054\n",
      "Losgistic Regression(    6200/10000): loss= 600.208899940773\n",
      "Losgistic Regression(    6300/10000): loss= 596.612733558828\n",
      "Losgistic Regression(    6400/10000): loss= 593.055360282949\n",
      "Losgistic Regression(    6500/10000): loss= 589.564670704755\n",
      "Losgistic Regression(    6600/10000): loss= 586.064886671487\n",
      "Losgistic Regression(    6700/10000): loss= 582.478904614424\n",
      "Losgistic Regression(    6800/10000): loss= 578.807667815052\n",
      "Losgistic Regression(    6900/10000): loss= 575.139770537637\n",
      "Losgistic Regression(    7000/10000): loss= 571.457331078017\n",
      "Losgistic Regression(    7100/10000): loss= 567.791989792294\n",
      "Losgistic Regression(    7200/10000): loss= 564.190084359913\n",
      "Losgistic Regression(    7300/10000): loss= 560.594023747242\n",
      "Losgistic Regression(    7400/10000): loss= 557.000330329675\n",
      "Losgistic Regression(    7500/10000): loss= 553.493435620425\n",
      "Losgistic Regression(    7600/10000): loss= 550.022585059118\n",
      "Losgistic Regression(    7700/10000): loss= 546.543312315894\n",
      "Losgistic Regression(    7800/10000): loss= 543.176677855272\n",
      "Losgistic Regression(    7900/10000): loss= 539.829639631784\n",
      "Losgistic Regression(    8000/10000): loss= 536.511912715939\n",
      "Losgistic Regression(    8100/10000): loss= 533.102453031655\n",
      "Losgistic Regression(    8200/10000): loss= 529.605874982239\n",
      "Losgistic Regression(    8300/10000): loss= 526.107643603748\n",
      "Losgistic Regression(    8400/10000): loss= 522.649664103714\n",
      "Losgistic Regression(    8500/10000): loss= 519.186301111433\n",
      "Losgistic Regression(    8600/10000): loss= 515.680347834085\n",
      "Losgistic Regression(    8700/10000): loss= 512.269813654403\n",
      "Losgistic Regression(    8800/10000): loss= 508.948761044703\n",
      "Losgistic Regression(    8900/10000): loss= 505.706879706238\n",
      "Losgistic Regression(    9000/10000): loss= 502.498736053514\n",
      "Losgistic Regression(    9100/10000): loss= 499.191257659525\n",
      "Losgistic Regression(    9200/10000): loss= 495.80351893355\n",
      "Losgistic Regression(    9300/10000): loss= 492.406080036687\n",
      "Losgistic Regression(    9400/10000): loss= 488.937774778576\n",
      "Losgistic Regression(    9500/10000): loss= 485.445418351685\n",
      "Losgistic Regression(    9600/10000): loss= 481.97563947292\n",
      "Losgistic Regression(    9700/10000): loss= 478.504847018547\n",
      "Losgistic Regression(    9800/10000): loss= 474.944333414014\n",
      "Losgistic Regression(    9900/10000): loss= 471.236586452744\n",
      "Time for  1th cross validation = 82.5535s\n",
      "Training Accuracy         =  0.884\n",
      "Cross Validation Accuracy = 0.776808\n",
      "Losgistic Regression(       0/10000): loss= 1726.06721756932\n",
      "Losgistic Regression(     100/10000): loss= 1224.13225543563\n",
      "Losgistic Regression(     200/10000): loss= 1067.14024532909\n",
      "Losgistic Regression(     300/10000): loss= 1005.50375790935\n",
      "Losgistic Regression(     400/10000): loss= 965.664995258297\n",
      "Losgistic Regression(     500/10000): loss= 936.840331079129\n",
      "Losgistic Regression(     600/10000): loss= 914.620320088978\n",
      "Losgistic Regression(     700/10000): loss= 896.664779632022\n",
      "Losgistic Regression(     800/10000): loss= 881.661319442159\n",
      "Losgistic Regression(     900/10000): loss= 868.900050872757\n",
      "Losgistic Regression(    1000/10000): loss= 856.61112351812\n",
      "Losgistic Regression(    1100/10000): loss= 845.075607892155\n",
      "Losgistic Regression(    1200/10000): loss= 834.711240997775\n",
      "Losgistic Regression(    1300/10000): loss= 825.408887775889\n",
      "Losgistic Regression(    1400/10000): loss= 817.041633410675\n",
      "Losgistic Regression(    1500/10000): loss= 809.484586673624\n",
      "Losgistic Regression(    1600/10000): loss= 802.641991717632\n",
      "Losgistic Regression(    1700/10000): loss= 796.410221159809\n",
      "Losgistic Regression(    1800/10000): loss= 790.426476461777\n",
      "Losgistic Regression(    1900/10000): loss= 784.153546478551\n",
      "Losgistic Regression(    2000/10000): loss= 777.804201655627\n",
      "Losgistic Regression(    2100/10000): loss= 771.477199159612\n",
      "Losgistic Regression(    2200/10000): loss= 765.535245504414\n",
      "Losgistic Regression(    2300/10000): loss= 759.950896384797\n",
      "Losgistic Regression(    2400/10000): loss= 754.140263532308\n",
      "Losgistic Regression(    2500/10000): loss= 748.182285206768\n",
      "Losgistic Regression(    2600/10000): loss= 742.555144351767\n",
      "Losgistic Regression(    2700/10000): loss= 737.226859808872\n",
      "Losgistic Regression(    2800/10000): loss= 732.167884820441\n",
      "Losgistic Regression(    2900/10000): loss= 727.398298496713\n",
      "Losgistic Regression(    3000/10000): loss= 722.933670697685\n",
      "Losgistic Regression(    3100/10000): loss= 718.462538433767\n",
      "Losgistic Regression(    3200/10000): loss= 714.276686488294\n",
      "Losgistic Regression(    3300/10000): loss= 710.351777217794\n",
      "Losgistic Regression(    3400/10000): loss= 706.667459394735\n",
      "Losgistic Regression(    3500/10000): loss= 703.243959374563\n",
      "Losgistic Regression(    3600/10000): loss= 700.083180727059\n",
      "Losgistic Regression(    3700/10000): loss= 697.136411310684\n",
      "Losgistic Regression(    3800/10000): loss= 694.350610363237\n",
      "Losgistic Regression(    3900/10000): loss= 691.701696996094\n",
      "Losgistic Regression(    4000/10000): loss= 689.189193898383\n",
      "Losgistic Regression(    4100/10000): loss= 686.811274597386\n",
      "Losgistic Regression(    4200/10000): loss= 684.566570106648\n",
      "Losgistic Regression(    4300/10000): loss= 682.444101033786\n",
      "Losgistic Regression(    4400/10000): loss= 680.320787047369\n",
      "Losgistic Regression(    4500/10000): loss= 678.241657987175\n",
      "Losgistic Regression(    4600/10000): loss= 676.249993790266\n",
      "Losgistic Regression(    4700/10000): loss= 674.349137416431\n",
      "Losgistic Regression(    4800/10000): loss= 672.544406148534\n",
      "Losgistic Regression(    4900/10000): loss= 670.807172736803\n",
      "Losgistic Regression(    5000/10000): loss= 669.125110757795\n",
      "Losgistic Regression(    5100/10000): loss= 667.514856793151\n",
      "Losgistic Regression(    5200/10000): loss= 666.009412988268\n",
      "Losgistic Regression(    5300/10000): loss= 664.605235988146\n",
      "Losgistic Regression(    5400/10000): loss= 663.283434330845\n",
      "Losgistic Regression(    5500/10000): loss= 662.019879168411\n",
      "Losgistic Regression(    5600/10000): loss= 660.778925295538\n",
      "Losgistic Regression(    5700/10000): loss= 659.570687589923\n",
      "Losgistic Regression(    5800/10000): loss= 658.393851513341\n",
      "Losgistic Regression(    5900/10000): loss= 657.237099793594\n",
      "Losgistic Regression(    6000/10000): loss= 656.116761522841\n",
      "Losgistic Regression(    6100/10000): loss= 654.961475447865\n",
      "Losgistic Regression(    6200/10000): loss= 653.769486676307\n",
      "Losgistic Regression(    6300/10000): loss= 652.641380143464\n",
      "Losgistic Regression(    6400/10000): loss= 651.570257462963\n",
      "Losgistic Regression(    6500/10000): loss= 650.510076377858\n",
      "Losgistic Regression(    6600/10000): loss= 649.446813185713\n",
      "Losgistic Regression(    6700/10000): loss= 648.394360042944\n",
      "Losgistic Regression(    6800/10000): loss= 647.377569796442\n",
      "Losgistic Regression(    6900/10000): loss= 646.384629881475\n",
      "Losgistic Regression(    7000/10000): loss= 645.436382885803\n",
      "Losgistic Regression(    7100/10000): loss= 644.554088916172\n",
      "Losgistic Regression(    7200/10000): loss= 643.720077552004\n",
      "Losgistic Regression(    7300/10000): loss= 642.910290041956\n",
      "Losgistic Regression(    7400/10000): loss= 642.141130468302\n",
      "Losgistic Regression(    7500/10000): loss= 641.385151924578\n",
      "Losgistic Regression(    7600/10000): loss= 640.529956035227\n",
      "Losgistic Regression(    7700/10000): loss= 639.686919766449\n",
      "Losgistic Regression(    7800/10000): loss= 638.854301576128\n",
      "Losgistic Regression(    7900/10000): loss= 638.025191967034\n",
      "Losgistic Regression(    8000/10000): loss= 637.196123340775\n",
      "Losgistic Regression(    8100/10000): loss= 636.383794067201\n",
      "Losgistic Regression(    8200/10000): loss= 635.591073674896\n",
      "Losgistic Regression(    8300/10000): loss= 634.828407212509\n",
      "Losgistic Regression(    8400/10000): loss= 634.089689728338\n",
      "Losgistic Regression(    8500/10000): loss= 633.376665537383\n",
      "Losgistic Regression(    8600/10000): loss= 632.668643043209\n",
      "Losgistic Regression(    8700/10000): loss= 631.969841987154\n",
      "Losgistic Regression(    8800/10000): loss= 631.236739538439\n",
      "Losgistic Regression(    8900/10000): loss= 630.544847761755\n",
      "Losgistic Regression(    9000/10000): loss= 629.868085361724\n",
      "Losgistic Regression(    9100/10000): loss= 629.203398891902\n",
      "Losgistic Regression(    9200/10000): loss= 628.588497530738\n",
      "Losgistic Regression(    9300/10000): loss= 628.046938289159\n",
      "Losgistic Regression(    9400/10000): loss= 627.553726703772\n",
      "Losgistic Regression(    9500/10000): loss= 627.092124437852\n",
      "Losgistic Regression(    9600/10000): loss= 626.648970411282\n",
      "Losgistic Regression(    9700/10000): loss= 626.217390741857\n",
      "Losgistic Regression(    9800/10000): loss= 625.783889736855\n",
      "Losgistic Regression(    9900/10000): loss= 625.327513289576\n",
      "Time for  2th cross validation = 80.853s\n",
      "Training Accuracy         = 0.8804\n",
      "Cross Validation Accuracy = 0.770928\n",
      "Losgistic Regression(       0/10000): loss= 1725.44136833601\n",
      "Losgistic Regression(     100/10000): loss= 1249.82612984231\n",
      "Losgistic Regression(     200/10000): loss= 1110.76450600815\n",
      "Losgistic Regression(     300/10000): loss= 1050.1396572981\n",
      "Losgistic Regression(     400/10000): loss= 1009.85186031506\n",
      "Losgistic Regression(     500/10000): loss= 979.694928256072\n",
      "Losgistic Regression(     600/10000): loss= 956.169382363926\n",
      "Losgistic Regression(     700/10000): loss= 936.965278559283\n",
      "Losgistic Regression(     800/10000): loss= 917.411876068588\n",
      "Losgistic Regression(     900/10000): loss= 899.174691425462\n",
      "Losgistic Regression(    1000/10000): loss= 878.593215865626\n",
      "Losgistic Regression(    1100/10000): loss= 860.237190165536\n",
      "Losgistic Regression(    1200/10000): loss= 844.198411196388\n",
      "Losgistic Regression(    1300/10000): loss= 830.221638515316\n",
      "Losgistic Regression(    1400/10000): loss= 818.200721581512\n",
      "Losgistic Regression(    1500/10000): loss= 807.89088104604\n",
      "Losgistic Regression(    1600/10000): loss= 798.860300828548\n",
      "Losgistic Regression(    1700/10000): loss= 790.662775508612\n",
      "Losgistic Regression(    1800/10000): loss= 783.060019052764\n",
      "Losgistic Regression(    1900/10000): loss= 776.160006907953\n",
      "Losgistic Regression(    2000/10000): loss= 769.863209200903\n",
      "Losgistic Regression(    2100/10000): loss= 764.059383683923\n",
      "Losgistic Regression(    2200/10000): loss= 758.64300848762\n",
      "Losgistic Regression(    2300/10000): loss= 753.567282803667\n",
      "Losgistic Regression(    2400/10000): loss= 748.533543390354\n",
      "Losgistic Regression(    2500/10000): loss= 743.875057804614\n",
      "Losgistic Regression(    2600/10000): loss= 739.507576468349\n",
      "Losgistic Regression(    2700/10000): loss= 735.392273227149\n",
      "Losgistic Regression(    2800/10000): loss= 731.545692355353\n",
      "Losgistic Regression(    2900/10000): loss= 727.996892820091\n",
      "Losgistic Regression(    3000/10000): loss= 724.723818426333\n",
      "Losgistic Regression(    3100/10000): loss= 721.699666132945\n",
      "Losgistic Regression(    3200/10000): loss= 718.926121461854\n",
      "Losgistic Regression(    3300/10000): loss= 716.370591720745\n",
      "Losgistic Regression(    3400/10000): loss= 714.010444344666\n",
      "Losgistic Regression(    3500/10000): loss= 711.41198279064\n",
      "Losgistic Regression(    3600/10000): loss= 708.980951330965\n",
      "Losgistic Regression(    3700/10000): loss= 706.686715823733\n",
      "Losgistic Regression(    3800/10000): loss= 704.491876100369\n",
      "Losgistic Regression(    3900/10000): loss= 702.374358225176\n",
      "Losgistic Regression(    4000/10000): loss= 700.143906865347\n",
      "Losgistic Regression(    4100/10000): loss= 697.670843751948\n",
      "Losgistic Regression(    4200/10000): loss= 695.291513195889\n",
      "Losgistic Regression(    4300/10000): loss= 692.970004526194\n",
      "Losgistic Regression(    4400/10000): loss= 690.696144876199\n",
      "Losgistic Regression(    4500/10000): loss= 688.472330304832\n",
      "Losgistic Regression(    4600/10000): loss= 686.145692108451\n",
      "Losgistic Regression(    4700/10000): loss= 683.7832850546\n",
      "Losgistic Regression(    4800/10000): loss= 681.522915811087\n",
      "Losgistic Regression(    4900/10000): loss= 679.358235543462\n",
      "Losgistic Regression(    5000/10000): loss= 677.305077884682\n",
      "Losgistic Regression(    5100/10000): loss= 675.358965469388\n",
      "Losgistic Regression(    5200/10000): loss= 673.519235439859\n",
      "Losgistic Regression(    5300/10000): loss= 671.785526668143\n",
      "Losgistic Regression(    5400/10000): loss= 670.164657347016\n",
      "Losgistic Regression(    5500/10000): loss= 668.658109591458\n",
      "Losgistic Regression(    5600/10000): loss= 667.283045690484\n",
      "Losgistic Regression(    5700/10000): loss= 666.011992590512\n",
      "Losgistic Regression(    5800/10000): loss= 664.825777213503\n",
      "Losgistic Regression(    5900/10000): loss= 663.737251244256\n",
      "Losgistic Regression(    6000/10000): loss= 662.73156151883\n",
      "Losgistic Regression(    6100/10000): loss= 661.656210844093\n",
      "Losgistic Regression(    6200/10000): loss= 660.662980869418\n",
      "Losgistic Regression(    6300/10000): loss= 659.767347953456\n",
      "Losgistic Regression(    6400/10000): loss= 658.939279330272\n",
      "Losgistic Regression(    6500/10000): loss= 658.159366922839\n",
      "Losgistic Regression(    6600/10000): loss= 657.407795905862\n",
      "Losgistic Regression(    6700/10000): loss= 656.685555828857\n",
      "Losgistic Regression(    6800/10000): loss= 655.999200839343\n",
      "Losgistic Regression(    6900/10000): loss= 655.35937158463\n",
      "Losgistic Regression(    7000/10000): loss= 654.751265375102\n",
      "Losgistic Regression(    7100/10000): loss= 654.184780760574\n",
      "Losgistic Regression(    7200/10000): loss= 653.65936783556\n",
      "Losgistic Regression(    7300/10000): loss= 653.182562080087\n",
      "Losgistic Regression(    7400/10000): loss= 652.71869239446\n",
      "Losgistic Regression(    7500/10000): loss= 652.234337709199\n",
      "Losgistic Regression(    7600/10000): loss= 651.607252646244\n",
      "Losgistic Regression(    7700/10000): loss= 650.684094590657\n",
      "Losgistic Regression(    7800/10000): loss= 649.759871668789\n",
      "Losgistic Regression(    7900/10000): loss= 648.841046716216\n",
      "Losgistic Regression(    8000/10000): loss= 647.965342933525\n",
      "Losgistic Regression(    8100/10000): loss= 647.127263414745\n",
      "Losgistic Regression(    8200/10000): loss= 646.308666267688\n",
      "Losgistic Regression(    8300/10000): loss= 645.499962127442\n",
      "Losgistic Regression(    8400/10000): loss= 644.685742889683\n",
      "Losgistic Regression(    8500/10000): loss= 643.895628021682\n",
      "Losgistic Regression(    8600/10000): loss= 643.114817181073\n",
      "Losgistic Regression(    8700/10000): loss= 642.30890092764\n",
      "Losgistic Regression(    8800/10000): loss= 641.53820261501\n",
      "Losgistic Regression(    8900/10000): loss= 640.845854629487\n",
      "Losgistic Regression(    9000/10000): loss= 640.19502569007\n",
      "Losgistic Regression(    9100/10000): loss= 639.600509257508\n",
      "Losgistic Regression(    9200/10000): loss= 639.092366086158\n",
      "Losgistic Regression(    9300/10000): loss= 638.623742246107\n",
      "Losgistic Regression(    9400/10000): loss= 638.170690974836\n",
      "Losgistic Regression(    9500/10000): loss= 637.789376087895\n",
      "Losgistic Regression(    9600/10000): loss= 637.441377461441\n",
      "Losgistic Regression(    9700/10000): loss= 637.110743922312\n",
      "Losgistic Regression(    9800/10000): loss= 636.828442818553\n",
      "Losgistic Regression(    9900/10000): loss= 636.567023880892\n",
      "Time for  3th cross validation = 80.9277s\n",
      "Training Accuracy         = 0.8716\n",
      "Cross Validation Accuracy = 0.781948\n",
      "Losgistic Regression(       0/10000): loss= 1726.04236710636\n",
      "Losgistic Regression(     100/10000): loss= 1230.69270454409\n",
      "Losgistic Regression(     200/10000): loss= 1074.80663958698\n",
      "Losgistic Regression(     300/10000): loss= 1012.75825400046\n",
      "Losgistic Regression(     400/10000): loss= 971.447969154807\n",
      "Losgistic Regression(     500/10000): loss= 940.331339742865\n",
      "Losgistic Regression(     600/10000): loss= 915.65255205285\n",
      "Losgistic Regression(     700/10000): loss= 895.504175918017\n",
      "Losgistic Regression(     800/10000): loss= 877.525479045076\n",
      "Losgistic Regression(     900/10000): loss= 859.465696738048\n",
      "Losgistic Regression(    1000/10000): loss= 843.486757705959\n",
      "Losgistic Regression(    1100/10000): loss= 828.94206243943\n",
      "Losgistic Regression(    1200/10000): loss= 815.712013648209\n",
      "Losgistic Regression(    1300/10000): loss= 803.862829102668\n",
      "Losgistic Regression(    1400/10000): loss= 793.203423184537\n",
      "Losgistic Regression(    1500/10000): loss= 782.789939399811\n",
      "Losgistic Regression(    1600/10000): loss= 772.379423423585\n",
      "Losgistic Regression(    1700/10000): loss= 762.73825160792\n",
      "Losgistic Regression(    1800/10000): loss= 753.64421794877\n",
      "Losgistic Regression(    1900/10000): loss= 744.682752222898\n",
      "Losgistic Regression(    2000/10000): loss= 736.329428786897\n",
      "Losgistic Regression(    2100/10000): loss= 728.538317680673\n",
      "Losgistic Regression(    2200/10000): loss= 721.177142398223\n",
      "Losgistic Regression(    2300/10000): loss= 714.162057448603\n",
      "Losgistic Regression(    2400/10000): loss= 707.485307511137\n",
      "Losgistic Regression(    2500/10000): loss= 701.161567229195\n",
      "Losgistic Regression(    2600/10000): loss= 695.137958970754\n",
      "Losgistic Regression(    2700/10000): loss= 689.08205539723\n",
      "Losgistic Regression(    2800/10000): loss= 683.2488872055\n",
      "Losgistic Regression(    2900/10000): loss= 677.609276726945\n",
      "Losgistic Regression(    3000/10000): loss= 672.152549737308\n",
      "Losgistic Regression(    3100/10000): loss= 666.83296284621\n",
      "Losgistic Regression(    3200/10000): loss= 661.608805888426\n",
      "Losgistic Regression(    3300/10000): loss= 656.416518835661\n",
      "Losgistic Regression(    3400/10000): loss= 650.955449971056\n",
      "Losgistic Regression(    3500/10000): loss= 645.397205303065\n",
      "Losgistic Regression(    3600/10000): loss= 639.79210548361\n",
      "Losgistic Regression(    3700/10000): loss= 634.113484059664\n",
      "Losgistic Regression(    3800/10000): loss= 628.35051904952\n",
      "Losgistic Regression(    3900/10000): loss= 622.514778267442\n",
      "Losgistic Regression(    4000/10000): loss= 616.618480298484\n",
      "Losgistic Regression(    4100/10000): loss= 610.652948269352\n",
      "Losgistic Regression(    4200/10000): loss= 604.604636421673\n",
      "Losgistic Regression(    4300/10000): loss= 598.443233371764\n",
      "Losgistic Regression(    4400/10000): loss= 592.223442827188\n",
      "Losgistic Regression(    4500/10000): loss= 585.841048372505\n",
      "Losgistic Regression(    4600/10000): loss= 579.295392092697\n",
      "Losgistic Regression(    4700/10000): loss= 572.470458988349\n",
      "Losgistic Regression(    4800/10000): loss= 565.576465771999\n",
      "Losgistic Regression(    4900/10000): loss= 558.674367441824\n",
      "Losgistic Regression(    5000/10000): loss= 551.633372541105\n",
      "Losgistic Regression(    5100/10000): loss= 544.649082862567\n",
      "Losgistic Regression(    5200/10000): loss= 537.665499506281\n",
      "Losgistic Regression(    5300/10000): loss= 530.660413679638\n",
      "Losgistic Regression(    5400/10000): loss= 523.650039154158\n",
      "Losgistic Regression(    5500/10000): loss= 516.688934264935\n",
      "Losgistic Regression(    5600/10000): loss= 509.820153051779\n",
      "Losgistic Regression(    5700/10000): loss= 503.019376600268\n",
      "Losgistic Regression(    5800/10000): loss= 496.187185011879\n",
      "Losgistic Regression(    5900/10000): loss= 489.291580183502\n",
      "Losgistic Regression(    6000/10000): loss= 482.416725098074\n",
      "Losgistic Regression(    6100/10000): loss= 475.560630985629\n",
      "Losgistic Regression(    6200/10000): loss= 468.725991022813\n",
      "Losgistic Regression(    6300/10000): loss= 461.829875139343\n",
      "Losgistic Regression(    6400/10000): loss= 454.929788533395\n",
      "Losgistic Regression(    6500/10000): loss= 447.988508379786\n",
      "Losgistic Regression(    6600/10000): loss= 441.053134130464\n",
      "Losgistic Regression(    6700/10000): loss= 434.130538072893\n",
      "Losgistic Regression(    6800/10000): loss= 427.214621666437\n",
      "Losgistic Regression(    6900/10000): loss= 420.357156687811\n",
      "Losgistic Regression(    7000/10000): loss= 413.585493952065\n",
      "Losgistic Regression(    7100/10000): loss= 406.869931923195\n",
      "Losgistic Regression(    7200/10000): loss= 400.200665426693\n",
      "Losgistic Regression(    7300/10000): loss= 393.560653414135\n",
      "Losgistic Regression(    7400/10000): loss= 386.969945360082\n",
      "Losgistic Regression(    7500/10000): loss= 380.492881131325\n",
      "Losgistic Regression(    7600/10000): loss= 374.116454652025\n",
      "Losgistic Regression(    7700/10000): loss= 367.73875959411\n",
      "Losgistic Regression(    7800/10000): loss= 361.328462152725\n",
      "Losgistic Regression(    7900/10000): loss= 354.902378933289\n",
      "Losgistic Regression(    8000/10000): loss= 348.480044483782\n",
      "Losgistic Regression(    8100/10000): loss= 342.067651218998\n",
      "Losgistic Regression(    8200/10000): loss= 335.647279330423\n",
      "Losgistic Regression(    8300/10000): loss= 329.234470908997\n",
      "Losgistic Regression(    8400/10000): loss= 322.872716713189\n",
      "Losgistic Regression(    8500/10000): loss= 316.588097548434\n",
      "Losgistic Regression(    8600/10000): loss= 310.416073275774\n",
      "Losgistic Regression(    8700/10000): loss= 304.355693848312\n",
      "Losgistic Regression(    8800/10000): loss= 298.374415041855\n",
      "Losgistic Regression(    8900/10000): loss= 292.396156885898\n",
      "Losgistic Regression(    9000/10000): loss= 286.392440402034\n",
      "Losgistic Regression(    9100/10000): loss= 280.352772204312\n",
      "Losgistic Regression(    9200/10000): loss= 274.2857965755\n",
      "Losgistic Regression(    9300/10000): loss= 268.232498330069\n",
      "Losgistic Regression(    9400/10000): loss= 262.235476788606\n",
      "Losgistic Regression(    9500/10000): loss= 256.330233517827\n",
      "Losgistic Regression(    9600/10000): loss= 250.463168603158\n",
      "Losgistic Regression(    9700/10000): loss= 244.560416582568\n",
      "Losgistic Regression(    9800/10000): loss= 238.663032903728\n",
      "Losgistic Regression(    9900/10000): loss= 232.876555682563\n",
      "Time for  4th cross validation = 81.2737s\n",
      "Training Accuracy         =  0.886\n",
      "Cross Validation Accuracy = 0.780192\n",
      "*************** ([0.88319999999999999, 0.88400000000000001, 0.88039999999999996, 0.87160000000000004, 0.88600000000000001], [0.78554000000000002, 0.77680800000000005, 0.77092799999999995, 0.78194799999999998, 0.780192])\n",
      "Losgistic Regression(       0/10000): loss= 1725.78056587255\n",
      "Losgistic Regression(     100/10000): loss= 1239.07644461936\n",
      "Losgistic Regression(     200/10000): loss= 1077.88026091558\n",
      "Losgistic Regression(     300/10000): loss= 1019.35573769087\n",
      "Losgistic Regression(     400/10000): loss= 984.775774923056\n",
      "Losgistic Regression(     500/10000): loss= 959.730658772812\n",
      "Losgistic Regression(     600/10000): loss= 940.107055248558\n",
      "Losgistic Regression(     700/10000): loss= 924.13644782797\n",
      "Losgistic Regression(     800/10000): loss= 910.459938720795\n",
      "Losgistic Regression(     900/10000): loss= 898.572825887378\n",
      "Losgistic Regression(    1000/10000): loss= 888.042623005712\n",
      "Losgistic Regression(    1100/10000): loss= 878.687969556373\n",
      "Losgistic Regression(    1200/10000): loss= 870.35462994593\n",
      "Losgistic Regression(    1300/10000): loss= 862.906361336751\n",
      "Losgistic Regression(    1400/10000): loss= 856.237810692363\n",
      "Losgistic Regression(    1500/10000): loss= 850.25187793777\n",
      "Losgistic Regression(    1600/10000): loss= 844.856008145075\n",
      "Losgistic Regression(    1700/10000): loss= 839.979742647806\n",
      "Losgistic Regression(    1800/10000): loss= 835.542434573382\n",
      "Losgistic Regression(    1900/10000): loss= 831.494206091028\n",
      "Losgistic Regression(    2000/10000): loss= 827.814187937587\n",
      "Losgistic Regression(    2100/10000): loss= 824.438862368978\n",
      "Losgistic Regression(    2200/10000): loss= 821.333385672163\n",
      "Losgistic Regression(    2300/10000): loss= 818.487245952812\n",
      "Losgistic Regression(    2400/10000): loss= 815.852777762021\n",
      "Losgistic Regression(    2500/10000): loss= 813.404793147458\n",
      "Losgistic Regression(    2600/10000): loss= 811.139038374307\n",
      "Losgistic Regression(    2700/10000): loss= 809.046058724041\n",
      "Losgistic Regression(    2800/10000): loss= 807.114276997949\n",
      "Losgistic Regression(    2900/10000): loss= 805.308146742891\n",
      "Losgistic Regression(    3000/10000): loss= 803.621470689627\n",
      "Losgistic Regression(    3100/10000): loss= 802.043881792647\n",
      "Losgistic Regression(    3200/10000): loss= 800.311171970264\n",
      "Losgistic Regression(    3300/10000): loss= 798.61755631771\n",
      "Losgistic Regression(    3400/10000): loss= 796.984478754787\n",
      "Losgistic Regression(    3500/10000): loss= 795.427984155912\n",
      "Losgistic Regression(    3600/10000): loss= 793.959210736603\n",
      "Losgistic Regression(    3700/10000): loss= 792.576890455688\n",
      "Losgistic Regression(    3800/10000): loss= 791.261781329128\n",
      "Losgistic Regression(    3900/10000): loss= 790.009065705553\n",
      "Losgistic Regression(    4000/10000): loss= 788.798428121199\n",
      "Losgistic Regression(    4100/10000): loss= 787.570520859419\n",
      "Losgistic Regression(    4200/10000): loss= 786.311252719678\n",
      "Losgistic Regression(    4300/10000): loss= 785.104084811113\n",
      "Losgistic Regression(    4400/10000): loss= 783.93086240715\n",
      "Losgistic Regression(    4500/10000): loss= 782.839183696378\n",
      "Losgistic Regression(    4600/10000): loss= 781.810710921492\n",
      "Losgistic Regression(    4700/10000): loss= 780.853223435559\n",
      "Losgistic Regression(    4800/10000): loss= 779.941290722733\n",
      "Losgistic Regression(    4900/10000): loss= 779.016079409583\n",
      "Losgistic Regression(    5000/10000): loss= 778.096797255165\n",
      "Losgistic Regression(    5100/10000): loss= 777.223004392085\n",
      "Losgistic Regression(    5200/10000): loss= 776.379595888977\n",
      "Losgistic Regression(    5300/10000): loss= 775.598895160451\n",
      "Losgistic Regression(    5400/10000): loss= 774.86265973106\n",
      "Losgistic Regression(    5500/10000): loss= 774.129629566246\n",
      "Losgistic Regression(    5600/10000): loss= 773.423533843865\n",
      "Losgistic Regression(    5700/10000): loss= 772.713472421998\n",
      "Losgistic Regression(    5800/10000): loss= 771.902402542763\n",
      "Losgistic Regression(    5900/10000): loss= 771.161652537226\n",
      "Losgistic Regression(    6000/10000): loss= 770.483522682291\n",
      "Losgistic Regression(    6100/10000): loss= 769.858205367769\n",
      "Losgistic Regression(    6200/10000): loss= 769.265164853726\n",
      "Losgistic Regression(    6300/10000): loss= 768.700853456423\n",
      "Losgistic Regression(    6400/10000): loss= 768.072026246037\n",
      "Losgistic Regression(    6500/10000): loss= 767.405364445939\n",
      "Losgistic Regression(    6600/10000): loss= 766.693098313721\n",
      "Losgistic Regression(    6700/10000): loss= 765.953206437012\n",
      "Losgistic Regression(    6800/10000): loss= 765.189668090111\n",
      "Losgistic Regression(    6900/10000): loss= 764.398988581147\n",
      "Losgistic Regression(    7000/10000): loss= 763.61568326461\n",
      "Losgistic Regression(    7100/10000): loss= 762.904283323402\n",
      "Losgistic Regression(    7200/10000): loss= 762.234917233063\n",
      "Losgistic Regression(    7300/10000): loss= 761.653789991937\n",
      "Losgistic Regression(    7400/10000): loss= 761.065713204502\n",
      "Losgistic Regression(    7500/10000): loss= 760.437960817592\n",
      "Losgistic Regression(    7600/10000): loss= 759.799972333978\n",
      "Losgistic Regression(    7700/10000): loss= 759.14399141697\n",
      "Losgistic Regression(    7800/10000): loss= 758.491310694796\n",
      "Losgistic Regression(    7900/10000): loss= 757.923925703967\n",
      "Losgistic Regression(    8000/10000): loss= 757.324900894945\n",
      "Losgistic Regression(    8100/10000): loss= 756.770560984719\n",
      "Losgistic Regression(    8200/10000): loss= 756.314019963768\n",
      "Losgistic Regression(    8300/10000): loss= 755.809271511409\n",
      "Losgistic Regression(    8400/10000): loss= 755.28721595165\n",
      "Losgistic Regression(    8500/10000): loss= 754.777750427969\n",
      "Losgistic Regression(    8600/10000): loss= 754.259413758329\n",
      "Losgistic Regression(    8700/10000): loss= 753.803516254934\n",
      "Losgistic Regression(    8800/10000): loss= 753.444545610283\n",
      "Losgistic Regression(    8900/10000): loss= 753.118976870119\n",
      "Losgistic Regression(    9000/10000): loss= 752.781534854801\n",
      "Losgistic Regression(    9100/10000): loss= 752.372599246686\n",
      "Losgistic Regression(    9200/10000): loss= 751.936605493129\n",
      "Losgistic Regression(    9300/10000): loss= 751.50963029908\n",
      "Losgistic Regression(    9400/10000): loss= 751.203525271118\n",
      "Losgistic Regression(    9500/10000): loss= 750.928308847147\n",
      "Losgistic Regression(    9600/10000): loss= 750.511334048677\n",
      "Losgistic Regression(    9700/10000): loss= 750.050302641206\n",
      "Losgistic Regression(    9800/10000): loss= 749.651748420093\n",
      "Losgistic Regression(    9900/10000): loss= 749.294534878476\n",
      "Time for  0th cross validation = 83.0543s\n",
      "Training Accuracy         = 0.8816\n",
      "Cross Validation Accuracy = 0.789588\n",
      "Losgistic Regression(       0/10000): loss= 1724.82229590019\n",
      "Losgistic Regression(     100/10000): loss= 1224.61431747928\n",
      "Losgistic Regression(     200/10000): loss= 1064.82870238259\n",
      "Losgistic Regression(     300/10000): loss= 1001.04395835549\n",
      "Losgistic Regression(     400/10000): loss= 964.316319619217\n",
      "Losgistic Regression(     500/10000): loss= 937.869059203012\n",
      "Losgistic Regression(     600/10000): loss= 916.980124138315\n",
      "Losgistic Regression(     700/10000): loss= 899.698996421594\n",
      "Losgistic Regression(     800/10000): loss= 885.005293381415\n",
      "Losgistic Regression(     900/10000): loss= 871.837938645472\n",
      "Losgistic Regression(    1000/10000): loss= 860.285681213658\n",
      "Losgistic Regression(    1100/10000): loss= 850.461020580162\n",
      "Losgistic Regression(    1200/10000): loss= 842.043448315129\n",
      "Losgistic Regression(    1300/10000): loss= 834.887317554441\n",
      "Losgistic Regression(    1400/10000): loss= 828.789358050462\n",
      "Losgistic Regression(    1500/10000): loss= 823.572779619077\n",
      "Losgistic Regression(    1600/10000): loss= 819.108516441818\n",
      "Losgistic Regression(    1700/10000): loss= 815.243698177656\n",
      "Losgistic Regression(    1800/10000): loss= 811.746500885549\n",
      "Losgistic Regression(    1900/10000): loss= 807.725599255393\n",
      "Losgistic Regression(    2000/10000): loss= 804.078276106937\n",
      "Losgistic Regression(    2100/10000): loss= 800.766664913274\n",
      "Losgistic Regression(    2200/10000): loss= 797.746087675304\n",
      "Losgistic Regression(    2300/10000): loss= 794.99284730289\n",
      "Losgistic Regression(    2400/10000): loss= 792.449617180891\n",
      "Losgistic Regression(    2500/10000): loss= 790.069507081943\n",
      "Losgistic Regression(    2600/10000): loss= 787.858465915446\n",
      "Losgistic Regression(    2700/10000): loss= 785.837573621939\n",
      "Losgistic Regression(    2800/10000): loss= 783.984440724469\n",
      "Losgistic Regression(    2900/10000): loss= 782.082207590142\n",
      "Losgistic Regression(    3000/10000): loss= 780.150641826463\n",
      "Losgistic Regression(    3100/10000): loss= 778.352570540888\n",
      "Losgistic Regression(    3200/10000): loss= 776.304573289921\n",
      "Losgistic Regression(    3300/10000): loss= 774.34081030916\n",
      "Losgistic Regression(    3400/10000): loss= 772.168717130312\n",
      "Losgistic Regression(    3500/10000): loss= 770.189299382866\n",
      "Losgistic Regression(    3600/10000): loss= 768.198170698355\n",
      "Losgistic Regression(    3700/10000): loss= 766.253652190856\n",
      "Losgistic Regression(    3800/10000): loss= 764.382994043899\n",
      "Losgistic Regression(    3900/10000): loss= 762.427148740748\n",
      "Losgistic Regression(    4000/10000): loss= 760.340504356036\n",
      "Losgistic Regression(    4100/10000): loss= 758.113797847188\n",
      "Losgistic Regression(    4200/10000): loss= 755.517370694636\n",
      "Losgistic Regression(    4300/10000): loss= 752.749923746933\n",
      "Losgistic Regression(    4400/10000): loss= 749.967455969184\n",
      "Losgistic Regression(    4500/10000): loss= 747.404395209879\n",
      "Losgistic Regression(    4600/10000): loss= 745.062191903319\n",
      "Losgistic Regression(    4700/10000): loss= 742.800161119479\n",
      "Losgistic Regression(    4800/10000): loss= 740.542088922025\n",
      "Losgistic Regression(    4900/10000): loss= 738.343109807947\n",
      "Losgistic Regression(    5000/10000): loss= 736.150198345377\n",
      "Losgistic Regression(    5100/10000): loss= 733.8520739028\n",
      "Losgistic Regression(    5200/10000): loss= 731.526805156837\n",
      "Losgistic Regression(    5300/10000): loss= 729.340227283312\n",
      "Losgistic Regression(    5400/10000): loss= 727.480324952093\n",
      "Losgistic Regression(    5500/10000): loss= 725.628402796353\n",
      "Losgistic Regression(    5600/10000): loss= 723.485162427811\n",
      "Losgistic Regression(    5700/10000): loss= 721.417876092098\n",
      "Losgistic Regression(    5800/10000): loss= 719.698839153317\n",
      "Losgistic Regression(    5900/10000): loss= 718.187170277802\n",
      "Losgistic Regression(    6000/10000): loss= 716.686184958549\n",
      "Losgistic Regression(    6100/10000): loss= 715.149888200548\n",
      "Losgistic Regression(    6200/10000): loss= 713.825360935957\n",
      "Losgistic Regression(    6300/10000): loss= 712.385002710232\n",
      "Losgistic Regression(    6400/10000): loss= 710.754253035909\n",
      "Losgistic Regression(    6500/10000): loss= 709.500371246053\n",
      "Losgistic Regression(    6600/10000): loss= 708.429723324671\n",
      "Losgistic Regression(    6700/10000): loss= 707.231817792754\n",
      "Losgistic Regression(    6800/10000): loss= 706.019918686812\n",
      "Losgistic Regression(    6900/10000): loss= 704.696774381814\n",
      "Losgistic Regression(    7000/10000): loss= 703.148364844079\n",
      "Losgistic Regression(    7100/10000): loss= 701.962747367195\n",
      "Losgistic Regression(    7200/10000): loss= 700.994929526507\n",
      "Losgistic Regression(    7300/10000): loss= 700.355434002166\n",
      "Losgistic Regression(    7400/10000): loss= 699.856511921211\n",
      "Losgistic Regression(    7500/10000): loss= 699.202282486335\n",
      "Losgistic Regression(    7600/10000): loss= 698.678571387784\n",
      "Losgistic Regression(    7700/10000): loss= 698.1796023867\n",
      "Losgistic Regression(    7800/10000): loss= 697.638122776683\n",
      "Losgistic Regression(    7900/10000): loss= 697.157320172073\n",
      "Losgistic Regression(    8000/10000): loss= 696.97543034096\n",
      "Losgistic Regression(    8100/10000): loss= 696.911066503568\n",
      "Losgistic Regression(    8200/10000): loss= 696.914228757235\n",
      "Totoal number of iterations =  8200\n",
      "Loss                        =  696.914228757\n",
      "Time for  1th cross validation = 67.7413s\n",
      "Training Accuracy         =  0.882\n",
      "Cross Validation Accuracy = 0.781796\n",
      "Losgistic Regression(       0/10000): loss= 1726.07241752705\n",
      "Losgistic Regression(     100/10000): loss= 1225.49486628735\n",
      "Losgistic Regression(     200/10000): loss= 1069.93894233674\n",
      "Losgistic Regression(     300/10000): loss= 1009.71424247759\n",
      "Losgistic Regression(     400/10000): loss= 971.227709699467\n",
      "Losgistic Regression(     500/10000): loss= 943.68166409249\n",
      "Losgistic Regression(     600/10000): loss= 922.691382451748\n",
      "Losgistic Regression(     700/10000): loss= 905.92284462176\n",
      "Losgistic Regression(     800/10000): loss= 892.054608326706\n",
      "Losgistic Regression(     900/10000): loss= 880.376878430199\n",
      "Losgistic Regression(    1000/10000): loss= 869.823411067926\n",
      "Losgistic Regression(    1100/10000): loss= 859.38516812558\n",
      "Losgistic Regression(    1200/10000): loss= 850.10737805291\n",
      "Losgistic Regression(    1300/10000): loss= 841.907181282327\n",
      "Losgistic Regression(    1400/10000): loss= 834.639132847135\n",
      "Losgistic Regression(    1500/10000): loss= 828.180697003914\n",
      "Losgistic Regression(    1600/10000): loss= 822.446781199965\n",
      "Losgistic Regression(    1700/10000): loss= 817.298161352792\n",
      "Losgistic Regression(    1800/10000): loss= 812.596249866124\n",
      "Losgistic Regression(    1900/10000): loss= 808.279087177029\n",
      "Losgistic Regression(    2000/10000): loss= 803.902511897051\n",
      "Losgistic Regression(    2100/10000): loss= 799.530831795236\n",
      "Losgistic Regression(    2200/10000): loss= 795.429733037868\n",
      "Losgistic Regression(    2300/10000): loss= 791.54006215897\n",
      "Losgistic Regression(    2400/10000): loss= 787.890682359321\n",
      "Losgistic Regression(    2500/10000): loss= 784.527189513716\n",
      "Losgistic Regression(    2600/10000): loss= 781.396722299738\n",
      "Losgistic Regression(    2700/10000): loss= 777.856387327501\n",
      "Losgistic Regression(    2800/10000): loss= 774.242443972114\n",
      "Losgistic Regression(    2900/10000): loss= 770.327406146347\n",
      "Losgistic Regression(    3000/10000): loss= 766.734597983234\n",
      "Losgistic Regression(    3100/10000): loss= 763.458931998276\n",
      "Losgistic Regression(    3200/10000): loss= 760.423865251662\n",
      "Losgistic Regression(    3300/10000): loss= 757.587492641912\n",
      "Losgistic Regression(    3400/10000): loss= 754.951980343234\n",
      "Losgistic Regression(    3500/10000): loss= 752.45907320759\n",
      "Losgistic Regression(    3600/10000): loss= 750.215376916122\n",
      "Losgistic Regression(    3700/10000): loss= 748.090475253\n",
      "Losgistic Regression(    3800/10000): loss= 746.029552764975\n",
      "Losgistic Regression(    3900/10000): loss= 744.187108411843\n",
      "Losgistic Regression(    4000/10000): loss= 742.575332167366\n",
      "Losgistic Regression(    4100/10000): loss= 741.208539473712\n",
      "Losgistic Regression(    4200/10000): loss= 739.863418072852\n",
      "Losgistic Regression(    4300/10000): loss= 738.413477745148\n",
      "Losgistic Regression(    4400/10000): loss= 737.105684526758\n",
      "Losgistic Regression(    4500/10000): loss= 735.935660513262\n",
      "Losgistic Regression(    4600/10000): loss= 734.839547406349\n",
      "Losgistic Regression(    4700/10000): loss= 734.017205045428\n",
      "Losgistic Regression(    4800/10000): loss= 733.404449300645\n",
      "Losgistic Regression(    4900/10000): loss= 732.755733871731\n",
      "Losgistic Regression(    5000/10000): loss= 732.076156465087\n",
      "Losgistic Regression(    5100/10000): loss= 731.438177155471\n",
      "Losgistic Regression(    5200/10000): loss= 730.823204151928\n",
      "Losgistic Regression(    5300/10000): loss= 730.277300554675\n",
      "Losgistic Regression(    5400/10000): loss= 729.636759312271\n",
      "Losgistic Regression(    5500/10000): loss= 728.907427798883\n",
      "Losgistic Regression(    5600/10000): loss= 728.268145624066\n",
      "Losgistic Regression(    5700/10000): loss= 727.652723619294\n",
      "Losgistic Regression(    5800/10000): loss= 727.075797935025\n",
      "Losgistic Regression(    5900/10000): loss= 726.517369054088\n",
      "Losgistic Regression(    6000/10000): loss= 725.872851604891\n",
      "Losgistic Regression(    6100/10000): loss= 725.110941248583\n",
      "Losgistic Regression(    6200/10000): loss= 724.474486584838\n",
      "Losgistic Regression(    6300/10000): loss= 723.999041336536\n",
      "Losgistic Regression(    6400/10000): loss= 723.570580112332\n",
      "Losgistic Regression(    6500/10000): loss= 723.244705182261\n",
      "Losgistic Regression(    6600/10000): loss= 722.96464960965\n",
      "Losgistic Regression(    6700/10000): loss= 722.78107941539\n",
      "Losgistic Regression(    6800/10000): loss= 722.528746502732\n",
      "Losgistic Regression(    6900/10000): loss= 722.286388259769\n",
      "Losgistic Regression(    7000/10000): loss= 722.246931249103\n",
      "Losgistic Regression(    7100/10000): loss= 722.249024218921\n",
      "Totoal number of iterations =  7100\n",
      "Loss                        =  722.249024219\n",
      "Time for  2th cross validation = 58.3343s\n",
      "Training Accuracy         = 0.8808\n",
      "Cross Validation Accuracy = 0.776188\n",
      "Losgistic Regression(       0/10000): loss= 1725.44692507244\n",
      "Losgistic Regression(     100/10000): loss= 1251.14932735084\n",
      "Losgistic Regression(     200/10000): loss= 1113.48516761705\n",
      "Losgistic Regression(     300/10000): loss= 1054.18943203284\n",
      "Losgistic Regression(     400/10000): loss= 1015.19004534061\n",
      "Losgistic Regression(     500/10000): loss= 986.275571215088\n",
      "Losgistic Regression(     600/10000): loss= 963.919837690285\n",
      "Losgistic Regression(     700/10000): loss= 945.821947033444\n",
      "Losgistic Regression(     800/10000): loss= 927.751420027749\n",
      "Losgistic Regression(     900/10000): loss= 910.73123515621\n",
      "Losgistic Regression(    1000/10000): loss= 892.103296250132\n",
      "Losgistic Regression(    1100/10000): loss= 875.020041833922\n",
      "Losgistic Regression(    1200/10000): loss= 860.217945936573\n",
      "Losgistic Regression(    1300/10000): loss= 847.428015655174\n",
      "Losgistic Regression(    1400/10000): loss= 836.586556030104\n",
      "Losgistic Regression(    1500/10000): loss= 827.408135972919\n",
      "Losgistic Regression(    1600/10000): loss= 819.390371454343\n",
      "Losgistic Regression(    1700/10000): loss= 812.2997863321\n",
      "Losgistic Regression(    1800/10000): loss= 805.945523252856\n",
      "Losgistic Regression(    1900/10000): loss= 799.864153335284\n",
      "Losgistic Regression(    2000/10000): loss= 794.356333841785\n",
      "Losgistic Regression(    2100/10000): loss= 789.375302651042\n",
      "Losgistic Regression(    2200/10000): loss= 784.877960581597\n",
      "Losgistic Regression(    2300/10000): loss= 780.891082640748\n",
      "Losgistic Regression(    2400/10000): loss= 777.358266755501\n",
      "Losgistic Regression(    2500/10000): loss= 774.223704891788\n",
      "Losgistic Regression(    2600/10000): loss= 771.049866963679\n",
      "Losgistic Regression(    2700/10000): loss= 768.117086075749\n",
      "Losgistic Regression(    2800/10000): loss= 765.463032354185\n",
      "Losgistic Regression(    2900/10000): loss= 763.094885778197\n",
      "Losgistic Regression(    3000/10000): loss= 761.055158355535\n",
      "Losgistic Regression(    3100/10000): loss= 759.220673415296\n",
      "Losgistic Regression(    3200/10000): loss= 757.63844162733\n",
      "Losgistic Regression(    3300/10000): loss= 756.353975652484\n",
      "Losgistic Regression(    3400/10000): loss= 755.072312218612\n",
      "Losgistic Regression(    3500/10000): loss= 753.714705213543\n",
      "Losgistic Regression(    3600/10000): loss= 752.492248063953\n",
      "Losgistic Regression(    3700/10000): loss= 751.431760982826\n",
      "Losgistic Regression(    3800/10000): loss= 750.424447408417\n",
      "Losgistic Regression(    3900/10000): loss= 749.43981619989\n",
      "Losgistic Regression(    4000/10000): loss= 748.588010402135\n",
      "Losgistic Regression(    4100/10000): loss= 747.803713852953\n",
      "Losgistic Regression(    4200/10000): loss= 747.089341755023\n",
      "Losgistic Regression(    4300/10000): loss= 746.369470800257\n",
      "Losgistic Regression(    4400/10000): loss= 745.667084811612\n",
      "Losgistic Regression(    4500/10000): loss= 745.019386790768\n",
      "Losgistic Regression(    4600/10000): loss= 744.054274312504\n",
      "Losgistic Regression(    4700/10000): loss= 743.099709792991\n",
      "Losgistic Regression(    4800/10000): loss= 742.171491203437\n",
      "Losgistic Regression(    4900/10000): loss= 741.278435102425\n",
      "Losgistic Regression(    5000/10000): loss= 740.482777915673\n",
      "Losgistic Regression(    5100/10000): loss= 739.848691264044\n",
      "Losgistic Regression(    5200/10000): loss= 739.334787411467\n",
      "Losgistic Regression(    5300/10000): loss= 738.753059025464\n",
      "Losgistic Regression(    5400/10000): loss= 738.308301764268\n",
      "Losgistic Regression(    5500/10000): loss= 737.940859609565\n",
      "Losgistic Regression(    5600/10000): loss= 737.614086638702\n",
      "Losgistic Regression(    5700/10000): loss= 737.144660643938\n",
      "Losgistic Regression(    5800/10000): loss= 736.673997893352\n",
      "Losgistic Regression(    5900/10000): loss= 736.165803955198\n",
      "Losgistic Regression(    6000/10000): loss= 735.487207550834\n",
      "Losgistic Regression(    6100/10000): loss= 734.743948428832\n",
      "Losgistic Regression(    6200/10000): loss= 734.166321444877\n",
      "Losgistic Regression(    6300/10000): loss= 733.437387704075\n",
      "Losgistic Regression(    6400/10000): loss= 732.59295478499\n",
      "Losgistic Regression(    6500/10000): loss= 731.767804756216\n",
      "Losgistic Regression(    6600/10000): loss= 730.807429791491\n",
      "Losgistic Regression(    6700/10000): loss= 729.874497612905\n",
      "Losgistic Regression(    6800/10000): loss= 729.031879416394\n",
      "Losgistic Regression(    6900/10000): loss= 727.991197710074\n",
      "Losgistic Regression(    7000/10000): loss= 726.816654353348\n",
      "Losgistic Regression(    7100/10000): loss= 725.663615493577\n",
      "Losgistic Regression(    7200/10000): loss= 724.682638922981\n",
      "Losgistic Regression(    7300/10000): loss= 723.776458691064\n",
      "Losgistic Regression(    7400/10000): loss= 722.923348967171\n",
      "Losgistic Regression(    7500/10000): loss= 722.183135852103\n",
      "Losgistic Regression(    7600/10000): loss= 721.587304488521\n",
      "Losgistic Regression(    7700/10000): loss= 721.007003076742\n",
      "Losgistic Regression(    7800/10000): loss= 720.39472554069\n",
      "Losgistic Regression(    7900/10000): loss= 719.761130617847\n",
      "Losgistic Regression(    8000/10000): loss= 719.104525816949\n",
      "Losgistic Regression(    8100/10000): loss= 718.465602433627\n",
      "Losgistic Regression(    8200/10000): loss= 717.702733536858\n",
      "Losgistic Regression(    8300/10000): loss= 717.013789549719\n",
      "Losgistic Regression(    8400/10000): loss= 716.489990769361\n",
      "Losgistic Regression(    8500/10000): loss= 716.031392411607\n",
      "Losgistic Regression(    8600/10000): loss= 715.570201240951\n",
      "Losgistic Regression(    8700/10000): loss= 714.990410706022\n",
      "Losgistic Regression(    8800/10000): loss= 714.337158247748\n",
      "Losgistic Regression(    8900/10000): loss= 713.761864424908\n",
      "Losgistic Regression(    9000/10000): loss= 713.213968122751\n",
      "Losgistic Regression(    9100/10000): loss= 712.70610987466\n",
      "Losgistic Regression(    9200/10000): loss= 712.248321747932\n",
      "Losgistic Regression(    9300/10000): loss= 711.788167196933\n",
      "Losgistic Regression(    9400/10000): loss= 711.23898284977\n",
      "Losgistic Regression(    9500/10000): loss= 710.712631788778\n",
      "Losgistic Regression(    9600/10000): loss= 710.164259772823\n",
      "Losgistic Regression(    9700/10000): loss= 709.542734317612\n",
      "Losgistic Regression(    9800/10000): loss= 708.907927754485\n",
      "Losgistic Regression(    9900/10000): loss= 708.324506586636\n",
      "Time for  3th cross validation = 81.4281s\n",
      "Training Accuracy         = 0.8696\n",
      "Cross Validation Accuracy = 0.785488\n",
      "Losgistic Regression(       0/10000): loss= 1726.04748874879\n",
      "Losgistic Regression(     100/10000): loss= 1231.92036185145\n",
      "Losgistic Regression(     200/10000): loss= 1077.48458935471\n",
      "Losgistic Regression(     300/10000): loss= 1016.76477531851\n",
      "Losgistic Regression(     400/10000): loss= 976.831144747524\n",
      "Losgistic Regression(     500/10000): loss= 947.026376324488\n",
      "Losgistic Regression(     600/10000): loss= 923.571595023161\n",
      "Losgistic Regression(     700/10000): loss= 904.597863279736\n",
      "Losgistic Regression(     800/10000): loss= 888.340338921599\n",
      "Losgistic Regression(     900/10000): loss= 872.068238365879\n",
      "Losgistic Regression(    1000/10000): loss= 857.360904006965\n",
      "Losgistic Regression(    1100/10000): loss= 844.600368674409\n",
      "Losgistic Regression(    1200/10000): loss= 833.068929668129\n",
      "Losgistic Regression(    1300/10000): loss= 822.650255475062\n",
      "Losgistic Regression(    1400/10000): loss= 813.337600297192\n",
      "Losgistic Regression(    1500/10000): loss= 805.231393001119\n",
      "Losgistic Regression(    1600/10000): loss= 798.23717465318\n",
      "Losgistic Regression(    1700/10000): loss= 790.752541475383\n",
      "Losgistic Regression(    1800/10000): loss= 783.76489811838\n",
      "Losgistic Regression(    1900/10000): loss= 777.313292941194\n",
      "Losgistic Regression(    2000/10000): loss= 771.324523458172\n",
      "Losgistic Regression(    2100/10000): loss= 765.649448190774\n",
      "Losgistic Regression(    2200/10000): loss= 760.164077265146\n",
      "Losgistic Regression(    2300/10000): loss= 754.808924024819\n",
      "Losgistic Regression(    2400/10000): loss= 749.523293910906\n",
      "Losgistic Regression(    2500/10000): loss= 744.543301378986\n",
      "Losgistic Regression(    2600/10000): loss= 739.951287071044\n",
      "Losgistic Regression(    2700/10000): loss= 735.763365138537\n",
      "Losgistic Regression(    2800/10000): loss= 731.876492778526\n",
      "Losgistic Regression(    2900/10000): loss= 728.058499716853\n",
      "Losgistic Regression(    3000/10000): loss= 724.431340561515\n",
      "Losgistic Regression(    3100/10000): loss= 720.956067798896\n",
      "Losgistic Regression(    3200/10000): loss= 717.537111091807\n",
      "Losgistic Regression(    3300/10000): loss= 714.002321816494\n",
      "Losgistic Regression(    3400/10000): loss= 710.249946855092\n",
      "Losgistic Regression(    3500/10000): loss= 706.466946205342\n",
      "Losgistic Regression(    3600/10000): loss= 702.765523502629\n",
      "Losgistic Regression(    3700/10000): loss= 699.057329666217\n",
      "Losgistic Regression(    3800/10000): loss= 695.247222233496\n",
      "Losgistic Regression(    3900/10000): loss= 691.398372434381\n",
      "Losgistic Regression(    4000/10000): loss= 687.514403470884\n",
      "Losgistic Regression(    4100/10000): loss= 683.720896142279\n",
      "Losgistic Regression(    4200/10000): loss= 679.977710536608\n",
      "Losgistic Regression(    4300/10000): loss= 676.104052566196\n",
      "Losgistic Regression(    4400/10000): loss= 672.198530454191\n",
      "Losgistic Regression(    4500/10000): loss= 668.316563983396\n",
      "Losgistic Regression(    4600/10000): loss= 664.527651696332\n",
      "Losgistic Regression(    4700/10000): loss= 660.796792922451\n",
      "Losgistic Regression(    4800/10000): loss= 657.099184328594\n",
      "Losgistic Regression(    4900/10000): loss= 653.450376782146\n",
      "Losgistic Regression(    5000/10000): loss= 649.928494761252\n",
      "Losgistic Regression(    5100/10000): loss= 646.544148043622\n",
      "Losgistic Regression(    5200/10000): loss= 643.191718994945\n",
      "Losgistic Regression(    5300/10000): loss= 639.763473572208\n",
      "Losgistic Regression(    5400/10000): loss= 636.42942008182\n",
      "Losgistic Regression(    5500/10000): loss= 633.165384991047\n",
      "Losgistic Regression(    5600/10000): loss= 629.97146993643\n",
      "Losgistic Regression(    5700/10000): loss= 626.56440804174\n",
      "Losgistic Regression(    5800/10000): loss= 623.041021788979\n",
      "Losgistic Regression(    5900/10000): loss= 619.396665977331\n",
      "Losgistic Regression(    6000/10000): loss= 615.761462377678\n",
      "Losgistic Regression(    6100/10000): loss= 612.312097032505\n",
      "Losgistic Regression(    6200/10000): loss= 608.935568347298\n",
      "Losgistic Regression(    6300/10000): loss= 605.549742197394\n",
      "Losgistic Regression(    6400/10000): loss= 602.070574693171\n",
      "Losgistic Regression(    6500/10000): loss= 598.460709685257\n",
      "Losgistic Regression(    6600/10000): loss= 594.9565036862\n",
      "Losgistic Regression(    6700/10000): loss= 591.823645813199\n",
      "Losgistic Regression(    6800/10000): loss= 588.860085871814\n",
      "Losgistic Regression(    6900/10000): loss= 585.911837017018\n",
      "Losgistic Regression(    7000/10000): loss= 582.783847321606\n",
      "Losgistic Regression(    7100/10000): loss= 579.580875016269\n",
      "Losgistic Regression(    7200/10000): loss= 576.501614209516\n",
      "Losgistic Regression(    7300/10000): loss= 573.523279985882\n",
      "Losgistic Regression(    7400/10000): loss= 570.386185064462\n",
      "Losgistic Regression(    7500/10000): loss= 567.074234782376\n",
      "Losgistic Regression(    7600/10000): loss= 563.695598648296\n",
      "Losgistic Regression(    7700/10000): loss= 560.56094940646\n",
      "Losgistic Regression(    7800/10000): loss= 557.855168703699\n",
      "Losgistic Regression(    7900/10000): loss= 555.322686414379\n",
      "Losgistic Regression(    8000/10000): loss= 552.644746651769\n",
      "Losgistic Regression(    8100/10000): loss= 549.989903025278\n",
      "Losgistic Regression(    8200/10000): loss= 547.634095356467\n",
      "Losgistic Regression(    8300/10000): loss= 545.568557300473\n",
      "Losgistic Regression(    8400/10000): loss= 543.270618896413\n",
      "Losgistic Regression(    8500/10000): loss= 540.421543033807\n",
      "Losgistic Regression(    8600/10000): loss= 537.398163520103\n",
      "Losgistic Regression(    8700/10000): loss= 534.401198881212\n",
      "Losgistic Regression(    8800/10000): loss= 531.61751968404\n",
      "Losgistic Regression(    8900/10000): loss= 528.756839282533\n",
      "Losgistic Regression(    9000/10000): loss= 525.798767550554\n",
      "Losgistic Regression(    9100/10000): loss= 522.997397798992\n",
      "Losgistic Regression(    9200/10000): loss= 520.532790731565\n",
      "Losgistic Regression(    9300/10000): loss= 518.227733599521\n",
      "Losgistic Regression(    9400/10000): loss= 515.907965243161\n",
      "Losgistic Regression(    9500/10000): loss= 513.505956853206\n",
      "Losgistic Regression(    9600/10000): loss= 511.147520390332\n",
      "Losgistic Regression(    9700/10000): loss= 508.955924325358\n",
      "Losgistic Regression(    9800/10000): loss= 507.097772614355\n",
      "Losgistic Regression(    9900/10000): loss= 505.29287292752\n",
      "Time for  4th cross validation = 81.3802s\n",
      "Training Accuracy         = 0.8836\n",
      "Cross Validation Accuracy = 0.784344\n",
      "*************** ([0.88160000000000005, 0.88200000000000001, 0.88080000000000003, 0.86960000000000004, 0.88360000000000005], [0.78958799999999996, 0.78179600000000005, 0.77618799999999999, 0.78548799999999996, 0.78434400000000004])\n",
      "Losgistic Regression(       0/10000): loss= 1725.79899674268\n",
      "Losgistic Regression(     100/10000): loss= 1243.46497328571\n",
      "Losgistic Regression(     200/10000): loss= 1087.44918080483\n",
      "Losgistic Regression(     300/10000): loss= 1033.09114967217\n",
      "Losgistic Regression(     400/10000): loss= 1002.20476392319\n",
      "Losgistic Regression(     500/10000): loss= 980.753019611808\n",
      "Losgistic Regression(     600/10000): loss= 964.581091515693\n",
      "Losgistic Regression(     700/10000): loss= 951.79738229357\n",
      "Losgistic Regression(     800/10000): loss= 941.214299551632\n",
      "Losgistic Regression(     900/10000): loss= 932.414420718683\n",
      "Losgistic Regression(    1000/10000): loss= 924.942929285134\n",
      "Losgistic Regression(    1100/10000): loss= 918.632380471554\n",
      "Losgistic Regression(    1200/10000): loss= 913.160573918665\n",
      "Losgistic Regression(    1300/10000): loss= 908.351573511357\n",
      "Losgistic Regression(    1400/10000): loss= 904.17962126163\n",
      "Losgistic Regression(    1500/10000): loss= 900.495181316916\n",
      "Losgistic Regression(    1600/10000): loss= 897.255011698786\n",
      "Losgistic Regression(    1700/10000): loss= 894.46773246005\n",
      "Losgistic Regression(    1800/10000): loss= 892.072614463078\n",
      "Losgistic Regression(    1900/10000): loss= 889.884054170937\n",
      "Losgistic Regression(    2000/10000): loss= 887.883267558013\n",
      "Losgistic Regression(    2100/10000): loss= 886.084368563479\n",
      "Losgistic Regression(    2200/10000): loss= 884.4640465854\n",
      "Losgistic Regression(    2300/10000): loss= 883.105087074809\n",
      "Losgistic Regression(    2400/10000): loss= 881.873222021066\n",
      "Losgistic Regression(    2500/10000): loss= 880.755902699147\n",
      "Losgistic Regression(    2600/10000): loss= 879.744351737825\n",
      "Losgistic Regression(    2700/10000): loss= 878.813659918976\n",
      "Losgistic Regression(    2800/10000): loss= 877.984904940009\n",
      "Losgistic Regression(    2900/10000): loss= 877.18809036118\n",
      "Losgistic Regression(    3000/10000): loss= 876.417358378106\n",
      "Losgistic Regression(    3100/10000): loss= 875.672464750104\n",
      "Losgistic Regression(    3200/10000): loss= 874.995503443351\n",
      "Losgistic Regression(    3300/10000): loss= 874.331221292796\n",
      "Losgistic Regression(    3400/10000): loss= 873.771257727142\n",
      "Losgistic Regression(    3500/10000): loss= 873.217601946992\n",
      "Losgistic Regression(    3600/10000): loss= 872.667034941413\n",
      "Losgistic Regression(    3700/10000): loss= 872.134649611435\n",
      "Losgistic Regression(    3800/10000): loss= 871.666700999647\n",
      "Losgistic Regression(    3900/10000): loss= 871.253064795045\n",
      "Losgistic Regression(    4000/10000): loss= 870.843488096327\n",
      "Losgistic Regression(    4100/10000): loss= 870.485235731907\n",
      "Losgistic Regression(    4200/10000): loss= 870.139480828383\n",
      "Losgistic Regression(    4300/10000): loss= 869.803850053402\n",
      "Losgistic Regression(    4400/10000): loss= 869.484188113848\n",
      "Losgistic Regression(    4500/10000): loss= 869.161557168936\n",
      "Losgistic Regression(    4600/10000): loss= 868.804721984413\n",
      "Losgistic Regression(    4700/10000): loss= 868.481178026018\n",
      "Losgistic Regression(    4800/10000): loss= 868.16781275534\n",
      "Losgistic Regression(    4900/10000): loss= 867.898683664048\n",
      "Losgistic Regression(    5000/10000): loss= 867.656474201623\n",
      "Losgistic Regression(    5100/10000): loss= 867.404053610636\n",
      "Losgistic Regression(    5200/10000): loss= 867.152329841874\n",
      "Losgistic Regression(    5300/10000): loss= 866.964262236778\n",
      "Losgistic Regression(    5400/10000): loss= 866.800985250463\n",
      "Losgistic Regression(    5500/10000): loss= 866.607695682238\n",
      "Losgistic Regression(    5600/10000): loss= 866.420400882091\n",
      "Losgistic Regression(    5700/10000): loss= 866.216344079766\n",
      "Losgistic Regression(    5800/10000): loss= 866.024780006052\n",
      "Losgistic Regression(    5900/10000): loss= 865.828384371057\n",
      "Losgistic Regression(    6000/10000): loss= 865.661728880854\n",
      "Losgistic Regression(    6100/10000): loss= 865.471944547983\n",
      "Losgistic Regression(    6200/10000): loss= 865.25972776908\n",
      "Losgistic Regression(    6300/10000): loss= 865.061401139261\n",
      "Losgistic Regression(    6400/10000): loss= 864.903454262965\n",
      "Losgistic Regression(    6500/10000): loss= 864.772354275806\n",
      "Losgistic Regression(    6600/10000): loss= 864.632274842138\n",
      "Losgistic Regression(    6700/10000): loss= 864.484235326331\n",
      "Losgistic Regression(    6800/10000): loss= 864.359016023619\n",
      "Losgistic Regression(    6900/10000): loss= 864.191682944625\n",
      "Losgistic Regression(    7000/10000): loss= 864.01173673941\n",
      "Losgistic Regression(    7100/10000): loss= 863.854421816271\n",
      "Losgistic Regression(    7200/10000): loss= 863.715714396967\n",
      "Losgistic Regression(    7300/10000): loss= 863.588996026005\n",
      "Losgistic Regression(    7400/10000): loss= 863.480757757871\n",
      "Losgistic Regression(    7500/10000): loss= 863.344649784974\n",
      "Losgistic Regression(    7600/10000): loss= 863.216575701267\n",
      "Losgistic Regression(    7700/10000): loss= 863.076188239847\n",
      "Losgistic Regression(    7800/10000): loss= 862.941508659638\n",
      "Losgistic Regression(    7900/10000): loss= 862.816032056826\n",
      "Losgistic Regression(    8000/10000): loss= 862.711486846472\n",
      "Losgistic Regression(    8100/10000): loss= 862.599985372075\n",
      "Losgistic Regression(    8200/10000): loss= 862.503515734557\n",
      "Losgistic Regression(    8300/10000): loss= 862.405180984532\n",
      "Losgistic Regression(    8400/10000): loss= 862.294016684601\n",
      "Losgistic Regression(    8500/10000): loss= 862.176228764969\n",
      "Losgistic Regression(    8600/10000): loss= 862.084941813902\n",
      "Losgistic Regression(    8700/10000): loss= 862.002883535174\n",
      "Losgistic Regression(    8800/10000): loss= 861.912721475852\n",
      "Losgistic Regression(    8900/10000): loss= 861.830812043506\n",
      "Losgistic Regression(    9000/10000): loss= 861.751874647394\n",
      "Losgistic Regression(    9100/10000): loss= 861.661856789978\n",
      "Losgistic Regression(    9200/10000): loss= 861.57208003701\n",
      "Losgistic Regression(    9300/10000): loss= 861.481149314465\n",
      "Losgistic Regression(    9400/10000): loss= 861.396135198296\n",
      "Losgistic Regression(    9500/10000): loss= 861.319857655024\n",
      "Losgistic Regression(    9600/10000): loss= 861.252200329659\n",
      "Losgistic Regression(    9700/10000): loss= 861.177293251995\n",
      "Losgistic Regression(    9800/10000): loss= 861.10781195718\n",
      "Losgistic Regression(    9900/10000): loss= 861.041299805381\n",
      "Time for  0th cross validation = 80.0237s\n",
      "Training Accuracy         = 0.8708\n",
      "Cross Validation Accuracy = 0.79714\n",
      "Losgistic Regression(       0/10000): loss= 1724.8424192068\n",
      "Losgistic Regression(     100/10000): loss= 1228.96250701568\n",
      "Losgistic Regression(     200/10000): loss= 1074.11944086632\n",
      "Losgistic Regression(     300/10000): loss= 1014.62769258242\n",
      "Losgistic Regression(     400/10000): loss= 981.898001823514\n",
      "Losgistic Regression(     500/10000): loss= 959.239631441532\n",
      "Losgistic Regression(     600/10000): loss= 941.813965240215\n",
      "Losgistic Regression(     700/10000): loss= 927.742412755788\n",
      "Losgistic Regression(     800/10000): loss= 916.016199263197\n",
      "Losgistic Regression(     900/10000): loss= 906.137803003709\n",
      "Losgistic Regression(    1000/10000): loss= 897.839112928563\n",
      "Losgistic Regression(    1100/10000): loss= 890.912662532104\n",
      "Losgistic Regression(    1200/10000): loss= 885.210704924219\n",
      "Losgistic Regression(    1300/10000): loss= 880.37563307611\n",
      "Losgistic Regression(    1400/10000): loss= 876.239595991317\n",
      "Losgistic Regression(    1500/10000): loss= 872.728953631304\n",
      "Losgistic Regression(    1600/10000): loss= 869.795076509553\n",
      "Losgistic Regression(    1700/10000): loss= 867.297863245005\n",
      "Losgistic Regression(    1800/10000): loss= 865.108588104531\n",
      "Losgistic Regression(    1900/10000): loss= 863.188731557776\n",
      "Losgistic Regression(    2000/10000): loss= 861.536383355359\n",
      "Losgistic Regression(    2100/10000): loss= 860.060501681281\n",
      "Losgistic Regression(    2200/10000): loss= 858.772996622229\n",
      "Losgistic Regression(    2300/10000): loss= 857.647500094238\n",
      "Losgistic Regression(    2400/10000): loss= 856.66044159612\n",
      "Losgistic Regression(    2500/10000): loss= 855.778020783573\n",
      "Losgistic Regression(    2600/10000): loss= 854.984265171937\n",
      "Losgistic Regression(    2700/10000): loss= 854.308647455975\n",
      "Losgistic Regression(    2800/10000): loss= 853.714701662589\n",
      "Losgistic Regression(    2900/10000): loss= 853.149712239132\n",
      "Losgistic Regression(    3000/10000): loss= 852.64233270528\n",
      "Losgistic Regression(    3100/10000): loss= 852.159454351375\n",
      "Losgistic Regression(    3200/10000): loss= 851.766790922903\n",
      "Losgistic Regression(    3300/10000): loss= 851.420371843217\n",
      "Losgistic Regression(    3400/10000): loss= 851.108047061237\n",
      "Losgistic Regression(    3500/10000): loss= 850.801997194697\n",
      "Losgistic Regression(    3600/10000): loss= 850.496967209959\n",
      "Losgistic Regression(    3700/10000): loss= 850.195619122666\n",
      "Losgistic Regression(    3800/10000): loss= 849.928548207016\n",
      "Losgistic Regression(    3900/10000): loss= 849.705092467731\n",
      "Losgistic Regression(    4000/10000): loss= 849.502525407582\n",
      "Losgistic Regression(    4100/10000): loss= 849.291248362863\n",
      "Losgistic Regression(    4200/10000): loss= 849.108976194509\n",
      "Losgistic Regression(    4300/10000): loss= 848.924187098309\n",
      "Losgistic Regression(    4400/10000): loss= 848.744031502528\n",
      "Losgistic Regression(    4500/10000): loss= 848.582318728577\n",
      "Losgistic Regression(    4600/10000): loss= 848.432938965227\n",
      "Losgistic Regression(    4700/10000): loss= 848.284118802763\n",
      "Losgistic Regression(    4800/10000): loss= 848.12384152616\n",
      "Losgistic Regression(    4900/10000): loss= 847.951454809247\n",
      "Losgistic Regression(    5000/10000): loss= 847.766527287278\n",
      "Losgistic Regression(    5100/10000): loss= 847.602941783583\n",
      "Losgistic Regression(    5200/10000): loss= 847.478139496402\n",
      "Losgistic Regression(    5300/10000): loss= 847.366924284903\n",
      "Losgistic Regression(    5400/10000): loss= 847.269864938295\n",
      "Losgistic Regression(    5500/10000): loss= 847.173566441555\n",
      "Losgistic Regression(    5600/10000): loss= 847.067112408374\n",
      "Losgistic Regression(    5700/10000): loss= 846.977624545061\n",
      "Losgistic Regression(    5800/10000): loss= 846.846401751716\n",
      "Losgistic Regression(    5900/10000): loss= 846.776765749296\n",
      "Losgistic Regression(    6000/10000): loss= 846.72122325166\n",
      "Losgistic Regression(    6100/10000): loss= 846.648033528613\n",
      "Losgistic Regression(    6200/10000): loss= 846.580604901034\n",
      "Losgistic Regression(    6300/10000): loss= 846.4889267818\n",
      "Losgistic Regression(    6400/10000): loss= 846.404869301503\n",
      "Losgistic Regression(    6500/10000): loss= 846.33333957167\n",
      "Losgistic Regression(    6600/10000): loss= 846.229883297835\n",
      "Losgistic Regression(    6700/10000): loss= 846.127151192494\n",
      "Losgistic Regression(    6800/10000): loss= 846.050397921278\n",
      "Losgistic Regression(    6900/10000): loss= 846.008788734307\n",
      "Losgistic Regression(    7000/10000): loss= 845.962159507359\n",
      "Losgistic Regression(    7100/10000): loss= 845.904692576605\n",
      "Losgistic Regression(    7200/10000): loss= 845.825200511151\n",
      "Losgistic Regression(    7300/10000): loss= 845.75291845133\n",
      "Losgistic Regression(    7400/10000): loss= 845.715313705714\n",
      "Losgistic Regression(    7500/10000): loss= 845.674027170978\n",
      "Losgistic Regression(    7600/10000): loss= 845.607649291769\n",
      "Losgistic Regression(    7700/10000): loss= 845.554610054086\n",
      "Losgistic Regression(    7800/10000): loss= 845.483939272432\n",
      "Losgistic Regression(    7900/10000): loss= 845.412182243677\n",
      "Losgistic Regression(    8000/10000): loss= 845.356785011631\n",
      "Losgistic Regression(    8100/10000): loss= 845.29301814693\n",
      "Losgistic Regression(    8200/10000): loss= 845.252678740749\n",
      "Losgistic Regression(    8300/10000): loss= 845.212775983103\n",
      "Losgistic Regression(    8400/10000): loss= 845.165794533276\n",
      "Losgistic Regression(    8500/10000): loss= 845.120541045759\n",
      "Losgistic Regression(    8600/10000): loss= 845.058449422886\n",
      "Losgistic Regression(    8700/10000): loss= 844.997304833031\n",
      "Losgistic Regression(    8800/10000): loss= 844.946291399358\n",
      "Losgistic Regression(    8900/10000): loss= 844.911388057238\n",
      "Losgistic Regression(    9000/10000): loss= 844.895566064973\n",
      "Losgistic Regression(    9100/10000): loss= 844.88489881353\n",
      "Losgistic Regression(    9200/10000): loss= 844.874155909726\n",
      "Losgistic Regression(    9300/10000): loss= 844.863772335293\n",
      "Losgistic Regression(    9400/10000): loss= 844.854180172452\n",
      "Losgistic Regression(    9500/10000): loss= 844.845090856395\n",
      "Losgistic Regression(    9600/10000): loss= 844.83564315769\n",
      "Losgistic Regression(    9700/10000): loss= 844.826653376216\n",
      "Losgistic Regression(    9800/10000): loss= 844.82653061175\n",
      "Totoal number of iterations =  9800\n",
      "Loss                        =  844.826530612\n",
      "Time for  1th cross validation = 78.3034s\n",
      "Training Accuracy         = 0.8744\n",
      "Cross Validation Accuracy = 0.790588\n",
      "Losgistic Regression(       0/10000): loss= 1726.09052483154\n",
      "Losgistic Regression(     100/10000): loss= 1230.17853290651\n",
      "Losgistic Regression(     200/10000): loss= 1079.46241352269\n",
      "Losgistic Regression(     300/10000): loss= 1023.87739614833\n",
      "Losgistic Regression(     400/10000): loss= 989.770739586209\n",
      "Losgistic Regression(     500/10000): loss= 966.260797082002\n",
      "Losgistic Regression(     600/10000): loss= 949.046782260174\n",
      "Losgistic Regression(     700/10000): loss= 935.747269451854\n",
      "Losgistic Regression(     800/10000): loss= 925.170874115064\n",
      "Losgistic Regression(     900/10000): loss= 916.47906870137\n",
      "Losgistic Regression(    1000/10000): loss= 909.290542702364\n",
      "Losgistic Regression(    1100/10000): loss= 903.32219725506\n",
      "Losgistic Regression(    1200/10000): loss= 897.638111035003\n",
      "Losgistic Regression(    1300/10000): loss= 892.558177849086\n",
      "Losgistic Regression(    1400/10000): loss= 888.299129552412\n",
      "Losgistic Regression(    1500/10000): loss= 884.673327534616\n",
      "Losgistic Regression(    1600/10000): loss= 881.574059411803\n",
      "Losgistic Regression(    1700/10000): loss= 878.8702520501\n",
      "Losgistic Regression(    1800/10000): loss= 876.494655418192\n",
      "Losgistic Regression(    1900/10000): loss= 874.279485922418\n",
      "Losgistic Regression(    2000/10000): loss= 872.211698894544\n",
      "Losgistic Regression(    2100/10000): loss= 870.404349540091\n",
      "Losgistic Regression(    2200/10000): loss= 868.99811468842\n",
      "Losgistic Regression(    2300/10000): loss= 867.576691112221\n",
      "Losgistic Regression(    2400/10000): loss= 866.181931431772\n",
      "Losgistic Regression(    2500/10000): loss= 864.954260241231\n",
      "Losgistic Regression(    2600/10000): loss= 863.878019534462\n",
      "Losgistic Regression(    2700/10000): loss= 862.981413078676\n",
      "Losgistic Regression(    2800/10000): loss= 862.194445215007\n",
      "Losgistic Regression(    2900/10000): loss= 861.336664265266\n",
      "Losgistic Regression(    3000/10000): loss= 860.520793188511\n",
      "Losgistic Regression(    3100/10000): loss= 859.88520025161\n",
      "Losgistic Regression(    3200/10000): loss= 859.33118033325\n",
      "Losgistic Regression(    3300/10000): loss= 858.713016399685\n",
      "Losgistic Regression(    3400/10000): loss= 858.094418444004\n",
      "Losgistic Regression(    3500/10000): loss= 857.638204153728\n",
      "Losgistic Regression(    3600/10000): loss= 857.287490685144\n",
      "Losgistic Regression(    3700/10000): loss= 857.057666583443\n",
      "Losgistic Regression(    3800/10000): loss= 856.760738734405\n",
      "Losgistic Regression(    3900/10000): loss= 856.467970926657\n",
      "Losgistic Regression(    4000/10000): loss= 856.118684414767\n",
      "Losgistic Regression(    4100/10000): loss= 855.886880183221\n",
      "Losgistic Regression(    4200/10000): loss= 855.72132645316\n",
      "Losgistic Regression(    4300/10000): loss= 855.5405851771\n",
      "Losgistic Regression(    4400/10000): loss= 855.356356725239\n",
      "Losgistic Regression(    4500/10000): loss= 855.192128484225\n",
      "Losgistic Regression(    4600/10000): loss= 854.940516396178\n",
      "Losgistic Regression(    4700/10000): loss= 854.678631094709\n",
      "Losgistic Regression(    4800/10000): loss= 854.509237402067\n",
      "Losgistic Regression(    4900/10000): loss= 854.440778942948\n",
      "Losgistic Regression(    5000/10000): loss= 854.441891792977\n",
      "Totoal number of iterations =  5000\n",
      "Loss                        =  854.441891793\n",
      "Time for  2th cross validation = 40.0047s\n",
      "Training Accuracy         = 0.8712\n",
      "Cross Validation Accuracy = 0.7839\n",
      "Losgistic Regression(       0/10000): loss= 1725.46627475255\n",
      "Losgistic Regression(     100/10000): loss= 1255.70016441706\n",
      "Losgistic Regression(     200/10000): loss= 1122.72490942613\n",
      "Losgistic Regression(     300/10000): loss= 1067.82603800295\n",
      "Losgistic Regression(     400/10000): loss= 1033.00666618691\n",
      "Losgistic Regression(     500/10000): loss= 1008.03102590861\n",
      "Losgistic Regression(     600/10000): loss= 989.252395642177\n",
      "Losgistic Regression(     700/10000): loss= 974.432309179666\n",
      "Losgistic Regression(     800/10000): loss= 961.100906746629\n",
      "Losgistic Regression(     900/10000): loss= 947.533997988228\n",
      "Losgistic Regression(    1000/10000): loss= 933.290842763874\n",
      "Losgistic Regression(    1100/10000): loss= 920.654806579567\n",
      "Losgistic Regression(    1200/10000): loss= 908.99183551521\n",
      "Losgistic Regression(    1300/10000): loss= 899.339752556916\n",
      "Losgistic Regression(    1400/10000): loss= 891.907185104555\n",
      "Losgistic Regression(    1500/10000): loss= 885.614507573038\n",
      "Losgistic Regression(    1600/10000): loss= 879.863953299146\n",
      "Losgistic Regression(    1700/10000): loss= 874.99528192842\n",
      "Losgistic Regression(    1800/10000): loss= 870.721500948182\n",
      "Losgistic Regression(    1900/10000): loss= 866.530561935184\n",
      "Losgistic Regression(    2000/10000): loss= 862.608168447497\n",
      "Losgistic Regression(    2100/10000): loss= 859.470658582194\n",
      "Losgistic Regression(    2200/10000): loss= 857.208949805174\n",
      "Losgistic Regression(    2300/10000): loss= 855.417815854299\n",
      "Losgistic Regression(    2400/10000): loss= 854.055334559741\n",
      "Losgistic Regression(    2500/10000): loss= 852.950739205548\n",
      "Losgistic Regression(    2600/10000): loss= 852.005452164479\n",
      "Losgistic Regression(    2700/10000): loss= 851.53588893528\n",
      "Losgistic Regression(    2800/10000): loss= 851.487085906038\n",
      "Losgistic Regression(    2900/10000): loss= 851.486996175348\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  851.486996175\n",
      "Time for  3th cross validation = 23.2629s\n",
      "Training Accuracy         = 0.8552\n",
      "Cross Validation Accuracy = 0.78924\n",
      "Losgistic Regression(       0/10000): loss= 1726.06532334358\n",
      "Losgistic Regression(     100/10000): loss= 1236.14885347198\n",
      "Losgistic Regression(     200/10000): loss= 1086.5801853589\n",
      "Losgistic Regression(     300/10000): loss= 1030.2654719416\n",
      "Losgistic Regression(     400/10000): loss= 994.824931533699\n",
      "Losgistic Regression(     500/10000): loss= 969.101023431867\n",
      "Losgistic Regression(     600/10000): loss= 949.405988074478\n",
      "Losgistic Regression(     700/10000): loss= 933.941710028095\n",
      "Losgistic Regression(     800/10000): loss= 921.41189012914\n",
      "Losgistic Regression(     900/10000): loss= 910.579336005022\n",
      "Losgistic Regression(    1000/10000): loss= 900.061571843982\n",
      "Losgistic Regression(    1100/10000): loss= 890.707266236863\n",
      "Losgistic Regression(    1200/10000): loss= 883.107119247108\n",
      "Losgistic Regression(    1300/10000): loss= 876.84639460241\n",
      "Losgistic Regression(    1400/10000): loss= 871.695824753656\n",
      "Losgistic Regression(    1500/10000): loss= 867.577050020174\n",
      "Losgistic Regression(    1600/10000): loss= 864.344943559195\n",
      "Losgistic Regression(    1700/10000): loss= 861.648145090514\n",
      "Losgistic Regression(    1800/10000): loss= 859.449224666592\n",
      "Losgistic Regression(    1900/10000): loss= 857.747296051236\n",
      "Losgistic Regression(    2000/10000): loss= 856.32135535049\n",
      "Losgistic Regression(    2100/10000): loss= 854.866351556659\n",
      "Losgistic Regression(    2200/10000): loss= 853.331312493314\n",
      "Losgistic Regression(    2300/10000): loss= 851.964117566148\n",
      "Losgistic Regression(    2400/10000): loss= 850.772807654144\n",
      "Losgistic Regression(    2500/10000): loss= 849.2421703326\n",
      "Losgistic Regression(    2600/10000): loss= 847.636092064047\n",
      "Losgistic Regression(    2700/10000): loss= 846.401615367233\n",
      "Losgistic Regression(    2800/10000): loss= 845.626339509484\n",
      "Losgistic Regression(    2900/10000): loss= 845.199329078286\n",
      "Losgistic Regression(    3000/10000): loss= 844.915822552224\n",
      "Losgistic Regression(    3100/10000): loss= 844.62911036182\n",
      "Losgistic Regression(    3200/10000): loss= 844.176727467463\n",
      "Losgistic Regression(    3300/10000): loss= 843.464318537966\n",
      "Losgistic Regression(    3400/10000): loss= 842.501975936824\n",
      "Losgistic Regression(    3500/10000): loss= 841.56800908557\n",
      "Losgistic Regression(    3600/10000): loss= 840.9259654193\n",
      "Losgistic Regression(    3700/10000): loss= 840.137413758613\n",
      "Losgistic Regression(    3800/10000): loss= 839.23515521781\n",
      "Losgistic Regression(    3900/10000): loss= 838.313934710848\n",
      "Losgistic Regression(    4000/10000): loss= 837.891467545815\n",
      "Losgistic Regression(    4100/10000): loss= 837.553510044974\n",
      "Losgistic Regression(    4200/10000): loss= 837.161646115408\n",
      "Losgistic Regression(    4300/10000): loss= 837.160005597052\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  837.160005597\n",
      "Time for  4th cross validation = 34.4017s\n",
      "Training Accuracy         = 0.8736\n",
      "Cross Validation Accuracy = 0.793172\n",
      "*************** ([0.87080000000000002, 0.87439999999999996, 0.87119999999999997, 0.85519999999999996, 0.87360000000000004], [0.79713999999999996, 0.79058799999999996, 0.78390000000000004, 0.78924000000000005, 0.79317199999999999])\n",
      "Losgistic Regression(       0/10000): loss= 1725.86317676019\n",
      "Losgistic Regression(     100/10000): loss= 1258.02257105804\n",
      "Losgistic Regression(     200/10000): loss= 1117.77206862883\n",
      "Losgistic Regression(     300/10000): loss= 1075.038016174\n",
      "Losgistic Regression(     400/10000): loss= 1053.6243292897\n",
      "Losgistic Regression(     500/10000): loss= 1040.2974747196\n",
      "Losgistic Regression(     600/10000): loss= 1030.7659114854\n",
      "Losgistic Regression(     700/10000): loss= 1023.67930652328\n",
      "Losgistic Regression(     800/10000): loss= 1018.64084131147\n",
      "Losgistic Regression(     900/10000): loss= 1014.75117269134\n",
      "Losgistic Regression(    1000/10000): loss= 1011.75860918403\n",
      "Losgistic Regression(    1100/10000): loss= 1009.34233852828\n",
      "Losgistic Regression(    1200/10000): loss= 1007.1632734975\n",
      "Losgistic Regression(    1300/10000): loss= 1005.39643720941\n",
      "Losgistic Regression(    1400/10000): loss= 1003.82312079098\n",
      "Losgistic Regression(    1500/10000): loss= 1002.51478207188\n",
      "Losgistic Regression(    1600/10000): loss= 1001.40167789613\n",
      "Losgistic Regression(    1700/10000): loss= 1000.5771533867\n",
      "Losgistic Regression(    1800/10000): loss= 999.807863275301\n",
      "Losgistic Regression(    1900/10000): loss= 999.168470909723\n",
      "Losgistic Regression(    2000/10000): loss= 998.586987987782\n",
      "Losgistic Regression(    2100/10000): loss= 997.99090759315\n",
      "Losgistic Regression(    2200/10000): loss= 997.539722269344\n",
      "Losgistic Regression(    2300/10000): loss= 997.025887664533\n",
      "Losgistic Regression(    2400/10000): loss= 996.568920511781\n",
      "Losgistic Regression(    2500/10000): loss= 996.093643045032\n",
      "Losgistic Regression(    2600/10000): loss= 995.67385651661\n",
      "Losgistic Regression(    2700/10000): loss= 995.283219771023\n",
      "Losgistic Regression(    2800/10000): loss= 994.873573975429\n",
      "Losgistic Regression(    2900/10000): loss= 994.484556622437\n",
      "Losgistic Regression(    3000/10000): loss= 994.105696731707\n",
      "Losgistic Regression(    3100/10000): loss= 993.756561753915\n",
      "Losgistic Regression(    3200/10000): loss= 993.559530699958\n",
      "Losgistic Regression(    3300/10000): loss= 993.521789522697\n",
      "Losgistic Regression(    3400/10000): loss= 993.4881886941\n",
      "Losgistic Regression(    3500/10000): loss= 993.450024307966\n",
      "Losgistic Regression(    3600/10000): loss= 993.406662122956\n",
      "Losgistic Regression(    3700/10000): loss= 993.357361973532\n",
      "Losgistic Regression(    3800/10000): loss= 993.30149293945\n",
      "Losgistic Regression(    3900/10000): loss= 993.238688577667\n",
      "Losgistic Regression(    4000/10000): loss= 993.168951748443\n",
      "Losgistic Regression(    4100/10000): loss= 993.153746501529\n",
      "Losgistic Regression(    4200/10000): loss= 993.153338315805\n",
      "Totoal number of iterations =  4200\n",
      "Loss                        =  993.153338316\n",
      "Time for  0th cross validation = 33.6376s\n",
      "Training Accuracy         = 0.8444\n",
      "Cross Validation Accuracy = 0.794224\n",
      "Losgistic Regression(       0/10000): loss= 1724.91249263041\n",
      "Losgistic Regression(     100/10000): loss= 1243.42048104811\n",
      "Losgistic Regression(     200/10000): loss= 1103.61235727625\n",
      "Losgistic Regression(     300/10000): loss= 1056.08519612249\n",
      "Losgistic Regression(     400/10000): loss= 1033.18780270874\n",
      "Losgistic Regression(     500/10000): loss= 1018.9883386976\n",
      "Losgistic Regression(     600/10000): loss= 1008.46523038071\n",
      "Losgistic Regression(     700/10000): loss= 1000.37421711857\n",
      "Losgistic Regression(     800/10000): loss= 993.842039158496\n",
      "Losgistic Regression(     900/10000): loss= 988.619050645563\n",
      "Losgistic Regression(    1000/10000): loss= 984.504819529528\n",
      "Losgistic Regression(    1100/10000): loss= 981.237496050812\n",
      "Losgistic Regression(    1200/10000): loss= 978.499029790103\n",
      "Losgistic Regression(    1300/10000): loss= 976.055340245308\n",
      "Losgistic Regression(    1400/10000): loss= 974.022451099015\n",
      "Losgistic Regression(    1500/10000): loss= 972.396349205805\n",
      "Losgistic Regression(    1600/10000): loss= 971.332524907097\n",
      "Losgistic Regression(    1700/10000): loss= 970.074434168718\n",
      "Losgistic Regression(    1800/10000): loss= 969.069825944072\n",
      "Losgistic Regression(    1900/10000): loss= 968.576307217151\n",
      "Losgistic Regression(    2000/10000): loss= 968.072246740538\n",
      "Losgistic Regression(    2100/10000): loss= 967.566056548985\n",
      "Losgistic Regression(    2200/10000): loss= 967.106745717744\n",
      "Losgistic Regression(    2300/10000): loss= 966.81182001203\n",
      "Losgistic Regression(    2400/10000): loss= 966.445342906229\n",
      "Losgistic Regression(    2500/10000): loss= 966.143661172804\n",
      "Losgistic Regression(    2600/10000): loss= 965.866055916278\n",
      "Losgistic Regression(    2700/10000): loss= 965.634811352725\n",
      "Losgistic Regression(    2800/10000): loss= 965.382485486789\n",
      "Losgistic Regression(    2900/10000): loss= 965.170203831558\n",
      "Losgistic Regression(    3000/10000): loss= 965.097631490302\n",
      "Losgistic Regression(    3100/10000): loss= 965.034264558271\n",
      "Losgistic Regression(    3200/10000): loss= 964.993263152474\n",
      "Losgistic Regression(    3300/10000): loss= 964.954414797576\n",
      "Losgistic Regression(    3400/10000): loss= 964.915324912549\n",
      "Losgistic Regression(    3500/10000): loss= 964.877265306878\n",
      "Losgistic Regression(    3600/10000): loss= 964.836703711773\n",
      "Losgistic Regression(    3700/10000): loss= 964.792989737443\n",
      "Losgistic Regression(    3800/10000): loss= 964.746634671798\n",
      "Losgistic Regression(    3900/10000): loss= 964.697285186836\n",
      "Losgistic Regression(    4000/10000): loss= 964.645025428096\n",
      "Losgistic Regression(    4100/10000): loss= 964.589884406431\n",
      "Losgistic Regression(    4200/10000): loss= 964.532195299117\n",
      "Losgistic Regression(    4300/10000): loss= 964.471838129107\n",
      "Losgistic Regression(    4400/10000): loss= 964.409136167059\n",
      "Losgistic Regression(    4500/10000): loss= 964.343828909403\n",
      "Losgistic Regression(    4600/10000): loss= 964.276749640352\n",
      "Losgistic Regression(    4700/10000): loss= 964.207326560787\n",
      "Losgistic Regression(    4800/10000): loss= 964.135809813043\n",
      "Losgistic Regression(    4900/10000): loss= 964.123827084957\n",
      "Losgistic Regression(    5000/10000): loss= 964.123643355598\n",
      "Totoal number of iterations =  5000\n",
      "Loss                        =  964.123643356\n",
      "Time for  1th cross validation = 39.897s\n",
      "Training Accuracy         = 0.8504\n",
      "Cross Validation Accuracy = 0.796292\n",
      "Losgistic Regression(       0/10000): loss= 1726.15357812805\n",
      "Losgistic Regression(     100/10000): loss= 1245.74267861601\n",
      "Losgistic Regression(     200/10000): loss= 1109.94751205473\n",
      "Losgistic Regression(     300/10000): loss= 1067.39702936261\n",
      "Losgistic Regression(     400/10000): loss= 1044.28776791808\n",
      "Losgistic Regression(     500/10000): loss= 1029.96614507008\n",
      "Losgistic Regression(     600/10000): loss= 1020.37981060765\n",
      "Losgistic Regression(     700/10000): loss= 1013.53967887292\n",
      "Losgistic Regression(     800/10000): loss= 1008.32959127879\n",
      "Losgistic Regression(     900/10000): loss= 1004.48166619655\n",
      "Losgistic Regression(    1000/10000): loss= 1001.73769513398\n",
      "Losgistic Regression(    1100/10000): loss= 999.330656891525\n",
      "Losgistic Regression(    1200/10000): loss= 997.496247140783\n",
      "Losgistic Regression(    1300/10000): loss= 996.088350172024\n",
      "Losgistic Regression(    1400/10000): loss= 994.745000396524\n",
      "Losgistic Regression(    1500/10000): loss= 993.783620025825\n",
      "Losgistic Regression(    1600/10000): loss= 992.859616345723\n",
      "Losgistic Regression(    1700/10000): loss= 992.369600259051\n",
      "Losgistic Regression(    1800/10000): loss= 991.898455605899\n",
      "Losgistic Regression(    1900/10000): loss= 991.453076429416\n",
      "Losgistic Regression(    2000/10000): loss= 991.004584194981\n",
      "Losgistic Regression(    2100/10000): loss= 990.684397984421\n",
      "Losgistic Regression(    2200/10000): loss= 990.406782346362\n",
      "Losgistic Regression(    2300/10000): loss= 990.17125827513\n",
      "Losgistic Regression(    2400/10000): loss= 990.060497777945\n",
      "Losgistic Regression(    2500/10000): loss= 989.945684138942\n",
      "Losgistic Regression(    2600/10000): loss= 989.893908246348\n",
      "Losgistic Regression(    2700/10000): loss= 989.849023091868\n",
      "Losgistic Regression(    2800/10000): loss= 989.802983802965\n",
      "Losgistic Regression(    2900/10000): loss= 989.756710379686\n",
      "Losgistic Regression(    3000/10000): loss= 989.705823325906\n",
      "Losgistic Regression(    3100/10000): loss= 989.653957977417\n",
      "Losgistic Regression(    3200/10000): loss= 989.602440847499\n",
      "Losgistic Regression(    3300/10000): loss= 989.547364274735\n",
      "Losgistic Regression(    3400/10000): loss= 989.491171556965\n",
      "Losgistic Regression(    3500/10000): loss= 989.436848712984\n",
      "Losgistic Regression(    3600/10000): loss= 989.379624897088\n",
      "Losgistic Regression(    3700/10000): loss= 989.322166252006\n",
      "Losgistic Regression(    3800/10000): loss= 989.26394974688\n",
      "Losgistic Regression(    3900/10000): loss= 989.204820861549\n",
      "Losgistic Regression(    4000/10000): loss= 989.144637644295\n",
      "Losgistic Regression(    4100/10000): loss= 989.08850723874\n",
      "Losgistic Regression(    4200/10000): loss= 989.031658839851\n",
      "Losgistic Regression(    4300/10000): loss= 988.973550779039\n",
      "Losgistic Regression(    4400/10000): loss= 988.915962832413\n",
      "Losgistic Regression(    4500/10000): loss= 988.858270930059\n",
      "Losgistic Regression(    4600/10000): loss= 988.80080837098\n",
      "Losgistic Regression(    4700/10000): loss= 988.744805453517\n",
      "Losgistic Regression(    4800/10000): loss= 988.687977106997\n",
      "Losgistic Regression(    4900/10000): loss= 988.630401449995\n",
      "Losgistic Regression(    5000/10000): loss= 988.573570869665\n",
      "Losgistic Regression(    5100/10000): loss= 988.515632917792\n",
      "Losgistic Regression(    5200/10000): loss= 988.458250522131\n",
      "Losgistic Regression(    5300/10000): loss= 988.400593612084\n",
      "Losgistic Regression(    5400/10000): loss= 988.342532072655\n",
      "Losgistic Regression(    5500/10000): loss= 988.28440116802\n",
      "Losgistic Regression(    5600/10000): loss= 988.266208572894\n",
      "Losgistic Regression(    5700/10000): loss= 988.266097121205\n",
      "Totoal number of iterations =  5700\n",
      "Loss                        =  988.266097121\n",
      "Time for  2th cross validation = 45.711s\n",
      "Training Accuracy         = 0.8528\n",
      "Cross Validation Accuracy = 0.788756\n",
      "Losgistic Regression(       0/10000): loss= 1725.53365425221\n",
      "Losgistic Regression(     100/10000): loss= 1270.85115702397\n",
      "Losgistic Regression(     200/10000): loss= 1152.17109455675\n",
      "Losgistic Regression(     300/10000): loss= 1109.64234637338\n",
      "Losgistic Regression(     400/10000): loss= 1085.90129204696\n",
      "Losgistic Regression(     500/10000): loss= 1070.18961089456\n",
      "Losgistic Regression(     600/10000): loss= 1058.91277760885\n",
      "Losgistic Regression(     700/10000): loss= 1050.70495120264\n",
      "Losgistic Regression(     800/10000): loss= 1044.40439225223\n",
      "Losgistic Regression(     900/10000): loss= 1039.58677465523\n",
      "Losgistic Regression(    1000/10000): loss= 1034.7547561204\n",
      "Losgistic Regression(    1100/10000): loss= 1028.65366301218\n",
      "Losgistic Regression(    1200/10000): loss= 1023.08781697131\n",
      "Losgistic Regression(    1300/10000): loss= 1020.08931893643\n",
      "Losgistic Regression(    1400/10000): loss= 1017.12168956449\n",
      "Losgistic Regression(    1500/10000): loss= 1014.83919446231\n",
      "Losgistic Regression(    1600/10000): loss= 1012.2691604068\n",
      "Losgistic Regression(    1700/10000): loss= 1010.74598005271\n",
      "Losgistic Regression(    1800/10000): loss= 1010.36392819695\n",
      "Losgistic Regression(    1900/10000): loss= 1010.28400609783\n",
      "Losgistic Regression(    2000/10000): loss= 1010.28119953622\n",
      "Totoal number of iterations =  2000\n",
      "Loss                        =  1010.28119954\n",
      "Time for  3th cross validation = 16.132s\n",
      "Training Accuracy         = 0.8368\n",
      "Cross Validation Accuracy = 0.79016\n",
      "Losgistic Regression(       0/10000): loss= 1726.12742700976\n",
      "Losgistic Regression(     100/10000): loss= 1250.22484354635\n",
      "Losgistic Regression(     200/10000): loss= 1115.54864069555\n",
      "Losgistic Regression(     300/10000): loss= 1072.03741827418\n",
      "Losgistic Regression(     400/10000): loss= 1047.98027980937\n",
      "Losgistic Regression(     500/10000): loss= 1031.59417785595\n",
      "Losgistic Regression(     600/10000): loss= 1020.07275943645\n",
      "Losgistic Regression(     700/10000): loss= 1011.38178238294\n",
      "Losgistic Regression(     800/10000): loss= 1004.68196240893\n",
      "Losgistic Regression(     900/10000): loss= 999.543475615582\n",
      "Losgistic Regression(    1000/10000): loss= 995.631900341518\n",
      "Losgistic Regression(    1100/10000): loss= 992.547603204356\n",
      "Losgistic Regression(    1200/10000): loss= 990.157988623512\n",
      "Losgistic Regression(    1300/10000): loss= 988.323005463385\n",
      "Losgistic Regression(    1400/10000): loss= 986.891564552197\n",
      "Losgistic Regression(    1500/10000): loss= 985.672595665704\n",
      "Losgistic Regression(    1600/10000): loss= 984.81617648566\n",
      "Losgistic Regression(    1700/10000): loss= 984.13852492944\n",
      "Losgistic Regression(    1800/10000): loss= 983.474282140323\n",
      "Losgistic Regression(    1900/10000): loss= 982.916792028621\n",
      "Losgistic Regression(    2000/10000): loss= 982.36457045209\n",
      "Losgistic Regression(    2100/10000): loss= 981.815363327131\n",
      "Losgistic Regression(    2200/10000): loss= 981.423499438799\n",
      "Losgistic Regression(    2300/10000): loss= 981.105347443756\n",
      "Losgistic Regression(    2400/10000): loss= 980.759767880127\n",
      "Losgistic Regression(    2500/10000): loss= 980.471481936473\n",
      "Losgistic Regression(    2600/10000): loss= 980.239006054775\n",
      "Losgistic Regression(    2700/10000): loss= 979.971188300447\n",
      "Losgistic Regression(    2800/10000): loss= 979.766631125158\n",
      "Losgistic Regression(    2900/10000): loss= 979.521860251717\n",
      "Losgistic Regression(    3000/10000): loss= 979.305274882403\n",
      "Losgistic Regression(    3100/10000): loss= 979.124520636221\n",
      "Losgistic Regression(    3200/10000): loss= 978.924639123091\n",
      "Losgistic Regression(    3300/10000): loss= 978.75773065549\n",
      "Losgistic Regression(    3400/10000): loss= 978.719211180597\n",
      "Losgistic Regression(    3500/10000): loss= 978.683561720698\n",
      "Losgistic Regression(    3600/10000): loss= 978.652757768462\n",
      "Losgistic Regression(    3700/10000): loss= 978.638263304874\n",
      "Losgistic Regression(    3800/10000): loss= 978.637912545913\n",
      "Totoal number of iterations =  3800\n",
      "Loss                        =  978.637912546\n",
      "Time for  4th cross validation = 30.4057s\n",
      "Training Accuracy         = 0.8516\n",
      "Cross Validation Accuracy = 0.797012\n",
      "*************** ([0.84440000000000004, 0.85040000000000004, 0.8528, 0.83679999999999999, 0.85160000000000002], [0.79422400000000004, 0.796292, 0.78875600000000001, 0.79015999999999997, 0.79701200000000005])\n",
      "Losgistic Regression(       0/10000): loss= 1726.08666456175\n",
      "Losgistic Regression(     100/10000): loss= 1300.36354249297\n",
      "Losgistic Regression(     200/10000): loss= 1193.96309696313\n",
      "Losgistic Regression(     300/10000): loss= 1170.89932138868\n",
      "Losgistic Regression(     400/10000): loss= 1161.43866121067\n",
      "Losgistic Regression(     500/10000): loss= 1155.69324715\n",
      "Losgistic Regression(     600/10000): loss= 1151.47518230298\n",
      "Losgistic Regression(     700/10000): loss= 1148.77003268003\n",
      "Losgistic Regression(     800/10000): loss= 1146.52796771531\n",
      "Losgistic Regression(     900/10000): loss= 1144.81985049757\n",
      "Losgistic Regression(    1000/10000): loss= 1143.71936119527\n",
      "Losgistic Regression(    1100/10000): loss= 1143.47525326723\n",
      "Losgistic Regression(    1200/10000): loss= 1143.29286327698\n",
      "Losgistic Regression(    1300/10000): loss= 1143.06063295362\n",
      "Losgistic Regression(    1400/10000): loss= 1142.78862026471\n",
      "Losgistic Regression(    1500/10000): loss= 1142.51627428924\n",
      "Losgistic Regression(    1600/10000): loss= 1142.2477815417\n",
      "Losgistic Regression(    1700/10000): loss= 1141.99400855458\n",
      "Losgistic Regression(    1800/10000): loss= 1141.73914454107\n",
      "Losgistic Regression(    1900/10000): loss= 1141.48351870918\n",
      "Losgistic Regression(    2000/10000): loss= 1141.23047346604\n",
      "Losgistic Regression(    2100/10000): loss= 1140.97048919962\n",
      "Losgistic Regression(    2200/10000): loss= 1140.71978445041\n",
      "Losgistic Regression(    2300/10000): loss= 1140.67507534059\n",
      "Losgistic Regression(    2400/10000): loss= 1140.67413089524\n",
      "Totoal number of iterations =  2400\n",
      "Loss                        =  1140.6741309\n",
      "Time for  0th cross validation = 19.3453s\n",
      "Training Accuracy         = 0.8216\n",
      "Cross Validation Accuracy = 0.788336\n",
      "Losgistic Regression(       0/10000): loss= 1725.156502464\n",
      "Losgistic Regression(     100/10000): loss= 1285.35489178162\n",
      "Losgistic Regression(     200/10000): loss= 1177.96082576228\n",
      "Losgistic Regression(     300/10000): loss= 1149.49045709893\n",
      "Losgistic Regression(     400/10000): loss= 1138.69844189958\n",
      "Losgistic Regression(     500/10000): loss= 1132.80886863098\n",
      "Losgistic Regression(     600/10000): loss= 1128.24425835467\n",
      "Losgistic Regression(     700/10000): loss= 1124.73181046334\n",
      "Losgistic Regression(     800/10000): loss= 1123.27928390893\n",
      "Losgistic Regression(     900/10000): loss= 1122.79140179055\n",
      "Losgistic Regression(    1000/10000): loss= 1122.29036288874\n",
      "Losgistic Regression(    1100/10000): loss= 1121.63389867757\n",
      "Losgistic Regression(    1200/10000): loss= 1120.87186278395\n",
      "Losgistic Regression(    1300/10000): loss= 1120.08258373612\n",
      "Losgistic Regression(    1400/10000): loss= 1119.29196963102\n",
      "Losgistic Regression(    1500/10000): loss= 1118.56015279564\n",
      "Losgistic Regression(    1600/10000): loss= 1117.82506287215\n",
      "Losgistic Regression(    1700/10000): loss= 1117.16483460254\n",
      "Losgistic Regression(    1800/10000): loss= 1116.65152178656\n",
      "Losgistic Regression(    1900/10000): loss= 1116.59782065764\n",
      "Losgistic Regression(    2000/10000): loss= 1116.59054899036\n",
      "Totoal number of iterations =  2000\n",
      "Loss                        =  1116.59054899\n",
      "Time for  1th cross validation = 16.1112s\n",
      "Training Accuracy         = 0.8196\n",
      "Cross Validation Accuracy = 0.787564\n",
      "Losgistic Regression(       0/10000): loss= 1726.37314245924\n",
      "Losgistic Regression(     100/10000): loss= 1291.5998449402\n",
      "Losgistic Regression(     200/10000): loss= 1187.67228034385\n",
      "Losgistic Regression(     300/10000): loss= 1165.9394083683\n",
      "Losgistic Regression(     400/10000): loss= 1156.1225559305\n",
      "Losgistic Regression(     500/10000): loss= 1149.87543554343\n",
      "Losgistic Regression(     600/10000): loss= 1145.29942075362\n",
      "Losgistic Regression(     700/10000): loss= 1141.80065853657\n",
      "Losgistic Regression(     800/10000): loss= 1139.6606556691\n",
      "Losgistic Regression(     900/10000): loss= 1137.84738671592\n",
      "Losgistic Regression(    1000/10000): loss= 1136.55680505432\n",
      "Losgistic Regression(    1100/10000): loss= 1135.88406109755\n",
      "Losgistic Regression(    1200/10000): loss= 1135.68556469792\n",
      "Losgistic Regression(    1300/10000): loss= 1135.48237833604\n",
      "Losgistic Regression(    1400/10000): loss= 1135.26271872986\n",
      "Losgistic Regression(    1500/10000): loss= 1135.00276326451\n",
      "Losgistic Regression(    1600/10000): loss= 1134.7361429932\n",
      "Losgistic Regression(    1700/10000): loss= 1134.45595753293\n",
      "Losgistic Regression(    1800/10000): loss= 1134.35442097325\n",
      "Losgistic Regression(    1900/10000): loss= 1134.3519344929\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  1134.35193449\n",
      "Time for  2th cross validation = 15.2619s\n",
      "Training Accuracy         = 0.8152\n",
      "Cross Validation Accuracy = 0.783424\n",
      "Losgistic Regression(       0/10000): loss= 1725.76828329777\n",
      "Losgistic Regression(     100/10000): loss= 1315.2366841062\n",
      "Losgistic Regression(     200/10000): loss= 1227.39578182893\n",
      "Losgistic Regression(     300/10000): loss= 1205.76459811404\n",
      "Losgistic Regression(     400/10000): loss= 1195.53591226958\n",
      "Losgistic Regression(     500/10000): loss= 1188.98824507913\n",
      "Losgistic Regression(     600/10000): loss= 1184.6220803083\n",
      "Losgistic Regression(     700/10000): loss= 1181.66840385826\n",
      "Losgistic Regression(     800/10000): loss= 1179.40226563972\n",
      "Losgistic Regression(     900/10000): loss= 1177.75521243068\n",
      "Losgistic Regression(    1000/10000): loss= 1176.54740106676\n",
      "Losgistic Regression(    1100/10000): loss= 1175.60565978357\n",
      "Losgistic Regression(    1200/10000): loss= 1174.86631306072\n",
      "Losgistic Regression(    1300/10000): loss= 1174.71688447973\n",
      "Losgistic Regression(    1400/10000): loss= 1174.57927336495\n",
      "Losgistic Regression(    1500/10000): loss= 1174.45770700792\n",
      "Losgistic Regression(    1600/10000): loss= 1174.33563108329\n",
      "Losgistic Regression(    1700/10000): loss= 1174.33392473444\n",
      "Totoal number of iterations =  1700\n",
      "Loss                        =  1174.33392473\n",
      "Time for  3th cross validation = 13.6464s\n",
      "Training Accuracy         = 0.8136\n",
      "Cross Validation Accuracy = 0.786216\n",
      "Losgistic Regression(       0/10000): loss= 1726.34368453606\n",
      "Losgistic Regression(     100/10000): loss= 1291.5794835707\n",
      "Losgistic Regression(     200/10000): loss= 1189.82428375296\n",
      "Losgistic Regression(     300/10000): loss= 1167.03830682009\n",
      "Losgistic Regression(     400/10000): loss= 1157.69353067344\n",
      "Losgistic Regression(     500/10000): loss= 1151.60536923104\n",
      "Losgistic Regression(     600/10000): loss= 1147.02268754233\n",
      "Losgistic Regression(     700/10000): loss= 1143.81266720475\n",
      "Losgistic Regression(     800/10000): loss= 1141.44562298156\n",
      "Losgistic Regression(     900/10000): loss= 1139.53843563392\n",
      "Losgistic Regression(    1000/10000): loss= 1138.22016368661\n",
      "Losgistic Regression(    1100/10000): loss= 1137.79689205482\n",
      "Losgistic Regression(    1200/10000): loss= 1137.63480609147\n",
      "Losgistic Regression(    1300/10000): loss= 1137.47087113593\n",
      "Losgistic Regression(    1400/10000): loss= 1137.2728586454\n",
      "Losgistic Regression(    1500/10000): loss= 1137.03849401344\n",
      "Losgistic Regression(    1600/10000): loss= 1136.77887862976\n",
      "Losgistic Regression(    1700/10000): loss= 1136.54330243629\n",
      "Losgistic Regression(    1800/10000): loss= 1136.32112238779\n",
      "Losgistic Regression(    1900/10000): loss= 1136.11795657918\n",
      "Losgistic Regression(    2000/10000): loss= 1135.9268204533\n",
      "Losgistic Regression(    2100/10000): loss= 1135.86037722756\n",
      "Losgistic Regression(    2200/10000): loss= 1135.85808332919\n",
      "Totoal number of iterations =  2200\n",
      "Loss                        =  1135.85808333\n",
      "Time for  4th cross validation = 17.8453s\n",
      "Training Accuracy         = 0.8192\n",
      "Cross Validation Accuracy = 0.791076\n",
      "*************** ([0.8216, 0.8196, 0.81520000000000004, 0.81359999999999999, 0.81920000000000004], [0.78833600000000004, 0.78756400000000004, 0.78342400000000001, 0.78621600000000003, 0.791076])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.88319999999999999,\n",
       "   0.88639999999999997,\n",
       "   0.88,\n",
       "   0.87439999999999996,\n",
       "   0.88680000000000003],\n",
       "  [0.78400800000000004,\n",
       "   0.77524000000000004,\n",
       "   0.76915599999999995,\n",
       "   0.78068000000000004,\n",
       "   0.77854400000000001]),\n",
       " ([0.88319999999999999,\n",
       "   0.88400000000000001,\n",
       "   0.88039999999999996,\n",
       "   0.87160000000000004,\n",
       "   0.88600000000000001],\n",
       "  [0.78554000000000002,\n",
       "   0.77680800000000005,\n",
       "   0.77092799999999995,\n",
       "   0.78194799999999998,\n",
       "   0.780192]),\n",
       " ([0.88160000000000005,\n",
       "   0.88200000000000001,\n",
       "   0.88080000000000003,\n",
       "   0.86960000000000004,\n",
       "   0.88360000000000005],\n",
       "  [0.78958799999999996,\n",
       "   0.78179600000000005,\n",
       "   0.77618799999999999,\n",
       "   0.78548799999999996,\n",
       "   0.78434400000000004]),\n",
       " ([0.87080000000000002,\n",
       "   0.87439999999999996,\n",
       "   0.87119999999999997,\n",
       "   0.85519999999999996,\n",
       "   0.87360000000000004],\n",
       "  [0.79713999999999996,\n",
       "   0.79058799999999996,\n",
       "   0.78390000000000004,\n",
       "   0.78924000000000005,\n",
       "   0.79317199999999999]),\n",
       " ([0.84440000000000004,\n",
       "   0.85040000000000004,\n",
       "   0.8528,\n",
       "   0.83679999999999999,\n",
       "   0.85160000000000002],\n",
       "  [0.79422400000000004,\n",
       "   0.796292,\n",
       "   0.78875600000000001,\n",
       "   0.79015999999999997,\n",
       "   0.79701200000000005]),\n",
       " ([0.8216,\n",
       "   0.8196,\n",
       "   0.81520000000000004,\n",
       "   0.81359999999999999,\n",
       "   0.81920000000000004],\n",
       "  [0.78833600000000004,\n",
       "   0.78756400000000004,\n",
       "   0.78342400000000001,\n",
       "   0.78621600000000003,\n",
       "   0.791076])]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_2500 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(2500, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_2500.append(tmp)\n",
    "\n",
    "accu_2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 692.02168650719\n",
      "Losgistic Regression(     100/10000): loss= 529.983357919194\n",
      "Losgistic Regression(     200/10000): loss= 448.861121341205\n",
      "Losgistic Regression(     300/10000): loss= 403.881813624913\n",
      "Losgistic Regression(     400/10000): loss= 377.543317717095\n",
      "Losgistic Regression(     500/10000): loss= 359.029957853866\n",
      "Losgistic Regression(     600/10000): loss= 344.698875916908\n",
      "Losgistic Regression(     700/10000): loss= 333.148166114654\n",
      "Losgistic Regression(     800/10000): loss= 323.582828803302\n",
      "Losgistic Regression(     900/10000): loss= 315.505541016704\n",
      "Losgistic Regression(    1000/10000): loss= 308.571558187071\n",
      "Losgistic Regression(    1100/10000): loss= 302.515268805259\n",
      "Losgistic Regression(    1200/10000): loss= 297.132600376463\n",
      "Losgistic Regression(    1300/10000): loss= 292.275267475195\n",
      "Losgistic Regression(    1400/10000): loss= 287.844959583579\n",
      "Losgistic Regression(    1500/10000): loss= 283.774537227454\n",
      "Losgistic Regression(    1600/10000): loss= 280.006580816188\n",
      "Losgistic Regression(    1700/10000): loss= 275.989327807963\n",
      "Losgistic Regression(    1800/10000): loss= 272.190270099556\n",
      "Losgistic Regression(    1900/10000): loss= 268.621496772294\n",
      "Losgistic Regression(    2000/10000): loss= 265.239433823676\n",
      "Losgistic Regression(    2100/10000): loss= 262.012677244256\n",
      "Losgistic Regression(    2200/10000): loss= 258.915592511772\n",
      "Losgistic Regression(    2300/10000): loss= 255.932411542768\n",
      "Losgistic Regression(    2400/10000): loss= 252.962996027102\n",
      "Losgistic Regression(    2500/10000): loss= 249.644591141931\n",
      "Losgistic Regression(    2600/10000): loss= 246.306020464386\n",
      "Losgistic Regression(    2700/10000): loss= 242.581430653251\n",
      "Losgistic Regression(    2800/10000): loss= 238.702869144437\n",
      "Losgistic Regression(    2900/10000): loss= 234.868692369182\n",
      "Losgistic Regression(    3000/10000): loss= 230.924592714222\n",
      "Losgistic Regression(    3100/10000): loss= 226.36702920405\n",
      "Losgistic Regression(    3200/10000): loss= 221.727199546061\n",
      "Losgistic Regression(    3300/10000): loss= 217.105509752377\n",
      "Losgistic Regression(    3400/10000): loss= 212.489563759754\n",
      "Losgistic Regression(    3500/10000): loss= 207.813650457642\n",
      "Losgistic Regression(    3600/10000): loss= 202.811302513616\n",
      "Losgistic Regression(    3700/10000): loss= 197.511851601435\n",
      "Losgistic Regression(    3800/10000): loss= 192.172114682069\n",
      "Losgistic Regression(    3900/10000): loss= 186.795460499802\n",
      "Losgistic Regression(    4000/10000): loss= 181.391946773122\n",
      "Losgistic Regression(    4100/10000): loss= 175.54059893143\n",
      "Losgistic Regression(    4200/10000): loss= 169.513667881115\n",
      "Losgistic Regression(    4300/10000): loss= 163.455159627514\n",
      "Losgistic Regression(    4400/10000): loss= 157.379861196324\n",
      "Losgistic Regression(    4500/10000): loss= 151.106228565337\n",
      "Losgistic Regression(    4600/10000): loss= 144.75000611563\n",
      "Losgistic Regression(    4700/10000): loss= 138.42062567377\n",
      "Losgistic Regression(    4800/10000): loss= 132.130567465499\n",
      "Losgistic Regression(    4900/10000): loss= 125.879388674214\n",
      "Losgistic Regression(    5000/10000): loss= 119.672038021833\n",
      "Losgistic Regression(    5100/10000): loss= 113.507238120256\n",
      "Losgistic Regression(    5200/10000): loss= 107.374817448803\n",
      "Losgistic Regression(    5300/10000): loss= 101.271251228069\n",
      "Losgistic Regression(    5400/10000): loss= 95.2032607124948\n",
      "Losgistic Regression(    5500/10000): loss= 89.1088436310155\n",
      "Losgistic Regression(    5600/10000): loss= 82.873955794874\n",
      "Losgistic Regression(    5700/10000): loss= 76.5893435160353\n",
      "Losgistic Regression(    5800/10000): loss= 70.1430522795845\n",
      "Losgistic Regression(    5900/10000): loss= 63.7457326088328\n",
      "Losgistic Regression(    6000/10000): loss= 57.4083464110788\n",
      "Losgistic Regression(    6100/10000): loss= 51.1304087661587\n",
      "Losgistic Regression(    6200/10000): loss= 44.9167266590826\n",
      "Losgistic Regression(    6300/10000): loss= 38.7646310913302\n",
      "Losgistic Regression(    6400/10000): loss= 32.672782369601\n",
      "Losgistic Regression(    6500/10000): loss= 26.4052657754953\n",
      "Losgistic Regression(    6600/10000): loss= 20.0308266497493\n",
      "Losgistic Regression(    6700/10000): loss= 13.7030416389331\n",
      "Losgistic Regression(    6800/10000): loss= 7.42029996295908\n",
      "Losgistic Regression(    6900/10000): loss= 1.18210401614384\n",
      "Losgistic Regression(    7000/10000): loss=-5.02578854886449\n",
      "Losgistic Regression(    7100/10000): loss=-11.1918808289585\n",
      "Losgistic Regression(    7200/10000): loss=-17.3337039010693\n",
      "Losgistic Regression(    7300/10000): loss=-23.4523806255212\n",
      "Losgistic Regression(    7400/10000): loss=-29.5517938663714\n",
      "Losgistic Regression(    7500/10000): loss=-35.6552079612132\n",
      "Losgistic Regression(    7600/10000): loss=-41.7655389594433\n",
      "Losgistic Regression(    7700/10000): loss=-47.8885514144733\n",
      "Losgistic Regression(    7800/10000): loss=-54.0259622685329\n",
      "Losgistic Regression(    7900/10000): loss=-60.1633281111363\n",
      "Losgistic Regression(    8000/10000): loss=-66.3061739859283\n",
      "Losgistic Regression(    8100/10000): loss=-72.4424297368805\n",
      "Losgistic Regression(    8200/10000): loss=-78.5714435452636\n",
      "Losgistic Regression(    8300/10000): loss=-84.7573699455431\n",
      "Losgistic Regression(    8400/10000): loss=-91.0069412616562\n",
      "Losgistic Regression(    8500/10000): loss=-97.2510331468751\n",
      "Losgistic Regression(    8600/10000): loss=-103.587924026993\n",
      "Losgistic Regression(    8700/10000): loss=-110.020279277412\n",
      "Losgistic Regression(    8800/10000): loss=-116.563184224181\n",
      "Losgistic Regression(    8900/10000): loss=-123.094694336708\n",
      "Losgistic Regression(    9000/10000): loss=-129.584132401583\n",
      "Losgistic Regression(    9100/10000): loss=-136.012926891661\n",
      "Losgistic Regression(    9200/10000): loss=-142.384678988016\n",
      "Losgistic Regression(    9300/10000): loss=-148.688395230924\n",
      "Losgistic Regression(    9400/10000): loss=-154.916758947018\n",
      "Losgistic Regression(    9500/10000): loss=-161.182768114046\n",
      "Losgistic Regression(    9600/10000): loss=-167.546638698547\n",
      "Losgistic Regression(    9700/10000): loss=-173.844528594598\n",
      "Losgistic Regression(    9800/10000): loss=-180.080098165705\n",
      "Losgistic Regression(    9900/10000): loss=-186.247528817362\n",
      "Time for  0th cross validation = 31.9072s\n",
      "Training Accuracy         =  0.944\n",
      "Cross Validation Accuracy = 0.735756\n",
      "Losgistic Regression(       0/10000): loss= 691.963949447064\n",
      "Losgistic Regression(     100/10000): loss= 517.968550182376\n",
      "Losgistic Regression(     200/10000): loss= 434.923460509036\n",
      "Losgistic Regression(     300/10000): loss= 387.214953061971\n",
      "Losgistic Regression(     400/10000): loss= 359.78430510325\n",
      "Losgistic Regression(     500/10000): loss= 341.053522030901\n",
      "Losgistic Regression(     600/10000): loss= 326.532391243624\n",
      "Losgistic Regression(     700/10000): loss= 314.640416278715\n",
      "Losgistic Regression(     800/10000): loss= 304.549436340835\n",
      "Losgistic Regression(     900/10000): loss= 295.735744260185\n",
      "Losgistic Regression(    1000/10000): loss= 287.82922444707\n",
      "Losgistic Regression(    1100/10000): loss= 279.367095212487\n",
      "Losgistic Regression(    1200/10000): loss= 270.696038198177\n",
      "Losgistic Regression(    1300/10000): loss= 262.762533575219\n",
      "Losgistic Regression(    1400/10000): loss= 255.533091251568\n",
      "Losgistic Regression(    1500/10000): loss= 248.957632383694\n",
      "Losgistic Regression(    1600/10000): loss= 242.946408430221\n",
      "Losgistic Regression(    1700/10000): loss= 237.199503509022\n",
      "Losgistic Regression(    1800/10000): loss= 231.701640487177\n",
      "Losgistic Regression(    1900/10000): loss= 226.416235551338\n",
      "Losgistic Regression(    2000/10000): loss= 221.247822723259\n",
      "Losgistic Regression(    2100/10000): loss= 216.094828876695\n",
      "Losgistic Regression(    2200/10000): loss= 210.54240496219\n",
      "Losgistic Regression(    2300/10000): loss= 205.070072415857\n",
      "Losgistic Regression(    2400/10000): loss= 199.696446373809\n",
      "Losgistic Regression(    2500/10000): loss= 194.436353720054\n",
      "Losgistic Regression(    2600/10000): loss= 189.309132312438\n",
      "Losgistic Regression(    2700/10000): loss= 184.3285630837\n",
      "Losgistic Regression(    2800/10000): loss= 179.505937817651\n",
      "Losgistic Regression(    2900/10000): loss= 174.843150456308\n",
      "Losgistic Regression(    3000/10000): loss= 170.333683622569\n",
      "Losgistic Regression(    3100/10000): loss= 165.561981808247\n",
      "Losgistic Regression(    3200/10000): loss= 160.622956523916\n",
      "Losgistic Regression(    3300/10000): loss= 155.842015820965\n",
      "Losgistic Regression(    3400/10000): loss= 151.215115497186\n",
      "Losgistic Regression(    3500/10000): loss= 146.73809027193\n",
      "Losgistic Regression(    3600/10000): loss= 142.404392975196\n",
      "Losgistic Regression(    3700/10000): loss= 138.208618840657\n",
      "Losgistic Regression(    3800/10000): loss= 134.017074618282\n",
      "Losgistic Regression(    3900/10000): loss= 129.685773674269\n",
      "Losgistic Regression(    4000/10000): loss= 125.458887143141\n",
      "Losgistic Regression(    4100/10000): loss= 121.316827092926\n",
      "Losgistic Regression(    4200/10000): loss= 117.084552548839\n",
      "Losgistic Regression(    4300/10000): loss= 112.774760148916\n",
      "Losgistic Regression(    4400/10000): loss= 108.532109297393\n",
      "Losgistic Regression(    4500/10000): loss= 104.188576933741\n",
      "Losgistic Regression(    4600/10000): loss= 99.3274613685628\n",
      "Losgistic Regression(    4700/10000): loss= 94.5142787749236\n",
      "Losgistic Regression(    4800/10000): loss= 89.7610609733463\n",
      "Losgistic Regression(    4900/10000): loss= 84.6504278508474\n",
      "Losgistic Regression(    5000/10000): loss= 79.573993851862\n",
      "Losgistic Regression(    5100/10000): loss= 74.5677771343127\n",
      "Losgistic Regression(    5200/10000): loss= 69.6454698250519\n",
      "Losgistic Regression(    5300/10000): loss= 64.8052193037033\n",
      "Losgistic Regression(    5400/10000): loss= 60.0411843607452\n",
      "Losgistic Regression(    5500/10000): loss= 55.2676176883622\n",
      "Losgistic Regression(    5600/10000): loss= 50.1734378769566\n",
      "Losgistic Regression(    5700/10000): loss= 45.1130813241505\n",
      "Losgistic Regression(    5800/10000): loss= 40.1206279894415\n",
      "Losgistic Regression(    5900/10000): loss= 35.2021617366469\n",
      "Losgistic Regression(    6000/10000): loss= 30.3153345785718\n",
      "Losgistic Regression(    6100/10000): loss= 25.4635229523115\n",
      "Losgistic Regression(    6200/10000): loss= 20.5668371080279\n",
      "Losgistic Regression(    6300/10000): loss= 15.567148163399\n",
      "Losgistic Regression(    6400/10000): loss= 10.4818893758031\n",
      "Losgistic Regression(    6500/10000): loss= 5.35771340821459\n",
      "Losgistic Regression(    6600/10000): loss= 0.123677031590135\n",
      "Losgistic Regression(    6700/10000): loss=-5.05610304987878\n",
      "Losgistic Regression(    6800/10000): loss=-10.3626562766438\n",
      "Losgistic Regression(    6900/10000): loss=-15.9137502563857\n",
      "Losgistic Regression(    7000/10000): loss=-21.5273276305919\n",
      "Losgistic Regression(    7100/10000): loss=-27.0880290656557\n",
      "Losgistic Regression(    7200/10000): loss=-32.6026185357839\n",
      "Losgistic Regression(    7300/10000): loss=-38.0780474321411\n",
      "Losgistic Regression(    7400/10000): loss=-43.5170668857869\n",
      "Losgistic Regression(    7500/10000): loss=-48.9154416429387\n",
      "Losgistic Regression(    7600/10000): loss=-54.2850754752649\n",
      "Losgistic Regression(    7700/10000): loss=-59.6403852606422\n",
      "Losgistic Regression(    7800/10000): loss=-64.979510003299\n",
      "Losgistic Regression(    7900/10000): loss=-70.2808930005456\n",
      "Losgistic Regression(    8000/10000): loss=-75.6313226398023\n",
      "Losgistic Regression(    8100/10000): loss=-81.0382055310375\n",
      "Losgistic Regression(    8200/10000): loss=-86.6734039041047\n",
      "Losgistic Regression(    8300/10000): loss=-92.3689410322817\n",
      "Losgistic Regression(    8400/10000): loss=-98.038391288342\n",
      "Losgistic Regression(    8500/10000): loss=-103.697631540933\n",
      "Losgistic Regression(    8600/10000): loss=-109.333194212235\n",
      "Losgistic Regression(    8700/10000): loss=-114.949835292882\n",
      "Losgistic Regression(    8800/10000): loss=-120.646278324041\n",
      "Losgistic Regression(    8900/10000): loss=-126.436689937832\n",
      "Losgistic Regression(    9000/10000): loss=-132.268429746651\n",
      "Losgistic Regression(    9100/10000): loss=-138.294412202538\n",
      "Losgistic Regression(    9200/10000): loss=-144.434199527754\n",
      "Losgistic Regression(    9300/10000): loss=-150.552831043578\n",
      "Losgistic Regression(    9400/10000): loss=-156.632479022463\n",
      "Losgistic Regression(    9500/10000): loss=-162.706451790387\n",
      "Losgistic Regression(    9600/10000): loss=-168.760016325898\n",
      "Losgistic Regression(    9700/10000): loss=-174.904605654788\n",
      "Losgistic Regression(    9800/10000): loss=-181.061729235232\n",
      "Losgistic Regression(    9900/10000): loss=-187.198530964329\n",
      "Time for  1th cross validation = 32.1332s\n",
      "Training Accuracy         =  0.948\n",
      "Cross Validation Accuracy = 0.759636\n",
      "Losgistic Regression(       0/10000): loss= 691.568934009718\n",
      "Losgistic Regression(     100/10000): loss= 521.234433411883\n",
      "Losgistic Regression(     200/10000): loss= 452.860911639271\n",
      "Losgistic Regression(     300/10000): loss= 413.415231912147\n",
      "Losgistic Regression(     400/10000): loss= 389.979885183556\n",
      "Losgistic Regression(     500/10000): loss= 373.849853800587\n",
      "Losgistic Regression(     600/10000): loss= 361.701303636928\n",
      "Losgistic Regression(     700/10000): loss= 352.130674344632\n",
      "Losgistic Regression(     800/10000): loss= 344.24174831467\n",
      "Losgistic Regression(     900/10000): loss= 337.401715722092\n",
      "Losgistic Regression(    1000/10000): loss= 331.263440043534\n",
      "Losgistic Regression(    1100/10000): loss= 325.62172724666\n",
      "Losgistic Regression(    1200/10000): loss= 319.160000084442\n",
      "Losgistic Regression(    1300/10000): loss= 312.680608150852\n",
      "Losgistic Regression(    1400/10000): loss= 306.600296246379\n",
      "Losgistic Regression(    1500/10000): loss= 300.862980335006\n",
      "Losgistic Regression(    1600/10000): loss= 295.401079812232\n",
      "Losgistic Regression(    1700/10000): loss= 290.177825819172\n",
      "Losgistic Regression(    1800/10000): loss= 284.495141563317\n",
      "Losgistic Regression(    1900/10000): loss= 278.476966073562\n",
      "Losgistic Regression(    2000/10000): loss= 272.641645361395\n",
      "Losgistic Regression(    2100/10000): loss= 267.072087640659\n",
      "Losgistic Regression(    2200/10000): loss= 261.793578500258\n",
      "Losgistic Regression(    2300/10000): loss= 256.781792580033\n",
      "Losgistic Regression(    2400/10000): loss= 251.962501723746\n",
      "Losgistic Regression(    2500/10000): loss= 247.288057849403\n",
      "Losgistic Regression(    2600/10000): loss= 241.856068761025\n",
      "Losgistic Regression(    2700/10000): loss= 235.984298754182\n",
      "Losgistic Regression(    2800/10000): loss= 230.282645835688\n",
      "Losgistic Regression(    2900/10000): loss= 224.732607159248\n",
      "Losgistic Regression(    3000/10000): loss= 218.296212644074\n",
      "Losgistic Regression(    3100/10000): loss= 211.359813951373\n",
      "Losgistic Regression(    3200/10000): loss= 204.297575994452\n",
      "Losgistic Regression(    3300/10000): loss= 197.351393009539\n",
      "Losgistic Regression(    3400/10000): loss= 190.544121176532\n",
      "Losgistic Regression(    3500/10000): loss= 183.875108812735\n",
      "Losgistic Regression(    3600/10000): loss= 177.358439825072\n",
      "Losgistic Regression(    3700/10000): loss= 171.017216619746\n",
      "Losgistic Regression(    3800/10000): loss= 164.491334987777\n",
      "Losgistic Regression(    3900/10000): loss= 157.993213374824\n",
      "Losgistic Regression(    4000/10000): loss= 151.603257233647\n",
      "Losgistic Regression(    4100/10000): loss= 145.300481067345\n",
      "Losgistic Regression(    4200/10000): loss= 139.077358737372\n",
      "Losgistic Regression(    4300/10000): loss= 132.673341401905\n",
      "Losgistic Regression(    4400/10000): loss= 126.2660967877\n",
      "Losgistic Regression(    4500/10000): loss= 119.893478277165\n",
      "Losgistic Regression(    4600/10000): loss= 113.549514352292\n",
      "Losgistic Regression(    4700/10000): loss= 107.244571502654\n",
      "Losgistic Regression(    4800/10000): loss= 100.968314633681\n",
      "Losgistic Regression(    4900/10000): loss= 94.7417614649726\n",
      "Losgistic Regression(    5000/10000): loss= 88.5500971095107\n",
      "Losgistic Regression(    5100/10000): loss= 82.2725448472615\n",
      "Losgistic Regression(    5200/10000): loss= 75.8189264139949\n",
      "Losgistic Regression(    5300/10000): loss= 69.4013394483292\n",
      "Losgistic Regression(    5400/10000): loss= 63.0352154689487\n",
      "Losgistic Regression(    5500/10000): loss= 56.6889964674364\n",
      "Losgistic Regression(    5600/10000): loss= 50.3704678466266\n",
      "Losgistic Regression(    5700/10000): loss= 44.0509306561198\n",
      "Losgistic Regression(    5800/10000): loss= 37.4237509674125\n",
      "Losgistic Regression(    5900/10000): loss= 30.834842680568\n",
      "Losgistic Regression(    6000/10000): loss= 24.2601082865889\n",
      "Losgistic Regression(    6100/10000): loss= 17.4201704583974\n",
      "Losgistic Regression(    6200/10000): loss= 10.2751553368905\n",
      "Losgistic Regression(    6300/10000): loss= 2.93411968120974\n",
      "Losgistic Regression(    6400/10000): loss=-4.40039268466127\n",
      "Losgistic Regression(    6500/10000): loss=-11.732086421107\n",
      "Losgistic Regression(    6600/10000): loss=-19.0366276710665\n",
      "Losgistic Regression(    6700/10000): loss=-26.3210486768152\n",
      "Losgistic Regression(    6800/10000): loss=-33.5783992071566\n",
      "Losgistic Regression(    6900/10000): loss=-40.8118638675456\n",
      "Losgistic Regression(    7000/10000): loss=-47.9997236129373\n",
      "Losgistic Regression(    7100/10000): loss=-55.1327660753311\n",
      "Losgistic Regression(    7200/10000): loss=-62.2205965571818\n",
      "Losgistic Regression(    7300/10000): loss=-69.2517158382717\n",
      "Losgistic Regression(    7400/10000): loss=-76.4047196179433\n",
      "Losgistic Regression(    7500/10000): loss=-83.6626895445614\n",
      "Losgistic Regression(    7600/10000): loss=-91.2892365587232\n",
      "Losgistic Regression(    7700/10000): loss=-98.8103431102695\n",
      "Losgistic Regression(    7800/10000): loss=-106.222701041419\n",
      "Losgistic Regression(    7900/10000): loss=-113.527682655896\n",
      "Losgistic Regression(    8000/10000): loss=-120.73781408205\n",
      "Losgistic Regression(    8100/10000): loss=-127.863710097674\n",
      "Losgistic Regression(    8200/10000): loss=-134.897837537204\n",
      "Losgistic Regression(    8300/10000): loss=-141.843033975683\n",
      "Losgistic Regression(    8400/10000): loss=-148.697445820218\n",
      "Losgistic Regression(    8500/10000): loss=-155.465992039125\n",
      "Losgistic Regression(    8600/10000): loss=-162.144743607242\n",
      "Losgistic Regression(    8700/10000): loss=-168.747108814487\n",
      "Losgistic Regression(    8800/10000): loss=-175.392849867172\n",
      "Losgistic Regression(    8900/10000): loss=-181.968330829302\n",
      "Losgistic Regression(    9000/10000): loss=-188.463580480216\n",
      "Losgistic Regression(    9100/10000): loss=-194.974665493555\n",
      "Losgistic Regression(    9200/10000): loss=-201.474588427671\n",
      "Losgistic Regression(    9300/10000): loss=-207.850814446346\n",
      "Losgistic Regression(    9400/10000): loss=-214.099542480876\n",
      "Losgistic Regression(    9500/10000): loss=-220.409860289028\n",
      "Losgistic Regression(    9600/10000): loss=-226.627850708227\n",
      "Losgistic Regression(    9700/10000): loss=-232.741214680408\n",
      "Losgistic Regression(    9800/10000): loss=-238.752644160606\n",
      "Losgistic Regression(    9900/10000): loss=-244.856219231888\n",
      "Time for  2th cross validation = 30.8741s\n",
      "Training Accuracy         =  0.931\n",
      "Cross Validation Accuracy = 0.737372\n",
      "Losgistic Regression(       0/10000): loss= 691.706602046285\n",
      "Losgistic Regression(     100/10000): loss= 511.989101618138\n",
      "Losgistic Regression(     200/10000): loss= 435.664603756968\n",
      "Losgistic Regression(     300/10000): loss= 392.564509424881\n",
      "Losgistic Regression(     400/10000): loss= 367.366572253557\n",
      "Losgistic Regression(     500/10000): loss= 349.418285911519\n",
      "Losgistic Regression(     600/10000): loss= 335.299997305472\n",
      "Losgistic Regression(     700/10000): loss= 322.962956859441\n",
      "Losgistic Regression(     800/10000): loss= 311.521044554984\n",
      "Losgistic Regression(     900/10000): loss= 301.385324801036\n",
      "Losgistic Regression(    1000/10000): loss= 292.203250227493\n",
      "Losgistic Regression(    1100/10000): loss= 283.773640009822\n",
      "Losgistic Regression(    1200/10000): loss= 275.407968104876\n",
      "Losgistic Regression(    1300/10000): loss= 266.447105744959\n",
      "Losgistic Regression(    1400/10000): loss= 257.91022515175\n",
      "Losgistic Regression(    1500/10000): loss= 249.777586387321\n",
      "Losgistic Regression(    1600/10000): loss= 242.039471607936\n",
      "Losgistic Regression(    1700/10000): loss= 234.06351987938\n",
      "Losgistic Regression(    1800/10000): loss= 226.099423405021\n",
      "Losgistic Regression(    1900/10000): loss= 217.685366318731\n",
      "Losgistic Regression(    2000/10000): loss= 209.001477663121\n",
      "Losgistic Regression(    2100/10000): loss= 199.5921308723\n",
      "Losgistic Regression(    2200/10000): loss= 189.778314598906\n",
      "Losgistic Regression(    2300/10000): loss= 180.221603539512\n",
      "Losgistic Regression(    2400/10000): loss= 170.069118437094\n",
      "Losgistic Regression(    2500/10000): loss= 160.174328381865\n",
      "Losgistic Regression(    2600/10000): loss= 150.55518678563\n",
      "Losgistic Regression(    2700/10000): loss= 141.211084357837\n",
      "Losgistic Regression(    2800/10000): loss= 132.139132433744\n",
      "Losgistic Regression(    2900/10000): loss= 123.032457867158\n",
      "Losgistic Regression(    3000/10000): loss= 113.904104066407\n",
      "Losgistic Regression(    3100/10000): loss= 104.546561291968\n",
      "Losgistic Regression(    3200/10000): loss= 95.3378497419556\n",
      "Losgistic Regression(    3300/10000): loss= 86.0815186405069\n",
      "Losgistic Regression(    3400/10000): loss= 77.025373670946\n",
      "Losgistic Regression(    3500/10000): loss= 68.15998310212\n",
      "Losgistic Regression(    3600/10000): loss= 58.994562838851\n",
      "Losgistic Regression(    3700/10000): loss= 49.9104625053748\n",
      "Losgistic Regression(    3800/10000): loss= 40.9295791756937\n",
      "Losgistic Regression(    3900/10000): loss= 32.0294511475104\n",
      "Losgistic Regression(    4000/10000): loss= 23.2042408946317\n",
      "Losgistic Regression(    4100/10000): loss= 14.4540457558822\n",
      "Losgistic Regression(    4200/10000): loss= 5.72978141188857\n",
      "Losgistic Regression(    4300/10000): loss=-3.13729168771041\n",
      "Losgistic Regression(    4400/10000): loss=-11.9101618388579\n",
      "Losgistic Regression(    4500/10000): loss=-20.5784965855077\n",
      "Losgistic Regression(    4600/10000): loss=-29.1864201072928\n",
      "Losgistic Regression(    4700/10000): loss=-37.8209812536444\n",
      "Losgistic Regression(    4800/10000): loss=-46.3768871790296\n",
      "Losgistic Regression(    4900/10000): loss=-55.1867225158961\n",
      "Losgistic Regression(    5000/10000): loss=-64.6100004770298\n",
      "Losgistic Regression(    5100/10000): loss=-74.0754436025194\n",
      "Losgistic Regression(    5200/10000): loss=-83.5291995515956\n",
      "Losgistic Regression(    5300/10000): loss=-93.1546915795169\n",
      "Losgistic Regression(    5400/10000): loss=-102.950407610099\n",
      "Losgistic Regression(    5500/10000): loss=-112.80602792232\n",
      "Losgistic Regression(    5600/10000): loss=-122.779695387036\n",
      "Losgistic Regression(    5700/10000): loss=-133.103839841931\n",
      "Losgistic Regression(    5800/10000): loss=-143.635264611982\n",
      "Losgistic Regression(    5900/10000): loss=-154.276475667387\n",
      "Losgistic Regression(    6000/10000): loss=-165.495694861715\n",
      "Losgistic Regression(    6100/10000): loss=-177.328597577286\n",
      "Losgistic Regression(    6200/10000): loss=-189.182948083955\n",
      "Losgistic Regression(    6300/10000): loss=-201.057522977245\n",
      "Losgistic Regression(    6400/10000): loss=-212.95343126354\n",
      "Losgistic Regression(    6500/10000): loss=-224.829170110516\n",
      "Losgistic Regression(    6600/10000): loss=-236.681639993827\n",
      "Losgistic Regression(    6700/10000): loss=-248.502521594735\n",
      "Losgistic Regression(    6800/10000): loss=-260.315091338845\n",
      "Losgistic Regression(    6900/10000): loss=-272.546940274116\n",
      "Losgistic Regression(    7000/10000): loss=-284.876277526292\n",
      "Losgistic Regression(    7100/10000): loss=-297.150727717941\n",
      "Losgistic Regression(    7200/10000): loss=-309.365949575243\n",
      "Losgistic Regression(    7300/10000): loss=-321.520110628482\n",
      "Losgistic Regression(    7400/10000): loss=-333.605927111525\n",
      "Losgistic Regression(    7500/10000): loss=-345.641707069602\n",
      "Losgistic Regression(    7600/10000): loss=-357.617362946598\n",
      "Losgistic Regression(    7700/10000): loss=-369.578324380271\n",
      "Losgistic Regression(    7800/10000): loss=-381.606101702755\n",
      "Losgistic Regression(    7900/10000): loss=-393.619486455251\n",
      "Losgistic Regression(    8000/10000): loss=-405.775732972169\n",
      "Losgistic Regression(    8100/10000): loss=-417.94576656274\n",
      "Losgistic Regression(    8200/10000): loss=-430.246564310129\n",
      "Losgistic Regression(    8300/10000): loss=-442.499733988645\n",
      "Losgistic Regression(    8400/10000): loss=-454.70519892623\n",
      "Losgistic Regression(    8500/10000): loss=-466.842153545102\n",
      "Losgistic Regression(    8600/10000): loss=-479.392670416173\n",
      "Losgistic Regression(    8700/10000): loss=-492.075032002005\n",
      "Losgistic Regression(    8800/10000): loss=-504.673474305194\n",
      "Losgistic Regression(    8900/10000): loss=-517.349795051556\n",
      "Losgistic Regression(    9000/10000): loss=-530.01134306882\n",
      "Losgistic Regression(    9100/10000): loss=-542.584176050878\n",
      "Losgistic Regression(    9200/10000): loss=-555.100070994685\n",
      "Losgistic Regression(    9300/10000): loss=-567.758210788586\n",
      "Losgistic Regression(    9400/10000): loss=-580.481819469542\n",
      "Losgistic Regression(    9500/10000): loss=-593.179432290626\n",
      "Losgistic Regression(    9600/10000): loss=-605.946879515617\n",
      "Losgistic Regression(    9700/10000): loss=-618.714380389586\n",
      "Losgistic Regression(    9800/10000): loss=-631.398970864512\n",
      "Losgistic Regression(    9900/10000): loss=-643.965893957604\n",
      "Time for  3th cross validation = 31.3074s\n",
      "Training Accuracy         =  0.942\n",
      "Cross Validation Accuracy = 0.737844\n",
      "Losgistic Regression(       0/10000): loss= 692.020799933278\n",
      "Losgistic Regression(     100/10000): loss= 517.759436318264\n",
      "Losgistic Regression(     200/10000): loss= 441.623575780799\n",
      "Losgistic Regression(     300/10000): loss= 399.307506555391\n",
      "Losgistic Regression(     400/10000): loss= 373.825268120459\n",
      "Losgistic Regression(     500/10000): loss= 355.773160833067\n",
      "Losgistic Regression(     600/10000): loss= 341.499587546336\n",
      "Losgistic Regression(     700/10000): loss= 329.500248684929\n",
      "Losgistic Regression(     800/10000): loss= 319.112279083389\n",
      "Losgistic Regression(     900/10000): loss= 309.93773920608\n",
      "Losgistic Regression(    1000/10000): loss= 301.708004128044\n",
      "Losgistic Regression(    1100/10000): loss= 294.26032639591\n",
      "Losgistic Regression(    1200/10000): loss= 287.476514240814\n",
      "Losgistic Regression(    1300/10000): loss= 281.264004504301\n",
      "Losgistic Regression(    1400/10000): loss= 275.548104219973\n",
      "Losgistic Regression(    1500/10000): loss= 270.265429832213\n",
      "Losgistic Regression(    1600/10000): loss= 265.35870649676\n",
      "Losgistic Regression(    1700/10000): loss= 260.782534296528\n",
      "Losgistic Regression(    1800/10000): loss= 255.698391269885\n",
      "Losgistic Regression(    1900/10000): loss= 250.51922674735\n",
      "Losgistic Regression(    2000/10000): loss= 245.565367221779\n",
      "Losgistic Regression(    2100/10000): loss= 240.816156149595\n",
      "Losgistic Regression(    2200/10000): loss= 236.25334282288\n",
      "Losgistic Regression(    2300/10000): loss= 231.864530960423\n",
      "Losgistic Regression(    2400/10000): loss= 227.6402900715\n",
      "Losgistic Regression(    2500/10000): loss= 223.567473003971\n",
      "Losgistic Regression(    2600/10000): loss= 219.63637227722\n",
      "Losgistic Regression(    2700/10000): loss= 215.836874822491\n",
      "Losgistic Regression(    2800/10000): loss= 212.155632313583\n",
      "Losgistic Regression(    2900/10000): loss= 208.081529172718\n",
      "Losgistic Regression(    3000/10000): loss= 203.848478104658\n",
      "Losgistic Regression(    3100/10000): loss= 199.712011408509\n",
      "Losgistic Regression(    3200/10000): loss= 195.672284382328\n",
      "Losgistic Regression(    3300/10000): loss= 191.731825372637\n",
      "Losgistic Regression(    3400/10000): loss= 187.730275397642\n",
      "Losgistic Regression(    3500/10000): loss= 183.411731682293\n",
      "Losgistic Regression(    3600/10000): loss= 179.086105783925\n",
      "Losgistic Regression(    3700/10000): loss= 174.490056067193\n",
      "Losgistic Regression(    3800/10000): loss= 170.034108622372\n",
      "Losgistic Regression(    3900/10000): loss= 165.463037896993\n",
      "Losgistic Regression(    4000/10000): loss= 160.729895854418\n",
      "Losgistic Regression(    4100/10000): loss= 155.807249918938\n",
      "Losgistic Regression(    4200/10000): loss= 151.030679763374\n",
      "Losgistic Regression(    4300/10000): loss= 146.395013475083\n",
      "Losgistic Regression(    4400/10000): loss= 141.554261282597\n",
      "Losgistic Regression(    4500/10000): loss= 136.78442325225\n",
      "Losgistic Regression(    4600/10000): loss= 132.127887172686\n",
      "Losgistic Regression(    4700/10000): loss= 127.283884490667\n",
      "Losgistic Regression(    4800/10000): loss= 122.322012896315\n",
      "Losgistic Regression(    4900/10000): loss= 117.422163530935\n",
      "Losgistic Regression(    5000/10000): loss= 112.618372722061\n",
      "Losgistic Regression(    5100/10000): loss= 107.900096319167\n",
      "Losgistic Regression(    5200/10000): loss= 103.272703953189\n",
      "Losgistic Regression(    5300/10000): loss= 98.718211896044\n",
      "Losgistic Regression(    5400/10000): loss= 94.2456691651739\n",
      "Losgistic Regression(    5500/10000): loss= 89.8489984072989\n",
      "Losgistic Regression(    5600/10000): loss= 85.5226332593926\n",
      "Losgistic Regression(    5700/10000): loss= 81.2604943761055\n",
      "Losgistic Regression(    5800/10000): loss= 77.0663459750414\n",
      "Losgistic Regression(    5900/10000): loss= 72.8452136898326\n",
      "Losgistic Regression(    6000/10000): loss= 68.5200723492377\n",
      "Losgistic Regression(    6100/10000): loss= 63.9497032868658\n",
      "Losgistic Regression(    6200/10000): loss= 59.4349479912077\n",
      "Losgistic Regression(    6300/10000): loss= 54.9717032706017\n",
      "Losgistic Regression(    6400/10000): loss= 50.553247476818\n",
      "Losgistic Regression(    6500/10000): loss= 46.1617818506773\n",
      "Losgistic Regression(    6600/10000): loss= 41.6697063828061\n",
      "Losgistic Regression(    6700/10000): loss= 37.1501341144839\n",
      "Losgistic Regression(    6800/10000): loss= 32.518108088059\n",
      "Losgistic Regression(    6900/10000): loss= 27.9118437967829\n",
      "Losgistic Regression(    7000/10000): loss= 23.3143967884694\n",
      "Losgistic Regression(    7100/10000): loss= 18.7387733431529\n",
      "Losgistic Regression(    7200/10000): loss= 14.1775389210089\n",
      "Losgistic Regression(    7300/10000): loss= 9.61013778588574\n",
      "Losgistic Regression(    7400/10000): loss= 5.04263557836655\n",
      "Losgistic Regression(    7500/10000): loss= 0.477912912925468\n",
      "Losgistic Regression(    7600/10000): loss=-4.08533846676874\n",
      "Losgistic Regression(    7700/10000): loss=-8.63923562410226\n",
      "Losgistic Regression(    7800/10000): loss=-13.1822208372438\n",
      "Losgistic Regression(    7900/10000): loss=-17.720340816035\n",
      "Losgistic Regression(    8000/10000): loss=-22.4409610909979\n",
      "Losgistic Regression(    8100/10000): loss=-27.1416248585494\n",
      "Losgistic Regression(    8200/10000): loss=-31.8189864273447\n",
      "Losgistic Regression(    8300/10000): loss=-36.4778236575031\n",
      "Losgistic Regression(    8400/10000): loss=-41.1191903916971\n",
      "Losgistic Regression(    8500/10000): loss=-45.7313134077689\n",
      "Losgistic Regression(    8600/10000): loss=-50.3280348501112\n",
      "Losgistic Regression(    8700/10000): loss=-54.9182688556143\n",
      "Losgistic Regression(    8800/10000): loss=-59.4913670742085\n",
      "Losgistic Regression(    8900/10000): loss=-64.140508283878\n",
      "Losgistic Regression(    9000/10000): loss=-68.8492483544749\n",
      "Losgistic Regression(    9100/10000): loss=-73.5486708403506\n",
      "Losgistic Regression(    9200/10000): loss=-78.2443379492713\n",
      "Losgistic Regression(    9300/10000): loss=-82.9392222354206\n",
      "Losgistic Regression(    9400/10000): loss=-87.6229434900284\n",
      "Losgistic Regression(    9500/10000): loss=-92.3139732823086\n",
      "Losgistic Regression(    9600/10000): loss=-97.004388452147\n",
      "Losgistic Regression(    9700/10000): loss=-101.688271868915\n",
      "Losgistic Regression(    9800/10000): loss=-106.376570590354\n",
      "Losgistic Regression(    9900/10000): loss=-111.063812288959\n",
      "Time for  4th cross validation = 31.0481s\n",
      "Training Accuracy         =  0.946\n",
      "Cross Validation Accuracy = 0.739404\n",
      "*************** ([0.94399999999999995, 0.94799999999999995, 0.93100000000000005, 0.94199999999999995, 0.94599999999999995], [0.73575599999999997, 0.75963599999999998, 0.73737200000000003, 0.73784400000000006, 0.73940399999999995])\n",
      "Losgistic Regression(       0/10000): loss= 692.022322883668\n",
      "Losgistic Regression(     100/10000): loss= 530.244082890699\n",
      "Losgistic Regression(     200/10000): loss= 449.459374407816\n",
      "Losgistic Regression(     300/10000): loss= 404.80551367435\n",
      "Losgistic Regression(     400/10000): loss= 378.77349817842\n",
      "Losgistic Regression(     500/10000): loss= 360.557728990852\n",
      "Losgistic Regression(     600/10000): loss= 346.518519028887\n",
      "Losgistic Regression(     700/10000): loss= 335.251161170844\n",
      "Losgistic Regression(     800/10000): loss= 325.960912783976\n",
      "Losgistic Regression(     900/10000): loss= 318.153128502817\n",
      "Losgistic Regression(    1000/10000): loss= 311.480109734271\n",
      "Losgistic Regression(    1100/10000): loss= 305.675622954445\n",
      "Losgistic Regression(    1200/10000): loss= 300.54030415154\n",
      "Losgistic Regression(    1300/10000): loss= 295.924042109435\n",
      "Losgistic Regression(    1400/10000): loss= 291.73453824578\n",
      "Losgistic Regression(    1500/10000): loss= 287.906262638902\n",
      "Losgistic Regression(    1600/10000): loss= 284.378632970203\n",
      "Losgistic Regression(    1700/10000): loss= 280.984090180399\n",
      "Losgistic Regression(    1800/10000): loss= 277.445574650485\n",
      "Losgistic Regression(    1900/10000): loss= 274.130950041555\n",
      "Losgistic Regression(    2000/10000): loss= 271.000413854891\n",
      "Losgistic Regression(    2100/10000): loss= 268.0202737057\n",
      "Losgistic Regression(    2200/10000): loss= 265.176479053767\n",
      "Losgistic Regression(    2300/10000): loss= 262.443350275484\n",
      "Losgistic Regression(    2400/10000): loss= 259.815747591176\n",
      "Losgistic Regression(    2500/10000): loss= 257.06130955293\n",
      "Losgistic Regression(    2600/10000): loss= 254.073646427782\n",
      "Losgistic Regression(    2700/10000): loss= 251.134255767641\n",
      "Losgistic Regression(    2800/10000): loss= 248.196409082953\n",
      "Losgistic Regression(    2900/10000): loss= 244.944680943353\n",
      "Losgistic Regression(    3000/10000): loss= 241.564149342176\n",
      "Losgistic Regression(    3100/10000): loss= 238.209245275716\n",
      "Losgistic Regression(    3200/10000): loss= 234.485696094175\n",
      "Losgistic Regression(    3300/10000): loss= 230.48316988516\n",
      "Losgistic Regression(    3400/10000): loss= 226.418698868813\n",
      "Losgistic Regression(    3500/10000): loss= 222.325122344581\n",
      "Losgistic Regression(    3600/10000): loss= 218.216799448239\n",
      "Losgistic Regression(    3700/10000): loss= 214.067072957759\n",
      "Losgistic Regression(    3800/10000): loss= 209.598274529081\n",
      "Losgistic Regression(    3900/10000): loss= 204.875520454147\n",
      "Losgistic Regression(    4000/10000): loss= 200.098341197251\n",
      "Losgistic Regression(    4100/10000): loss= 195.312343746621\n",
      "Losgistic Regression(    4200/10000): loss= 190.524155134631\n",
      "Losgistic Regression(    4300/10000): loss= 185.190001104465\n",
      "Losgistic Regression(    4400/10000): loss= 179.825582412921\n",
      "Losgistic Regression(    4500/10000): loss= 174.493894788774\n",
      "Losgistic Regression(    4600/10000): loss= 169.169377202414\n",
      "Losgistic Regression(    4700/10000): loss= 163.820035837116\n",
      "Losgistic Regression(    4800/10000): loss= 158.324307514284\n",
      "Losgistic Regression(    4900/10000): loss= 152.903269456376\n",
      "Losgistic Regression(    5000/10000): loss= 147.48275796013\n",
      "Losgistic Regression(    5100/10000): loss= 142.122784178579\n",
      "Losgistic Regression(    5200/10000): loss= 136.807151973802\n",
      "Losgistic Regression(    5300/10000): loss= 131.525924616948\n",
      "Losgistic Regression(    5400/10000): loss= 126.307272400329\n",
      "Losgistic Regression(    5500/10000): loss= 121.133144756459\n",
      "Losgistic Regression(    5600/10000): loss= 116.035343117829\n",
      "Losgistic Regression(    5700/10000): loss= 110.98143521004\n",
      "Losgistic Regression(    5800/10000): loss= 105.974453775793\n",
      "Losgistic Regression(    5900/10000): loss= 101.020225520188\n",
      "Losgistic Regression(    6000/10000): loss= 96.0699590720559\n",
      "Losgistic Regression(    6100/10000): loss= 91.0812180717309\n",
      "Losgistic Regression(    6200/10000): loss= 86.1988764901067\n",
      "Losgistic Regression(    6300/10000): loss= 81.253898973415\n",
      "Losgistic Regression(    6400/10000): loss= 76.2925756289342\n",
      "Losgistic Regression(    6500/10000): loss= 71.4594295874204\n",
      "Losgistic Regression(    6600/10000): loss= 66.7164513339284\n",
      "Losgistic Regression(    6700/10000): loss= 62.003643159503\n",
      "Losgistic Regression(    6800/10000): loss= 57.118645988151\n",
      "Losgistic Regression(    6900/10000): loss= 52.1714657745526\n",
      "Losgistic Regression(    7000/10000): loss= 47.2570627112097\n",
      "Losgistic Regression(    7100/10000): loss= 42.3987244083946\n",
      "Losgistic Regression(    7200/10000): loss= 37.5072591563649\n",
      "Losgistic Regression(    7300/10000): loss= 32.6423773999487\n",
      "Losgistic Regression(    7400/10000): loss= 27.7642171967972\n",
      "Losgistic Regression(    7500/10000): loss= 22.8720427498964\n",
      "Losgistic Regression(    7600/10000): loss= 18.054654996706\n",
      "Losgistic Regression(    7700/10000): loss= 13.2269448327886\n",
      "Losgistic Regression(    7800/10000): loss= 8.35370474154358\n",
      "Losgistic Regression(    7900/10000): loss= 3.45746390525563\n",
      "Losgistic Regression(    8000/10000): loss=-1.40244199665175\n",
      "Losgistic Regression(    8100/10000): loss=-6.26876430738498\n",
      "Losgistic Regression(    8200/10000): loss=-11.1007947874876\n",
      "Losgistic Regression(    8300/10000): loss=-15.874536366149\n",
      "Losgistic Regression(    8400/10000): loss=-20.6590281184192\n",
      "Losgistic Regression(    8500/10000): loss=-25.4342704043166\n",
      "Losgistic Regression(    8600/10000): loss=-30.1339165376555\n",
      "Losgistic Regression(    8700/10000): loss=-34.8423701322149\n",
      "Losgistic Regression(    8800/10000): loss=-39.6032274463085\n",
      "Losgistic Regression(    8900/10000): loss=-44.2997777577886\n",
      "Losgistic Regression(    9000/10000): loss=-48.9871125238007\n",
      "Losgistic Regression(    9100/10000): loss=-53.6668599537307\n",
      "Losgistic Regression(    9200/10000): loss=-58.3137916462444\n",
      "Losgistic Regression(    9300/10000): loss=-62.9039338978606\n",
      "Losgistic Regression(    9400/10000): loss=-67.3810302281422\n",
      "Losgistic Regression(    9500/10000): loss=-71.7869550619289\n",
      "Losgistic Regression(    9600/10000): loss=-76.165425626358\n",
      "Losgistic Regression(    9700/10000): loss=-80.6243896129785\n",
      "Losgistic Regression(    9800/10000): loss=-85.0888184555669\n",
      "Losgistic Regression(    9900/10000): loss=-89.5052503631128\n",
      "Time for  0th cross validation = 30.6125s\n",
      "Training Accuracy         =   0.94\n",
      "Cross Validation Accuracy = 0.738636\n",
      "Losgistic Regression(       0/10000): loss= 691.964598136934\n",
      "Losgistic Regression(     100/10000): loss= 518.245340473712\n",
      "Losgistic Regression(     200/10000): loss= 435.517402640865\n",
      "Losgistic Regression(     300/10000): loss= 388.13640551179\n",
      "Losgistic Regression(     400/10000): loss= 361.039361726647\n",
      "Losgistic Regression(     500/10000): loss= 342.64327530631\n",
      "Losgistic Regression(     600/10000): loss= 328.447977723925\n",
      "Losgistic Regression(     700/10000): loss= 316.862446694558\n",
      "Losgistic Regression(     800/10000): loss= 307.067301372105\n",
      "Losgistic Regression(     900/10000): loss= 298.545771101284\n",
      "Losgistic Regression(    1000/10000): loss= 290.998657691756\n",
      "Losgistic Regression(    1100/10000): loss= 283.299815614631\n",
      "Losgistic Regression(    1200/10000): loss= 275.017211030087\n",
      "Losgistic Regression(    1300/10000): loss= 267.45456774065\n",
      "Losgistic Regression(    1400/10000): loss= 260.597704437285\n",
      "Losgistic Regression(    1500/10000): loss= 254.400041657419\n",
      "Losgistic Regression(    1600/10000): loss= 248.758562464851\n",
      "Losgistic Regression(    1700/10000): loss= 243.566064520788\n",
      "Losgistic Regression(    1800/10000): loss= 238.698551679347\n",
      "Losgistic Regression(    1900/10000): loss= 233.857768854192\n",
      "Losgistic Regression(    2000/10000): loss= 229.060401587265\n",
      "Losgistic Regression(    2100/10000): loss= 224.338467955938\n",
      "Losgistic Regression(    2200/10000): loss= 219.50168231801\n",
      "Losgistic Regression(    2300/10000): loss= 214.444028269925\n",
      "Losgistic Regression(    2400/10000): loss= 209.491853148414\n",
      "Losgistic Regression(    2500/10000): loss= 204.660880335323\n",
      "Losgistic Regression(    2600/10000): loss= 199.98547966336\n",
      "Losgistic Regression(    2700/10000): loss= 195.461976278976\n",
      "Losgistic Regression(    2800/10000): loss= 191.104740185472\n",
      "Losgistic Regression(    2900/10000): loss= 186.914588620201\n",
      "Losgistic Regression(    3000/10000): loss= 182.871069817364\n",
      "Losgistic Regression(    3100/10000): loss= 178.970817671317\n",
      "Losgistic Regression(    3200/10000): loss= 174.567261384779\n",
      "Losgistic Regression(    3300/10000): loss= 170.264617304623\n",
      "Losgistic Regression(    3400/10000): loss= 166.114739341431\n",
      "Losgistic Regression(    3500/10000): loss= 162.144672707693\n",
      "Losgistic Regression(    3600/10000): loss= 158.334880116749\n",
      "Losgistic Regression(    3700/10000): loss= 154.675221666699\n",
      "Losgistic Regression(    3800/10000): loss= 151.149453912084\n",
      "Losgistic Regression(    3900/10000): loss= 147.73533931104\n",
      "Losgistic Regression(    4000/10000): loss= 144.418204334144\n",
      "Losgistic Regression(    4100/10000): loss= 141.085514175227\n",
      "Losgistic Regression(    4200/10000): loss= 137.610513894557\n",
      "Losgistic Regression(    4300/10000): loss= 134.21081030577\n",
      "Losgistic Regression(    4400/10000): loss= 130.904821495773\n",
      "Losgistic Regression(    4500/10000): loss= 127.664274047931\n",
      "Losgistic Regression(    4600/10000): loss= 124.406548746143\n",
      "Losgistic Regression(    4700/10000): loss= 120.918672994695\n",
      "Losgistic Regression(    4800/10000): loss= 117.128739823254\n",
      "Losgistic Regression(    4900/10000): loss= 113.387903977786\n",
      "Losgistic Regression(    5000/10000): loss= 109.713792783458\n",
      "Losgistic Regression(    5100/10000): loss= 106.114747990836\n",
      "Losgistic Regression(    5200/10000): loss= 102.546598061021\n",
      "Losgistic Regression(    5300/10000): loss= 98.6792791179203\n",
      "Losgistic Regression(    5400/10000): loss= 94.8346621338613\n",
      "Losgistic Regression(    5500/10000): loss= 90.9454925409407\n",
      "Losgistic Regression(    5600/10000): loss= 87.0980943467336\n",
      "Losgistic Regression(    5700/10000): loss= 83.3205265445785\n",
      "Losgistic Regression(    5800/10000): loss= 79.601216284455\n",
      "Losgistic Regression(    5900/10000): loss= 76.0261607438079\n",
      "Losgistic Regression(    6000/10000): loss= 72.1648828160051\n",
      "Losgistic Regression(    6100/10000): loss= 68.2847371138966\n",
      "Losgistic Regression(    6200/10000): loss= 64.5449199319426\n",
      "Losgistic Regression(    6300/10000): loss= 60.8660600605369\n",
      "Losgistic Regression(    6400/10000): loss= 57.2758296738814\n",
      "Losgistic Regression(    6500/10000): loss= 53.7293775760489\n",
      "Losgistic Regression(    6600/10000): loss= 50.1998318179031\n",
      "Losgistic Regression(    6700/10000): loss= 46.7426253739362\n",
      "Losgistic Regression(    6800/10000): loss= 43.2765146669278\n",
      "Losgistic Regression(    6900/10000): loss= 39.6625811374334\n",
      "Losgistic Regression(    7000/10000): loss= 36.0907511413789\n",
      "Losgistic Regression(    7100/10000): loss= 32.4883837542602\n",
      "Losgistic Regression(    7200/10000): loss= 28.702408262412\n",
      "Losgistic Regression(    7300/10000): loss= 24.9086587945238\n",
      "Losgistic Regression(    7400/10000): loss= 21.0602922772587\n",
      "Losgistic Regression(    7500/10000): loss= 17.1032302555044\n",
      "Losgistic Regression(    7600/10000): loss= 12.9786075064751\n",
      "Losgistic Regression(    7700/10000): loss= 8.90769694202789\n",
      "Losgistic Regression(    7800/10000): loss= 4.78482395995144\n",
      "Losgistic Regression(    7900/10000): loss= 0.635051223667173\n",
      "Losgistic Regression(    8000/10000): loss=-3.5228408454123\n",
      "Losgistic Regression(    8100/10000): loss=-7.69827396966333\n",
      "Losgistic Regression(    8200/10000): loss=-11.7203555774572\n",
      "Losgistic Regression(    8300/10000): loss=-15.6539297410146\n",
      "Losgistic Regression(    8400/10000): loss=-19.5970772395394\n",
      "Losgistic Regression(    8500/10000): loss=-23.4153946350108\n",
      "Losgistic Regression(    8600/10000): loss=-27.2765784208327\n",
      "Losgistic Regression(    8700/10000): loss=-31.2026781946174\n",
      "Losgistic Regression(    8800/10000): loss=-35.0790594218997\n",
      "Losgistic Regression(    8900/10000): loss=-39.0452473170501\n",
      "Losgistic Regression(    9000/10000): loss=-43.1487706006102\n",
      "Losgistic Regression(    9100/10000): loss=-47.1564821284559\n",
      "Losgistic Regression(    9200/10000): loss=-51.0805709328608\n",
      "Losgistic Regression(    9300/10000): loss=-54.8762764227989\n",
      "Losgistic Regression(    9400/10000): loss=-58.5889230848284\n",
      "Losgistic Regression(    9500/10000): loss=-62.2510480792276\n",
      "Losgistic Regression(    9600/10000): loss=-65.8638143149373\n",
      "Losgistic Regression(    9700/10000): loss=-69.4669868711156\n",
      "Losgistic Regression(    9800/10000): loss=-73.0341908580786\n",
      "Losgistic Regression(    9900/10000): loss=-76.5186981234218\n",
      "Time for  1th cross validation = 31.137s\n",
      "Training Accuracy         =  0.947\n",
      "Cross Validation Accuracy = 0.761612\n",
      "Losgistic Regression(       0/10000): loss= 691.569707340736\n",
      "Losgistic Regression(     100/10000): loss= 521.479457171315\n",
      "Losgistic Regression(     200/10000): loss= 453.364801897028\n",
      "Losgistic Regression(     300/10000): loss= 414.212734960755\n",
      "Losgistic Regression(     400/10000): loss= 391.069047580342\n",
      "Losgistic Regression(     500/10000): loss= 375.219930435098\n",
      "Losgistic Regression(     600/10000): loss= 363.355618631938\n",
      "Losgistic Regression(     700/10000): loss= 354.059426472869\n",
      "Losgistic Regression(     800/10000): loss= 346.431550031187\n",
      "Losgistic Regression(     900/10000): loss= 339.850464714783\n",
      "Losgistic Regression(    1000/10000): loss= 333.972094040198\n",
      "Losgistic Regression(    1100/10000): loss= 328.593514786542\n",
      "Losgistic Regression(    1200/10000): loss= 322.76161618043\n",
      "Losgistic Regression(    1300/10000): loss= 316.606841409217\n",
      "Losgistic Regression(    1400/10000): loss= 310.85502414739\n",
      "Losgistic Regression(    1500/10000): loss= 305.444648507184\n",
      "Losgistic Regression(    1600/10000): loss= 300.306788890227\n",
      "Losgistic Regression(    1700/10000): loss= 295.42376430706\n",
      "Losgistic Regression(    1800/10000): loss= 290.507139875548\n",
      "Losgistic Regression(    1900/10000): loss= 285.389379958795\n",
      "Losgistic Regression(    2000/10000): loss= 280.062830776984\n",
      "Losgistic Regression(    2100/10000): loss= 274.948252396113\n",
      "Losgistic Regression(    2200/10000): loss= 270.111664524823\n",
      "Losgistic Regression(    2300/10000): loss= 265.523805854994\n",
      "Losgistic Regression(    2400/10000): loss= 261.133463682555\n",
      "Losgistic Regression(    2500/10000): loss= 256.900533301479\n",
      "Losgistic Regression(    2600/10000): loss= 252.699552360283\n",
      "Losgistic Regression(    2700/10000): loss= 247.945264390331\n",
      "Losgistic Regression(    2800/10000): loss= 242.857383426194\n",
      "Losgistic Regression(    2900/10000): loss= 237.921617739285\n",
      "Losgistic Regression(    3000/10000): loss= 232.8634820421\n",
      "Losgistic Regression(    3100/10000): loss= 227.265054861435\n",
      "Losgistic Regression(    3200/10000): loss= 221.351090102879\n",
      "Losgistic Regression(    3300/10000): loss= 215.409085619152\n",
      "Losgistic Regression(    3400/10000): loss= 209.397356175957\n",
      "Losgistic Regression(    3500/10000): loss= 203.537432280962\n",
      "Losgistic Regression(    3600/10000): loss= 197.849471890346\n",
      "Losgistic Regression(    3700/10000): loss= 192.337300126874\n",
      "Losgistic Regression(    3800/10000): loss= 186.971715584054\n",
      "Losgistic Regression(    3900/10000): loss= 181.742779736271\n",
      "Losgistic Regression(    4000/10000): loss= 176.240780645448\n",
      "Losgistic Regression(    4100/10000): loss= 170.820947102611\n",
      "Losgistic Regression(    4200/10000): loss= 165.491696893418\n",
      "Losgistic Regression(    4300/10000): loss= 160.248976259738\n",
      "Losgistic Regression(    4400/10000): loss= 155.039507400511\n",
      "Losgistic Regression(    4500/10000): loss= 149.872941888839\n",
      "Losgistic Regression(    4600/10000): loss= 144.74094878894\n",
      "Losgistic Regression(    4700/10000): loss= 139.657960145756\n",
      "Losgistic Regression(    4800/10000): loss= 134.455478852255\n",
      "Losgistic Regression(    4900/10000): loss= 129.305206627331\n",
      "Losgistic Regression(    5000/10000): loss= 124.263365808947\n",
      "Losgistic Regression(    5100/10000): loss= 119.285070013459\n",
      "Losgistic Regression(    5200/10000): loss= 114.35696851511\n",
      "Losgistic Regression(    5300/10000): loss= 109.503284081184\n",
      "Losgistic Regression(    5400/10000): loss= 104.697266766771\n",
      "Losgistic Regression(    5500/10000): loss= 99.7857003831572\n",
      "Losgistic Regression(    5600/10000): loss= 94.7371384462676\n",
      "Losgistic Regression(    5700/10000): loss= 89.7216180956019\n",
      "Losgistic Regression(    5800/10000): loss= 84.5223202227888\n",
      "Losgistic Regression(    5900/10000): loss= 79.2421853895556\n",
      "Losgistic Regression(    6000/10000): loss= 73.754683166654\n",
      "Losgistic Regression(    6100/10000): loss= 68.0139590291239\n",
      "Losgistic Regression(    6200/10000): loss= 62.2718942293941\n",
      "Losgistic Regression(    6300/10000): loss= 56.4909085747623\n",
      "Losgistic Regression(    6400/10000): loss= 50.7484551619389\n",
      "Losgistic Regression(    6500/10000): loss= 45.0677935816802\n",
      "Losgistic Regression(    6600/10000): loss= 39.44854070003\n",
      "Losgistic Regression(    6700/10000): loss= 33.8613695311654\n",
      "Losgistic Regression(    6800/10000): loss= 28.2310572658861\n",
      "Losgistic Regression(    6900/10000): loss= 22.2266175901212\n",
      "Losgistic Regression(    7000/10000): loss= 16.1049532736572\n",
      "Losgistic Regression(    7100/10000): loss= 10.0728682241617\n",
      "Losgistic Regression(    7200/10000): loss= 4.17122580230678\n",
      "Losgistic Regression(    7300/10000): loss=-1.68034167867656\n",
      "Losgistic Regression(    7400/10000): loss=-7.44098216532905\n",
      "Losgistic Regression(    7500/10000): loss=-13.1020883775288\n",
      "Losgistic Regression(    7600/10000): loss=-18.6573169715753\n",
      "Losgistic Regression(    7700/10000): loss=-24.0437480263689\n",
      "Losgistic Regression(    7800/10000): loss=-29.3881574671537\n",
      "Losgistic Regression(    7900/10000): loss=-34.575634205947\n",
      "Losgistic Regression(    8000/10000): loss=-39.7370951089148\n",
      "Losgistic Regression(    8100/10000): loss=-44.868192520936\n",
      "Losgistic Regression(    8200/10000): loss=-50.2563374640942\n",
      "Losgistic Regression(    8300/10000): loss=-55.7554488174516\n",
      "Losgistic Regression(    8400/10000): loss=-61.2814890222188\n",
      "Losgistic Regression(    8500/10000): loss=-66.8144787790518\n",
      "Losgistic Regression(    8600/10000): loss=-72.3469418865564\n",
      "Losgistic Regression(    8700/10000): loss=-77.8392627551823\n",
      "Losgistic Regression(    8800/10000): loss=-83.2850720008589\n",
      "Losgistic Regression(    8900/10000): loss=-88.6391799310827\n",
      "Losgistic Regression(    9000/10000): loss=-93.8905992692898\n",
      "Losgistic Regression(    9100/10000): loss=-99.0035397400544\n",
      "Losgistic Regression(    9200/10000): loss=-104.01401901669\n",
      "Losgistic Regression(    9300/10000): loss=-108.901073952275\n",
      "Losgistic Regression(    9400/10000): loss=-113.726320131835\n",
      "Losgistic Regression(    9500/10000): loss=-118.491789198807\n",
      "Losgistic Regression(    9600/10000): loss=-123.19975676398\n",
      "Losgistic Regression(    9700/10000): loss=-127.862730754814\n",
      "Losgistic Regression(    9800/10000): loss=-132.457118564016\n",
      "Losgistic Regression(    9900/10000): loss=-136.947727635091\n",
      "Time for  2th cross validation = 31.4181s\n",
      "Training Accuracy         =  0.928\n",
      "Cross Validation Accuracy = 0.739488\n",
      "Losgistic Regression(       0/10000): loss= 691.707325662028\n",
      "Losgistic Regression(     100/10000): loss= 512.257118517144\n",
      "Losgistic Regression(     200/10000): loss= 436.215229431893\n",
      "Losgistic Regression(     300/10000): loss= 393.449655910208\n",
      "Losgistic Regression(     400/10000): loss= 368.571823510648\n",
      "Losgistic Regression(     500/10000): loss= 350.923313151256\n",
      "Losgistic Regression(     600/10000): loss= 337.098215933545\n",
      "Losgistic Regression(     700/10000): loss= 325.321370689701\n",
      "Losgistic Regression(     800/10000): loss= 314.228094111496\n",
      "Losgistic Regression(     900/10000): loss= 304.435843734171\n",
      "Losgistic Regression(    1000/10000): loss= 295.582978546133\n",
      "Losgistic Regression(    1100/10000): loss= 287.472157003685\n",
      "Losgistic Regression(    1200/10000): loss= 279.722077485034\n",
      "Losgistic Regression(    1300/10000): loss= 271.126388798671\n",
      "Losgistic Regression(    1400/10000): loss= 262.95713972136\n",
      "Losgistic Regression(    1500/10000): loss= 255.206027347989\n",
      "Losgistic Regression(    1600/10000): loss= 247.851207320106\n",
      "Losgistic Regression(    1700/10000): loss= 240.67250511232\n",
      "Losgistic Regression(    1800/10000): loss= 233.15205032266\n",
      "Losgistic Regression(    1900/10000): loss= 225.724012176882\n",
      "Losgistic Regression(    2000/10000): loss= 217.900837185281\n",
      "Losgistic Regression(    2100/10000): loss= 209.593237920577\n",
      "Losgistic Regression(    2200/10000): loss= 200.535444786176\n",
      "Losgistic Regression(    2300/10000): loss= 191.667045474096\n",
      "Losgistic Regression(    2400/10000): loss= 182.720568209042\n",
      "Losgistic Regression(    2500/10000): loss= 173.61642437858\n",
      "Losgistic Regression(    2600/10000): loss= 164.794804724139\n",
      "Losgistic Regression(    2700/10000): loss= 156.201687993839\n",
      "Losgistic Regression(    2800/10000): loss= 147.859757477079\n",
      "Losgistic Regression(    2900/10000): loss= 139.765904006398\n",
      "Losgistic Regression(    3000/10000): loss= 131.833820719503\n",
      "Losgistic Regression(    3100/10000): loss= 123.759307034112\n",
      "Losgistic Regression(    3200/10000): loss= 115.754641030566\n",
      "Losgistic Regression(    3300/10000): loss= 107.713381199684\n",
      "Losgistic Regression(    3400/10000): loss= 99.8861264016526\n",
      "Losgistic Regression(    3500/10000): loss= 92.0794568582925\n",
      "Losgistic Regression(    3600/10000): loss= 84.3662117008077\n",
      "Losgistic Regression(    3700/10000): loss= 76.8126436183943\n",
      "Losgistic Regression(    3800/10000): loss= 68.955275536512\n",
      "Losgistic Regression(    3900/10000): loss= 61.1477394028232\n",
      "Losgistic Regression(    4000/10000): loss= 53.4698379056411\n",
      "Losgistic Regression(    4100/10000): loss= 45.9008058601788\n",
      "Losgistic Regression(    4200/10000): loss= 38.4139904194437\n",
      "Losgistic Regression(    4300/10000): loss= 31.0215047611811\n",
      "Losgistic Regression(    4400/10000): loss= 23.8005388315211\n",
      "Losgistic Regression(    4500/10000): loss= 16.6629710467235\n",
      "Losgistic Regression(    4600/10000): loss= 9.590846438843\n",
      "Losgistic Regression(    4700/10000): loss= 2.4028024654316\n",
      "Losgistic Regression(    4800/10000): loss=-4.77295245296881\n",
      "Losgistic Regression(    4900/10000): loss=-11.9248515905905\n",
      "Losgistic Regression(    5000/10000): loss=-19.0764728874144\n",
      "Losgistic Regression(    5100/10000): loss=-26.5018366917335\n",
      "Losgistic Regression(    5200/10000): loss=-34.4372351469172\n",
      "Losgistic Regression(    5300/10000): loss=-42.6136386800762\n",
      "Losgistic Regression(    5400/10000): loss=-51.0931603778373\n",
      "Losgistic Regression(    5500/10000): loss=-59.7349026951574\n",
      "Losgistic Regression(    5600/10000): loss=-68.4834769816452\n",
      "Losgistic Regression(    5700/10000): loss=-77.425183230251\n",
      "Losgistic Regression(    5800/10000): loss=-86.4969900951359\n",
      "Losgistic Regression(    5900/10000): loss=-95.6447204108442\n",
      "Losgistic Regression(    6000/10000): loss=-104.865274575131\n",
      "Losgistic Regression(    6100/10000): loss=-114.084136628641\n",
      "Losgistic Regression(    6200/10000): loss=-123.219300915458\n",
      "Losgistic Regression(    6300/10000): loss=-132.510241385547\n",
      "Losgistic Regression(    6400/10000): loss=-141.848472701673\n",
      "Losgistic Regression(    6500/10000): loss=-151.573108048484\n",
      "Losgistic Regression(    6600/10000): loss=-161.335252931197\n",
      "Losgistic Regression(    6700/10000): loss=-171.127050796375\n",
      "Losgistic Regression(    6800/10000): loss=-180.959387620078\n",
      "Losgistic Regression(    6900/10000): loss=-190.658709251393\n",
      "Losgistic Regression(    7000/10000): loss=-200.324652730412\n",
      "Losgistic Regression(    7100/10000): loss=-210.004250975039\n",
      "Losgistic Regression(    7200/10000): loss=-219.742190047165\n",
      "Losgistic Regression(    7300/10000): loss=-229.335136640363\n",
      "Losgistic Regression(    7400/10000): loss=-238.976295874759\n",
      "Losgistic Regression(    7500/10000): loss=-248.551317797774\n",
      "Losgistic Regression(    7600/10000): loss=-258.089461209675\n",
      "Losgistic Regression(    7700/10000): loss=-267.833142835643\n",
      "Losgistic Regression(    7800/10000): loss=-277.693245877379\n",
      "Losgistic Regression(    7900/10000): loss=-287.445878721153\n",
      "Losgistic Regression(    8000/10000): loss=-297.076340846318\n",
      "Losgistic Regression(    8100/10000): loss=-306.615226555933\n",
      "Losgistic Regression(    8200/10000): loss=-316.012894438942\n",
      "Losgistic Regression(    8300/10000): loss=-325.2085403529\n",
      "Losgistic Regression(    8400/10000): loss=-334.331730705987\n",
      "Losgistic Regression(    8500/10000): loss=-343.462483528311\n",
      "Losgistic Regression(    8600/10000): loss=-352.551010933155\n",
      "Losgistic Regression(    8700/10000): loss=-361.450807227585\n",
      "Losgistic Regression(    8800/10000): loss=-370.237764102703\n",
      "Losgistic Regression(    8900/10000): loss=-379.075922128183\n",
      "Losgistic Regression(    9000/10000): loss=-387.888960000056\n",
      "Losgistic Regression(    9100/10000): loss=-397.113515965542\n",
      "Losgistic Regression(    9200/10000): loss=-406.295242856487\n",
      "Losgistic Regression(    9300/10000): loss=-415.375027697925\n",
      "Losgistic Regression(    9400/10000): loss=-424.416917270442\n",
      "Losgistic Regression(    9500/10000): loss=-433.37036368909\n",
      "Losgistic Regression(    9600/10000): loss=-442.359577826905\n",
      "Losgistic Regression(    9700/10000): loss=-451.573479662188\n",
      "Losgistic Regression(    9800/10000): loss=-460.504220744578\n",
      "Losgistic Regression(    9900/10000): loss=-469.022344389197\n",
      "Time for  3th cross validation = 31.2862s\n",
      "Training Accuracy         =   0.94\n",
      "Cross Validation Accuracy = 0.740492\n",
      "Losgistic Regression(       0/10000): loss= 692.021458023298\n",
      "Losgistic Regression(     100/10000): loss= 518.021422636273\n",
      "Losgistic Regression(     200/10000): loss= 442.199081567282\n",
      "Losgistic Regression(     300/10000): loss= 400.193542946546\n",
      "Losgistic Regression(     400/10000): loss= 375.011027598533\n",
      "Losgistic Regression(     500/10000): loss= 357.248937612831\n",
      "Losgistic Regression(     600/10000): loss= 343.260489771233\n",
      "Losgistic Regression(     700/10000): loss= 331.540001441599\n",
      "Losgistic Regression(     800/10000): loss= 321.427756296329\n",
      "Losgistic Regression(     900/10000): loss= 312.534456786129\n",
      "Losgistic Regression(    1000/10000): loss= 304.582123303461\n",
      "Losgistic Regression(    1100/10000): loss= 297.416375356103\n",
      "Losgistic Regression(    1200/10000): loss= 290.925879940141\n",
      "Losgistic Regression(    1300/10000): loss= 285.006962508137\n",
      "Losgistic Regression(    1400/10000): loss= 279.587926442942\n",
      "Losgistic Regression(    1500/10000): loss= 274.60038532527\n",
      "Losgistic Regression(    1600/10000): loss= 269.98527191271\n",
      "Losgistic Regression(    1700/10000): loss= 265.699057365436\n",
      "Losgistic Regression(    1800/10000): loss= 261.310430681368\n",
      "Losgistic Regression(    1900/10000): loss= 256.466578304345\n",
      "Losgistic Regression(    2000/10000): loss= 251.848365496337\n",
      "Losgistic Regression(    2100/10000): loss= 247.4372214543\n",
      "Losgistic Regression(    2200/10000): loss= 243.206580818132\n",
      "Losgistic Regression(    2300/10000): loss= 239.148799608748\n",
      "Losgistic Regression(    2400/10000): loss= 235.243601449261\n",
      "Losgistic Regression(    2500/10000): loss= 231.485173525571\n",
      "Losgistic Regression(    2600/10000): loss= 227.857260898104\n",
      "Losgistic Regression(    2700/10000): loss= 224.359117879473\n",
      "Losgistic Regression(    2800/10000): loss= 220.978356577681\n",
      "Losgistic Regression(    2900/10000): loss= 217.701007151773\n",
      "Losgistic Regression(    3000/10000): loss= 213.905815289417\n",
      "Losgistic Regression(    3100/10000): loss= 210.110675338819\n",
      "Losgistic Regression(    3200/10000): loss= 206.419394113616\n",
      "Losgistic Regression(    3300/10000): loss= 202.828922182426\n",
      "Losgistic Regression(    3400/10000): loss= 199.337837565294\n",
      "Losgistic Regression(    3500/10000): loss= 195.955745431246\n",
      "Losgistic Regression(    3600/10000): loss= 192.223677854265\n",
      "Losgistic Regression(    3700/10000): loss= 188.572948337466\n",
      "Losgistic Regression(    3800/10000): loss= 184.717021525884\n",
      "Losgistic Regression(    3900/10000): loss= 180.851319165542\n",
      "Losgistic Regression(    4000/10000): loss= 177.113320146009\n",
      "Losgistic Regression(    4100/10000): loss= 173.337895954108\n",
      "Losgistic Regression(    4200/10000): loss= 169.495485683797\n",
      "Losgistic Regression(    4300/10000): loss= 165.757565519177\n",
      "Losgistic Regression(    4400/10000): loss= 162.127481395027\n",
      "Losgistic Regression(    4500/10000): loss= 158.37681736118\n",
      "Losgistic Regression(    4600/10000): loss= 154.570922303646\n",
      "Losgistic Regression(    4700/10000): loss= 150.607101383598\n",
      "Losgistic Regression(    4800/10000): loss= 146.736943089761\n",
      "Losgistic Regression(    4900/10000): loss= 142.960977208248\n",
      "Losgistic Regression(    5000/10000): loss= 139.295218831051\n",
      "Losgistic Regression(    5100/10000): loss= 135.694598363681\n",
      "Losgistic Regression(    5200/10000): loss= 131.843038765195\n",
      "Losgistic Regression(    5300/10000): loss= 127.973540605704\n",
      "Losgistic Regression(    5400/10000): loss= 124.191277559719\n",
      "Losgistic Regression(    5500/10000): loss= 120.494019296972\n",
      "Losgistic Regression(    5600/10000): loss= 116.854479169078\n",
      "Losgistic Regression(    5700/10000): loss= 113.280454963123\n",
      "Losgistic Regression(    5800/10000): loss= 109.79496809412\n",
      "Losgistic Regression(    5900/10000): loss= 106.364767567779\n",
      "Losgistic Regression(    6000/10000): loss= 103.010003675274\n",
      "Losgistic Regression(    6100/10000): loss= 99.7403590040103\n",
      "Losgistic Regression(    6200/10000): loss= 96.5127983762611\n",
      "Losgistic Regression(    6300/10000): loss= 93.3861854800489\n",
      "Losgistic Regression(    6400/10000): loss= 90.3389579685602\n",
      "Losgistic Regression(    6500/10000): loss= 87.3076003195207\n",
      "Losgistic Regression(    6600/10000): loss= 84.1728793390464\n",
      "Losgistic Regression(    6700/10000): loss= 81.0053186368037\n",
      "Losgistic Regression(    6800/10000): loss= 77.8163704666501\n",
      "Losgistic Regression(    6900/10000): loss= 74.414512326324\n",
      "Losgistic Regression(    7000/10000): loss= 71.0058421121052\n",
      "Losgistic Regression(    7100/10000): loss= 67.6221096691548\n",
      "Losgistic Regression(    7200/10000): loss= 64.2677234965933\n",
      "Losgistic Regression(    7300/10000): loss= 60.9443859849422\n",
      "Losgistic Regression(    7400/10000): loss= 57.6541853136415\n",
      "Losgistic Regression(    7500/10000): loss= 54.3507965896246\n",
      "Losgistic Regression(    7600/10000): loss= 51.0234793990343\n",
      "Losgistic Regression(    7700/10000): loss= 47.7587253262471\n",
      "Losgistic Regression(    7800/10000): loss= 44.5181748871937\n",
      "Losgistic Regression(    7900/10000): loss= 41.2703365913861\n",
      "Losgistic Regression(    8000/10000): loss= 37.9960697335244\n",
      "Losgistic Regression(    8100/10000): loss= 34.7116974990247\n",
      "Losgistic Regression(    8200/10000): loss= 31.4142863126366\n",
      "Losgistic Regression(    8300/10000): loss= 28.1517180671069\n",
      "Losgistic Regression(    8400/10000): loss= 24.879246716342\n",
      "Losgistic Regression(    8500/10000): loss= 21.5955525520149\n",
      "Losgistic Regression(    8600/10000): loss= 18.282629388867\n",
      "Losgistic Regression(    8700/10000): loss= 14.9625466119408\n",
      "Losgistic Regression(    8800/10000): loss= 11.6349652218817\n",
      "Losgistic Regression(    8900/10000): loss= 8.32269499890186\n",
      "Losgistic Regression(    9000/10000): loss= 5.02628909293416\n",
      "Losgistic Regression(    9100/10000): loss= 1.7705512000586\n",
      "Losgistic Regression(    9200/10000): loss=-1.55723535268011\n",
      "Losgistic Regression(    9300/10000): loss=-4.91386369943795\n",
      "Losgistic Regression(    9400/10000): loss=-8.34709870897223\n",
      "Losgistic Regression(    9500/10000): loss=-11.7149263253015\n",
      "Losgistic Regression(    9600/10000): loss=-15.0656422727736\n",
      "Losgistic Regression(    9700/10000): loss=-18.4079096785764\n",
      "Losgistic Regression(    9800/10000): loss=-21.6828305248945\n",
      "Losgistic Regression(    9900/10000): loss=-24.9020629599682\n",
      "Time for  4th cross validation = 31.8249s\n",
      "Training Accuracy         =  0.943\n",
      "Cross Validation Accuracy = 0.742312\n",
      "*************** ([0.93999999999999995, 0.94699999999999995, 0.92800000000000005, 0.93999999999999995, 0.94299999999999995], [0.73863599999999996, 0.76161199999999996, 0.73948800000000003, 0.74049200000000004, 0.74231199999999997])\n",
      "Losgistic Regression(       0/10000): loss= 692.024538875273\n",
      "Losgistic Regression(     100/10000): loss= 531.146758700516\n",
      "Losgistic Regression(     200/10000): loss= 451.524603037748\n",
      "Losgistic Regression(     300/10000): loss= 407.976857299804\n",
      "Losgistic Regression(     400/10000): loss= 382.980468980066\n",
      "Losgistic Regression(     500/10000): loss= 365.755911137163\n",
      "Losgistic Regression(     600/10000): loss= 352.686174542224\n",
      "Losgistic Regression(     700/10000): loss= 342.34633802605\n",
      "Losgistic Regression(     800/10000): loss= 333.948114840886\n",
      "Losgistic Regression(     900/10000): loss= 327.002953448001\n",
      "Losgistic Regression(    1000/10000): loss= 321.147267370627\n",
      "Losgistic Regression(    1100/10000): loss= 316.142605212711\n",
      "Losgistic Regression(    1200/10000): loss= 311.776525145734\n",
      "Losgistic Regression(    1300/10000): loss= 307.931469993441\n",
      "Losgistic Regression(    1400/10000): loss= 304.509577513143\n",
      "Losgistic Regression(    1500/10000): loss= 301.43773256735\n",
      "Losgistic Regression(    1600/10000): loss= 298.642438300574\n",
      "Losgistic Regression(    1700/10000): loss= 296.080375488867\n",
      "Losgistic Regression(    1800/10000): loss= 293.718803952646\n",
      "Losgistic Regression(    1900/10000): loss= 291.526903857147\n",
      "Losgistic Regression(    2000/10000): loss= 289.441726841813\n",
      "Losgistic Regression(    2100/10000): loss= 287.179497547128\n",
      "Losgistic Regression(    2200/10000): loss= 285.042722705402\n",
      "Losgistic Regression(    2300/10000): loss= 283.039148434734\n",
      "Losgistic Regression(    2400/10000): loss= 281.146620418252\n",
      "Losgistic Regression(    2500/10000): loss= 279.356083822949\n",
      "Losgistic Regression(    2600/10000): loss= 277.624415445779\n",
      "Losgistic Regression(    2700/10000): loss= 275.935847180302\n",
      "Losgistic Regression(    2800/10000): loss= 274.339966708471\n",
      "Losgistic Regression(    2900/10000): loss= 272.686735265571\n",
      "Losgistic Regression(    3000/10000): loss= 270.917168830777\n",
      "Losgistic Regression(    3100/10000): loss= 269.161250684754\n",
      "Losgistic Regression(    3200/10000): loss= 267.440528838011\n",
      "Losgistic Regression(    3300/10000): loss= 265.703006429641\n",
      "Losgistic Regression(    3400/10000): loss= 263.975784637079\n",
      "Losgistic Regression(    3500/10000): loss= 262.279026605692\n",
      "Losgistic Regression(    3600/10000): loss= 260.615934962681\n",
      "Losgistic Regression(    3700/10000): loss= 258.955895400165\n",
      "Losgistic Regression(    3800/10000): loss= 257.274410548715\n",
      "Losgistic Regression(    3900/10000): loss= 255.632648937451\n",
      "Losgistic Regression(    4000/10000): loss= 254.057913794971\n",
      "Losgistic Regression(    4100/10000): loss= 252.583301397204\n",
      "Losgistic Regression(    4200/10000): loss= 250.727114923488\n",
      "Losgistic Regression(    4300/10000): loss= 248.936205608939\n",
      "Losgistic Regression(    4400/10000): loss= 247.14338268367\n",
      "Losgistic Regression(    4500/10000): loss= 245.48213949815\n",
      "Losgistic Regression(    4600/10000): loss= 243.592787790525\n",
      "Losgistic Regression(    4700/10000): loss= 241.477441178472\n",
      "Losgistic Regression(    4800/10000): loss= 239.525500454419\n",
      "Losgistic Regression(    4900/10000): loss= 237.414129593289\n",
      "Losgistic Regression(    5000/10000): loss= 235.392374664933\n",
      "Losgistic Regression(    5100/10000): loss= 232.856184521942\n",
      "Losgistic Regression(    5200/10000): loss= 230.461397923848\n",
      "Losgistic Regression(    5300/10000): loss= 228.051656614953\n",
      "Losgistic Regression(    5400/10000): loss= 225.948772202787\n",
      "Losgistic Regression(    5500/10000): loss= 224.083678912082\n",
      "Losgistic Regression(    5600/10000): loss= 222.501620048213\n",
      "Losgistic Regression(    5700/10000): loss= 220.552046187767\n",
      "Losgistic Regression(    5800/10000): loss= 218.473234706097\n",
      "Losgistic Regression(    5900/10000): loss= 215.931427816147\n",
      "Losgistic Regression(    6000/10000): loss= 213.185757411609\n",
      "Losgistic Regression(    6100/10000): loss= 210.409654466337\n",
      "Losgistic Regression(    6200/10000): loss= 207.845157093406\n",
      "Losgistic Regression(    6300/10000): loss= 205.13454746156\n",
      "Losgistic Regression(    6400/10000): loss= 202.84000326616\n",
      "Losgistic Regression(    6500/10000): loss= 200.281331667834\n",
      "Losgistic Regression(    6600/10000): loss= 197.6760525778\n",
      "Losgistic Regression(    6700/10000): loss= 195.36445781064\n",
      "Losgistic Regression(    6800/10000): loss= 192.934281840794\n",
      "Losgistic Regression(    6900/10000): loss= 190.758578565975\n",
      "Losgistic Regression(    7000/10000): loss= 188.643008066044\n",
      "Losgistic Regression(    7100/10000): loss= 186.591573340149\n",
      "Losgistic Regression(    7200/10000): loss= 184.528745120775\n",
      "Losgistic Regression(    7300/10000): loss= 182.575260465147\n",
      "Losgistic Regression(    7400/10000): loss= 180.690218943762\n",
      "Losgistic Regression(    7500/10000): loss= 178.709031990074\n",
      "Losgistic Regression(    7600/10000): loss= 176.642237402751\n",
      "Losgistic Regression(    7700/10000): loss= 174.726997124473\n",
      "Losgistic Regression(    7800/10000): loss= 172.811323817632\n",
      "Losgistic Regression(    7900/10000): loss= 170.909007089941\n",
      "Losgistic Regression(    8000/10000): loss= 169.193439404711\n",
      "Losgistic Regression(    8100/10000): loss= 167.491276316208\n",
      "Losgistic Regression(    8200/10000): loss= 165.990133380678\n",
      "Losgistic Regression(    8300/10000): loss= 164.282778425568\n",
      "Losgistic Regression(    8400/10000): loss= 162.616357848339\n",
      "Losgistic Regression(    8500/10000): loss= 160.454173237503\n",
      "Losgistic Regression(    8600/10000): loss= 158.377306342388\n",
      "Losgistic Regression(    8700/10000): loss= 156.381564416661\n",
      "Losgistic Regression(    8800/10000): loss= 154.053088525129\n",
      "Losgistic Regression(    8900/10000): loss= 152.072225553039\n",
      "Losgistic Regression(    9000/10000): loss= 149.883148491149\n",
      "Losgistic Regression(    9100/10000): loss= 147.982772391088\n",
      "Losgistic Regression(    9200/10000): loss= 145.821361227084\n",
      "Losgistic Regression(    9300/10000): loss= 143.609052140532\n",
      "Losgistic Regression(    9400/10000): loss= 141.669261559916\n",
      "Losgistic Regression(    9500/10000): loss= 140.223649951685\n",
      "Losgistic Regression(    9600/10000): loss= 138.949283686298\n",
      "Losgistic Regression(    9700/10000): loss= 137.635866539717\n",
      "Losgistic Regression(    9800/10000): loss= 136.185800301784\n",
      "Losgistic Regression(    9900/10000): loss= 134.260513600001\n",
      "Time for  0th cross validation = 31.8911s\n",
      "Training Accuracy         =  0.931\n",
      "Cross Validation Accuracy = 0.75012\n",
      "Losgistic Regression(       0/10000): loss= 691.966857006262\n",
      "Losgistic Regression(     100/10000): loss= 519.203982936664\n",
      "Losgistic Regression(     200/10000): loss= 437.564328297982\n",
      "Losgistic Regression(     300/10000): loss= 391.298690438588\n",
      "Losgistic Regression(     400/10000): loss= 365.332492328538\n",
      "Losgistic Regression(     500/10000): loss= 348.054539838285\n",
      "Losgistic Regression(     600/10000): loss= 334.931608326533\n",
      "Losgistic Regression(     700/10000): loss= 324.350791907485\n",
      "Losgistic Regression(     800/10000): loss= 315.525454801257\n",
      "Losgistic Regression(     900/10000): loss= 307.94573607932\n",
      "Losgistic Regression(    1000/10000): loss= 301.300170808401\n",
      "Losgistic Regression(    1100/10000): loss= 295.387478757695\n",
      "Losgistic Regression(    1200/10000): loss= 289.391624032992\n",
      "Losgistic Regression(    1300/10000): loss= 282.978517770786\n",
      "Losgistic Regression(    1400/10000): loss= 277.247466454495\n",
      "Losgistic Regression(    1500/10000): loss= 272.140106599754\n",
      "Losgistic Regression(    1600/10000): loss= 267.563719239486\n",
      "Losgistic Regression(    1700/10000): loss= 263.428463498173\n",
      "Losgistic Regression(    1800/10000): loss= 259.621111720305\n",
      "Losgistic Regression(    1900/10000): loss= 256.0281229574\n",
      "Losgistic Regression(    2000/10000): loss= 252.623421890985\n",
      "Losgistic Regression(    2100/10000): loss= 249.344726438162\n",
      "Losgistic Regression(    2200/10000): loss= 246.208818651707\n",
      "Losgistic Regression(    2300/10000): loss= 243.186494525846\n",
      "Losgistic Regression(    2400/10000): loss= 240.002758534376\n",
      "Losgistic Regression(    2500/10000): loss= 236.632268317232\n",
      "Losgistic Regression(    2600/10000): loss= 233.227983252404\n",
      "Losgistic Regression(    2700/10000): loss= 229.972260534034\n",
      "Losgistic Regression(    2800/10000): loss= 226.933370779737\n",
      "Losgistic Regression(    2900/10000): loss= 224.008759462852\n",
      "Losgistic Regression(    3000/10000): loss= 221.255600433746\n",
      "Losgistic Regression(    3100/10000): loss= 218.638822271476\n",
      "Losgistic Regression(    3200/10000): loss= 216.091730724178\n",
      "Losgistic Regression(    3300/10000): loss= 213.708683835769\n",
      "Losgistic Regression(    3400/10000): loss= 211.453158936873\n",
      "Losgistic Regression(    3500/10000): loss= 209.137292063893\n",
      "Losgistic Regression(    3600/10000): loss= 206.631727196773\n",
      "Losgistic Regression(    3700/10000): loss= 204.326021074852\n",
      "Losgistic Regression(    3800/10000): loss= 202.140820383208\n",
      "Losgistic Regression(    3900/10000): loss= 200.006653866421\n",
      "Losgistic Regression(    4000/10000): loss= 197.987221905614\n",
      "Losgistic Regression(    4100/10000): loss= 196.008898829502\n",
      "Losgistic Regression(    4200/10000): loss= 194.315311191255\n",
      "Losgistic Regression(    4300/10000): loss= 192.774831931449\n",
      "Losgistic Regression(    4400/10000): loss= 191.310873832132\n",
      "Losgistic Regression(    4500/10000): loss= 189.898973967797\n",
      "Losgistic Regression(    4600/10000): loss= 188.631052210654\n",
      "Losgistic Regression(    4700/10000): loss= 187.350472277121\n",
      "Losgistic Regression(    4800/10000): loss= 186.098762061045\n",
      "Losgistic Regression(    4900/10000): loss= 184.908941462458\n",
      "Losgistic Regression(    5000/10000): loss= 183.805447570519\n",
      "Losgistic Regression(    5100/10000): loss= 182.633823121092\n",
      "Losgistic Regression(    5200/10000): loss= 181.507564651455\n",
      "Losgistic Regression(    5300/10000): loss= 180.369557251184\n",
      "Losgistic Regression(    5400/10000): loss= 179.17232696394\n",
      "Losgistic Regression(    5500/10000): loss= 178.180707773381\n",
      "Losgistic Regression(    5600/10000): loss= 177.241658922918\n",
      "Losgistic Regression(    5700/10000): loss= 175.972495206524\n",
      "Losgistic Regression(    5800/10000): loss= 174.734243495204\n",
      "Losgistic Regression(    5900/10000): loss= 173.386640649095\n",
      "Losgistic Regression(    6000/10000): loss= 172.157881731528\n",
      "Losgistic Regression(    6100/10000): loss= 170.94617985381\n",
      "Losgistic Regression(    6200/10000): loss= 169.6895963011\n",
      "Losgistic Regression(    6300/10000): loss= 168.446219670409\n",
      "Losgistic Regression(    6400/10000): loss= 167.40327952222\n",
      "Losgistic Regression(    6500/10000): loss= 166.233643203948\n",
      "Losgistic Regression(    6600/10000): loss= 165.281424695902\n",
      "Losgistic Regression(    6700/10000): loss= 164.408812220921\n",
      "Losgistic Regression(    6800/10000): loss= 163.63315602003\n",
      "Losgistic Regression(    6900/10000): loss= 162.705461277474\n",
      "Losgistic Regression(    7000/10000): loss= 161.986764299906\n",
      "Losgistic Regression(    7100/10000): loss= 161.368637339573\n",
      "Losgistic Regression(    7200/10000): loss= 160.766783724607\n",
      "Losgistic Regression(    7300/10000): loss= 160.26004092196\n",
      "Losgistic Regression(    7400/10000): loss= 159.916068550723\n",
      "Losgistic Regression(    7500/10000): loss= 159.392327445816\n",
      "Losgistic Regression(    7600/10000): loss= 158.661742637983\n",
      "Losgistic Regression(    7700/10000): loss= 158.111643391485\n",
      "Losgistic Regression(    7800/10000): loss= 157.378871931646\n",
      "Losgistic Regression(    7900/10000): loss= 156.620871098288\n",
      "Losgistic Regression(    8000/10000): loss= 155.806991981369\n",
      "Losgistic Regression(    8100/10000): loss= 154.99592566619\n",
      "Losgistic Regression(    8200/10000): loss= 154.375041928744\n",
      "Losgistic Regression(    8300/10000): loss= 153.947204121653\n",
      "Losgistic Regression(    8400/10000): loss= 153.476721246206\n",
      "Losgistic Regression(    8500/10000): loss= 153.015036696721\n",
      "Losgistic Regression(    8600/10000): loss= 152.411523836293\n",
      "Losgistic Regression(    8700/10000): loss= 151.800830173589\n",
      "Losgistic Regression(    8800/10000): loss= 150.806847686163\n",
      "Losgistic Regression(    8900/10000): loss= 149.980142967139\n",
      "Losgistic Regression(    9000/10000): loss= 149.154798925362\n",
      "Losgistic Regression(    9100/10000): loss= 148.482039350042\n",
      "Losgistic Regression(    9200/10000): loss= 147.743423257501\n",
      "Losgistic Regression(    9300/10000): loss= 147.147018082944\n",
      "Losgistic Regression(    9400/10000): loss= 146.36806939982\n",
      "Losgistic Regression(    9500/10000): loss= 145.399659474952\n",
      "Losgistic Regression(    9600/10000): loss= 144.261556330789\n",
      "Losgistic Regression(    9700/10000): loss= 143.44384236041\n",
      "Losgistic Regression(    9800/10000): loss= 142.269784659585\n",
      "Losgistic Regression(    9900/10000): loss= 141.506590650045\n",
      "Time for  1th cross validation = 31.7393s\n",
      "Training Accuracy         =  0.942\n",
      "Cross Validation Accuracy = 0.766416\n",
      "Losgistic Regression(       0/10000): loss= 691.57240023575\n",
      "Losgistic Regression(     100/10000): loss= 522.3280246139\n",
      "Losgistic Regression(     200/10000): loss= 455.1017185204\n",
      "Losgistic Regression(     300/10000): loss= 416.949810672041\n",
      "Losgistic Regression(     400/10000): loss= 394.789986692352\n",
      "Losgistic Regression(     500/10000): loss= 379.883034722865\n",
      "Losgistic Regression(     600/10000): loss= 368.951303021102\n",
      "Losgistic Regression(     700/10000): loss= 360.540932732663\n",
      "Losgistic Regression(     800/10000): loss= 353.760733165435\n",
      "Losgistic Regression(     900/10000): loss= 348.012206075916\n",
      "Losgistic Regression(    1000/10000): loss= 342.971473759466\n",
      "Losgistic Regression(    1100/10000): loss= 338.432995146218\n",
      "Losgistic Regression(    1200/10000): loss= 334.23880349019\n",
      "Losgistic Regression(    1300/10000): loss= 329.472572737733\n",
      "Losgistic Regression(    1400/10000): loss= 324.722990368497\n",
      "Losgistic Regression(    1500/10000): loss= 320.329297259993\n",
      "Losgistic Regression(    1600/10000): loss= 316.223133805894\n",
      "Losgistic Regression(    1700/10000): loss= 312.393597065415\n",
      "Losgistic Regression(    1800/10000): loss= 308.831029816108\n",
      "Losgistic Regression(    1900/10000): loss= 305.485882996708\n",
      "Losgistic Regression(    2000/10000): loss= 302.3503987088\n",
      "Losgistic Regression(    2100/10000): loss= 298.993805100516\n",
      "Losgistic Regression(    2200/10000): loss= 295.769089644622\n",
      "Losgistic Regression(    2300/10000): loss= 292.803092143615\n",
      "Losgistic Regression(    2400/10000): loss= 290.062757727167\n",
      "Losgistic Regression(    2500/10000): loss= 287.045549013827\n",
      "Losgistic Regression(    2600/10000): loss= 284.267328723441\n",
      "Losgistic Regression(    2700/10000): loss= 281.703471361881\n",
      "Losgistic Regression(    2800/10000): loss= 279.338320135151\n",
      "Losgistic Regression(    2900/10000): loss= 277.021710120186\n",
      "Losgistic Regression(    3000/10000): loss= 274.511543696344\n",
      "Losgistic Regression(    3100/10000): loss= 271.882921402247\n",
      "Losgistic Regression(    3200/10000): loss= 269.162823269543\n",
      "Losgistic Regression(    3300/10000): loss= 266.440641637214\n",
      "Losgistic Regression(    3400/10000): loss= 262.790006824236\n",
      "Losgistic Regression(    3500/10000): loss= 258.924294604384\n",
      "Losgistic Regression(    3600/10000): loss= 255.381718253969\n",
      "Losgistic Regression(    3700/10000): loss= 252.079943457861\n",
      "Losgistic Regression(    3800/10000): loss= 249.02419712337\n",
      "Losgistic Regression(    3900/10000): loss= 246.074576917587\n",
      "Losgistic Regression(    4000/10000): loss= 243.223296961639\n",
      "Losgistic Regression(    4100/10000): loss= 240.537718797173\n",
      "Losgistic Regression(    4200/10000): loss= 237.631674740812\n",
      "Losgistic Regression(    4300/10000): loss= 234.755635513742\n",
      "Losgistic Regression(    4400/10000): loss= 231.925513763355\n",
      "Losgistic Regression(    4500/10000): loss= 229.253616923281\n",
      "Losgistic Regression(    4600/10000): loss= 226.447402055704\n",
      "Losgistic Regression(    4700/10000): loss= 223.657050557139\n",
      "Losgistic Regression(    4800/10000): loss= 220.614118224525\n",
      "Losgistic Regression(    4900/10000): loss= 217.708556738757\n",
      "Losgistic Regression(    5000/10000): loss= 214.652978774407\n",
      "Losgistic Regression(    5100/10000): loss= 211.274177851371\n",
      "Losgistic Regression(    5200/10000): loss= 207.927491158412\n",
      "Losgistic Regression(    5300/10000): loss= 204.911346422848\n",
      "Losgistic Regression(    5400/10000): loss= 201.909456959822\n",
      "Losgistic Regression(    5500/10000): loss= 198.981787296472\n",
      "Losgistic Regression(    5600/10000): loss= 196.260321896047\n",
      "Losgistic Regression(    5700/10000): loss= 193.562427909781\n",
      "Losgistic Regression(    5800/10000): loss= 191.137299502182\n",
      "Losgistic Regression(    5900/10000): loss= 188.919400665759\n",
      "Losgistic Regression(    6000/10000): loss= 186.455817256677\n",
      "Losgistic Regression(    6100/10000): loss= 184.178084181417\n",
      "Losgistic Regression(    6200/10000): loss= 181.853685181036\n",
      "Losgistic Regression(    6300/10000): loss= 179.357230430158\n",
      "Losgistic Regression(    6400/10000): loss= 177.092393267145\n",
      "Losgistic Regression(    6500/10000): loss= 174.783455063722\n",
      "Losgistic Regression(    6600/10000): loss= 172.244370002467\n",
      "Losgistic Regression(    6700/10000): loss= 169.643655604917\n",
      "Losgistic Regression(    6800/10000): loss= 166.756946258031\n",
      "Losgistic Regression(    6900/10000): loss= 163.990939267841\n",
      "Losgistic Regression(    7000/10000): loss= 161.168434154879\n",
      "Losgistic Regression(    7100/10000): loss= 158.452363756051\n",
      "Losgistic Regression(    7200/10000): loss= 155.851532764295\n",
      "Losgistic Regression(    7300/10000): loss= 153.319019174719\n",
      "Losgistic Regression(    7400/10000): loss= 151.443252534566\n",
      "Losgistic Regression(    7500/10000): loss= 149.332135120545\n",
      "Losgistic Regression(    7600/10000): loss= 147.46861505095\n",
      "Losgistic Regression(    7700/10000): loss= 145.406304333774\n",
      "Losgistic Regression(    7800/10000): loss= 143.422400428456\n",
      "Losgistic Regression(    7900/10000): loss= 141.68892659423\n",
      "Losgistic Regression(    8000/10000): loss= 139.119422791352\n",
      "Losgistic Regression(    8100/10000): loss= 137.174778601897\n",
      "Losgistic Regression(    8200/10000): loss= 134.91049717903\n",
      "Losgistic Regression(    8300/10000): loss= 132.677110740616\n",
      "Losgistic Regression(    8400/10000): loss= 130.838433914034\n",
      "Losgistic Regression(    8500/10000): loss= 128.992244758392\n",
      "Losgistic Regression(    8600/10000): loss= 127.38099132156\n",
      "Losgistic Regression(    8700/10000): loss= 125.906458658852\n",
      "Losgistic Regression(    8800/10000): loss= 124.554173887375\n",
      "Losgistic Regression(    8900/10000): loss= 123.313735956144\n",
      "Losgistic Regression(    9000/10000): loss= 122.138567260004\n",
      "Losgistic Regression(    9100/10000): loss= 120.563274040693\n",
      "Losgistic Regression(    9200/10000): loss= 119.117678469364\n",
      "Losgistic Regression(    9300/10000): loss= 117.500604161754\n",
      "Losgistic Regression(    9400/10000): loss= 116.032020036175\n",
      "Losgistic Regression(    9500/10000): loss= 114.865035740361\n",
      "Losgistic Regression(    9600/10000): loss= 113.981600431506\n",
      "Losgistic Regression(    9700/10000): loss= 113.027420391703\n",
      "Losgistic Regression(    9800/10000): loss= 112.017859793289\n",
      "Losgistic Regression(    9900/10000): loss= 111.079281862841\n",
      "Time for  2th cross validation = 31.5059s\n",
      "Training Accuracy         =  0.922\n",
      "Cross Validation Accuracy = 0.748204\n",
      "Losgistic Regression(       0/10000): loss= 691.709845438396\n",
      "Losgistic Regression(     100/10000): loss= 513.18547899407\n",
      "Losgistic Regression(     200/10000): loss= 438.113671647637\n",
      "Losgistic Regression(     300/10000): loss= 396.486722460064\n",
      "Losgistic Regression(     400/10000): loss= 372.688909419636\n",
      "Losgistic Regression(     500/10000): loss= 356.040516739411\n",
      "Losgistic Regression(     600/10000): loss= 343.186961678383\n",
      "Losgistic Regression(     700/10000): loss= 332.665466661025\n",
      "Losgistic Regression(     800/10000): loss= 323.317988527156\n",
      "Losgistic Regression(     900/10000): loss= 314.582877438278\n",
      "Losgistic Regression(    1000/10000): loss= 306.702187097838\n",
      "Losgistic Regression(    1100/10000): loss= 299.527255364269\n",
      "Losgistic Regression(    1200/10000): loss= 292.975908902264\n",
      "Losgistic Regression(    1300/10000): loss= 286.211918743835\n",
      "Losgistic Regression(    1400/10000): loss= 279.200669232021\n",
      "Losgistic Regression(    1500/10000): loss= 272.623925807372\n",
      "Losgistic Regression(    1600/10000): loss= 266.469079958813\n",
      "Losgistic Regression(    1700/10000): loss= 260.733832187432\n",
      "Losgistic Regression(    1800/10000): loss= 255.396103883889\n",
      "Losgistic Regression(    1900/10000): loss= 249.837583952657\n",
      "Losgistic Regression(    2000/10000): loss= 244.341301547909\n",
      "Losgistic Regression(    2100/10000): loss= 239.139750506023\n",
      "Losgistic Regression(    2200/10000): loss= 233.49077643669\n",
      "Losgistic Regression(    2300/10000): loss= 227.437467811869\n",
      "Losgistic Regression(    2400/10000): loss= 221.163084899379\n",
      "Losgistic Regression(    2500/10000): loss= 214.601795619728\n",
      "Losgistic Regression(    2600/10000): loss= 208.423477743963\n",
      "Losgistic Regression(    2700/10000): loss= 202.500463377329\n",
      "Losgistic Regression(    2800/10000): loss= 196.781726715077\n",
      "Losgistic Regression(    2900/10000): loss= 190.958810697872\n",
      "Losgistic Regression(    3000/10000): loss= 185.300169295536\n",
      "Losgistic Regression(    3100/10000): loss= 179.915206084456\n",
      "Losgistic Regression(    3200/10000): loss= 174.762892499756\n",
      "Losgistic Regression(    3300/10000): loss= 169.783233540911\n",
      "Losgistic Regression(    3400/10000): loss= 165.015634122336\n",
      "Losgistic Regression(    3500/10000): loss= 160.267789875639\n",
      "Losgistic Regression(    3600/10000): loss= 155.580621972829\n",
      "Losgistic Regression(    3700/10000): loss= 150.802953811562\n",
      "Losgistic Regression(    3800/10000): loss= 146.057768143535\n",
      "Losgistic Regression(    3900/10000): loss= 141.27382166979\n",
      "Losgistic Regression(    4000/10000): loss= 136.279485140321\n",
      "Losgistic Regression(    4100/10000): loss= 131.571803053647\n",
      "Losgistic Regression(    4200/10000): loss= 126.851657943605\n",
      "Losgistic Regression(    4300/10000): loss= 122.149052137307\n",
      "Losgistic Regression(    4400/10000): loss= 117.377688351326\n",
      "Losgistic Regression(    4500/10000): loss= 112.52347612831\n",
      "Losgistic Regression(    4600/10000): loss= 107.613416833\n",
      "Losgistic Regression(    4700/10000): loss= 102.87254963162\n",
      "Losgistic Regression(    4800/10000): loss= 98.2552415445198\n",
      "Losgistic Regression(    4900/10000): loss= 93.7868057201029\n",
      "Losgistic Regression(    5000/10000): loss= 89.3375548713202\n",
      "Losgistic Regression(    5100/10000): loss= 85.0412941667404\n",
      "Losgistic Regression(    5200/10000): loss= 80.7789699842131\n",
      "Losgistic Regression(    5300/10000): loss= 76.3464369545916\n",
      "Losgistic Regression(    5400/10000): loss= 71.9125121009147\n",
      "Losgistic Regression(    5500/10000): loss= 67.3281286052122\n",
      "Losgistic Regression(    5600/10000): loss= 62.9944471379609\n",
      "Losgistic Regression(    5700/10000): loss= 58.6915932095603\n",
      "Losgistic Regression(    5800/10000): loss= 54.2785372198886\n",
      "Losgistic Regression(    5900/10000): loss= 49.8406967493743\n",
      "Losgistic Regression(    6000/10000): loss= 45.6238942974568\n",
      "Losgistic Regression(    6100/10000): loss= 41.5572605062966\n",
      "Losgistic Regression(    6200/10000): loss= 37.6163254271631\n",
      "Losgistic Regression(    6300/10000): loss= 33.8748985282999\n",
      "Losgistic Regression(    6400/10000): loss= 29.9976297912094\n",
      "Losgistic Regression(    6500/10000): loss= 26.2618868792632\n",
      "Losgistic Regression(    6600/10000): loss= 22.6950518323911\n",
      "Losgistic Regression(    6700/10000): loss= 19.1566930017364\n",
      "Losgistic Regression(    6800/10000): loss= 15.843822958672\n",
      "Losgistic Regression(    6900/10000): loss= 12.5525668438538\n",
      "Losgistic Regression(    7000/10000): loss= 8.98544332371912\n",
      "Losgistic Regression(    7100/10000): loss= 5.34957674338834\n",
      "Losgistic Regression(    7200/10000): loss= 1.47335847732644\n",
      "Losgistic Regression(    7300/10000): loss=-2.7087280785046\n",
      "Losgistic Regression(    7400/10000): loss=-6.84299947075709\n",
      "Losgistic Regression(    7500/10000): loss=-11.2859759424812\n",
      "Losgistic Regression(    7600/10000): loss=-15.9607462972018\n",
      "Losgistic Regression(    7700/10000): loss=-20.7212797833765\n",
      "Losgistic Regression(    7800/10000): loss=-25.4672935998032\n",
      "Losgistic Regression(    7900/10000): loss=-29.7512786869687\n",
      "Losgistic Regression(    8000/10000): loss=-33.9875382284193\n",
      "Losgistic Regression(    8100/10000): loss=-37.2512441721085\n",
      "Losgistic Regression(    8200/10000): loss=-39.4796369725486\n",
      "Losgistic Regression(    8300/10000): loss=-41.0374088489394\n",
      "Losgistic Regression(    8400/10000): loss=-41.7536967653283\n",
      "Losgistic Regression(    8500/10000): loss=-42.8470681018213\n",
      "Losgistic Regression(    8600/10000): loss=-44.0468623337075\n",
      "Losgistic Regression(    8700/10000): loss=-45.4175186743748\n",
      "Losgistic Regression(    8800/10000): loss=-47.0234043274009\n",
      "Losgistic Regression(    8900/10000): loss=-48.9406767334172\n",
      "Losgistic Regression(    9000/10000): loss=-51.9175479783753\n",
      "Losgistic Regression(    9100/10000): loss=-54.676396557335\n",
      "Losgistic Regression(    9200/10000): loss=-56.7891206724353\n",
      "Losgistic Regression(    9300/10000): loss=-58.6670634836654\n",
      "Losgistic Regression(    9400/10000): loss=-60.4626891002775\n",
      "Losgistic Regression(    9500/10000): loss=-62.0049875649644\n",
      "Losgistic Regression(    9600/10000): loss=-63.9886755512337\n",
      "Losgistic Regression(    9700/10000): loss=-65.344520353103\n",
      "Losgistic Regression(    9800/10000): loss=-66.8168232002767\n",
      "Losgistic Regression(    9900/10000): loss=-68.2687694114034\n",
      "Time for  3th cross validation = 31.4016s\n",
      "Training Accuracy         =  0.935\n",
      "Cross Validation Accuracy = 0.750784\n",
      "Losgistic Regression(       0/10000): loss= 692.023749625848\n",
      "Losgistic Regression(     100/10000): loss= 518.928494467493\n",
      "Losgistic Regression(     200/10000): loss= 444.18496787749\n",
      "Losgistic Regression(     300/10000): loss= 403.236032203022\n",
      "Losgistic Regression(     400/10000): loss= 379.065324591848\n",
      "Losgistic Regression(     500/10000): loss= 362.276603351915\n",
      "Losgistic Regression(     600/10000): loss= 349.234807751679\n",
      "Losgistic Regression(     700/10000): loss= 338.433774929878\n",
      "Losgistic Regression(     800/10000): loss= 329.231638354003\n",
      "Losgistic Regression(     900/10000): loss= 321.248800515243\n",
      "Losgistic Regression(    1000/10000): loss= 314.184461007832\n",
      "Losgistic Regression(    1100/10000): loss= 307.934918336977\n",
      "Losgistic Regression(    1200/10000): loss= 302.375327789198\n",
      "Losgistic Regression(    1300/10000): loss= 297.369263365155\n",
      "Losgistic Regression(    1400/10000): loss= 292.866736443301\n",
      "Losgistic Regression(    1500/10000): loss= 288.772802587705\n",
      "Losgistic Regression(    1600/10000): loss= 285.036322105766\n",
      "Losgistic Regression(    1700/10000): loss= 281.602101197492\n",
      "Losgistic Regression(    1800/10000): loss= 278.439464553167\n",
      "Losgistic Regression(    1900/10000): loss= 275.519669221577\n",
      "Losgistic Regression(    2000/10000): loss= 271.967276302953\n",
      "Losgistic Regression(    2100/10000): loss= 268.549102438679\n",
      "Losgistic Regression(    2200/10000): loss= 265.28851041105\n",
      "Losgistic Regression(    2300/10000): loss= 262.155985051147\n",
      "Losgistic Regression(    2400/10000): loss= 259.162368808805\n",
      "Losgistic Regression(    2500/10000): loss= 256.300545744645\n",
      "Losgistic Regression(    2600/10000): loss= 253.570888369065\n",
      "Losgistic Regression(    2700/10000): loss= 250.950933826793\n",
      "Losgistic Regression(    2800/10000): loss= 248.407177271458\n",
      "Losgistic Regression(    2900/10000): loss= 245.950246853446\n",
      "Losgistic Regression(    3000/10000): loss= 243.599581919308\n",
      "Losgistic Regression(    3100/10000): loss= 241.352823545141\n",
      "Losgistic Regression(    3200/10000): loss= 239.214162508565\n",
      "Losgistic Regression(    3300/10000): loss= 237.026010812861\n",
      "Losgistic Regression(    3400/10000): loss= 234.494882139032\n",
      "Losgistic Regression(    3500/10000): loss= 232.089702239787\n",
      "Losgistic Regression(    3600/10000): loss= 229.764671954467\n",
      "Losgistic Regression(    3700/10000): loss= 227.502316594692\n",
      "Losgistic Regression(    3800/10000): loss= 225.318124817185\n",
      "Losgistic Regression(    3900/10000): loss= 223.327532873127\n",
      "Losgistic Regression(    4000/10000): loss= 221.418873949319\n",
      "Losgistic Regression(    4100/10000): loss= 219.702456772719\n",
      "Losgistic Regression(    4200/10000): loss= 218.057333368323\n",
      "Losgistic Regression(    4300/10000): loss= 216.516286873637\n",
      "Losgistic Regression(    4400/10000): loss= 214.747303773547\n",
      "Losgistic Regression(    4500/10000): loss= 212.970927237334\n",
      "Losgistic Regression(    4600/10000): loss= 211.297102282578\n",
      "Losgistic Regression(    4700/10000): loss= 209.817657149832\n",
      "Losgistic Regression(    4800/10000): loss= 208.397495197035\n",
      "Losgistic Regression(    4900/10000): loss= 207.073660761524\n",
      "Losgistic Regression(    5000/10000): loss= 205.630844985425\n",
      "Losgistic Regression(    5100/10000): loss= 204.110924975235\n",
      "Losgistic Regression(    5200/10000): loss= 202.759685164884\n",
      "Losgistic Regression(    5300/10000): loss= 201.501649462901\n",
      "Losgistic Regression(    5400/10000): loss= 200.164871160488\n",
      "Losgistic Regression(    5500/10000): loss= 198.711158643346\n",
      "Losgistic Regression(    5600/10000): loss= 197.370975092424\n",
      "Losgistic Regression(    5700/10000): loss= 196.065554713096\n",
      "Losgistic Regression(    5800/10000): loss= 194.742156167595\n",
      "Losgistic Regression(    5900/10000): loss= 193.616123385567\n",
      "Losgistic Regression(    6000/10000): loss= 192.437334787434\n",
      "Losgistic Regression(    6100/10000): loss= 191.308683998825\n",
      "Losgistic Regression(    6200/10000): loss= 190.182432724491\n",
      "Losgistic Regression(    6300/10000): loss= 188.991833005858\n",
      "Losgistic Regression(    6400/10000): loss= 187.970934193364\n",
      "Losgistic Regression(    6500/10000): loss= 186.961592293149\n",
      "Losgistic Regression(    6600/10000): loss= 185.915084307962\n",
      "Losgistic Regression(    6700/10000): loss= 185.030358004297\n",
      "Losgistic Regression(    6800/10000): loss= 184.033377022178\n",
      "Losgistic Regression(    6900/10000): loss= 183.021864919481\n",
      "Losgistic Regression(    7000/10000): loss= 182.084916324606\n",
      "Losgistic Regression(    7100/10000): loss= 181.153757251248\n",
      "Losgistic Regression(    7200/10000): loss= 180.156670372923\n",
      "Losgistic Regression(    7300/10000): loss= 179.104218018704\n",
      "Losgistic Regression(    7400/10000): loss= 178.167643842494\n",
      "Losgistic Regression(    7500/10000): loss= 177.140299774068\n",
      "Losgistic Regression(    7600/10000): loss= 176.252853321551\n",
      "Losgistic Regression(    7700/10000): loss= 175.510689716845\n",
      "Losgistic Regression(    7800/10000): loss= 174.794587465603\n",
      "Losgistic Regression(    7900/10000): loss= 173.79988388115\n",
      "Losgistic Regression(    8000/10000): loss= 172.805467756969\n",
      "Losgistic Regression(    8100/10000): loss= 171.866104911566\n",
      "Losgistic Regression(    8200/10000): loss= 171.089220037254\n",
      "Losgistic Regression(    8300/10000): loss= 170.557563709628\n",
      "Losgistic Regression(    8400/10000): loss= 169.646408906589\n",
      "Losgistic Regression(    8500/10000): loss= 168.944878934117\n",
      "Losgistic Regression(    8600/10000): loss= 168.494894094389\n",
      "Losgistic Regression(    8700/10000): loss= 167.811703837284\n",
      "Losgistic Regression(    8800/10000): loss= 167.719813111308\n",
      "Losgistic Regression(    8900/10000): loss= 167.72143420652\n",
      "Totoal number of iterations =  8900\n",
      "Loss                        =  167.721434207\n",
      "Time for  4th cross validation = 28.2543s\n",
      "Training Accuracy         =  0.932\n",
      "Cross Validation Accuracy = 0.75098\n",
      "*************** ([0.93100000000000005, 0.94199999999999995, 0.92200000000000004, 0.93500000000000005, 0.93200000000000005], [0.75012000000000001, 0.76641599999999999, 0.74820399999999998, 0.75078400000000001, 0.75097999999999998])\n",
      "Losgistic Regression(       0/10000): loss= 692.032255406233\n",
      "Losgistic Regression(     100/10000): loss= 534.231981715536\n",
      "Losgistic Regression(     200/10000): loss= 458.488804914881\n",
      "Losgistic Regression(     300/10000): loss= 418.478337083546\n",
      "Losgistic Regression(     400/10000): loss= 396.693045577261\n",
      "Losgistic Regression(     500/10000): loss= 382.445401825215\n",
      "Losgistic Regression(     600/10000): loss= 372.112876291553\n",
      "Losgistic Regression(     700/10000): loss= 364.354636246018\n",
      "Losgistic Regression(     800/10000): loss= 358.323726818357\n",
      "Losgistic Regression(     900/10000): loss= 353.555137291435\n",
      "Losgistic Regression(    1000/10000): loss= 349.780587243687\n",
      "Losgistic Regression(    1100/10000): loss= 346.774118892124\n",
      "Losgistic Regression(    1200/10000): loss= 344.298331505\n",
      "Losgistic Regression(    1300/10000): loss= 342.178251339044\n",
      "Losgistic Regression(    1400/10000): loss= 340.366278787606\n",
      "Losgistic Regression(    1500/10000): loss= 338.720331424221\n",
      "Losgistic Regression(    1600/10000): loss= 337.246947127112\n",
      "Losgistic Regression(    1700/10000): loss= 335.948275347584\n",
      "Losgistic Regression(    1800/10000): loss= 334.801292577518\n",
      "Losgistic Regression(    1900/10000): loss= 333.820134786438\n",
      "Losgistic Regression(    2000/10000): loss= 332.886612435506\n",
      "Losgistic Regression(    2100/10000): loss= 332.045121611211\n",
      "Losgistic Regression(    2200/10000): loss= 331.304012439164\n",
      "Losgistic Regression(    2300/10000): loss= 330.585180002578\n",
      "Losgistic Regression(    2400/10000): loss= 329.991722128055\n",
      "Losgistic Regression(    2500/10000): loss= 329.446080937098\n",
      "Losgistic Regression(    2600/10000): loss= 328.889100909345\n",
      "Losgistic Regression(    2700/10000): loss= 328.348734000623\n",
      "Losgistic Regression(    2800/10000): loss= 327.872028133184\n",
      "Losgistic Regression(    2900/10000): loss= 327.405613395757\n",
      "Losgistic Regression(    3000/10000): loss= 326.956811068551\n",
      "Losgistic Regression(    3100/10000): loss= 326.531931512069\n",
      "Losgistic Regression(    3200/10000): loss= 326.124064523627\n",
      "Losgistic Regression(    3300/10000): loss= 325.753781257206\n",
      "Losgistic Regression(    3400/10000): loss= 325.346544547721\n",
      "Losgistic Regression(    3500/10000): loss= 324.998576366831\n",
      "Losgistic Regression(    3600/10000): loss= 324.719537239694\n",
      "Losgistic Regression(    3700/10000): loss= 324.449438539868\n",
      "Losgistic Regression(    3800/10000): loss= 324.166473213142\n",
      "Losgistic Regression(    3900/10000): loss= 323.873018017512\n",
      "Losgistic Regression(    4000/10000): loss= 323.583305073471\n",
      "Losgistic Regression(    4100/10000): loss= 323.369294717379\n",
      "Losgistic Regression(    4200/10000): loss= 323.183430306701\n",
      "Losgistic Regression(    4300/10000): loss= 322.945349407153\n",
      "Losgistic Regression(    4400/10000): loss= 322.724370155478\n",
      "Losgistic Regression(    4500/10000): loss= 322.524371154427\n",
      "Losgistic Regression(    4600/10000): loss= 322.351729658125\n",
      "Losgistic Regression(    4700/10000): loss= 322.2340812345\n",
      "Losgistic Regression(    4800/10000): loss= 322.092547102525\n",
      "Losgistic Regression(    4900/10000): loss= 321.934433533132\n",
      "Losgistic Regression(    5000/10000): loss= 321.79473653035\n",
      "Losgistic Regression(    5100/10000): loss= 321.660557266327\n",
      "Losgistic Regression(    5200/10000): loss= 321.549828086128\n",
      "Losgistic Regression(    5300/10000): loss= 321.398814487574\n",
      "Losgistic Regression(    5400/10000): loss= 321.270649961894\n",
      "Losgistic Regression(    5500/10000): loss= 321.170820868545\n",
      "Losgistic Regression(    5600/10000): loss= 321.065411999317\n",
      "Losgistic Regression(    5700/10000): loss= 320.932758288263\n",
      "Losgistic Regression(    5800/10000): loss= 320.835011511102\n",
      "Losgistic Regression(    5900/10000): loss= 320.738278751875\n",
      "Losgistic Regression(    6000/10000): loss= 320.652946713557\n",
      "Losgistic Regression(    6100/10000): loss= 320.572703955006\n",
      "Losgistic Regression(    6200/10000): loss= 320.548849710593\n",
      "Losgistic Regression(    6300/10000): loss= 320.514065028909\n",
      "Losgistic Regression(    6400/10000): loss= 320.477423659802\n",
      "Losgistic Regression(    6500/10000): loss= 320.450901550792\n",
      "Losgistic Regression(    6600/10000): loss= 320.423503209808\n",
      "Losgistic Regression(    6700/10000): loss= 320.403268819428\n",
      "Losgistic Regression(    6800/10000): loss= 320.381326445407\n",
      "Losgistic Regression(    6900/10000): loss= 320.360618118669\n",
      "Losgistic Regression(    7000/10000): loss= 320.341115726658\n",
      "Losgistic Regression(    7100/10000): loss= 320.320785081273\n",
      "Losgistic Regression(    7200/10000): loss= 320.302116009261\n",
      "Losgistic Regression(    7300/10000): loss= 320.29062290848\n",
      "Losgistic Regression(    7400/10000): loss= 320.27157334695\n",
      "Losgistic Regression(    7500/10000): loss= 320.258379929982\n",
      "Losgistic Regression(    7600/10000): loss= 320.241407066846\n",
      "Losgistic Regression(    7700/10000): loss= 320.225034486024\n",
      "Losgistic Regression(    7800/10000): loss= 320.207657293703\n",
      "Losgistic Regression(    7900/10000): loss= 320.191797921006\n",
      "Losgistic Regression(    8000/10000): loss= 320.174479989311\n",
      "Losgistic Regression(    8100/10000): loss= 320.157676201824\n",
      "Losgistic Regression(    8200/10000): loss= 320.13988371059\n",
      "Losgistic Regression(    8300/10000): loss= 320.122773543295\n",
      "Losgistic Regression(    8400/10000): loss= 320.104293927842\n",
      "Losgistic Regression(    8500/10000): loss= 320.08636221388\n",
      "Losgistic Regression(    8600/10000): loss= 320.067481973039\n",
      "Losgistic Regression(    8700/10000): loss= 320.048214206586\n",
      "Losgistic Regression(    8800/10000): loss= 320.028579624796\n",
      "Losgistic Regression(    8900/10000): loss= 320.008857481932\n",
      "Losgistic Regression(    9000/10000): loss= 319.988929708572\n",
      "Losgistic Regression(    9100/10000): loss= 319.968946735816\n",
      "Losgistic Regression(    9200/10000): loss= 319.948955720924\n",
      "Losgistic Regression(    9300/10000): loss= 319.92814064701\n",
      "Losgistic Regression(    9400/10000): loss= 319.907327359879\n",
      "Losgistic Regression(    9500/10000): loss= 319.886137743761\n",
      "Losgistic Regression(    9600/10000): loss= 319.865051759649\n",
      "Losgistic Regression(    9700/10000): loss= 319.843602220721\n",
      "Losgistic Regression(    9800/10000): loss= 319.821965048144\n",
      "Losgistic Regression(    9900/10000): loss= 319.800250231441\n",
      "Time for  0th cross validation = 31.7513s\n",
      "Training Accuracy         =  0.896\n",
      "Cross Validation Accuracy = 0.769864\n",
      "Losgistic Regression(       0/10000): loss= 691.974722846127\n",
      "Losgistic Regression(     100/10000): loss= 522.475039229994\n",
      "Losgistic Regression(     200/10000): loss= 444.448789843303\n",
      "Losgistic Regression(     300/10000): loss= 401.772520281694\n",
      "Losgistic Regression(     400/10000): loss= 379.335216004559\n",
      "Losgistic Regression(     500/10000): loss= 365.391333549916\n",
      "Losgistic Regression(     600/10000): loss= 355.358882807224\n",
      "Losgistic Regression(     700/10000): loss= 347.573004839934\n",
      "Losgistic Regression(     800/10000): loss= 341.368582655993\n",
      "Losgistic Regression(     900/10000): loss= 336.219379890494\n",
      "Losgistic Regression(    1000/10000): loss= 331.795088411237\n",
      "Losgistic Regression(    1100/10000): loss= 327.957625917355\n",
      "Losgistic Regression(    1200/10000): loss= 324.532214515065\n",
      "Losgistic Regression(    1300/10000): loss= 321.532086571185\n",
      "Losgistic Regression(    1400/10000): loss= 318.944312794909\n",
      "Losgistic Regression(    1500/10000): loss= 316.715174713799\n",
      "Losgistic Regression(    1600/10000): loss= 314.777949765123\n",
      "Losgistic Regression(    1700/10000): loss= 313.155143320317\n",
      "Losgistic Regression(    1800/10000): loss= 311.680652570245\n",
      "Losgistic Regression(    1900/10000): loss= 310.356242967633\n",
      "Losgistic Regression(    2000/10000): loss= 309.161373395717\n",
      "Losgistic Regression(    2100/10000): loss= 307.803462886884\n",
      "Losgistic Regression(    2200/10000): loss= 306.56367261448\n",
      "Losgistic Regression(    2300/10000): loss= 305.365756402516\n",
      "Losgistic Regression(    2400/10000): loss= 304.453981583725\n",
      "Losgistic Regression(    2500/10000): loss= 303.57811996477\n",
      "Losgistic Regression(    2600/10000): loss= 302.710486793369\n",
      "Losgistic Regression(    2700/10000): loss= 301.85373679806\n",
      "Losgistic Regression(    2800/10000): loss= 301.066744875463\n",
      "Losgistic Regression(    2900/10000): loss= 300.429128625458\n",
      "Losgistic Regression(    3000/10000): loss= 299.921645216489\n",
      "Losgistic Regression(    3100/10000): loss= 299.49384677251\n",
      "Losgistic Regression(    3200/10000): loss= 299.138149685189\n",
      "Losgistic Regression(    3300/10000): loss= 298.664472162587\n",
      "Losgistic Regression(    3400/10000): loss= 298.167879511231\n",
      "Losgistic Regression(    3500/10000): loss= 297.753877139329\n",
      "Losgistic Regression(    3600/10000): loss= 297.507378267884\n",
      "Losgistic Regression(    3700/10000): loss= 297.406722172726\n",
      "Losgistic Regression(    3800/10000): loss= 297.238118464946\n",
      "Losgistic Regression(    3900/10000): loss= 297.017473567856\n",
      "Losgistic Regression(    4000/10000): loss= 296.82295089888\n",
      "Losgistic Regression(    4100/10000): loss= 296.581836664892\n",
      "Losgistic Regression(    4200/10000): loss= 296.328487653898\n",
      "Losgistic Regression(    4300/10000): loss= 296.130194360193\n",
      "Losgistic Regression(    4400/10000): loss= 295.920015489599\n",
      "Losgistic Regression(    4500/10000): loss= 295.842270171205\n",
      "Losgistic Regression(    4600/10000): loss= 295.724756472072\n",
      "Losgistic Regression(    4700/10000): loss= 295.463648328598\n",
      "Losgistic Regression(    4800/10000): loss= 295.204644392223\n",
      "Losgistic Regression(    4900/10000): loss= 294.827259765093\n",
      "Losgistic Regression(    5000/10000): loss= 294.683734782165\n",
      "Losgistic Regression(    5100/10000): loss= 294.462799205173\n",
      "Losgistic Regression(    5200/10000): loss= 294.250088433225\n",
      "Losgistic Regression(    5300/10000): loss= 294.178423289862\n",
      "Losgistic Regression(    5400/10000): loss= 294.09683835401\n",
      "Losgistic Regression(    5500/10000): loss= 294.096617457925\n",
      "Totoal number of iterations =  5500\n",
      "Loss                        =  294.096617458\n",
      "Time for  1th cross validation = 17.3121s\n",
      "Training Accuracy         =  0.917\n",
      "Cross Validation Accuracy = 0.774824\n",
      "Losgistic Regression(       0/10000): loss= 691.581777440834\n",
      "Losgistic Regression(     100/10000): loss= 525.224443542299\n",
      "Losgistic Regression(     200/10000): loss= 460.944340249038\n",
      "Losgistic Regression(     300/10000): loss= 426.023657659818\n",
      "Losgistic Regression(     400/10000): loss= 406.901976707294\n",
      "Losgistic Regression(     500/10000): loss= 394.821526485095\n",
      "Losgistic Regression(     600/10000): loss= 386.499374689415\n",
      "Losgistic Regression(     700/10000): loss= 380.501600263081\n",
      "Losgistic Regression(     800/10000): loss= 375.939110786759\n",
      "Losgistic Regression(     900/10000): loss= 372.338331034583\n",
      "Losgistic Regression(    1000/10000): loss= 369.289034619688\n",
      "Losgistic Regression(    1100/10000): loss= 366.654419584399\n",
      "Losgistic Regression(    1200/10000): loss= 364.279787160653\n",
      "Losgistic Regression(    1300/10000): loss= 362.175629747179\n",
      "Losgistic Regression(    1400/10000): loss= 360.245666212581\n",
      "Losgistic Regression(    1500/10000): loss= 358.486087572493\n",
      "Losgistic Regression(    1600/10000): loss= 356.872428285404\n",
      "Losgistic Regression(    1700/10000): loss= 355.360155688533\n",
      "Losgistic Regression(    1800/10000): loss= 353.949468153265\n",
      "Losgistic Regression(    1900/10000): loss= 352.614253787241\n",
      "Losgistic Regression(    2000/10000): loss= 351.365129562728\n",
      "Losgistic Regression(    2100/10000): loss= 350.206210387538\n",
      "Losgistic Regression(    2200/10000): loss= 349.173055960688\n",
      "Losgistic Regression(    2300/10000): loss= 348.222919846499\n",
      "Losgistic Regression(    2400/10000): loss= 347.309102843668\n",
      "Losgistic Regression(    2500/10000): loss= 346.48161112424\n",
      "Losgistic Regression(    2600/10000): loss= 345.692072913433\n",
      "Losgistic Regression(    2700/10000): loss= 345.011878695374\n",
      "Losgistic Regression(    2800/10000): loss= 344.359728564301\n",
      "Losgistic Regression(    2900/10000): loss= 343.755548707254\n",
      "Losgistic Regression(    3000/10000): loss= 343.196094216944\n",
      "Losgistic Regression(    3100/10000): loss= 342.704716746405\n",
      "Losgistic Regression(    3200/10000): loss= 342.238528246215\n",
      "Losgistic Regression(    3300/10000): loss= 341.795345662318\n",
      "Losgistic Regression(    3400/10000): loss= 341.405224767507\n",
      "Losgistic Regression(    3500/10000): loss= 341.103696253479\n",
      "Losgistic Regression(    3600/10000): loss= 340.817895509555\n",
      "Losgistic Regression(    3700/10000): loss= 340.528196953701\n",
      "Losgistic Regression(    3800/10000): loss= 340.244359998414\n",
      "Losgistic Regression(    3900/10000): loss= 339.984392269993\n",
      "Losgistic Regression(    4000/10000): loss= 339.809054347403\n",
      "Losgistic Regression(    4100/10000): loss= 339.651968773185\n",
      "Losgistic Regression(    4200/10000): loss= 339.480208844479\n",
      "Losgistic Regression(    4300/10000): loss= 339.338058261598\n",
      "Losgistic Regression(    4400/10000): loss= 339.181513039707\n",
      "Losgistic Regression(    4500/10000): loss= 339.021452916946\n",
      "Losgistic Regression(    4600/10000): loss= 338.876339453282\n",
      "Losgistic Regression(    4700/10000): loss= 338.750540306106\n",
      "Losgistic Regression(    4800/10000): loss= 338.618071592883\n",
      "Losgistic Regression(    4900/10000): loss= 338.517049180327\n",
      "Losgistic Regression(    5000/10000): loss= 338.422909063392\n",
      "Losgistic Regression(    5100/10000): loss= 338.337089290885\n",
      "Losgistic Regression(    5200/10000): loss= 338.2680424741\n",
      "Losgistic Regression(    5300/10000): loss= 338.194593402026\n",
      "Losgistic Regression(    5400/10000): loss= 338.102071109984\n",
      "Losgistic Regression(    5500/10000): loss= 337.985450646245\n",
      "Losgistic Regression(    5600/10000): loss= 337.931499817028\n",
      "Losgistic Regression(    5700/10000): loss= 337.878108015133\n",
      "Losgistic Regression(    5800/10000): loss= 337.809112154259\n",
      "Losgistic Regression(    5900/10000): loss= 337.742963631278\n",
      "Losgistic Regression(    6000/10000): loss= 337.678940780909\n",
      "Losgistic Regression(    6100/10000): loss= 337.476823550743\n",
      "Losgistic Regression(    6200/10000): loss= 337.31698202061\n",
      "Losgistic Regression(    6300/10000): loss= 337.112370170368\n",
      "Losgistic Regression(    6400/10000): loss= 336.900985416512\n",
      "Losgistic Regression(    6500/10000): loss= 336.809119064207\n",
      "Losgistic Regression(    6600/10000): loss= 336.670020548582\n",
      "Losgistic Regression(    6700/10000): loss= 336.579794918005\n",
      "Losgistic Regression(    6800/10000): loss= 336.57965062587\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  336.579650626\n",
      "Time for  2th cross validation = 21.1029s\n",
      "Training Accuracy         =  0.889\n",
      "Cross Validation Accuracy = 0.761128\n",
      "Losgistic Regression(       0/10000): loss= 691.718619809344\n",
      "Losgistic Regression(     100/10000): loss= 516.354540217655\n",
      "Losgistic Regression(     200/10000): loss= 444.491644690766\n",
      "Losgistic Regression(     300/10000): loss= 406.522698785679\n",
      "Losgistic Regression(     400/10000): loss= 386.047190091791\n",
      "Losgistic Regression(     500/10000): loss= 372.390509889713\n",
      "Losgistic Regression(     600/10000): loss= 362.384037147815\n",
      "Losgistic Regression(     700/10000): loss= 354.587639385707\n",
      "Losgistic Regression(     800/10000): loss= 348.317443595209\n",
      "Losgistic Regression(     900/10000): loss= 343.203245064006\n",
      "Losgistic Regression(    1000/10000): loss= 338.651792698438\n",
      "Losgistic Regression(    1100/10000): loss= 333.657266833092\n",
      "Losgistic Regression(    1200/10000): loss= 329.47024948725\n",
      "Losgistic Regression(    1300/10000): loss= 326.041457335334\n",
      "Losgistic Regression(    1400/10000): loss= 322.990937739946\n",
      "Losgistic Regression(    1500/10000): loss= 320.082375057625\n",
      "Losgistic Regression(    1600/10000): loss= 317.260937673483\n",
      "Losgistic Regression(    1700/10000): loss= 313.901367177045\n",
      "Losgistic Regression(    1800/10000): loss= 310.627542512757\n",
      "Losgistic Regression(    1900/10000): loss= 307.772790763341\n",
      "Losgistic Regression(    2000/10000): loss= 305.493425249825\n",
      "Losgistic Regression(    2100/10000): loss= 303.154430781117\n",
      "Losgistic Regression(    2200/10000): loss= 300.774909152536\n",
      "Losgistic Regression(    2300/10000): loss= 298.708089041311\n",
      "Losgistic Regression(    2400/10000): loss= 296.95081412536\n",
      "Losgistic Regression(    2500/10000): loss= 295.356253303284\n",
      "Losgistic Regression(    2600/10000): loss= 293.911615130704\n",
      "Losgistic Regression(    2700/10000): loss= 292.675754611072\n",
      "Losgistic Regression(    2800/10000): loss= 291.429001097741\n",
      "Losgistic Regression(    2900/10000): loss= 289.927399496676\n",
      "Losgistic Regression(    3000/10000): loss= 288.546364328322\n",
      "Losgistic Regression(    3100/10000): loss= 287.119038180163\n",
      "Losgistic Regression(    3200/10000): loss= 285.790019774212\n",
      "Losgistic Regression(    3300/10000): loss= 284.871861383959\n",
      "Losgistic Regression(    3400/10000): loss= 284.291199259497\n",
      "Losgistic Regression(    3500/10000): loss= 283.830335148482\n",
      "Losgistic Regression(    3600/10000): loss= 282.989068721755\n",
      "Losgistic Regression(    3700/10000): loss= 281.791463850898\n",
      "Losgistic Regression(    3800/10000): loss= 280.713385882978\n",
      "Losgistic Regression(    3900/10000): loss= 279.794972678856\n",
      "Losgistic Regression(    4000/10000): loss= 279.529346056696\n",
      "Losgistic Regression(    4100/10000): loss= 279.471295414668\n",
      "Losgistic Regression(    4200/10000): loss= 279.474435624202\n",
      "Totoal number of iterations =  4200\n",
      "Loss                        =  279.474435624\n",
      "Time for  3th cross validation = 13.0744s\n",
      "Training Accuracy         =  0.898\n",
      "Cross Validation Accuracy = 0.769624\n",
      "Losgistic Regression(       0/10000): loss= 692.031729449412\n",
      "Losgistic Regression(     100/10000): loss= 522.026768029092\n",
      "Losgistic Regression(     200/10000): loss= 450.872163720412\n",
      "Losgistic Regression(     300/10000): loss= 413.320407782111\n",
      "Losgistic Regression(     400/10000): loss= 392.318139623993\n",
      "Losgistic Regression(     500/10000): loss= 378.45151255742\n",
      "Losgistic Regression(     600/10000): loss= 368.134237852003\n",
      "Losgistic Regression(     700/10000): loss= 359.964325864828\n",
      "Losgistic Regression(     800/10000): loss= 353.278782171325\n",
      "Losgistic Regression(     900/10000): loss= 347.688750405911\n",
      "Losgistic Regression(    1000/10000): loss= 343.015547533504\n",
      "Losgistic Regression(    1100/10000): loss= 339.069174687845\n",
      "Losgistic Regression(    1200/10000): loss= 335.654989855544\n",
      "Losgistic Regression(    1300/10000): loss= 332.783764953506\n",
      "Losgistic Regression(    1400/10000): loss= 330.260190485618\n",
      "Losgistic Regression(    1500/10000): loss= 328.010502983914\n",
      "Losgistic Regression(    1600/10000): loss= 326.062116076466\n",
      "Losgistic Regression(    1700/10000): loss= 324.340001401824\n",
      "Losgistic Regression(    1800/10000): loss= 322.820968758989\n",
      "Losgistic Regression(    1900/10000): loss= 321.440824935824\n",
      "Losgistic Regression(    2000/10000): loss= 320.304616731818\n",
      "Losgistic Regression(    2100/10000): loss= 319.24563502622\n",
      "Losgistic Regression(    2200/10000): loss= 318.269096500668\n",
      "Losgistic Regression(    2300/10000): loss= 317.397716736142\n",
      "Losgistic Regression(    2400/10000): loss= 316.621562037852\n",
      "Losgistic Regression(    2500/10000): loss= 315.895400889335\n",
      "Losgistic Regression(    2600/10000): loss= 315.264600161863\n",
      "Losgistic Regression(    2700/10000): loss= 314.700343766577\n",
      "Losgistic Regression(    2800/10000): loss= 314.204789468267\n",
      "Losgistic Regression(    2900/10000): loss= 313.759899902487\n",
      "Losgistic Regression(    3000/10000): loss= 313.34676073884\n",
      "Losgistic Regression(    3100/10000): loss= 312.900612752859\n",
      "Losgistic Regression(    3200/10000): loss= 312.188557767265\n",
      "Losgistic Regression(    3300/10000): loss= 311.412905663438\n",
      "Losgistic Regression(    3400/10000): loss= 310.788570643796\n",
      "Losgistic Regression(    3500/10000): loss= 310.081534982376\n",
      "Losgistic Regression(    3600/10000): loss= 309.451795016364\n",
      "Losgistic Regression(    3700/10000): loss= 308.910717688913\n",
      "Losgistic Regression(    3800/10000): loss= 308.586600642724\n",
      "Losgistic Regression(    3900/10000): loss= 308.285848965814\n",
      "Losgistic Regression(    4000/10000): loss= 307.978860058173\n",
      "Losgistic Regression(    4100/10000): loss= 307.57997489017\n",
      "Losgistic Regression(    4200/10000): loss= 307.140155936106\n",
      "Losgistic Regression(    4300/10000): loss= 306.702765751222\n",
      "Losgistic Regression(    4400/10000): loss= 306.410602990473\n",
      "Losgistic Regression(    4500/10000): loss= 306.120892210608\n",
      "Losgistic Regression(    4600/10000): loss= 305.89322179169\n",
      "Losgistic Regression(    4700/10000): loss= 305.733821426002\n",
      "Losgistic Regression(    4800/10000): loss= 305.509614382736\n",
      "Losgistic Regression(    4900/10000): loss= 305.411771921943\n",
      "Losgistic Regression(    5000/10000): loss= 305.304661002174\n",
      "Losgistic Regression(    5100/10000): loss= 305.202187688542\n",
      "Losgistic Regression(    5200/10000): loss= 305.113712191569\n",
      "Losgistic Regression(    5300/10000): loss= 304.98343786396\n",
      "Losgistic Regression(    5400/10000): loss= 304.899836929171\n",
      "Losgistic Regression(    5500/10000): loss= 304.810155773047\n",
      "Losgistic Regression(    5600/10000): loss= 304.71543668038\n",
      "Losgistic Regression(    5700/10000): loss= 304.677291186967\n",
      "Losgistic Regression(    5800/10000): loss= 304.672483927011\n",
      "Losgistic Regression(    5900/10000): loss= 304.672131233343\n",
      "Totoal number of iterations =  5900\n",
      "Loss                        =  304.672131233\n",
      "Time for  4th cross validation = 18.2783s\n",
      "Training Accuracy         =  0.907\n",
      "Cross Validation Accuracy = 0.762292\n",
      "*************** ([0.89600000000000002, 0.91700000000000004, 0.88900000000000001, 0.89800000000000002, 0.90700000000000003], [0.76986399999999999, 0.77482399999999996, 0.76112800000000003, 0.76962399999999997, 0.76229199999999997])\n",
      "Losgistic Regression(       0/10000): loss= 692.05912592773\n",
      "Losgistic Regression(     100/10000): loss= 544.269085679418\n",
      "Losgistic Regression(     200/10000): loss= 479.886243137128\n",
      "Losgistic Regression(     300/10000): loss= 448.78540868959\n",
      "Losgistic Regression(     400/10000): loss= 434.130939291728\n",
      "Losgistic Regression(     500/10000): loss= 425.574507990988\n",
      "Losgistic Regression(     600/10000): loss= 420.046066273545\n",
      "Losgistic Regression(     700/10000): loss= 416.307375744936\n",
      "Losgistic Regression(     800/10000): loss= 413.662085641974\n",
      "Losgistic Regression(     900/10000): loss= 411.816156964451\n",
      "Losgistic Regression(    1000/10000): loss= 410.257328707955\n",
      "Losgistic Regression(    1100/10000): loss= 409.205400085849\n",
      "Losgistic Regression(    1200/10000): loss= 408.24751316117\n",
      "Losgistic Regression(    1300/10000): loss= 407.518082692345\n",
      "Losgistic Regression(    1400/10000): loss= 406.896527414479\n",
      "Losgistic Regression(    1500/10000): loss= 406.312755026396\n",
      "Losgistic Regression(    1600/10000): loss= 405.788084159281\n",
      "Losgistic Regression(    1700/10000): loss= 405.302491663629\n",
      "Losgistic Regression(    1800/10000): loss= 404.864270652278\n",
      "Losgistic Regression(    1900/10000): loss= 404.495396301742\n",
      "Losgistic Regression(    2000/10000): loss= 404.247048459406\n",
      "Losgistic Regression(    2100/10000): loss= 403.916441220213\n",
      "Losgistic Regression(    2200/10000): loss= 403.694900119069\n",
      "Losgistic Regression(    2300/10000): loss= 403.475200977213\n",
      "Losgistic Regression(    2400/10000): loss= 403.233625572236\n",
      "Losgistic Regression(    2500/10000): loss= 403.060754638696\n",
      "Losgistic Regression(    2600/10000): loss= 402.880953280704\n",
      "Losgistic Regression(    2700/10000): loss= 402.830536614308\n",
      "Losgistic Regression(    2800/10000): loss= 402.791301771326\n",
      "Losgistic Regression(    2900/10000): loss= 402.762679954243\n",
      "Losgistic Regression(    3000/10000): loss= 402.735191164219\n",
      "Losgistic Regression(    3100/10000): loss= 402.704952075042\n",
      "Losgistic Regression(    3200/10000): loss= 402.675980832038\n",
      "Losgistic Regression(    3300/10000): loss= 402.645138142795\n",
      "Losgistic Regression(    3400/10000): loss= 402.612142689156\n",
      "Losgistic Regression(    3500/10000): loss= 402.576609826214\n",
      "Losgistic Regression(    3600/10000): loss= 402.538628357762\n",
      "Losgistic Regression(    3700/10000): loss= 402.499070326654\n",
      "Losgistic Regression(    3800/10000): loss= 402.457069714581\n",
      "Losgistic Regression(    3900/10000): loss= 402.412484150862\n",
      "Losgistic Regression(    4000/10000): loss= 402.367934382038\n",
      "Losgistic Regression(    4100/10000): loss= 402.327605733955\n",
      "Losgistic Regression(    4200/10000): loss= 402.283081713408\n",
      "Losgistic Regression(    4300/10000): loss= 402.242187840088\n",
      "Losgistic Regression(    4400/10000): loss= 402.199293455295\n",
      "Losgistic Regression(    4500/10000): loss= 402.154163473313\n",
      "Losgistic Regression(    4600/10000): loss= 402.109615544021\n",
      "Losgistic Regression(    4700/10000): loss= 402.064981927219\n",
      "Losgistic Regression(    4800/10000): loss= 402.020240701099\n",
      "Losgistic Regression(    4900/10000): loss= 401.974411318971\n",
      "Losgistic Regression(    5000/10000): loss= 401.932447449959\n",
      "Losgistic Regression(    5100/10000): loss= 401.932290855958\n",
      "Totoal number of iterations =  5100\n",
      "Loss                        =  401.932290856\n",
      "Time for  0th cross validation = 15.9317s\n",
      "Training Accuracy         =  0.858\n",
      "Cross Validation Accuracy = 0.77658\n",
      "Losgistic Regression(       0/10000): loss= 692.002113291426\n",
      "Losgistic Regression(     100/10000): loss= 533.045927475285\n",
      "Losgistic Regression(     200/10000): loss= 465.393943023057\n",
      "Losgistic Regression(     300/10000): loss= 432.082809838541\n",
      "Losgistic Regression(     400/10000): loss= 417.568565506157\n",
      "Losgistic Regression(     500/10000): loss= 410.113738887878\n",
      "Losgistic Regression(     600/10000): loss= 405.358687418833\n",
      "Losgistic Regression(     700/10000): loss= 401.835197561385\n",
      "Losgistic Regression(     800/10000): loss= 399.114220634845\n",
      "Losgistic Regression(     900/10000): loss= 396.898571401568\n",
      "Losgistic Regression(    1000/10000): loss= 395.133315999526\n",
      "Losgistic Regression(    1100/10000): loss= 393.73634708042\n",
      "Losgistic Regression(    1200/10000): loss= 392.591885843016\n",
      "Losgistic Regression(    1300/10000): loss= 391.649898625263\n",
      "Losgistic Regression(    1400/10000): loss= 390.842399005792\n",
      "Losgistic Regression(    1500/10000): loss= 390.098419967314\n",
      "Losgistic Regression(    1600/10000): loss= 389.517873815858\n",
      "Losgistic Regression(    1700/10000): loss= 389.090038144259\n",
      "Losgistic Regression(    1800/10000): loss= 388.62735320403\n",
      "Losgistic Regression(    1900/10000): loss= 388.345695500618\n",
      "Losgistic Regression(    2000/10000): loss= 388.100025026747\n",
      "Losgistic Regression(    2100/10000): loss= 387.766199376055\n",
      "Losgistic Regression(    2200/10000): loss= 387.566477155477\n",
      "Losgistic Regression(    2300/10000): loss= 387.394704657247\n",
      "Losgistic Regression(    2400/10000): loss= 387.191933854999\n",
      "Losgistic Regression(    2500/10000): loss= 387.072220551721\n",
      "Losgistic Regression(    2600/10000): loss= 386.920432527236\n",
      "Losgistic Regression(    2700/10000): loss= 386.873683875172\n",
      "Losgistic Regression(    2800/10000): loss= 386.831548431169\n",
      "Losgistic Regression(    2900/10000): loss= 386.792851175805\n",
      "Losgistic Regression(    3000/10000): loss= 386.753584129237\n",
      "Losgistic Regression(    3100/10000): loss= 386.718594725648\n",
      "Losgistic Regression(    3200/10000): loss= 386.714082978878\n",
      "Losgistic Regression(    3300/10000): loss= 386.713490990592\n",
      "Totoal number of iterations =  3300\n",
      "Loss                        =  386.713490991\n",
      "Time for  1th cross validation = 10.7337s\n",
      "Training Accuracy         =  0.872\n",
      "Cross Validation Accuracy = 0.779004\n",
      "Losgistic Regression(       0/10000): loss= 691.614430765508\n",
      "Losgistic Regression(     100/10000): loss= 534.583423515397\n",
      "Losgistic Regression(     200/10000): loss= 478.780751543609\n",
      "Losgistic Regression(     300/10000): loss= 451.98519081709\n",
      "Losgistic Regression(     400/10000): loss= 439.712319338279\n",
      "Losgistic Regression(     500/10000): loss= 432.838774679885\n",
      "Losgistic Regression(     600/10000): loss= 428.668233425023\n",
      "Losgistic Regression(     700/10000): loss= 426.009784698878\n",
      "Losgistic Regression(     800/10000): loss= 424.176682333278\n",
      "Losgistic Regression(     900/10000): loss= 423.00916093269\n",
      "Losgistic Regression(    1000/10000): loss= 421.992473106274\n",
      "Losgistic Regression(    1100/10000): loss= 421.169371179038\n",
      "Losgistic Regression(    1200/10000): loss= 420.394250303495\n",
      "Losgistic Regression(    1300/10000): loss= 419.728074582245\n",
      "Losgistic Regression(    1400/10000): loss= 419.134196656329\n",
      "Losgistic Regression(    1500/10000): loss= 418.533587762691\n",
      "Losgistic Regression(    1600/10000): loss= 418.068209968823\n",
      "Losgistic Regression(    1700/10000): loss= 417.607230081694\n",
      "Losgistic Regression(    1800/10000): loss= 417.179914165641\n",
      "Losgistic Regression(    1900/10000): loss= 416.818712612604\n",
      "Losgistic Regression(    2000/10000): loss= 416.527049829512\n",
      "Losgistic Regression(    2100/10000): loss= 416.287626980779\n",
      "Losgistic Regression(    2200/10000): loss= 416.043901942241\n",
      "Losgistic Regression(    2300/10000): loss= 415.987942569493\n",
      "Losgistic Regression(    2400/10000): loss= 415.932919307356\n",
      "Losgistic Regression(    2500/10000): loss= 415.886213633991\n",
      "Losgistic Regression(    2600/10000): loss= 415.846968855094\n",
      "Losgistic Regression(    2700/10000): loss= 415.81002083798\n",
      "Losgistic Regression(    2800/10000): loss= 415.771147668427\n",
      "Losgistic Regression(    2900/10000): loss= 415.729552411798\n",
      "Losgistic Regression(    3000/10000): loss= 415.683789980869\n",
      "Losgistic Regression(    3100/10000): loss= 415.634553534625\n",
      "Losgistic Regression(    3200/10000): loss= 415.581943223456\n",
      "Losgistic Regression(    3300/10000): loss= 415.526360972236\n",
      "Losgistic Regression(    3400/10000): loss= 415.468125844633\n",
      "Losgistic Regression(    3500/10000): loss= 415.410696643335\n",
      "Losgistic Regression(    3600/10000): loss= 415.354424265217\n",
      "Losgistic Regression(    3700/10000): loss= 415.295629581269\n",
      "Losgistic Regression(    3800/10000): loss= 415.234075937345\n",
      "Losgistic Regression(    3900/10000): loss= 415.172736535863\n",
      "Losgistic Regression(    4000/10000): loss= 415.111931394793\n",
      "Losgistic Regression(    4100/10000): loss= 415.049924262669\n",
      "Losgistic Regression(    4200/10000): loss= 414.991435173987\n",
      "Losgistic Regression(    4300/10000): loss= 414.963546820659\n",
      "Losgistic Regression(    4400/10000): loss= 414.963328608778\n",
      "Totoal number of iterations =  4400\n",
      "Loss                        =  414.963328609\n",
      "Time for  2th cross validation = 15.032s\n",
      "Training Accuracy         =  0.842\n",
      "Cross Validation Accuracy = 0.759964\n",
      "Losgistic Regression(       0/10000): loss= 691.749173943629\n",
      "Losgistic Regression(     100/10000): loss= 526.601630056708\n",
      "Losgistic Regression(     200/10000): loss= 463.90186696492\n",
      "Losgistic Regression(     300/10000): loss= 434.992473032814\n",
      "Losgistic Regression(     400/10000): loss= 421.652857861071\n",
      "Losgistic Regression(     500/10000): loss= 413.98442773519\n",
      "Losgistic Regression(     600/10000): loss= 408.84254925982\n",
      "Losgistic Regression(     700/10000): loss= 405.404635727264\n",
      "Losgistic Regression(     800/10000): loss= 402.864267071185\n",
      "Losgistic Regression(     900/10000): loss= 400.997256966557\n",
      "Losgistic Regression(    1000/10000): loss= 399.479283989158\n",
      "Losgistic Regression(    1100/10000): loss= 398.248383120007\n",
      "Losgistic Regression(    1200/10000): loss= 397.22852714205\n",
      "Losgistic Regression(    1300/10000): loss= 396.29294823973\n",
      "Losgistic Regression(    1400/10000): loss= 395.562913743393\n",
      "Losgistic Regression(    1500/10000): loss= 394.935287544577\n",
      "Losgistic Regression(    1600/10000): loss= 394.374929601511\n",
      "Losgistic Regression(    1700/10000): loss= 393.885284319073\n",
      "Losgistic Regression(    1800/10000): loss= 393.487300992207\n",
      "Losgistic Regression(    1900/10000): loss= 393.126774415099\n",
      "Losgistic Regression(    2000/10000): loss= 392.80559957941\n",
      "Losgistic Regression(    2100/10000): loss= 392.511438601824\n",
      "Losgistic Regression(    2200/10000): loss= 392.303487783216\n",
      "Losgistic Regression(    2300/10000): loss= 392.109722464222\n",
      "Losgistic Regression(    2400/10000): loss= 391.903800053858\n",
      "Losgistic Regression(    2500/10000): loss= 391.716517129968\n",
      "Losgistic Regression(    2600/10000): loss= 391.652994494679\n",
      "Losgistic Regression(    2700/10000): loss= 391.622555606486\n",
      "Losgistic Regression(    2800/10000): loss= 391.592703469047\n",
      "Losgistic Regression(    2900/10000): loss= 391.557133001098\n",
      "Losgistic Regression(    3000/10000): loss= 391.519172217874\n",
      "Losgistic Regression(    3100/10000): loss= 391.489258067362\n",
      "Losgistic Regression(    3200/10000): loss= 391.455008808706\n",
      "Losgistic Regression(    3300/10000): loss= 391.419509098029\n",
      "Losgistic Regression(    3400/10000): loss= 391.380275115375\n",
      "Losgistic Regression(    3500/10000): loss= 391.339002519801\n",
      "Losgistic Regression(    3600/10000): loss= 391.294406957832\n",
      "Losgistic Regression(    3700/10000): loss= 391.24819087449\n",
      "Losgistic Regression(    3800/10000): loss= 391.201066405662\n",
      "Losgistic Regression(    3900/10000): loss= 391.150667402655\n",
      "Losgistic Regression(    4000/10000): loss= 391.098184618053\n",
      "Losgistic Regression(    4100/10000): loss= 391.04417882607\n",
      "Losgistic Regression(    4200/10000): loss= 390.98845242756\n",
      "Losgistic Regression(    4300/10000): loss= 390.9597467304\n",
      "Losgistic Regression(    4400/10000): loss= 390.959568637489\n",
      "Totoal number of iterations =  4400\n",
      "Loss                        =  390.959568637\n",
      "Time for  3th cross validation = 13.9822s\n",
      "Training Accuracy         =  0.854\n",
      "Cross Validation Accuracy = 0.775084\n",
      "Losgistic Regression(       0/10000): loss= 692.059516809006\n",
      "Losgistic Regression(     100/10000): loss= 532.127177288546\n",
      "Losgistic Regression(     200/10000): loss= 471.443928015304\n",
      "Losgistic Regression(     300/10000): loss= 442.517164505323\n",
      "Losgistic Regression(     400/10000): loss= 428.298245349957\n",
      "Losgistic Regression(     500/10000): loss= 420.283074346855\n",
      "Losgistic Regression(     600/10000): loss= 415.004412307381\n",
      "Losgistic Regression(     700/10000): loss= 410.948672385029\n",
      "Losgistic Regression(     800/10000): loss= 407.857872039171\n",
      "Losgistic Regression(     900/10000): loss= 405.681380281042\n",
      "Losgistic Regression(    1000/10000): loss= 403.785681131872\n",
      "Losgistic Regression(    1100/10000): loss= 402.445488911684\n",
      "Losgistic Regression(    1200/10000): loss= 401.134739975795\n",
      "Losgistic Regression(    1300/10000): loss= 400.074737259502\n",
      "Losgistic Regression(    1400/10000): loss= 399.189657019202\n",
      "Losgistic Regression(    1500/10000): loss= 398.583999852742\n",
      "Losgistic Regression(    1600/10000): loss= 397.980814405163\n",
      "Losgistic Regression(    1700/10000): loss= 397.464552576176\n",
      "Losgistic Regression(    1800/10000): loss= 397.20034380817\n",
      "Losgistic Regression(    1900/10000): loss= 396.807735875928\n",
      "Losgistic Regression(    2000/10000): loss= 396.538329128312\n",
      "Losgistic Regression(    2100/10000): loss= 396.338645035601\n",
      "Losgistic Regression(    2200/10000): loss= 396.15951273298\n",
      "Losgistic Regression(    2300/10000): loss= 395.971207320273\n",
      "Losgistic Regression(    2400/10000): loss= 395.893064889981\n",
      "Losgistic Regression(    2500/10000): loss= 395.845716605686\n",
      "Losgistic Regression(    2600/10000): loss= 395.803297261518\n",
      "Losgistic Regression(    2700/10000): loss= 395.76097791578\n",
      "Losgistic Regression(    2800/10000): loss= 395.726951446438\n",
      "Losgistic Regression(    2900/10000): loss= 395.697908316099\n",
      "Losgistic Regression(    3000/10000): loss= 395.667279522654\n",
      "Losgistic Regression(    3100/10000): loss= 395.635257251963\n",
      "Losgistic Regression(    3200/10000): loss= 395.60103836923\n",
      "Losgistic Regression(    3300/10000): loss= 395.56645765119\n",
      "Losgistic Regression(    3400/10000): loss= 395.531357202486\n",
      "Losgistic Regression(    3500/10000): loss= 395.495217553948\n",
      "Losgistic Regression(    3600/10000): loss= 395.457945749741\n",
      "Losgistic Regression(    3700/10000): loss= 395.420242665271\n",
      "Losgistic Regression(    3800/10000): loss= 395.381933087732\n",
      "Losgistic Regression(    3900/10000): loss= 395.343168830967\n",
      "Losgistic Regression(    4000/10000): loss= 395.30406518878\n",
      "Losgistic Regression(    4100/10000): loss= 395.264858831902\n",
      "Losgistic Regression(    4200/10000): loss= 395.22784250382\n",
      "Losgistic Regression(    4300/10000): loss= 395.192661142394\n",
      "Losgistic Regression(    4400/10000): loss= 395.156901580743\n",
      "Losgistic Regression(    4500/10000): loss= 395.12211081839\n",
      "Losgistic Regression(    4600/10000): loss= 395.08829067276\n",
      "Losgistic Regression(    4700/10000): loss= 395.068591342024\n",
      "Losgistic Regression(    4800/10000): loss= 395.06845919079\n",
      "Totoal number of iterations =  4800\n",
      "Loss                        =  395.068459191\n",
      "Time for  4th cross validation = 17.6859s\n",
      "Training Accuracy         =  0.865\n",
      "Cross Validation Accuracy = 0.769916\n",
      "*************** ([0.85799999999999998, 0.872, 0.84199999999999997, 0.85399999999999998, 0.86499999999999999], [0.77658000000000005, 0.77900400000000003, 0.75996399999999997, 0.775084, 0.76991600000000004])\n",
      "Losgistic Regression(       0/10000): loss= 692.15269451823\n",
      "Losgistic Regression(     100/10000): loss= 571.158446818561\n",
      "Losgistic Regression(     200/10000): loss= 526.871468442973\n",
      "Losgistic Regression(     300/10000): loss= 508.372708318406\n",
      "Losgistic Regression(     400/10000): loss= 502.246229971392\n",
      "Losgistic Regression(     500/10000): loss= 499.678578208716\n",
      "Losgistic Regression(     600/10000): loss= 497.756799888517\n",
      "Losgistic Regression(     700/10000): loss= 496.671963390385\n",
      "Losgistic Regression(     800/10000): loss= 495.877429016682\n",
      "Losgistic Regression(     900/10000): loss= 495.762334506192\n",
      "Losgistic Regression(    1000/10000): loss= 495.605103691817\n",
      "Losgistic Regression(    1100/10000): loss= 495.43705496928\n",
      "Losgistic Regression(    1200/10000): loss= 495.245768788307\n",
      "Losgistic Regression(    1300/10000): loss= 495.022520558488\n",
      "Losgistic Regression(    1400/10000): loss= 494.782005255243\n",
      "Losgistic Regression(    1500/10000): loss= 494.714072797923\n",
      "Losgistic Regression(    1600/10000): loss= 494.712017906628\n",
      "Totoal number of iterations =  1600\n",
      "Loss                        =  494.712017907\n",
      "Time for  0th cross validation = 5.23146s\n",
      "Training Accuracy         =  0.803\n",
      "Cross Validation Accuracy = 0.766652\n",
      "Losgistic Regression(       0/10000): loss= 692.097492361762\n",
      "Losgistic Regression(     100/10000): loss= 561.206698069302\n",
      "Losgistic Regression(     200/10000): loss= 511.356941424488\n",
      "Losgistic Regression(     300/10000): loss= 490.78547076547\n",
      "Losgistic Regression(     400/10000): loss= 484.998842859899\n",
      "Losgistic Regression(     500/10000): loss= 482.514563714572\n",
      "Losgistic Regression(     600/10000): loss= 480.815072388472\n",
      "Losgistic Regression(     700/10000): loss= 479.757492922157\n",
      "Losgistic Regression(     800/10000): loss= 478.956080710328\n",
      "Losgistic Regression(     900/10000): loss= 478.72934918774\n",
      "Losgistic Regression(    1000/10000): loss= 478.717106476853\n",
      "Losgistic Regression(    1100/10000): loss= 478.707102551776\n",
      "Losgistic Regression(    1200/10000): loss= 478.697906674986\n",
      "Losgistic Regression(    1300/10000): loss= 478.688868497865\n",
      "Losgistic Regression(    1400/10000): loss= 478.678901116313\n",
      "Losgistic Regression(    1500/10000): loss= 478.670262384174\n",
      "Losgistic Regression(    1600/10000): loss= 478.661852762032\n",
      "Losgistic Regression(    1700/10000): loss= 478.653395304952\n",
      "Losgistic Regression(    1800/10000): loss= 478.644629429115\n",
      "Losgistic Regression(    1900/10000): loss= 478.636393137547\n",
      "Losgistic Regression(    2000/10000): loss= 478.627539067494\n",
      "Losgistic Regression(    2100/10000): loss= 478.620048115323\n",
      "Losgistic Regression(    2200/10000): loss= 478.612966896197\n",
      "Losgistic Regression(    2300/10000): loss= 478.605115028177\n",
      "Losgistic Regression(    2400/10000): loss= 478.598247792708\n",
      "Losgistic Regression(    2500/10000): loss= 478.591775544538\n",
      "Losgistic Regression(    2600/10000): loss= 478.585183233266\n",
      "Losgistic Regression(    2700/10000): loss= 478.578653335305\n",
      "Losgistic Regression(    2800/10000): loss= 478.572863718784\n",
      "Losgistic Regression(    2900/10000): loss= 478.568471777017\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  478.568471777\n",
      "Time for  1th cross validation = 9.8831s\n",
      "Training Accuracy         =  0.814\n",
      "Cross Validation Accuracy = 0.76954\n",
      "Losgistic Regression(       0/10000): loss= 691.72813624626\n",
      "Losgistic Regression(     100/10000): loss= 559.749364857921\n",
      "Losgistic Regression(     200/10000): loss= 518.428173684912\n",
      "Losgistic Regression(     300/10000): loss= 502.332098564658\n",
      "Losgistic Regression(     400/10000): loss= 497.297150692099\n",
      "Losgistic Regression(     500/10000): loss= 494.949721032093\n",
      "Losgistic Regression(     600/10000): loss= 493.745576976932\n",
      "Losgistic Regression(     700/10000): loss= 493.064529319883\n",
      "Losgistic Regression(     800/10000): loss= 492.934912413949\n",
      "Losgistic Regression(     900/10000): loss= 492.924743794366\n",
      "Losgistic Regression(    1000/10000): loss= 492.917470641734\n",
      "Losgistic Regression(    1100/10000): loss= 492.911266526549\n",
      "Losgistic Regression(    1200/10000): loss= 492.904853136347\n",
      "Losgistic Regression(    1300/10000): loss= 492.898891347259\n",
      "Losgistic Regression(    1400/10000): loss= 492.892632594255\n",
      "Losgistic Regression(    1500/10000): loss= 492.889191825524\n",
      "Totoal number of iterations =  1500\n",
      "Loss                        =  492.889191826\n",
      "Time for  2th cross validation = 5.02489s\n",
      "Training Accuracy         =  0.795\n",
      "Cross Validation Accuracy = 0.75424\n",
      "Losgistic Regression(       0/10000): loss= 691.855569618879\n",
      "Losgistic Regression(     100/10000): loss= 554.243509911927\n",
      "Losgistic Regression(     200/10000): loss= 507.48536478956\n",
      "Losgistic Regression(     300/10000): loss= 489.424040524135\n",
      "Losgistic Regression(     400/10000): loss= 483.706749434954\n",
      "Losgistic Regression(     500/10000): loss= 481.262377438413\n",
      "Losgistic Regression(     600/10000): loss= 479.777774905372\n",
      "Losgistic Regression(     700/10000): loss= 478.901623717855\n",
      "Losgistic Regression(     800/10000): loss= 478.275114763973\n",
      "Losgistic Regression(     900/10000): loss= 477.947449981735\n",
      "Losgistic Regression(    1000/10000): loss= 477.940164066587\n",
      "Losgistic Regression(    1100/10000): loss= 477.934825526535\n",
      "Losgistic Regression(    1200/10000): loss= 477.93011277501\n",
      "Totoal number of iterations =  1200\n",
      "Loss                        =  477.930112775\n",
      "Time for  3th cross validation = 3.82811s\n",
      "Training Accuracy         =  0.813\n",
      "Cross Validation Accuracy = 0.760576\n",
      "Losgistic Regression(       0/10000): loss= 692.156278015194\n",
      "Losgistic Regression(     100/10000): loss= 559.77259796423\n",
      "Losgistic Regression(     200/10000): loss= 516.630960858453\n",
      "Losgistic Regression(     300/10000): loss= 498.506881165699\n",
      "Losgistic Regression(     400/10000): loss= 491.763758851327\n",
      "Losgistic Regression(     500/10000): loss= 488.490803842087\n",
      "Losgistic Regression(     600/10000): loss= 486.316915544351\n",
      "Losgistic Regression(     700/10000): loss= 484.879870301058\n",
      "Losgistic Regression(     800/10000): loss= 483.936475354637\n",
      "Losgistic Regression(     900/10000): loss= 483.268010801293\n",
      "Losgistic Regression(    1000/10000): loss= 483.145316247887\n",
      "Losgistic Regression(    1100/10000): loss= 483.139322902811\n",
      "Losgistic Regression(    1200/10000): loss= 483.134036303571\n",
      "Losgistic Regression(    1300/10000): loss= 483.129401970153\n",
      "Totoal number of iterations =  1300\n",
      "Loss                        =  483.12940197\n",
      "Time for  4th cross validation = 4.26068s\n",
      "Training Accuracy         =   0.81\n",
      "Cross Validation Accuracy = 0.764156\n",
      "*************** ([0.80300000000000005, 0.81399999999999995, 0.79500000000000004, 0.81299999999999994, 0.81000000000000005], [0.766652, 0.76954, 0.75424000000000002, 0.76057600000000003, 0.76415599999999995])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.94399999999999995,\n",
       "   0.94799999999999995,\n",
       "   0.93100000000000005,\n",
       "   0.94199999999999995,\n",
       "   0.94599999999999995],\n",
       "  [0.73575599999999997,\n",
       "   0.75963599999999998,\n",
       "   0.73737200000000003,\n",
       "   0.73784400000000006,\n",
       "   0.73940399999999995]),\n",
       " ([0.93999999999999995,\n",
       "   0.94699999999999995,\n",
       "   0.92800000000000005,\n",
       "   0.93999999999999995,\n",
       "   0.94299999999999995],\n",
       "  [0.73863599999999996,\n",
       "   0.76161199999999996,\n",
       "   0.73948800000000003,\n",
       "   0.74049200000000004,\n",
       "   0.74231199999999997]),\n",
       " ([0.93100000000000005,\n",
       "   0.94199999999999995,\n",
       "   0.92200000000000004,\n",
       "   0.93500000000000005,\n",
       "   0.93200000000000005],\n",
       "  [0.75012000000000001,\n",
       "   0.76641599999999999,\n",
       "   0.74820399999999998,\n",
       "   0.75078400000000001,\n",
       "   0.75097999999999998]),\n",
       " ([0.89600000000000002,\n",
       "   0.91700000000000004,\n",
       "   0.88900000000000001,\n",
       "   0.89800000000000002,\n",
       "   0.90700000000000003],\n",
       "  [0.76986399999999999,\n",
       "   0.77482399999999996,\n",
       "   0.76112800000000003,\n",
       "   0.76962399999999997,\n",
       "   0.76229199999999997]),\n",
       " ([0.85799999999999998,\n",
       "   0.872,\n",
       "   0.84199999999999997,\n",
       "   0.85399999999999998,\n",
       "   0.86499999999999999],\n",
       "  [0.77658000000000005,\n",
       "   0.77900400000000003,\n",
       "   0.75996399999999997,\n",
       "   0.775084,\n",
       "   0.76991600000000004]),\n",
       " ([0.80300000000000005,\n",
       "   0.81399999999999995,\n",
       "   0.79500000000000004,\n",
       "   0.81299999999999994,\n",
       "   0.81000000000000005],\n",
       "  [0.766652,\n",
       "   0.76954,\n",
       "   0.75424000000000002,\n",
       "   0.76057600000000003,\n",
       "   0.76415599999999995])]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_1000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(1000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_1000.append(tmp)\n",
    "\n",
    "accu_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processing(accu_, lambdas):\n",
    "    tr_accs = [np.array(i[0]) for i in accu_]\n",
    "    cv_accs = [np.array(i[1]) for i in accu_]\n",
    "    tr_error = [1-np.mean(i) for i in tr_accs]\n",
    "    cv_error = [1-np.mean(i) for i in cv_accs]\n",
    "    plt.semilogx(lambdas, tr_error,'r*-', lambdas, cv_error, 'bs-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.015625  ,  0.05440941,  0.18946457,  0.65975396,  2.29739671,  8.        ])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-3, 1.5,num=6,base=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trs.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw8AAAGNCAYAAABXO00sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCQu/x97J5JuCSL27wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c308cf780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "accus = [accu_1000, accu_2500, accu_5000, accu_10000, accu_25000]\n",
    "sizes = [1000, 2500,5000,10000, 25000]\n",
    "trs = []\n",
    "cvs = []\n",
    "for accu_ in accus:\n",
    "    tr_accs = [np.array(i[0]) for i in accu_]\n",
    "    cv_accs = [np.array(i[1]) for i in accu_]\n",
    "    tr_error = [1-np.mean(i) for i in tr_accs]\n",
    "    cv_error = [1-np.mean(i) for i in cv_accs]\n",
    "    trs.append(tr_error)\n",
    "    cvs.append(cv_error)\n",
    "\n",
    "trs = np.array(trs)\n",
    "cvs = np.array(cvs)\n",
    "\n",
    "plt.semilogx(sizes, (trs.T)[0],'r*-', label='tr lambda=0.0156')\n",
    "plt.semilogx(sizes, (trs.T)[1],'rs-',label='tr lambda=0.0544')\n",
    "plt.semilogx(sizes, (trs.T)[2],'ro:', label='tr lambda=0.1895')\n",
    "plt.semilogx(sizes, (trs.T)[3],'k*-', label='tr lambda=0.6598')\n",
    "plt.semilogx(sizes, (trs.T)[4],'ks-', label='tr lambda=2.297')\n",
    "plt.semilogx(sizes, (trs.T)[5],'ko-', label='tr lambda=8')\n",
    "\n",
    "plt.semilogx(sizes, (cvs.T)[0],'b*-', label='cv lambda=0.0156')\n",
    "plt.semilogx(sizes, (cvs.T)[1],'bs-',label='cv lambda=0.0544')\n",
    "plt.semilogx(sizes, (cvs.T)[2],'bo:', label='cv lambda=0.1895')\n",
    "plt.semilogx(sizes, (cvs.T)[3],'g*-', label='cv lambda=0.6598')\n",
    "plt.semilogx(sizes, (cvs.T)[4],'gs-', label='cv lambda=2.297')\n",
    "plt.semilogx(sizes, (cvs.T)[5],'go:', label='cv lambda=8')\n",
    "\n",
    "plt.xlim([10**3, 3*10**4])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Learning curve or the model\")\n",
    "plt.xlabel(\"size of training data size\")\n",
    "plt.ylabel(\"Error prediction rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ", sizes, (cvs.T)[0], 'bs-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d190ef3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVDAGC0mSVDAGC0mSVDD/H1CLcSbNmt8NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c3dc79080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmcVXX9x/HXFxhk0zFFEEMRVNxNZnJLSRPDJcPSUnFX\nUinFoMwUtzLTUElzwfxZClpiLqGWGiCuuDsDai64IYIKbkmIoMB8f398Zxp2mOHOnLu8no/HeYz3\n3HPOfIYHl3n7XUOMEUmSpFxokXUBkiSpeBgsJElSzhgsJElSzhgsJElSzhgsJElSzhgsJElSzhgs\nJElSzhgsJElSzhgsJElSzhgsJElSzjQqWIQQTgkhTA0hzAshPBVC2Gkl1/4ohPBoCOGT2mP80teH\nEG4MIdQsddzXmNokSVJ2GhwsQgiHAcOB84FewPPA2BBCxxXcsidwC7AXsCswHRgXQuiy1HX3A52B\nDWuP/g2tTZIkZSs0dBOyEMJTwNMxxp/Wvg6ksHBljPGS1bi/BfAf4JQY419qz90IlMcYD25g/ZIk\nKY80qMUihFAGVAIT6s7FlEweAHZbzce0B8qAT5Y6v1cIYVYI4dUQwogQwnoNqU2SJGWvoV0hHYGW\nwKylzs8idV+sjmHAu6QwUud+4Bhgb+AMUvfJfbWtIZIkqUC0ytFzArDKPpUQwpnAocCeMcYv687H\nGG9b7LKXQggvAm+SxmU8tJznrA/sC7wNzF+TwiVJKjFtgE2BsTHGj3P98IYGi4+ARaRBlovrxLKt\nGEsIIZxOao3oE2N8aWXXxhinhhA+AjZnOcGCFCr+urpFS5KkZRxJmlyRUw0KFjHGBSGEKqAPcA/8\nb/BmH+DKFd0XQvgFMBToG2OctKrvE0LoCqwPvL+CS94G2HDDXrRu3QGALl06MGLEhav9sxSrIUOG\ncPnll2ddxkplUWNTfs9cPXtNn9OY+xt6T0OuL4S/i1kohD+XYvqM5vK5a/Ksxt7bFJ/RV155haOO\nOgpqf5fmWmO6Qn4PjKoNGM8AQ4B2wEiAEMJNwIwY49Da12cAF5Cmj74TQqhr7fgsxjg3hNCeNHX1\nTmAmqZViGPAaMHYFNcwHmDnzT0AFAG3a9KOioqIRP05xKS8vz/s/hyxqbMrvmatnr+lzGnN/Q+9p\nyPWF8HcxC4Xw51JMn9FcPndNntXYe5vyM0oTDSVocLCIMd5Wu2bFBaQukcnAvjHGD2sv6QosXOyW\nH5Nmgdyx1KN+XfuMRcAOpMGb6wLvkQLFeTHGBQ2tr9T175//y39kUWNTfs9cPXtNn9OY+xt6TyH8\n/cp3hfBnWEyf0Vw+d02e1dh7C/Ez2uB1LPJBCKECqIIq6losevbsx5Qp92Ral6R6/fr14557/ExK\n+aa6uprKykqAyhhjda6f714hkiQpZ4omWLz3Hrz8ctZVSKqTD02ykppf0QSL+fNhu+3giCPg1Vez\nrkaSwUIqTUUTLLp3h2uvhYkTYdtt4eij4bXXsq5KkqTSUtDBYpNNBtOzZz969uzHppuWc/LJ8Prr\ncNVV8NBDsPXWcOyx8MYbWVcqSVJpKOhgMWbMFUyZcg9TptzDuHE3A7DWWvCTn6QwccUVMH48bLUV\nnHACvPVWxgVLklTkCjpYrEybNjBoELz5Jlx2Gdx3H2y5JZx4Irz9dtbVSZJUnIo2WNRp2xYGD06t\nFcOGwd13Q8+eMHAgvPNO1tVJklRcij5Y1GnXDn72M5g6FX77W7jzTthiCzjlFJgxI+vqJEkqDiUT\nLOq0bw+/+EUKGL/+Ndx6K2y2GZx2WloLQ5IkNV7JBYs6HTrAmWemgHHeeXDzzSlgDBkCM2dmXZ0k\nSYWpZINFnXXWgbPPTgM6zzoLbrwRevSA00+HDz7IujpJkgpLyQeLOuXlqeXi7bdTV8n116dFt844\nAz78cJW3S5IkDBbLWHfdNPZi6tTULXLttSlgnHUWfPxx1tVJkpTfDBYrsN56cOGFKWAMGpRW89x0\nUzjnHPjkk6yrkyQpPxksVqFjR7j44hQwfvITuPzy1IJx/vnw6adZVydJUn4xWKymDTZIC2xNnZpW\n77zkktSCccEFMHt21tVJkpQfDBYN1KlTWiJ86lQ4/ni46KLUgnHhhfDf/2ZdnSRJ2TJYNNKGG6Zu\nkbfegqOOgt/8JgWMiy+Gzz7LujpJkrJhsFhDG20EV16ZNjs7/PA09qJ799RVMndu1tVJktS8DBY5\n0rUrXHNN2q79kEPS7JEePWD4cPj886yrkySpeRgscmyTTeCPf4TXXoN+/dKy4T16wBVXwLx5WVcn\nSVLTMlg0kU03Tat3TpkCBxyQlgjfbLO0Hsb8+VlXJ0lS0zBYNLEePeCGG+DVV6FvXxg8GDbfHEaM\ngC++yLo6SZJyy2DRTDbfHEaOhFdegW99K63mucUWcN118OWXWVcnSVJuGCyaWc+eaYv2l16CPfaA\nH/84nfvTn2DBgqyrkyRpzRgsMrLVVnDLLfDii7DLLmk1zy23TN0mBgxJUqEyWGRs223hb3+DF16A\nigoYMAC23hpGjYKFC7OuTpKkhjFY5Intt4c77oDJk9N/H3ccbLMN/OUvsGhR1tVJkrR6DBZ55mtf\ngzFjoKoqdZccfTRstx2MHm3AkCTlP4NFnqqogHvugWeeSVNWjzgitWTcdhvU1GRdnSRJy2ewyHM7\n7QT33gtPPQXdusFhh6VWjTvvNGBIkvKPwaJA7LIL3H8/PPEEdOkCP/gB9OqVuk1izLo6SZISg0WB\n2W03GDcOHnsMOnaEgw+GysrUbWLAkCRlzWBRoPbYAyZMgIcfhrXXhoMOgp13Tt0mBgxJUlYMFgVu\nzz1TuJgwAdq0gQMPhF13hX/9y4AhSWp+BosiEALsvTc8+mjqJmnZEvbfH77xDRg/3oAhSWo+Bosi\nEgJ8+9vw+OOpxaKmJu2o2rt3atEwYEiSmprBogiFAPvum6ao3nsvzJ8P++wDe+0FjzySdXWSpGJm\nsChiIcABB8Czz6ZZI3PmpHCx995pVokkSblmsCgBIcB3v5uWCR8zBj75BL75zdRt8sQTWVcnSSom\nBosSEgJ873tQXZ02PJs5E3bfHfbbD55+OuvqJEnFwGBRglq0gEMOgeefT1u2T5+epqh+5zvw3HNZ\nVydJKmStsi5A2WnRAg49NIWM22+HX/0q7U1y4IHw61/DmWcezbRps1d4f7du5Ywbd3PzFSxJynsG\nC9GyJRx+OPzwh3DrrSlUVFZC+/azmTv3npXc2a/ZapQkFQa7QvQ/LVvCkUfCyy/DqFHw5ZdZVyRJ\nKjQGCy2jVSs45hjYdNOsK5EkFRqDhVYohKwrkCQVGoOFGm3atLQuRk1N1pVIkvKFwUKN1qIFHHww\nbLstjBzpmAxJksFCa2DjjdPKnVtsAccfD5tvDn/4A8ydm3VlkqSsON1UK9StWzkrm1LarVs5u+2W\n9iH5979h2DD4+c/hwgvhtNPg1FPhK19pvnolSdkLsQD30g4hVABVVVVVVFRUZF2OFjN1Klx2Gdxw\nQ5pdMnAgDBkCG22UdWWSJIDq6moqKysBKmOM1bl+vl0hyqnu3eGaa+Dtt2HQIPi//0vnTjoJ3ngj\n6+okSU3NYKEm0bkzXHQRvPNOWsnz7rthyy3TCp+TJ2ddnSSpqRgs1KTKy+HMM1MLxtVXp11Ue/WC\n/feHRx+FAuyJkySthMFCzaJtW/jxj+H11+Evf4EZM2DPPWGPPeAf/zBgSFKxMFioWbVqlfYjef75\nFCgA+vWDHXaAv/4VFi7Mtj5J0poxWCgTLVqk7dknToRHHoGuXeGoo9KaGCNGwLx5WVcoSWoMg4Uy\nFQJ885tw//0waRLsskuaTdK9O/zudzB7dtYVSpIawmChvLHjjnDrrTBlSuoeOf982GQTGDoUZs3K\nujpJ0uowWCjvbL55Wv9i6tS0/sVVV6Ut3E85Jc0ukSTlL4OF8tZGG8Gll6ZdVIcOhb/9LYWOo49O\nS4hLkvKPwUJ5b7314NxzU8D4/e/TYM/tt0/dJU8+mXV1kqTFGSxUMNq3T5ubvfEG3HhjWhPjG9+A\nvfaCf/3LtTAkKR8YLFRwWreG446Dl16Cv/8dPv88reRZWQm33QaLFmVdoSSVrkYFixDCKSGEqSGE\neSGEp0IIO63k2h+FEB4NIXxSe4xf3vUhhAtCCO+FED6vvWbzxtSm0tGiBXz/+2mZ8AcegPXXh8MO\ng622guuvhy++yLpCSSo9DQ4WIYTDgOHA+UAv4HlgbAih4wpu2RO4BdgL2BWYDowLIXRZ7Jm/BE4F\nTgZ2BubWPrN1Q+tT6QkB+vSB8ePhmWfSKp4nnww9esDw4TBnTtYVSlLpaEyLxRDguhjjTTHGV4GB\nwOfACcu7OMZ4dIzxjzHGF2KMrwE/qv2+fRa77KfAb2KM/4gx/hs4BtgI+F4j6lMJ22knuPPO1E3S\nt2/aAK1bNzjvPPjoo6yrk6Ti16BgEUIoAyqBCXXnYowReADYbTUf0x4oAz6pfWZ3YMOlnvlf4OkG\nPFNawtZbpwGeb70FxxyTWi66dYPBg2H69Kyrk6Ti1dAWi45AS2DpdRBnkcLB6hgGvEsKI9TeF9fw\nmdJybbwxXHFFmqr685/DqFGw2WZwwgnw6qtZVydJxSdXs0ICKRys/KIQzgQOBb4XY/wyF8+UVkfH\njnDBBfDOO3DxxWl66jbbwCGHwHPPZV2dJBWPVg28/iNgEdB5qfOdWLbFYQkhhNOBM4A+McaXFntr\nJilEdF7qGZ2ASSt75pAhQygvL1/iXP/+/enfv//KblMJW3vt1HJx6qlw001wySVpXEafPnDWWbD3\n3mkwqCQVg9GjRzN69Oglzs1u4t0dQ2zgqkIhhKeAp2OMP619HYB3gCtjjJeu4J5fAEOBvjHGZ5fz\n/nvApTHGy2tfr0MKGcfEGG9fzvUVQFVVVRUVFRUNql9a3KJFabDnxRfD5Mmw885pwOdBB6XprJJU\nbKqrq6msrASojDFW5/r5jfmn8/fASSGEY0IIWwF/BNoBIwFCCDeFEC6quziEcAbwG9KskXdCCJ1r\nj/aLPfMK4JwQwndDCNsDNwEzgLsb80NJq6tlSzj0UKiuTlu3t2kDBx8M224LI0fCggVZVyhJhaXB\nwSLGeBvwc+ACUlfFDsC+McYPay/pypKDLn9MmgVyB/DeYsfPF3vmJcBVwHWk2SBtgf1XYxyGlBMh\nwH77pX1IHn8cttgCjj8+DfS88sq0uqckadUa3BWSD+wKUXN48cU0BmP0aPjKV+CnP01btx922NFM\nm7biPspu3coZN+7mZqxUklZfU3eFNHTwplQytt8ebr45zSa57DK48EIYNgzKymbzn//cs5I7+zVb\njZKUbxyeJq1C9+5wzTVpLYxTT4VPP826IknKXwYLaTV17pxmj2y2WdaVSFL+MlhIDbSqaahvv51a\nNm69Fd59t1lKkqS84RgLKcfatIFx41L3CaSulD32gN6909ettnIRLknFy2Ah5diGG8KUKTBzZpq6\n+thjMHEi/PWvUFMD669fHzR694ZevaCsLOuqJSk3DBZSE9lww7QXySGHpNdz5sCTT6aQ8dhjcO65\nMG8etGsHu+5aHzZ23RU6dMi2dklqLIOF1EDdupWzsiml6f1lrb029O2bDoAvv0wrfta1aFx9dZra\n2rJlasWo6zrZYw/o1KkJfhBJagIukCXliZqatJX7Y4/Vh41p09J7PXvWd53ssQf06OE4DUmN4wJZ\nUolo0SJt5b7NNnDyyenc9On1XSePPQZ//nM636VLfcjo3Tst5tWyZXa1S1Idg4WUxzbeGPr3TwfA\nf/6TBoTWhY2f/zxtlLbOOvCNb9QHjZ13TrNTJKm5GSykAvKVr8CBB6YD0uDPZ5+t7zoZNgzOOQda\nt4avf72+VWP33dO9ktTUDBZSAWvbFr75zXQALFoEL7xQ36IxalQKGyHAdtstuZ7GxhtnW7uk4mSw\nkIpI3YySXr1g0CCIEd56qz5oTJgA116bru3Wbcn1NLbaatWrikrSqhgspCIWQtrbZLPN4Nhj07kP\nPkhBoy5s3HpraulYf/3UZVLXolFRkbpUJKkhDBZSienUCQ4+OB0An30GTz1VP/PkvPPS2I22bWGX\nXeqDxm67pbU4VqRv36OZNm32Ct/v1q2cceNuzvFPIynfGCykEtehA+yzTzogzTKprq5v0RgxAn7z\nm9RN0qvXkuM0Oneuf860abN57bV7VvKdVryomKTiYbCQtISystRSscsuaTprjPULd02cCHffDX/4\nQ7p2iy3qQ8aCBdnWLSk/GCwkrVQIsPXW6TjppHRuxoz6Fo2JE+HGG1MAkSSDhaQG69oVDj88HZAW\n7tp+e3j33RXf8/bbsP/+6d6uXdN017r/7to1LfIlqfAZLCStsa98Bdq3X/k17drBWmvBpEnwj3/A\nrFlLvr/OOksGjaWDx8Ybp2vcI0XKbwYLSc2iUye46676119+mVo4ZsyoP6ZPT19ffBHuvx9mzlyy\ni6VDh5UHj65dYd11DR9SlgwWkjLRujV0756OFVmwAN57b/nh4+WXYdw4eP/9tDNsnXbtVh48unaF\n9dYzfEhNxWAhKSe6dStnZVNK0/sNU1aWVgjt1m3F1yxYkFo2lg4eM2bAa6/Bgw+mcLJoUf09bdsu\nGTqWF0Q6djR8SI1hsJCUE1ktflVWlkLByvY+WbgwjelYXvh46y145JEUPhYurL9nrbVWPeajY0eX\nQZeWZrCQVPRatYKvfjUdu+yy/GsWLUrLnS8dPGbMgGnT0nb177675HodrVunZ66s66VTJ8OHSovB\nQpJIG7h16ZKOnXZa/jU1NfDhh8sPH9Onp6XRZ8xIA1PrlJXBRhutfMxH587p+zeES6grXxksJGk1\ntWiRQkDnzlBZufxrYoSPPlp+8JgxA557Ln2dP7/+nlatUvhYWddLly5Lhg+XUFe+MlhIUg6FABts\nkI6KiuVfEyN88smKw8fkyem/582rv6euRaUuaHz4YfP8PFJDGSwkqZmFkLapX3992HHH5V8TY1rR\ndHnBY8aMtCutlI8MFpKUh0JI622stx7ssMOy72+5ZZpOK+UbxypLkqScMVhIkqScMVhIkqSccYyF\nJBWgplhCXcoFg4UkFSAXv1K+sitEkiTljMFCkiTljMFCkgpcjJGhgwcTY8y6FMlgIUmFrqqqiquu\nuYbq6uqsS5EMFpJU6G6/9lqGL1zI7ddem3UpksFCkgrRxeeey5brr8/+G27Ia7feyonAlH/+k/02\n35wtO3Xi4vPOy7pElSinm0pSvps/H158ESZN+t9x+vPP02n+fO4CxtReNmbWLL4LnHHuuRwzcGCG\nBauUGSwkKZ98+mnaN32xEMErr8CiRdCiBWy9NfTqRdmhhzKgVy/uPvFEePPN/90eyssZMGhQhj+A\nSp3BQpKyECO8//6SAWLSJJg6Nb3fpk3a1nT33eHUU6FXL9h+e2jbdonHLAyBP7Vty9/WXZfDPv2U\nhTU1GfwwUj2DhSQ1tZqa1KqwdIj44IP0/rrrpuDw/e+nr716pX3RW636n+juFRWE007j7gEDGP3n\nP9N94sQm/mGklQuFOO85hFABVFVVVVFRUZF1OZJU78sv4eWXlwwQzz8Pc+ak97/61frwUHd06wYh\nZFu3SkZ1dTWVlZUAlTHGnM9RtsVCkhrrs89SaFg8RPz737BgQQoKPXum4HDggfUhYoMNsq5aalIG\nC0laHR9+uGxXxuuvp7ESrVvDdttBRQUMGJACxA47QIcOWVctNTuDhSQtLkZ4++0lA8TkyfDuu+n9\ntdeGHXeE/faDs85KIWLrrVO4kGSwkFTCFi6EV19dNkR8+ml6v3PnFByOOaa+K6NHjzTtU9JyGSwk\nlYbPP19mkSlefDEtPgUpMPTqBaefXh8iunTJtmapABksJBWf//xn2fEQr76apn22bAnbbJOCwxFH\npK877gjl5VlXLRUFg4WkwhVjGvuwdIiYNi29365dGkS5554weHAKEdttlxafktQkDBaSCkNNTZqF\nsXSI+Oij9P7666fgcOih9a0QPXumFgpJzcZgISn/fPEFvPTSsotMzZ2b3t9kkxQe6pa67tULunZ1\nkSkpDxgsJOVcjJGzhwzht5dfTljVL/v//nfZRaZeeinN2GjRIi1tvfhy1zvumFonJOUlg4WknKuq\nquKqa67hkKOPrls6OJk5c9mpnW+8kd5ba620ydbOO8PJJ9cvMtWuXTY/hKRGMVhIyrnbR4xg+MKF\n3H7GGVTuumt9kJg5M11QXp5aHhZf6nqrraCsLNvCJa0xg4WknLh48GBG3nADPWKkzdy5/A44+MEH\n2e+RR5jasiXH7bwzZ119dQoR3bs7HkIqUgYLSY0zdy48+iiMGwdjx3L6K6/QCbirrIwxtbsmjwG+\n27EjZ5x9NscMHGiLhFQCXJdW0uqpqUndGcOGQZ8+sN56cMABcOedsPvulN1+OwM+/pjQvfsSt4Xy\ncgYMGkSZoUIqCbZYSFqx99+H8eNTq8T48fDBB9C+PXzrW3DZZdC3b1orYrFujYU1NfypbVv+tu66\nHPbppyysqcnwB5DU3AwWkurNmwcTJ6YgMW4cvPBCOl9ZmbYD79sXdtstzeBYge4VFYTTTuPuAQMY\n/ec/033ixGYqXlI+CLG2L7SQhBAqgKqqqioqKiqyLkcqXDGmNSPGjk1B4tFH06ZcG22UQkTfvrDP\nPrDBBllXKilHqqur66aBV8YYq3P9fFsspFLzwQfwwAP1rRLvvw9t26b9NC66KIWJbbZx1oakRjFY\nSMXuiy/giSf+N3uDSZPS+a99DY46KgWJPfZwYy5JOdGoWSEhhFNCCFNDCPNCCE+FEHZaybXbhBDu\nqL2+JoRw2nKuOb/2vcWPlxtTm1TyYkxbhF95JXznO2n2xt57ww03wLbbwk03pVaKyZPhkktSV4eh\nQlKONLjFIoRwGDAcOAl4BhgCjA0h9IwxfrScW9oBbwK3AZev5NH/BvoAde2vCxtam1SyPv4YJkyo\n796YPh1at4beveFXv0qtEttvn/bekKQm1JiukCHAdTHGmwBCCAOB7wAnAJcsfXGM8Tngudprh63k\nuQtjjB82oh6p9CxYAE89VR8knn02tVRsuy384AcpSHzzm+6zIanZNShYhBDKgErgorpzMcYYQngA\n2G0Na9kihPAuMB94Ejgrxjh9DZ8pFYcY4c0362dvPPQQzJmTdvn89rdh4MD0tWvXrCuVVOIa2mLR\nEWgJzFrq/CxgyzWo4yngOGAK0AX4FfBoCGG7GOPcNXiuVLg+/RQefLC+VWLqVGjVCnbfHc48E/bd\nN+27YfeGpDySq1khAWj0ghgxxrGLvfx3COEZYBpwKHDjGtYmFYaFC1OXRl2QePppWLQIttwy7QLa\nt2+aErr22llXKkkr1NBg8RGwCOi81PlOLNuK0WgxxtkhhNeAzVd23ZAhQygvL1/iXP/+/enfv3+u\nSpGa1ttv13dvTJgAs2fDuuummRrXXpu6NzbdNOsqJRWo0aNHM3r06CXOzZ49u0m/Z4NX3gwhPAU8\nHWP8ae3rALwDXBljvHQV904FLo8xXrmK6zqQWizOjzFevZz3XXlThWnOnDQ+oq5V4vXXoWVL2HXX\n1CKx777w9a+nc5LUBPJx5c3fA6NCCFXUTzdtB4wECCHcBMyIMQ6tfV0GbEPqLmkNfDWE8DXgsxjj\nm7XXXAr8gxQmvgr8mjTddMmYJeWZGCNnDxnCby+/nLC8lSoXLYLq6vrFqZ58MnV59OiRQsQll6QN\nvZZqeZOkQtXgYBFjvC2E0BG4gNQlMhnYd7Gpol1Zcg2KjYBJ1I/BOL32eATYe7F7bgHWBz4EJgK7\nxhg/bmh9UnOqqqriqmuu4ZCjj677P4C0hsT48SlIPPAAfPJJGhfRp09atKpvX9hss2wLl6Qm0qjB\nmzHGEcCIFby391Kvp7GKFT5jjA6KUEG6/dprGb5wIbcPHUrl1lunlolXXkkzNXbaCU45JQWJXXaB\nsrKsy5WkJudeIVIDXXzeeYz84x/p0aYNbWbN4nfAwePGsd+ECUwtK+O4H/yAs667Li2lLUklxgnw\nUgOdPnAgZ/TsSavp0xnz5ZcEYAxQ1rEjZ1xyCaffcouhQlLJMlhIq+uLL+DSSynbemsGvPQSYYMN\nlng7lJczYNAgyuzykFTCDBbSqsQIf/87bLMNnHUWHHMMvP46C8vL+VPbtny7Sxf+1LYtC2tqsq5U\nkjJnsJBWpro6TQc95JC0AuYLL8BVV0HHjnSvqCAMG8bdb7xBGDaM7q6pIkkNXyArH7hAlprc++/D\n2WfDyJGw9dYwfDjst1/WVUnSGsvHBbKk4jVvHvz+93DxxdCmDVx9NZx0Utr8S5K0Sv5rKUEaR/G3\nv8Evf5laKwYNgnPPTft2SJJWm8FCevppGDIkLbd90EFptcwttsi6KkkqSA7eVOmaPh2OOiptAPb5\n52l30bvuMlRI0hqwxUKl57PP0uZfl10G66wD118Pxx/vjqKSlAMGC5WOmhq4+WYYOhQ+/hh+9rO0\nLsXaa2ddmSQVDbtCVBomTkwbgR13HOyxB7z6Klx0kaFCknLMYKHiNnUq/PCH0Lt3ev3YY2n2x6ab\nZlqWJBUrg4WK03//C2eeCVttBU88AaNGpdkfe+yRdWWSVNQcY6HismgR3HADnHMOzJmTxlD84hfQ\nvn3WlUlSSbDFQsVjwgSoqEgrZfbtC6+9Br/6laFCkpqRwUKF7/XX08JW++wDHTqkLo+bb4auXbOu\nTJJKjsFChes//0lTRrfdFp5/Hm69Nc3+2HnnrCuTpJLlGAsVngUL4LrrUjfHF1+kr0OGQNu2WVcm\nSSXPYKHCcv/98POfp3Uojj8eLrwQunTJuipJUi27QlQYXnoJ9tsPDjgAOneGqir4858NFZKUZwwW\nym8ffQSnnAJf+xq88QaMGQMPPgi9emVdmSRpOewKUX768ku4+mq44AKIEYYNg1NPhbXWyroySdJK\nGCyUX2KEe+6B00+Ht96Ck0+GX/8aNtgg68okSavBrhDlj+efhz594Hvfgx490usRIwwVklRADBbK\n3syZcOLVDMtrAAARaUlEQVSJadzEe+/BvffCv/4F222XdWWSpAayK0TZmT8fLr88bV/eujX84Q8w\ncCCUlWVdmSSpkQwWan4xwu23wy9/CTNmpFkf550H662XdWWSpDVksFDzeu45GDwYHn8cDjwwdXls\nuWXWVUmScsQxFmoe774Lxx4LO+0Es2fD+PHwj38YKiSpyNhioab1+edw6aVwySVp+/I//hEGDIBW\n/tWTpGLkv+5qGjU1cMstcOaZ8OGHqftj6FAoL8+6MklSE7IrRLn3xBOw665w9NHp6yuvpJUzDRWS\nVPQMFsqdadPg8MNh991h4UJ4+GG444602JUkqSQYLLTm5syBs89OAzEfeQRuvDHN/thzz6wrkyQ1\nM8dYqPEWLYJRo1Ko+PRT+MUv0toUHTpkXZkkKSO2WGi1xBgZOngwMcZ04uGH4etfTzM8vvUtmDIF\nfvMbQ4UklTiDhVZLVVUVV11zDdV33w3f/34KE2utBU8+mWZ/bLJJ1iVKkvKAXSFaLbdfcQXDFy7k\n9oMPprJrV/jrX6F/fwgh69IkSXnEFgut0MXnnceWnTqx/yab8Nro0ZwITGnXjv3Kythy8GAuPv/8\nrEuUJOUZg4VW6PRzz+WMww6j1YwZjKmpIQBj5s6lbO5czjj3XE4/99ysS5Qk5RmDhVao7PbbGXDd\ndYS2bZc4H8rLGTBoEGVuby5JWorBQsuKMa2UeeSRcMQRLOzShT+1bcu3a78urKnJukJJUp4yWGhJ\nixbBaaelPT7OOQduvJHulZWEYcO4+403CMOG0b2iIusqJUl5KvxvXYICEkKoAKqqqqqo8Jdc7syb\nl1op7r4bRoyAk0/OuiJJUo5VV1dTWVkJUBljrM71851uquTjj6FfP5g0Ce66C7773awrkiQVIIOF\nYOpU2H//FC4efhh23jnriiRJBcoxFqWuuhp22y3tRvrkk4YKSdIaMViUsrFj0w6km2wCTzwBm2+e\ndUWSpAJnsChVI0fCd74De+0FDz0EnTplXZEkqQgYLEpNjGkX0uOPhxNOgDFjoH37rKuSJBUJB2+W\nkoUL4Sc/geuvhwsuSOtUuImYJCmHDBalYu5cOOww+Ne/4IYbUouFJEk5ZrAoBR98AAceCC+/DPfe\nC/vum3VFkqQiZbAodm+8AfvtB599Bo8+Cq5UKklqQg7eLGZPP53WqGjVKq1RYaiQJDUxg0Wx+sc/\n4Fvfgp494fHHoXv3rCuSJJUAg0Uxuu46+N73UhfIAw/A+utnXZEkqUQYLIpJjGkK6cCBaVrp7bdD\n27ZZVyVJKiEO3iwWCxbAiSfCqFEwbBj84heuUSFJanYGi2IwZw4cckjamfSvf4Ujjsi6IklSiTJY\nFLr334cDDoC33kqLX+29d9YVSZJKmMGikL3yCuy/f1qq+7HHYIcdsq5IklTiHLxZqCZOhN13hw4d\n0hoVhgpJUh4wWBSiO++EffZJYWLiRNh446wrkiQJMFgUniuvhB/+MK1TMXYsrLtu1hVJkvQ/jQoW\nIYRTQghTQwjzQghPhRB2Wsm124QQ7qi9viaEcNqaPrMk1dSkKaQ//Sn87Gdwyy2w1lpZVyVJ0hIa\nHCxCCIcBw4HzgV7A88DYEELHFdzSDngT+CXwfo6eWVq++AKOPBKGD4crroDLLoMWNjZJkvJPY347\nDQGuizHeFGN8FRgIfA6csLyLY4zPxRh/GWO8DfgyF88sKZ9+mpbmHjMGbrsttVhIkpSnGhQsQghl\nQCUwoe5cjDECDwC7NaaApnhm0Zg+HXr3huefh/Hj4Qc/yLoiSZJWqqEtFh2BlsCspc7PAjZsZA1N\n8czC9+KLacvz//437U7au3fWFUmStEq5WiArADFHz1rtZw4ZMoTy8vIlzvXv35/+/fvnuJRm9tBD\nadZH9+5w332w0UZZVyRJKkCjR49m9OjRS5ybPXt2k37PhgaLj4BFQOelzndi2RaHJn/m5ZdfTkVF\nRSO/bZ4aPRqOPRb22gvuuAPWWSfriiRJBWp5/7NdXV1NZWVlk33PBnWFxBgXAFVAn7pzIYRQ+/qJ\nxhTQFM8sSDHCpZemDcT694d//tNQIUkqOI3pCvk9MCqEUAU8Q5rR0Q4YCRBCuAmYEWMcWvu6DNiG\n1LXRGvhqCOFrwGcxxjdX55lFb9EiGDIErroKhg6FCy90y3NJUkFqcLCIMd5Wu77EBaTui8nAvjHG\nD2sv6QosXOyWjYBJ1I+XOL32eATYezWfWbzmzYOjjoK77oJrr4WBA7OuSJKkRmvU4M0Y4whgxAre\n23up19NYjS6XlT2zaH38MRx0EFRXp3Uq+vXLuiJJktaI26Zn5e2308JXH38MDz4Iu+6adUWSJK0x\n14XOwqRJaY2KBQvgiScMFZKkomGwaG7jxsE3vwldu6ZQscUWWVckSVLOGCya06hR8J3vpGDx8MPQ\neemlOyRJKmwGi+YQI/z2t3Dccem4+25o3z7rqiRJyjmDRVNbuBB+/GM45xz49a/h//4PWjlmVpJU\nnPwN15Tmzk2raN53H9xwAxx/fNYVSZLUpAwWTeXDD+HAA+Gll9Ly3Pvtl3VFkiQ1OYNFU3jzzRQk\n5syBRx6BJtzsRZKkfOIYi1x75pm0RkWLFvDkk4YKSVJJMVjk0j//Cd/6Fmy+OTz+OHTvnnVFkiQ1\nK4NFrlx/fdr3o29fmDABOnbMuiJJkpqdwWJNxQjnnQcnnZSmld5xB7Rtm3VVkiRlwsGba2LBghQo\nRo6E3/0OzjgDQsi6KkmSMmOwaKw5c+CHP0w7k/7lL3DkkVlXJElS5gwWjTFzZtrz44034P77oU+f\nrCuSJCkvGCwaasqUtEbFl1/CY4/BDjtkXZEkSXnDwZsN8fjj8I1vQLt2aY0KQ4UkSUswWKyuMWNg\nn31g++1h4kTYZJOsK5IkKe8YLFbH1VfDIYdAv34wdix85StZVyRJUl4yWKxMTQ388pcwaBAMGQKj\nR8Naa2VdlSRJecvBmyvyxRdwwgkpTFx+OQwenHVFkiTlPYPF8syeDd//PjzxBPztb2m9CkmStEoG\ni6XNmAEHHADTp8P48dC7d9YVSZJUMAwWi/v3v2H//dOW548/Dttsk3VFkiQVFAdv1nn4YdhjD1h/\n/bRGhaFCkqQGM1gA3Hor7Lsv7LQTPPoobLRR1hVJklSQSjtYxAjDh0P//nDYYXDvvbDOOllXJUlS\nwSrdYLFoUVqb4vTT4ayzYNQoaN0666okSSpopTl4c/58OOqotEz3iBHw4x9nXZEkSUWh9ILFJ5/A\nQQdBVRX8/e/pvyVJUk6UVrCYNi1NJ/3gA3jwQdh116wrkiSpqJTOGIvJk1OQmD8/rahpqJAkKedK\nI1jUraD51a+mNSp69sy6IkmSilLxB4ubbkpLdPfunRbB6tw564okSSpaxRssYoSLLoJjj4VjjoG7\n74YOHbKuSpKkolacwWLhQvjJT+Dss+FXv4I//QnKyrKuSpKkold8s0I+/zytpHnvvSlQDBiQdUWS\nJJWM4goWH30E3/0uvPAC3HNPGlshSZKaTfEEizffTGtUzJ4NjzwCX/961hVJklRyimOMxbPPwje+\nkf77yScNFZIkZaSgg0WMEe67D/baC3r0SAtf9eiRdVmSJJWsgg4Wr157LfTrB9/+NkyYAB07Zl2S\nJEklraCDxfg//xlOOgnuvBPatcu6HEmSSl5BB4tpbduy37hxbNmlCxefd17W5UiSVPIKOlgMnzeP\nss8+44xzz+X0c8/NuhxJkkpewU83DeXlDBg0KOsyJEkSBd5iMWattVhYU5N1GZIkqVZBt1iE006j\n+7RpWZchSZJqFXSw+N7hh1NRUZF1GZIkqVZBd4VIkqT8YrCQJEk5Y7CQJEk5Y7CQJEk5Y7CQJEk5\nY7CQJEk5Y7CQJEk5Y7CQJEk5Y7CQJEk5Y7CQJEk5Y7CQJEk5Y7CQJEk5Y7CQJEk5Y7CQJEk5Y7CQ\nJEk5Y7CQJEk506hgEUI4JYQwNYQwL4TwVAhhp1Vc/8MQwiu11z8fQth/qfdvDCHULHXc15jaJOWH\n0aNHZ12CpAw0OFiEEA4DhgPnA72A54GxIYSOK7h+N+AW4HpgR+Au4K4QwjZLXXo/0BnYsPbo39Da\nJOUPg4VUmhrTYjEEuC7GeFOM8VVgIPA5cMIKrv8pcH+M8fcxxikxxvOBauDUpa77Isb4YYzxg9pj\ndiNqkyRJGWpQsAghlAGVwIS6czHGCDwA7LaC23arfX9xY5dz/V4hhFkhhFdDCCNCCOs1pDYlhfB/\niVnU2JTfM1fPXtPnNOb+ht5TCH+/8l0h/BkW02c0l89dk2c19t5C/Iw2tMWiI9ASmLXU+Vmk7ovl\n2XA1rr8fOAbYGzgD2BO4L4QQGlhfycuHv1SrUkz/aOXy2QaL0lAIf4bF9Bk1WDS/Vjl6TgBiY6+P\nMd622HsvhRBeBN4E9gIeWs79bQBeeeWVBhda7GbPnk11dXXWZaxUFjU25ffM1bPX9DmNub+h9zTk\n+kL4u5iFQvhzKabPaC6fuybPauy9TfEZXex3Z5sGF7Q6YoyrfQBlwAKg31LnRwJjVnDPNOC0pc79\nCpi0iu/1AXDiCt47ghRMPDw8PDw8PBp3HNGQDLC6R4NaLGKMC0IIVUAf4B6A2u6KPsCVK7jtyeW8\n/+3a88sVQugKrA+8v4JLxgJHAm8D81f/J5AkqeS1ATYl/S7NuVDbArD6N4RwKDAKOBl4hjRL5AfA\nVjHGD0MINwEzYoxDa6/fDXgEOBO4lzSN9EygIsb4cgihPWnq6p3ATGBzYBjQHtghxrhgjX9KSZLU\nLBo8xiLGeFvtmhUXkNadmAzsG2P8sPaSrsDCxa5/MoTQH/ht7fE6cFCM8eXaSxYBO5AGb64LvEdK\nUecZKiRJKiwNbrGQJElaEfcKkSRJOWOwkCRJOVN0wSKEUB5CeDaEUB1CeCGE8KOsa5K0rBBC2xDC\n2yGES7KuRVK92s/l5BDCpBDChFXfsaRcLZCVT/4L9I4xzg8htCUtuHVnjPE/WRcmaQlnA09lXYSk\nZdQAu8UY5zXm5qJrsYhJ3doWbWu/ujS4lEdCCJsDWwL3ZV2LpGUE1iAfFF2wgP91h0wG3gEujTF+\nknVNkpZwGXAWhn4pH9UAD4cQng4hHNHQm/MqWIQQeocQ7gkhvBtCqAkh9FvONaeEEKaGEOaFEJ4K\nIey09DUxxtkxxh2B7sCRIYQNmqN+qdjl4jNae8+UGOMbdaeao3ap2OXqdyiwe4xxJ+AgYGgIYduG\n1JFXwYK02uZk4BTSOuZLCCEcBgwnrdTZC3geGFu7YNcyahftegHo3VQFSyUmF5/RXYHDQwhvkVou\nfhRCOKepC5dKQE5+h8YYZy729T6gsiFF5O0CWSGEGuB7McZ7Fjv3FPB0jPGnta8DMB24MsZ4Se25\nzsDcGONnIYRyYCJweIzxpWb/IaQi1tjP6FLPOBbYNsZ4RjOVLZWENfgd2g5oUfs7tAPwMHByjLFq\ndb93vrVYrFAIoYyUmv439SWmVPQAsNtil24CPBZCmETao+QPhgqp6TXgMyqpmTXg89kZmFj7O/QJ\nYGRDQgUU1nTTjkBLYNZS52eRRpcDEGN8ltTEI6l5rdZndHExxlFNXZQkYPV/h04FdlyTb1QwLRYr\nEVhOX5KkvOFnVMpfOf98FlKw+Ii0E2rnpc53YtkEJqn5+RmV8lezfT4LJljUbqFeBfSpO1c78KQP\nqR9IUob8jEr5qzk/n3k1xiKE0B7YnPp57T1CCF8DPokxTgd+D4wKIVQBzwBDgHbAyAzKlUqOn1Ep\nf+XL5zOvppuGEPYEHmLZ/p5RMcYTaq/5CXAGqTlnMjAoxvhcsxYqlSg/o1L+ypfPZ14FC0mSVNgK\nZoyFJEnKfwYLSZKUMwYLSZKUMwYLSZKUMwYLSZKUMwYLSZKUMwYLSZKUMwYLSZKUMwYLSZKUMwYL\nSZKUMwYLSZKUMwYLSZKUMwYLSZKUM/8PDkriy1h80hUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c34aea438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8/9Yrj9Dg2/jWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d07b09dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xl8VNX9//HXBwiyqEGLARUF3BBcwARUXKoFFZdqcStG\nxaqAooian4gVrQtaFS2iKCIuLeASxV37te5tVZQtAdxFEUFBKbhEWVQCn98fZ2IWkpCZTHJnkvfz\n8ZhHnDv3nvkMD8a8Oefcc8zdEREREUmGJlEXICIiIg2HgoWIiIgkjYKFiIiIJI2ChYiIiCSNgoWI\niIgkjYKFiIiIJI2ChYiIiCSNgoWIiIgkjYKFiIiIJI2ChYiIiCRNQsHCzIaZ2SIzW2tmM8ysVzXn\nDjaz183s29jj5Yrnm9nVZvahma0qc86+idQmIiIi0Yk7WJjZAGAscDWwDzAfeNHM2lZxySHAw8Ch\nwP7AF8BLZrZtmXM+BoYBewIHAp/HzvlNvPWJiIhIdCzeTcjMbAYw090vij03QlgY7+431+D6JsB3\nwDB3f7CKc7YAioC+7v7vuAoUERGRyMTVY2FmGUAO8GrJMQ/J5BWgdw2baQ1kAN9W8x7nAt8TekNE\nREQkTcQ7FNIWaAosr3B8OdC+hm2MAZYSwsivzOwYM/sR+Am4CDjc3SsNHyIiIpKamiWpHQM2OaZi\nZn8G/ggc4u6/VHj5NaA7IbwMAR4zs33dfWUl7fwG6EeYi/FT7UoXERFpVFoAnYAX3f2bZDceb7BY\nCawH2lU4nsXGvRjlmNkIYCRh3sT7FV9397XAZ7HHLDNbAAwi9HBU1A94KM7aRUREpNRphJsrkiqu\nYOHu68ysAOgLPAu/Tt7sC4yv6jozuxQYBRzh7nNr+HZNgM2qeO1zgAcffJCuXbvWsLnGIS8vj3Hj\nxkVdRrWiqLEu3zNZbde2nUSuj/eaeM5Ph7+LUUiHP5eG9B1NZru1aSvRa+viO/rhhx9y+umnQ+x3\nabIlMhRyKzAlFjBmAXlAK2AygJlNBb5091Gx5yOB0UAusMTMSno7Vrn7ajNrBVxBCCpfEYZCLgC2\nAx6rooafALp27Up2dnYCH6HhyszMTPk/kyhqrMv3TFbbtW0nkevjvSae89Ph72IU0uHPpSF9R5PZ\nbm3aSvTauvyOUkdTCeIOFu4+LbZmxWjCkMg8oJ+7r4id0gEoLnPJeYS7QB6v0NS1sTbWA7sDZxBC\nxTfAbOAgd/+wulqOP/5iWrRoA0DHjpm89NID8X6cBic3NzfqEjYpihrr8j2T1XZt20nk+nivSYe/\nX6kuHf4MG9J3NJnt1qatRK9Nx+9o3OtYpAIzywYKoAAIyWy33Y7j44+fjbQuESl13HHH8eyz+k6K\npJrCwkJycnIActy9MNnta68QERERSRoFCxGpE6nQJSsi9U/BQkTqhIKFSOOkYCEiIiJJ02CCxYoV\nsH591FWIiIg0bg0mWHz3HRx5JKzcaAFwERERqS/J2iskEjvuWLqOxRZbZDJvHuTkwJNPhp8iIiJS\nv9I6WDz11G3lVhhbsgROOgkOPBDuvhvOPDO62kRERBqjBjMUArDjjvD66zBwIJx1Fpx/PvxScQ9V\nERERqTMNKlgAtGgB994L99wD998PhxwCS5dGXZWIiEjj0OCCRYkhQ0LvxRdfQHZ2+G8RERGpWw02\nWADstx8UFkK3btCnD9x+O6Th1igiIiJpo0EHC4CsLHj5Zbj44vA4/XRYvTrqqkRERBqmBh8sAJo1\ng7/9DR55BJ5+Gg44ABYujLoqERGRhqdRBIsSAwbAzJmwZg307AnPPx91RSIiIg1LowoWAHvuCbNn\nw0EHwe9/D6NHw4YNUVclIiLSMDS6YAHQpg088wxcc014/OEP8P33UVclIiKS/hplsABo0gSuugr+\n+U94803o1Qveey/qqkRERNJbow0WJY4+GubMgZYtw+2pjz4adUUiIiLpq9EHC4Cdd4a334b+/eGU\nU2DECCgujroqERGR9KNgEdO6NTz4INx2W3gcfjj8739RVyUiIpJeFCzKMIOLLoLXXoMPPghbr8+c\nGXVVIiIi6UPBohK//W1YCrxDh/Df994bdUUiIiLpIaFgYWbDzGyRma01sxlm1quacweb2etm9m3s\n8XLZ882smZmNMbN3zGyVmS01sylmtm0itSXL9tvDf/4DgwbBOeeETc1++inKikRERFJf3MHCzAYA\nY4GrgX2A+cCLZta2iksOAR4GDgX2B74AXioTHFoBPYBrY+0dD3QBnom3tmTbbDO46y74xz/ggQdC\n78UXX0RdlYiISOpKpMciD5jk7lPd/SNgKLAGOLuyk919oLvf7e7vuPsCYHDsffvGXv/B3fu5+xPu\n/om7zwIuAHLMrEMiHyrZzjwTpk+H5cvDFuz//nfUFYmIiKSmuIKFmWUAOcCrJcfc3YFXgN41bKY1\nkAF8W805bQAHUmY9zJwcKCiAHj3gsMPCpmbagl1ERKS8eHss2gJNgeUVji8H2tewjTHAUkIY2YiZ\nbQbcBDzs7qvirK9OtW0LL7wAI0fCpZeGTc1WpVSFIiIi0UrWXSFG6GGo/iSzPwN/BPq7+y+VvN4M\neCzW1vlJqi2pmjaFG2+Exx+Hf/0rrNa5YEHUVYmIiKSGZnGevxJYD7SrcDyLjXsxyjGzEcBIoK+7\nv1/J6yWhYgegT016K/Ly8sjMzCx3LDc3l9zc3E1dWmsnngjdusHxx4d9Rh54AI47rs7fVkREpMby\n8/PJz88vd6yoqKhO39M8zokCZjYDmOnuF8WeG7AEGO/ut1RxzaXAKOAId59dyesloWIn4HfuXt38\nC8wsGygoKCggOzs7rvqT7YcfwuTOp56CK68Mu6U2bRppSSIiIlUqLCwkJycHIMfdC5PdfiJDIbcC\n55jZGWa2O3A34ZbRyQBmNtXMbig52cxGAtcR7hpZYmbtYo/WsdebAk8A2cDpQEaZczJq8dnqxZZb\nwhNPwA03wF//Cr//PXxbbSwSERFpuOIOFu4+DbgEGA3MBfYG+rn7itgpHSg/kfM8wl0gjwPLyjwu\nKXP+72M/58Ve+yr2s6Z3mkTKDC6/PEzsnDULevaE+fOjrkpERKT+xTvHAgB3vwu4q4rX+lR43nkT\nbS0m3GmS9o44ItySesIJ0Ls33HMPnH561FWJiIjUH+0VkmSdOoXFtE4+GQYODJuarVsXdVUiIiL1\nQ8GiDrRsCZMnw4QJYUnwvn3h66+jrkpERKTuKVjUETM4/3z473/h00/DUuBvvRV1VSIiInVLwaKO\nHXBA2IJ9553h0ENDD4aWAhcRkYZKwaIetG8Pr70G550Hw4bBWWfB2rVRVyUiIpJ8Chb1JCMDbr89\nrNA5bRoceCB8/nnUVYmIiCSXgkU9O/30MNfi++/Djqkvvxx1RSIiIsmjYBGBHj1gzhzYd1848ki4\n6SbNuxARkYZBwSIiW28N//wnXHFFWLXzxBPDviMiIiLpTMEiQk2bwujR8Mwz8OqroQfjww+jrkpE\nRCRxChYp4LjjYPbsEDT23ReefDLqikRERBKjYJEidtsNZs6Eo44KwyKXXw7r10ddlYiISHwULFLI\n5pvDo4/CLbfAzTeHiZ0rV0ZdlYiISM0pWKQYMxgxItyGOm9e2IK9sDDqqkRERGpGwSJF9ekTtmDP\nygrLgk+eHHVFIiIim6ZgkcJ23BFefz0sqnXWWWFTs19+iboqERGRqilYpLgWLeC+++Cee+D++8NG\nZkuXRl2ViIhI5RQs0sSQIaH3YsmSsBT4669HXZGIiMjGFCzSyH77hYmcu+8OffvC+PFaClxERFKL\ngkWaycqCV16BCy+Eiy4K8y/WrIm6KhERkUDBIg01awZjx8Ijj8DTT0Pv3rBwYdRViYiIKFiktQED\nwmqda9aE9S7+9a+oKxIRkcZOwSLN7bln2GfkoIPgmGPguutgw4aoqxIRkcZKwaIBaNMm7JB6zTVw\n9dXQvz98/33UVYmISGOUULAws2FmtsjM1prZDDPrVc25g83sdTP7NvZ4ueL5Zna8mb1gZivMbIOZ\n7Z1IXY1ZkyZw1VXwz3/CG2+EXVLfey/qqkREpLGJO1iY2QBgLHA1sA8wH3jRzNpWcckhwMPAocD+\nwBfAS2a2bZlzWgNvApcBuoGyFo4+GubMCQtr7b8/TJsWdUUiItKYJNJjkQdMcvep7v4RMBRYA5xd\n2cnuPtDd73b3d9x9ATA49r59y5zzoLtfD7wKWAI1SRk77wxvvw1/+EOY4DliBBQXR12ViIg0BnEF\nCzPLAHIIAQAAd3fgFaB3DZtpDWQA38bz3hKf1q3hwQfhttvC4/DD4X//i7oqERFp6OLtsWgLNAWW\nVzi+HGhfwzbGAEsJYUTqkFlYROvVV+GDD8JS4LNmRV2ViIg0ZMm6K8SowdwIM/sz8Eegv7trn856\ncsghYSnwDh3g4IPh3nujrkhERBqqZnGevxJYD7SrcDyLjXsxyjGzEcBIoK+7vx/n+1YqLy+PzMzM\ncsdyc3PJzc1NRvMNyvbbw3/+AxdfDOecE3ou7rwTNtss6spERKSu5Ofnk5+fX+5YUVFRnb6neZy7\nWJnZDGCmu18Ue27AEmC8u99SxTWXAqOAI9x9djVtdwQ+A/Zx93eqOS8bKCgoKCA7Ozuu+gX+8Q84\n7zzYe2944gnYYYeoKxIRkfpSWFhITk4OQI67Fya7/USGQm4FzjGzM8xsd+BuoBUwGcDMpprZDSUn\nm9lI4DrCXSNLzKxd7NG6zDlbmVl3YA/CsMruZtbdzCr2jEgSnHUWTJ8Oy5eHeRf//nfUFYmISEMR\nd7Bw92nAJcBoYC6wN9DP3VfETulA+Ymc5xHuAnkcWFbmcUmZc46LtfUcYa5GPlAInBtvfVIzOTlQ\nUBB6LQ47LGxqpi3YRUSktuKdYwGAu98F3FXFa30qPO9cg/amAFMSqUUS17YtvPACXHllWOti1iy4\n/37YfPOoKxMRkXSlvUIauWbN4Kab4PHH4fnnw2qdCxZEXZWIiKQrBQsB4MQTQ49FcTH06gXPPht1\nRSIiko4SGgqRhqlr1xAu/vSnsBx4584DadasCKtikfWOHTN56aUH6rdIERFJaQoWUs6WW4ZbUMeM\ngVGjioDqui6Oq6+yREQkTWgoRDbSpAlcfnlYVEtERCQeChZSpdatN32OiIhIWQoWIiIikjQKFiIi\nIpI0ChaSsGXLYOXKqKsQEZFUomAhCVuzBvbYA55+OupKREQkVeh2U6lSx46ZVHdLafv2mbRpA8cf\nD6efDuPHw1Zb1V99IiKSehQspEo1WfzKHR54AC68EF57Lew1cuSR9VCciIikJA2FSK2YwRlnwHvv\nwV57wVFHwZAh8MMPUVcmIiJRULCQpOjQAf71L7jnHnjkkRAyXnst6qpERKS+KVhI0piF3op334Wd\ndoK+feGCC2D16qgrExGR+qJgIUnXqRO8+mqYzPn3v0P37jB9etRViYhIfVCwkDrRpAkMHw7z50NW\nFhx8MIwYAWvXRl2ZiIjUJQULqVO77gpvvBF2S73zTsjODluzi4hIw6RgIXWuaVO49FIoLITNN4cD\nDoArr4Rffom6MhERSTYFC6k33brBW2/BNdfAzTdDr14wb17UVYmISDIpWEi9ysgIvRUlwyG9esF1\n18G6ddHWJSIiyaFgIZHo0QNmz4bLLoNrr4XeveH996OuSkREakvBQiLTvDlcfz28/XbY0Cw7G265\nBdavj7oyERFJVELBwsyGmdkiM1trZjPMrFc15w42s9fN7NvY4+XKzjez0Wa2zMzWxM7ZJZHaJP30\n6hUmdl54YejB+O1v4ZNPoq5KREQSEXewMLMBwFjgamAfYD7wopm1reKSQ4CHgUOB/YEvgJfMbNsy\nbV4GXACcC+wLrI612Tze+iQ9tWgReitefx2WLw+Lao0fDxs2RF2ZiIjEI5EeizxgkrtPdfePgKHA\nGuDsyk5294Hufre7v+PuC4DBsfftW+a0i4Dr3P05d38POAPYDuifQH2Sxg46KCyqNWgQXHRRWBb8\n88+jrkpERGoqrmBhZhlADvBqyTF3d+AVoHcNm2kNZADfxtrsDLSv0OYPwMw42pQGpHVruOOOsCz4\nokVhQ7N77glbtIuISGqLt8eiLdAUWF7h+HJCOKiJMcBSQhghdp3Xsk1pgPr0gXfegdxcOPfcsCX7\nl19GXZWIiFQnWXeFGCEcVH+S2Z+BPwL93X1T6y7WqE1p2LbcMvRWPP982DV1zz1hyhT1XoiIpKpm\ncZ6/ElgPtKtwPIuNexzKMbMRwEigr7uXXbHga0KIaFehjSxgbnVt5uXlkZmZWe5Ybm4uubm51V0m\naeioo+C998K8izPPhCeeCIGjvfq0RESqlJ+fT35+frljRUVFdfqe5nH+08/MZgAz3f2i2HMDlgDj\n3f2WKq65FBgFHOHusyt5fRlwi7uPiz3fkhAyznD3xyo5PxsoKCgoIDs7O676Jf098wyccw4UF8Nd\nd8GAAVFXJCKSPgoLC8nJyQHIcffCZLefyFDIrcA5ZnaGme0O3A20AiYDmNlUM7uh5GQzGwlcR7hr\nZImZtYs9Wpdp8zbgSjM71sz2AqYCXwLPJPKhpGH7wx/CKp19+8Ipp8Af/wgrV0ZdlYiIQALBwt2n\nAZcAowlDFXsD/dx9ReyUDpSfdHke4S6Qx4FlZR6XlGnzZuAOYBLhbpCWwFE1mIchjVTbtjBtGjzy\nSLh7ZI894Omno65KRETiHgpJBRoKkbK+/jrcNfLsszBwINx+O2y1VdRViYikplQcChFJKe3bh96K\nKVNCuNhzT3jhhairEhFpnBQspEEwgzPOCHeO7LlnuItkyBD44YeoKxMRaVwULKRB6dAh9FZMmhTm\nX+y9N7z2WtRViYg0HgoW0uCYhdtR330XOncOd48MHw6rV0ddmYhIw6dgIQ1Wp07hjpHx4+H++6FH\nD5g+PeqqREQaNgULadCaNAm9FfPnwzbbwMEHw4gR8NNPUVcmItIwKVhIo7DrrvDGGzBmDNx5J+yz\nD8yaFXVVIiINj4KFNBpNm8Kll0JhIWy+ORxwAFx5JfyiZdhERJJGwUIanW7d4K234JprQg9Gr15h\nqERERGpPwUIapYyM0FsxO7YlXs+ecN11sG5dtHWJiKQ7BQtp1Hr0COHissvg2mvD8MgHH0RdlYhI\n+lKwkEaveXO4/np4++2w1kV2NtxyC6xfH3VlIiLpR8FCJKZXrzCxc/jw0IPx29/CJ59EXZWISHpR\nsBApo0WL0Fvx+uuwfDl07w533AEbNkRdmYhIelCwEKnEQQeFO0UGDYILL4TDDoPPP4+6KhGR1Ncs\n6gJEUlXr1qG34vjj4ayzYK+9YOxYeOyxgSxZUlTldR07ZvLSSw/UY6UiIqlDwUJkE/r0CRuaXXIJ\nnHsutGpVxJo1z1ZzxXH1VpuISKrRUIhIDWy5Jdx7Lzz/PPz8c9TViIikLgULkTgcdVTYNVVERCqn\nYCESp6ZNo65ARCR1KViIJNnq1bo9VUQaLwULkSRbuhS6doWJE0PIEBFpTBQsRJJshx3CwloXXAA7\n7gijRsGyZVFXJSJSP3S7qUicOnbMpLpbSjt2zGTatLCg1h13wJ13wt/+BgMGQF5e2ItERKShMneP\n/yKzYcAIoD0wHxju7rOrOLcbMBrIAToCF7v7+ArnbA5cD/QHsoDC2HlzqmgzGygoKCggW/+XlhT3\nww/w97/D7beHsHHIISFg/P73mggqIvWvsLCQnJwcgBx3L0x2+3EPhZjZAGAscDWwDyFYvGhmbau4\npBWwELgM+KqKc+4H+gKnAXsCLwOvmNm28dYnkmq23BIuvjhsaPb441BcDP37w+67h96MVauirlBE\nJHkSmWORB0xy96nu/hEwFFgDnF3Zye4+x90vc/dpwC8VXzezFsAJwKXuPt3dP3P3a4FPgfMSqE8k\nJTVrBieeCG++CTNnQs+eIXDssEPYTfXLL6OuUESk9uIKFmaWQRjSeLXkmIexlFeA3gnW0AxoClRc\nz3AtcFCCbYqktH33hfx8+OwzGDIEJk2Czp3h1FNhdqWDiiIi6SHeHou2hBCwvMLx5YT5FnFz91XA\n28BfzGxbM2tiZqcTgoqGQqRB23FHuPlm+OILuPXW0JOx775w8MHw5JOwfn3UFYqIxCdZt5saEP8s\n0FKnx9pYCvwEXAA8DOh/q9IobLEFDB8OCxbAU09BkyZh2GTXXcOkzx9/jLpCEZGaifd205WEX/bt\nKhzPYuNejBpz90XA78ysJbCluy83s0eARdVdl5eXR2ZmZrljubm55ObmJlqKSKSaNg0TO/v3h4IC\nGDcORoyAq64KQybDh0PHjlFXKSLpIj8/n/z8/HLHioqK6vQ9477d1MxmADPd/aLYcwOWAOPd/ZZN\nXLsIGFfxdtNKztsK+AwY4e73V/K6bjeVRmPp0nD3yKRJ4dbVE08Mt6vuv3/UlYlIOkq5202BW4Fz\nzOwMM9sduJtwS+lkADObamY3lJxsZhlm1t3MegDNge1jz3cuc84RZtbPzDqZ2eHAa8CHJW2KNGbb\nbw833hjmYYwfD3PnQu/ecMAB8Nhj4fZVEZFUEXewiN02eglh0au5wN5AP3dfETulA+Uncm4XO68g\ndnwEYQGse8uckwlMoDRMvB5rU3MsRGJat4bzz4ePPoJnn4UWLeCPf4RddgkTP+u4d1NEpEYSWnkz\nahoKEQnmzQvzMPLzQ9AYNAguvDDcuioiUplUHAoRkRTRowdMmQKLF4dAMXVq6ME46SSYPh3S8N8N\nIpLmFCxEGoBtt4Xrrw/zMO66C95/Hw46KEzwfOQRWLcu6gpFpLFQsBBpQFq1gnPPDcHi//4v7FOS\nmws77QS33ALffx91hSLS0ClYiDRATZrA0UfDyy/D/Plw+OFw5ZXQoUNYC+PTT6OuUEQaKgULkQZu\n773Dtu1LlsAll4Shkd12C4twvf665mGISHIpWIg0Eu3awbXXhoBxzz1hG/dDDgm7rD70EPyy0d7D\nIiLxU7AQaWRatoTBg+G99+CFF2CbbeD008MtqjfdBN9+G3WFIpLOFCxEGikz6NcvhIv33gtzMq65\nBnbYISzEtWBB1BWKSDpSsBAR9tgD7r03DJNcdhk88QR06QLHHgv//rfmYYhIzSlYiMivsrLCTqqL\nF4cJn4sXQ58+sM8+YfEtzcMQkU1RsBCRjbRoAWedFW5VffnlsBHan/4Utmz/619h5cqoKxSRVNUs\n6gJEJHWZwWGHhcdHH8Ftt4Vgcf31cMYZcPHF0LVrOPeIIwayeHHVO6F17JjJSy89UE+Vi0hUFCxE\npEZ23x3uvjuEikmT4M47w22rRx0FeXmweHERCxY8W00Lx9VbrSISHQ2FiEhc2raFK66Azz8PG6B9\n9RUccUR4LiKiYCEiCdlsszAcUlgY7hzJyIi6IhFJBQoWIlIrZnDooWGCZ3VWrYLly+ulJBGJkOZY\niEi9WLYM2rcPC3Dtuy/06hV+5uSEXVhFpGFQsBCRetG5M9x8M8yaBbNnh0mgq1aFHo/ddy8NGr16\nQffuYahFRNKPgoWI1IuMDDjppPAAWL8ePv64NGjMmgX5+bBuXTi3e/cQNErCRpcu0LRptJ9BRDZN\nwUJEkqJjx0yqu6U0vF6qaVPo1i08zjwzHPv557AoV0nYeO01mDgxLCm+xRZh2KTsMMoOO4QeDxFJ\nHQoWIpIUyVj8arPNSnspShQVQUFB+V6Nm28Or2VllQ8avXrBb35T6zJEpBYULEQkpWVmhv1K+vQp\nPfb116VBY/bssCLod9+F13baqXzQyM6G1q2jqV2kMVKwEJG007592Hn12GPDc3f47LPy8zWefRbW\nroUmTcLurWV7NvbcU+tuiNSVhIKFmQ0DRgDtgfnAcHefXcW53YDRQA7QEbjY3cdXOKcJcC1wWqzN\nZcBkd78+kfpEpHExg513Do/c3HCsuBjef798z8bkyWHSaIsWYcfWsj0bu+wSQoiI1E7cwcLMBgBj\ngXOAWUAe8KKZ7ebule152ApYCEwDxlXR7J+Bc4EzgA+AnsBkM/ve3e+Mt0YRkWbNwp0l3bvD4MHh\n2Jo1MG9eadB4/nkYH/tnTps20LNn+Z6N7baLrn6RdJVIj0UeMMndpwKY2VDgGOBs4OaKJ7v7HGBO\n7NwxVbTZG3jG3V+IPV9iZqcC+1ZxvohI3Fq1ggMOCI8S334Lc+aU9mz8/e9www3hte22Kx80evYM\nAUREqhZXsDCzDMKQxg0lx9zdzewVQjhI1FvAEDPb1d0/MbPuwIGEECMiUme23jpsonbEEeG5Oyxd\nWn4IZcwY+OGH8Ppuu5UfQunRA1q2jK5+kVQTb49FW6ApUHHF/+VAl1rUcROwJfCRma0n7GFyhbs/\nUos2RUTiZgYdOoTH8ceHYxs2wCeflJ8c+vjjYd2NZs1gr73K92x066bFvKTxStZdIQZ4La4fAJwK\nnEKYY9EDuN3Mlrl77W+OFxGphSZNwsqfXbrAwIHh2C+/wLvvlgaN6dPhnntCj0erVmExr7I9G507\nazEvaRziDRYrgfVAuwrHs9i4FyMeNwM3uPtjsefvm1kn4HKgymCRl5dHZmb51fxyc3PJLZkWLiJS\nR5o3D+EhJweGDg3HVq0K28iX9Gw8+STcemt47Te/KR80evWCdhX/TyqSZPn5+eTn55c7VlRUVKfv\nae7xdTSY2QxgprtfFHtuwBJgvLvfsolrFwHjKrnddCVh6GNSmWOXA39y990raScbKCgoKCA7Ozuu\n+kVE6tOKFSFklJ2zsWJFeG3HHTfe6XWLLaKtVxq+wsJCcnJyAHLcvTDZ7ScyFHIrMMXMCii93bQV\nMBnAzKYCX7r7qNjzDKAbYbikObB9bHLmKndfGGvzOeAKM/sCeB/IjrV7X4KfS0QkJWyzDRx9dHhA\nGCpZvLg0aMyaBaNHw+rVYaika9fyPRt77135Tq9HHDGQxYur/pdnx46ZSVlmXSRecQcLd59mZm0J\ni161A+YB/dw9lsHpABSXuWQ7YC6lczBGxB7/BUoW6b0AuA6YQBhWWQZMjB0TEWkwzKBTp/A4+eRw\nbP16+PAox8nnAAAb4UlEQVTD8r0aDz0UFvlq3rx0p9eSwNGlCyxeXMSCBc9W805VbwgnUpcSmrzp\n7ncBd1XxWp8KzxcT7vKorr3VwP+LPUREGpWmTcMy43vuCWedFY799FP5nV5feQUmTAivbbFFCB0i\nqUh7hYiIpKAWLWC//cKjRFFR6WJef/1rdLWJVEcr44uIpInMTOjbF/78Zy03LqlLwUJERESSRsFC\nREREkkbBQkRERJJGkzdFRNJQx46ZVHdLaXhdpP4pWIiIpCEtfiWpSkMhIiIikjQKFiIiIpI0ChYi\nIiKSNAoWIiIikjQKFiIiIpI0ChYiIiKSNAoWIiIikjQKFiIiIpI0ChYiIiKSNAoWIiIikjQKFiIi\nac7dGXXxxbh71KWIKFiIiKS7goIC7pgwgcLCwqhLEVGwEBFJd49NnMjY4mIemzgx6lJEFCxERNLR\njVddRZesLI7aaScWPPkkQ4CPn3uOI3fZhS5ZWdx41VVRlyiNlLZNFxFJF+6wZAm8+SYjvv6arGbN\neHrRIp6KvfzU//7HsWaM/MtfOGPo0EhLlcZLwUJEJFWtXw/vvQdvvln6+PJLADK6dmXQ73/PM889\nB19//esllpnJoOHDo6pYRMFCRCRlrF0Ls2eXhoi33oKiIsjIgJ49ITcXDjoIDjgA2rYFoHjXXbmv\nZUsebdOGAd9/T/GGDRF/CGnsEgoWZjYMGAG0B+YDw919dhXndgNGAzlAR+Bidx9f4ZxFsdcqmuDu\nit4i0jB98w1Mn14aJObMgXXrYMstQ3gYOTIEiV69oGXLSpvonJ2NXXghzwwaRP7999P5zTfr+UOI\nlBd3sDCzAcBY4BxgFpAHvGhmu7n7ykouaQUsBKYB46potifQtMzzvYCXYteIiKQ/d/j8c3jjjdIg\n8eGH4bXtt4eDD4bTTgtBYs89oWnTapsrMeHRR3/970HDh2sYRCKXSI9FHjDJ3acCmNlQ4BjgbODm\niie7+xxgTuzcMZU16O7flH1uZscCC939jQTqExGJ3vr18M475edHLFsWXttzTzjkELjiihAkdtwR\nzKKtVyRJ4goWZpZBGNK4oeSYu7uZvQL0TkZBsfc4DfhbMtoTEakXa9bAzJmlIeLtt+HHH6F58zCU\nMXBg6fyIrbeOulqROhNvj0VbwpDF8grHlwNdklIRHA9kAlOS1J6ISPKtWFF+fkRBARQXQ5s2cOCB\nMGpUCBI9e0KLFlFXK1JvknVXiAHJWqT+bOBf7v71Js8UEakP7rBwYflhjY8/Dq/tuGOYH3HmmSFI\ndOsGTbT2oDRe8QaLlcB6oF2F41ls3IsRNzPbETgM6F+T8/Py8sjMzCx3LDc3l9zc3NqWIiKNWXEx\nzJtXPkgsXx7mQey1F/TtC1dfHXomdtwx6mpFqpSfn09+fn65Y0VFRXX6nhbvbnhmNgOY6e4XxZ4b\nsAQY7+63bOLaRcC4ireblnn9GmAIsIO7V3kztpllAwUFBQVkZ2fHVb+IyEZWrdp4fsTq1bDZZrDf\nfqEn4qCDoHfvMNQhksYKCwvJyckByHH3pO9cl8hQyK3AFDMroPR201bAZAAzmwp86e6jYs8zgG6E\n4ZLmwPZm1h1Y5e4LSxqNBZQzgcnVhQoRkVr7+uvy8yPmzg13cWy9deiFuOqqMLyRnR3ChYjUWNzB\nwt2nmVlbwqJX7YB5QD93XxE7pQNQXOaS7YC5lM7BGBF7/BfoU+a8w4AdgH/EW5OISJXc4ZNPQoAo\nWUPi00/Da507h56IIUPCz9131/wIkVpKaPKmu98F3FXFa30qPF9MDXZRdfeXKb9IlohI/NatCz0Q\nZedHrFgRAkP37nDUUSFEHHhgWJhKRJJKe4WISHr74QeYMaM0RMyYEfbcaNkyzI8499wwrLH//mGp\nbBGpUwoWIpJeli0rnR/xxhswfz5s2BA25TroILjuuvBzn33C4lQiUq8ULEQkdbnDRx+VH9b47LPw\n2i67hAAxbFj4udtuWhZbJAUoWIhI6vjll7CCZUmImD497ADapEnogTj22NL5EdtuG3W1IlIJBQsR\nSTp354q8PP46bhxWXS9CUVFYM6IkSMycCT/9BK1ahTUjLrggBIn99oMttqi/DyAiCVOwEJGkKygo\n4I4JEzhx4MCShXiCL78sP6zxzjthuCMrK0ywvPHGECS6d4eMjOg+gIgkTMFCRJLusYkTGVtczGM3\n3EDO4YeXBonFi8MJu+0WAsRFF4Wfu+yi+REiDYSChYgkxY2jRjF54kR2atKEFqtWcRNwwpNPcuST\nT7KoWTPOzMnh8nHjwvyIrKyoyxWROqJgISKJWb48TK6MPUYUFJBVXMzTZjwV24PoKeDYrCxGXnkl\nZwwdquENkUZAwUJENm3DBvjgA3jrrdIwsTC21U/HjnDggWSccQaDDjyQZ04+OSyhHWNt2jBo+PCI\nCheR+qZgISIbW7MGZs0qDRFvvw3ffw9Nm0KPHnDMMWFIo5JlsYvdua9lSx5t04YB339P8QbtKSjS\nmChYiAh89VW5YQ3mzoXi4rAEdu/e8P/+XwgR++4Lm29ebVOds7OxCy/kmUGDyL//fjq/+WY9fQgR\nSQXm7ps+K8WYWTZQUFBQQHZ2dtTliKSXDRvg/ffLB4lFi8JrnTqV9kQceCDssUfopRCRBqOwsLDk\nNvAcdy9MdvvqsRBp6Fav3nhYo6goBIZ99oHjjisNEtttF3W1IpLmFCxEGpqlS8tPspw7F9avh8xM\nOOAAuPTSECJ69YLWraOuVkQaGAULkXS2fj289175YY2SRah22ikEiMGDw89u3cKeGyIidUjBQiSd\nrFoV9tMoCREzZsAPP0CzZpCdDSecEELEAQdoky4RiYSChUgq+/LL8r0R8+eHXoo2bUJ4uOyy0mGN\nVq2irlZERMFCJGWsXw/vvls+SCxZEl7beecQIM49N/zs2lXDGiKSkhQsRKLy449hKKNkouWMGeFY\nRkYY1jjppNK7Ndq1i7paEZEaUbAQqQV354q8PP46bhy2qd05lywp3xvxzjthTYmttw7DGqNGhRDR\nsye0bFk/H0BEJMkULERqoaCggDsmTODEgQNLFpwJiotDcCgbJL78Mry2664hQAwbFn526aJhDRFp\nMBQsRGrhsYkTGVtczGPjx5Nz2mmlIWLmzHAHR/PmkJMDp5xSereGtgwXkQYsoX8mmdkwM1tkZmvN\nbIaZ9arm3G5m9njs/A1mdmEV521nZg+Y2UozW2Nm82NLd4uklBuvuoouWVkc1akTCx55hCHAx1On\ncmS/fnS57jpuXLIErrwS3ngjrHD51ltwyy3Qv79ChYg0eHEHCzMbAIwFrgb2AeYDL5pZ2youaQUs\nBC4DvqqizTbAdOBnoB/QFbgE+C7e+kTqlDsjDjyQke3b02zxYp5aswYDngIy2rZl5G23MeL998Nt\noAcdBC1aRF2xiEi9SqTHIg+Y5O5T3f0jYCiwBji7spPdfY67X+bu04Bfqmjzz8ASdx/s7gXuvtjd\nX3H3RQnUJ5J8q1bBXXfBHnuQceSRDHLHKtypYVtvzaALLyQjIyOiIkVEohdXsDCzDCAHeLXkmIft\nUV8BeteijmOBOWY2zcyWm1mhmQ2uRXsiyfHJJ3DxxbD99jB8eFg/4t//hnfeoXiLLbivZUsO33Zb\n7mvZkuING6KuVkQkcvH2WLQFmgLLKxxfDrSvRR07AecBHwNHAHcD483s9Fq0KZKYDRvgX/+Co4+G\n3XaDBx8Md3AsWgRPPAGHHgpmdM7OxsaM4ZlPP8XGjKFztqYEiYgk664QA7wW1zcBZrn7X2LP55vZ\nHoSw8WBtixOpkaIimDwZJkwIPRXZ2fCPf4Q7OiqZKzHh0Ud//e9Bw4czaPjweixWRCQ1xRssVgLr\ngYrLAGaxcS9GPL4CPqxw7EPghOouysvLIzMzs9yx3NxccnNza1GKNDoffAB33glTp8LPP4cVLydP\nht69YVOLXomIpLD8/Hzy8/PLHSsqKqrT94wrWLj7OjMrAPoCzwJYWG6wLzC+FnVMB7pUONYFWFzd\nRePGjSNb3c+SiPXr4Z//hDvugFdfDUtmjxgR9uLQrqAi0kBU9o/twsLC8gv6JVkiQyG3AlNiAWMW\n4S6RVsBkADObCnzp7qNizzOAboThkubA9mbWHVjl7gtjbY4DppvZ5cA0YD9gMDAkwc8lUrlvvoH7\n7w93eCxeDPvvDw89FHopmjePujoRkbQXd7Bw92mxNStGE4ZE5gH93H1F7JQOQHGZS7YD5lI6B2NE\n7PFfoE+szTlmdjxwE/AXYBFwkbs/EvcnEqnM/Pmhd+Khh8LkzFNOCXd59OwZdWUiIg1KQpM33f0u\n4K4qXutT4flianD3ibs/DzyfSD0ilVq3Dp56KsyfeOMN6NAB/vIXGDxYK2CKiNQR7RUiDc///gf3\n3AN33w1Ll8JvfwuPPRaW1G6mv/IiInVJ/5eVhmP27DDc8eij0LQpnH56WH+ie/eoKxMRaTQULCS9\n/fxz6I24886wo2inTvDXv8LZZ8PWW0ddnYhIo6NgIelp2bIw1HHPPbB8ORx2GDzzDBxzTOitEBGR\nSChYSPpwD1uQ33FHWFq7RQv405/CcEfXrlFXJyIiKFhIOli7Fh55JASKuXNh111h7NgQKiqsvCoi\nItFSsJDUtWRJWMjqvvvg22/hqKPghhvgiCOgSbz754mISH1QsJDU4g7/+U/onXjmGdhiizAR8/zz\nYZddoq5OREQ2QcFCUsPq1WF78jvvhPfeg27dwi6jp58Om28edXUiIlJDChYSrYULQ4D4+9/hxx/h\nuOPg9tvhd7/TzqIiImlIwULq34YN8PLLYbjj+edhq61g6FA47zzo2DHq6kREpBYULKT+/PADTJkS\nhjsWLIAePcLEzNxcaNky6upERCQJFCyk7n30UQgTU6aEW0dPPDFsXX7ggRruEBFpYBQspG6sXx+G\nOe64Iwx7ZGVBXh6cey5sv33U1YmISB1RsJDk+u67MBFzwgRYtAj23RceeABOPhk22yzq6kREpI4p\nWEhyvPtu6J148EEoLoYBA8JqmfvuG3VlIiJSj7R8odSIuzPq4otx99KDxcVhz45DD4W994b/+z+4\n/HL44ovQS6FQISLS6ChYSI0UFBRwx4QJFBYWwooVYWntzp3hpJPCfIpHH4XPP4e//AXatYu6XBER\niYiGQqRGHps4kbHFxTx2yinkfPFFuJvj1FPhggtgn32iLk9ERFKEeiykSjdedRVdttmGo9q1Y8ED\nDzAE+PizzziydWu6bL45N26/vUKFiIiUox4LqdwnnzDixx/JWr2ap9eu5anY4ac2bODYjAxGXnEF\nZwwdGmmJIiKSetRjIaWKi+HJJ+Hww2G33ciYOpVB55+PdepU7jTLzGTQ8OFkZGREUqaIiKQuBQuB\npUvhmmvCPh0nnhh2Gp06NRz/298obtaM+1q25PBtt+W+li0p3rAh6opFRCRFKVg0Vhs2wEsvwfHH\nh0Dxt7/BscfCvHnw1lswcCC0aAFA5+xsbMwYnvn0U2zMGDpnZ0dcvIiIpKqEgoWZDTOzRWa21sxm\nmFmvas7tZmaPx87fYGYXVnLO1bHXyj4+SKQ22YRvvgkhoksX6NcPPv0Uxo+HZcvg7ruhe/eNLpnw\n6KMMGj6cVq1aMWj4cCY8+mgEhYuISDqIe/KmmQ0AxgLnALOAPOBFM9vN3VdWckkrYCEwDRhXTdPv\nAX2Bkl2piuOtTargDjNmwMSJMG1aeH7yyTB5MhxwgDYCExGRpEnkrpA8YJK7TwUws6HAMcDZwM0V\nT3b3OcCc2Lljqmm32N1XJFCPVOXHH+Ghh0JPxPz5sNNOMHo0nHUWbLNN1NWJiEgDFNdQiJllADnA\nqyXHPKzx/ArQu5a17GpmS81soZk9aGY71LK9xuvdd+H888MuosOGQadO8MIL8MknMHKkQoWIiNSZ\neHss2gJNgeUVji8HutSijhnAmcDHwLbANcDrZranu6+uRbuNx08/hX07Jk6E6dNh223h4othyBDY\nQRlNRETqR7IWyDLAN3lWFdz9xTJP3zOzWcBi4I/AP6q6Li8vj8zMzHLHcnNzyc3NTbSU9LNwIUya\nBP/4B6xcCX37wuOPw3HHgdaZEBFp1PLz88nPzy93rKioqE7fM95gsRJYD1TcZSqLjXsxEubuRWa2\nANiluvPGjRtHdmO89bG4OOwkOnEivPgibLUVnHkmnHtuuNtDRESEyv+xXVhYSE5OTp29Z1xzLNx9\nHVBAuHsDADOz2PO3klWUmW0O7Ax8law2G4Rly8Lky86doX9/+O670FOxdCnceqtChYiIRC6RoZBb\ngSlmVkDp7aatgMkAZjYV+NLdR8WeZwDdCMMlzYHtzaw7sMrdF8bOuQV4jjD8sT1wLeF20/L9N42R\nO7z2WuidePpp2GwzOO00GDoUGmNvjYiIpLS4g4W7TzOztsBowpDIPKBfmVtFO1B+DYrtgLmUzsEY\nEXv8F+hT5pqHgd8AK4A3gf3d/Zt462swvv02rDMxaRIsWADdusFtt4UVMSvMKxEREUkVCU3edPe7\ngLuqeK1PheeL2cSQi7s3otmW1XCHWbNC78Sjj8L69XDSSXDffXDQQVrISkREUp62TU8Fq1fDww+H\nQDF3blh34uqr4eyzISsr6upERERqTMEiSu+/H8LEAw+EVTKPOQauvz7s4dG0adTViYiIxE3Bor79\n/DM8+WQIFG+8Ae3awfDhYSGrjh2jrk5ERKRWFCzqy6JFcM89cP/9sGIFHHpomEfRvz80bx51dSIi\nIkmhYFGX1q+H558PvRMvvABbbgl/+lO4VbRr16irExERSToFi7rw9dehZ+Kee2DJEujZM9zZccop\n0KpV1NWJiIjUGQWLZHGH//wn9E489VTYp+PUU0PvRM+eUVcnIiJSLxQsauv772HKFLj7bvjoI9h9\ndxg7Fs44A9q0ibo6ERGReqVgkajZs0OYyM+HdevghBNCb8Uhh2ghKxERabQULOKxZk0IEhMnQkEB\n7LgjXHllWMiqffuoqxMREYmcgkVNfPhh6J2YMgV++AGOOgqeey781EJWIiIiv1KwqMovv4RJmHff\nHSZlZmXB+eeHhaw6d466OhERkZSkYFHR4sWlC1ktXw6//W0Y/jjhBC1kJSIisgkKFhAWsnrxxTB3\n4vnnYfPNw10dQ4fCHntEXZ2IiEjaaNzB4n//K13I6vPPITsbJk2C3Fxo3Trq6kRERNJO4wsW7mHz\nr4kT4YknwuTLU06B886DXr10q6iIiEgtNJ5gUVQEU6eGyZgffAC77QZjxoS9O7beOurqREREGoSG\nHywKC0PvxMMPhzs9+veHO+6A3/1OvRMiIiJJ1jCDxZo1MG1aCBSzZkGHDvDnP8PgwbDttlFXJyIi\n0mCldbBw9/IHPv44DHVMnhyGPvr1g2eegaOPhmZp/VFFRETSQlr/tv3oo4/I2XvvEB4mToTXXoO2\nbeGcc+Dcc2GnnaIuUUREpFFpEnUBtfHy6NHQsSOcfHKYP/HQQ/Dll2FSpkKFiIhIvUvrYLF4wQKO\nXL2aLlttxY2/+x2ceipstlnUZYmIiDRaCQULMxtmZovMbK2ZzTCzXtWc283MHo+dv8HMLtxE25fH\nzrt1U3WMBTJatmTktdcy4i9/SeCTiEhdyc/Pj7oEEYlA3MHCzAYQfqdfDewDzAdeNLO2VVzSClgI\nXAZ8tYm2ewFDYm3WrJ7MTAYNH05GRkZNLxGReqBgIdI4JdJjkQdMcvep7v4RMBRYA5xd2cnuPsfd\nL3P3acAvVTVqZpsDDwKDge9rUshTm21G8YYN8dYvIiIidSSuYGFmGUAO8GrJMQ/3fL4C9K5lLROA\n59z9tRrXc+GFdM7OruXbNizp8K/EKGqsy/dMVtu1bSeR6+O9Jh3+fqW6dPgzbEjf0WS2W5u2Er02\nHb+j8fZYtAWaAssrHF8OtE+0CDM7BegBXB7Pdf1POYUJjz6a6Ns2SKnwl2pTGtL/tJLZtoJF45AO\nf4YN6TuqYFH/krWOhQG+ybMqu9CsA3AbcLi7r6vhZS0APvzww0TeskErKiqisLAw6jKqFUWNdfme\nyWq7tu0kcn2818Rzfjr8XYxCOvy5NKTvaDLbrU1biV5bF9/RMr87W8RdUA3YRqtXVndyGApZA5zo\n7s+WOT4ZyHT34zdx/SJgnLuPL3PsD8CTwHpCQIHQK+KxY5t5hSLN7FTgoRoXLiIiIhWd5u4PJ7vR\nuHos3H2dmRUAfYFnAczMYs/HV3dtNV4B9qpwbDLwIXBTxVAR8yJwGvA58FOC7ysiItIYtQA6EX6X\nJl0iQyG3AlNiAWMW4S6RVoQwgJlNBb5091Gx5xlAN0JvRHNgezPrDqxy94Xuvhr4oOwbmNlq4Bt3\nr3Ssw92/AZKeskRERBqJt+qq4biDhbtPi61ZMRpoB8wD+rn7itgpHYDiMpdsB8yldA7GiNjjv0Cf\nqt4m3rpEREQkenHNsRARERGpTlrvFSIiIiKpRcFCREREkqZBBgszyzSz2WZWaGbvmNngqGsSkfLM\nrKWZfW5mN0ddi4iUF/tuzjOzuWb26qavKJWsBbJSzQ/Awe7+k5m1BN43syfc/buoCxORX10BzIi6\nCBGp1Aagt7uvjffCBtlj4UHJ+hYtYz+tqvNFpH6Z2S5AF+D5qGsRkUoZCWaEBhks4NfhkHnAEuAW\nd/826ppE5Fd/I+wNpMAvkpo2AP8xs5mx1a5rLOWChZkdbGbPmtlSM9tgZsdVcs4wM1tkZmvNbIaZ\n9ap4jrsXuXsPoDNwmpltUx/1izRkyfh+xq752N0/LTlUH7WLNAbJ+h0KHOjuvYA/AKPMbI+a1pBy\nwQJoTVh0axiVLJRlZgOAscDVwD7AfODF2KJdG4kt3PUOcHBdFSzSiCTj+7k/cIqZfUbouRhsZlfW\ndeEijURSfoe6+9dlfj4P5NS0gJReIMvMNgD9K2x4NgOY6e4XxZ4b8AUw3t1vjh1rB6x291Vmlgm8\nCZzi7u/X+4cQaaAS/X5WaONPwB7uPrKeyhZpNGrxO7QV0CT2O3Rz4D/Aue5eUJP3TcUeiyrF9h3J\nAX699SW2SdkrQO8yp+4IvGFmcwlLh9+uUCFSt+L4fopIBOL4jrYD3oz9Dn0LmFzTUAHpd7tpW8KW\n6ssrHF9OmGEOgLvPJnTxiEj9qdH3syx3n1LXRYnIr2r6O3QR0CPRN0mrHotqGNq4TCRV6fspktqS\n+h1Nt2CxElhP6KYpK4uNE5iI1C99P0VSW718R9MqWLj7OqAA6FtyLDbxpC91uLe8iGyavp8iqa2+\nvqMpN8fCzFoDu1B6b/tOZtYd+NbdvwBuBaaYWQEwC8gDWgGTIyhXpFHR91MktaXCdzTlbjc1s0OA\nf7PxeM8Udz87ds75wEhCd848YLi7z6nXQkUaIX0/RVJbKnxHUy5YiIiISPpKqzkWIiIiktoULERE\nRCRpFCxEREQkaRQsREREJGkULERERCRpFCxEREQkaRQsREREJGkULERERCRpFCxEREQkaRQsRERE\nJGkULERERCRpFCxEREQkaRQsREREJGn+P6LUsxOHnWzjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c3f6bb7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_curve([accu_1000, accu_2500, accu_5000, accu_10000, accu_25000],\n",
    "              np.logspace(-3, 1.5, 6, 4), [1000, 2500,5000,10000, 25000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing(accu_1000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhkFCxEREQkMgoWIiIiEpn/D5dfQwi1nlk8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c3234d2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_2500, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM1k66cdj7b0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c38cf5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_5000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFqCAYAAAC6Wjg+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh4iIiMRK4UNERERi9f8BOoyzK+0PHaAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d49ad2f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_10000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFqCAYAAAC6Wjg+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d190dfda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_25000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.92272838311708e-07\n",
      "Losgistic Regression(       0/100000): loss= 13558.3986235257\n",
      "Losgistic Regression(     100/100000): loss= 9015.03859638758\n",
      "Losgistic Regression(     200/100000): loss= 8564.93889943202\n",
      "Losgistic Regression(     300/100000): loss= 8347.85518261214\n",
      "Losgistic Regression(     400/100000): loss= 8223.67747944826\n",
      "Losgistic Regression(     500/100000): loss= 8144.97019368849\n",
      "Losgistic Regression(     600/100000): loss= 8091.40659649746\n",
      "Losgistic Regression(     700/100000): loss= 8052.09745046569\n",
      "Losgistic Regression(     800/100000): loss= 8023.83028131998\n",
      "Losgistic Regression(     900/100000): loss= 8001.74640004757\n",
      "Losgistic Regression(    1000/100000): loss= 7984.34061861021\n",
      "Losgistic Regression(    1100/100000): loss= 7971.13626974165\n",
      "Losgistic Regression(    1200/100000): loss= 7961.27649743662\n",
      "Losgistic Regression(    1300/100000): loss= 7953.50760543862\n",
      "Losgistic Regression(    1400/100000): loss= 7949.1291842453\n",
      "Losgistic Regression(    1500/100000): loss= 7947.5050682421\n",
      "Losgistic Regression(    1600/100000): loss= 7947.44470976324\n",
      "Totoal number of iterations =  1600\n",
      "Loss                        =  7947.44470976\n",
      "1 0.824 0.82076\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(20000)\n",
    "cols = np.arange(train_tX9.shape[1])\n",
    "\n",
    "L = np.linalg.eigvals(train_tX9[idxes][:, cols].T @ train_tX9[idxes][:, cols]).max()\n",
    "print(np.real(1/L))\n",
    "\n",
    "lambda_ = 1\n",
    "w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L),\n",
    "                       max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso)\n",
    "\n",
    "y_pred = predict_labels(w, train_tX9[idxes][:, cols])\n",
    "tr_acc = accuracy(y_pred, transform_y_back(train_y9[idxes]))\n",
    "y_pred = predict_labels(w, cv_tX9[:, cols])\n",
    "te_acc = accuracy(y_pred, transform_y_back(cv_y9))\n",
    "print(lambda_, tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81706153846153851"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, train_tX9[30000:][:, cols])\n",
    "acc = accuracy(y_pred, transform_y_back(train_y9[30000:]))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-37bb4d1dd0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1], \n\u001b[1;32m      6\u001b[0m           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1]], [-5, 5], 'g-')\n\u001b[0;32m----> 7\u001b[0;31m plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1], \n\u001b[0m\u001b[1;32m      8\u001b[0m           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1]],\n\u001b[1;32m      9\u001b[0m          [-5, 5], 'r--')\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAFkCAYAAAB4sKK5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7Jg+iVkHHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d0699a2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w)\n",
    "\n",
    "plt.plot([poly_tX.shape[1], poly_tX.shape[1]], [-5, 5], 'k-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1], poly_tX.shape[1]+decomposed_tX.shape[1]], [-5, 5], 'r-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1], \n",
    "          poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1]], [-5, 5], 'g-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1], \n",
    "          poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1]],\n",
    "         [-5, 5], 'r--')\n",
    "# plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1], \n",
    "#           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1]],\n",
    "#          [-5, 5], 'k--')\n",
    "# plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] \n",
    "#           + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1]\n",
    "#           + mixed_tX.shape[1], \n",
    "#           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] \n",
    "#           + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1]\n",
    "#           + mixed_tX.shape[1]], [-5, 5], 'r:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose features that is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,   9,  12,  14,  15,  16,  23,  26,  52,  58,  65,  69,  75,\n",
       "        76,  77,  84,  88,  98, 106, 107, 108, 112, 114, 117, 123, 124,\n",
       "       136, 138, 142, 143, 144, 150, 151, 155, 156, 162, 169, 170, 171,\n",
       "       173, 175, 189, 202, 205, 210, 214, 222, 225, 226, 227, 228, 231,\n",
       "       241, 247, 256, 260, 270, 282, 287, 289, 309, 315, 322, 323, 326,\n",
       "       333, 342, 345, 346, 349, 351, 358, 366, 367, 368, 369, 371, 373,\n",
       "       376, 379, 383, 384, 386, 387, 391, 403, 404, 407, 416, 418, 421,\n",
       "       423, 424, 434, 442, 443, 447, 448, 453, 458, 462, 465, 466, 467,\n",
       "       469, 471, 480, 481, 487, 494, 497, 498, 502, 513, 516, 519, 520,\n",
       "       521, 523, 524, 525])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = np.arange(len(w_cached))[abs(w_cached) < 0.025]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225000, 529)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tX9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(train_tX9[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(np.max(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002,)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov[abs(cov)>0.99].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7532857516227975e-06\n",
      "Losgistic Regression(       0/100000): loss= 44027.643871765\n",
      "Losgistic Regression(     100/100000): loss= 7149.23068313139\n",
      "Losgistic Regression(     200/100000): loss= 5830.61785617616\n",
      "Losgistic Regression(     300/100000): loss= 5651.0690221374\n",
      "Losgistic Regression(     400/100000): loss= 5610.96407601787\n",
      "Losgistic Regression(     500/100000): loss= 5584.15653210648\n",
      "Losgistic Regression(     600/100000): loss= 5565.44509463008\n",
      "Losgistic Regression(     700/100000): loss= 5554.08864337397\n",
      "Losgistic Regression(     800/100000): loss= 5550.08004866419\n",
      "Losgistic Regression(     900/100000): loss= 5548.08541955738\n",
      "Losgistic Regression(    1000/100000): loss= 5546.07400213894\n",
      "Losgistic Regression(    1100/100000): loss= 5544.52349151197\n",
      "Losgistic Regression(    1200/100000): loss= 5543.66170086858\n",
      "Losgistic Regression(    1300/100000): loss= 5542.96201864574\n",
      "Losgistic Regression(    1400/100000): loss= 5542.35387058249\n",
      "Losgistic Regression(    1500/100000): loss= 5541.91065810279\n",
      "Losgistic Regression(    1600/100000): loss= 5541.71519164196\n",
      "Losgistic Regression(    1700/100000): loss= 5541.62394167513\n",
      "Losgistic Regression(    1800/100000): loss= 5541.48745172193\n",
      "Losgistic Regression(    1900/100000): loss= 5541.3325065692\n",
      "Losgistic Regression(    2000/100000): loss= 5541.11067539367\n",
      "Losgistic Regression(    2100/100000): loss= 5540.8999691436\n",
      "Losgistic Regression(    2200/100000): loss= 5540.70706251728\n",
      "Losgistic Regression(    2300/100000): loss= 5540.5060690598\n",
      "Losgistic Regression(    2400/100000): loss= 5540.29062187811\n",
      "Losgistic Regression(    2500/100000): loss= 5540.08415859742\n",
      "Losgistic Regression(    2600/100000): loss= 5539.88127961009\n",
      "Losgistic Regression(    2700/100000): loss= 5539.6689181606\n",
      "Losgistic Regression(    2800/100000): loss= 5539.44714530173\n",
      "Losgistic Regression(    2900/100000): loss= 5539.216574923\n",
      "Losgistic Regression(    3000/100000): loss= 5538.98013852817\n",
      "Losgistic Regression(    3100/100000): loss= 5538.75102911006\n",
      "Losgistic Regression(    3200/100000): loss= 5538.52371263354\n",
      "Losgistic Regression(    3300/100000): loss= 5538.28912437458\n",
      "Losgistic Regression(    3400/100000): loss= 5538.0534078974\n",
      "Losgistic Regression(    3500/100000): loss= 5537.8140283405\n",
      "Losgistic Regression(    3600/100000): loss= 5537.57023645811\n",
      "Losgistic Regression(    3700/100000): loss= 5537.3218375429\n",
      "Losgistic Regression(    3800/100000): loss= 5537.06784699668\n",
      "Losgistic Regression(    3900/100000): loss= 5536.80731519535\n",
      "Losgistic Regression(    4000/100000): loss= 5536.54498148511\n",
      "Losgistic Regression(    4100/100000): loss= 5536.28038103831\n",
      "Losgistic Regression(    4200/100000): loss= 5536.01240976893\n",
      "Losgistic Regression(    4300/100000): loss= 5535.74071208025\n",
      "Losgistic Regression(    4400/100000): loss= 5535.46573713438\n",
      "Losgistic Regression(    4500/100000): loss= 5535.18777530807\n",
      "Losgistic Regression(    4600/100000): loss= 5534.90763353057\n",
      "Losgistic Regression(    4700/100000): loss= 5534.62584810594\n",
      "Losgistic Regression(    4800/100000): loss= 5534.34252010805\n",
      "Losgistic Regression(    4900/100000): loss= 5534.06008181016\n",
      "Losgistic Regression(    5000/100000): loss= 5533.77558244073\n",
      "Losgistic Regression(    5100/100000): loss= 5533.48646011176\n",
      "Losgistic Regression(    5200/100000): loss= 5533.19142459606\n",
      "Losgistic Regression(    5300/100000): loss= 5532.88908367772\n",
      "Losgistic Regression(    5400/100000): loss= 5532.58130183602\n",
      "Losgistic Regression(    5500/100000): loss= 5532.26956285377\n",
      "Losgistic Regression(    5600/100000): loss= 5531.95588454197\n",
      "Losgistic Regression(    5700/100000): loss= 5531.64108535763\n",
      "Losgistic Regression(    5800/100000): loss= 5531.32588390287\n",
      "Losgistic Regression(    5900/100000): loss= 5531.00973926472\n",
      "Losgistic Regression(    6000/100000): loss= 5530.69070445144\n",
      "Losgistic Regression(    6100/100000): loss= 5530.36814401525\n",
      "Losgistic Regression(    6200/100000): loss= 5530.04175659677\n",
      "Losgistic Regression(    6300/100000): loss= 5529.7130126271\n",
      "Losgistic Regression(    6400/100000): loss= 5529.38165142565\n",
      "Losgistic Regression(    6500/100000): loss= 5529.04745201874\n",
      "Losgistic Regression(    6600/100000): loss= 5528.7110806547\n",
      "Losgistic Regression(    6700/100000): loss= 5528.3724889422\n",
      "Losgistic Regression(    6800/100000): loss= 5528.03113446775\n",
      "Losgistic Regression(    6900/100000): loss= 5527.68744216968\n",
      "Losgistic Regression(    7000/100000): loss= 5527.3418397663\n",
      "Losgistic Regression(    7100/100000): loss= 5526.99439188549\n",
      "Losgistic Regression(    7200/100000): loss= 5526.64458487969\n",
      "Losgistic Regression(    7300/100000): loss= 5526.29283405827\n",
      "Losgistic Regression(    7400/100000): loss= 5525.93974523049\n",
      "Losgistic Regression(    7500/100000): loss= 5525.58526343156\n",
      "Losgistic Regression(    7600/100000): loss= 5525.22916856464\n",
      "Losgistic Regression(    7700/100000): loss= 5524.87154170972\n",
      "Losgistic Regression(    7800/100000): loss= 5524.51261290613\n",
      "Losgistic Regression(    7900/100000): loss= 5524.15253952439\n",
      "Losgistic Regression(    8000/100000): loss= 5523.7913003972\n",
      "Losgistic Regression(    8100/100000): loss= 5523.42947475149\n",
      "Losgistic Regression(    8200/100000): loss= 5523.06646667013\n",
      "Losgistic Regression(    8300/100000): loss= 5522.70282574999\n",
      "Losgistic Regression(    8400/100000): loss= 5522.43071213075\n",
      "Losgistic Regression(    8500/100000): loss= 5521.9864399381\n",
      "Losgistic Regression(    8600/100000): loss= 5521.03663115989\n",
      "Losgistic Regression(    8700/100000): loss= 5520.16355490774\n",
      "Losgistic Regression(    8800/100000): loss= 5519.30518081302\n",
      "Losgistic Regression(    8900/100000): loss= 5518.47143132678\n",
      "Losgistic Regression(    9000/100000): loss= 5517.66730608177\n",
      "Losgistic Regression(    9100/100000): loss= 5516.8848320604\n",
      "Losgistic Regression(    9200/100000): loss= 5516.12045766804\n",
      "Losgistic Regression(    9300/100000): loss= 5515.36861301696\n",
      "Losgistic Regression(    9400/100000): loss= 5514.64873309955\n",
      "Losgistic Regression(    9500/100000): loss= 5513.95395597739\n",
      "Losgistic Regression(    9600/100000): loss= 5513.27296627707\n",
      "Losgistic Regression(    9700/100000): loss= 5512.60656585565\n",
      "Losgistic Regression(    9800/100000): loss= 5511.96209419897\n",
      "Losgistic Regression(    9900/100000): loss= 5511.35179831282\n",
      "Losgistic Regression(   10000/100000): loss= 5510.75848649488\n",
      "Losgistic Regression(   10100/100000): loss= 5510.17028203018\n",
      "Losgistic Regression(   10200/100000): loss= 5509.63955915718\n",
      "Losgistic Regression(   10300/100000): loss= 5508.97815195267\n",
      "Losgistic Regression(   10400/100000): loss= 5508.06982248843\n",
      "Losgistic Regression(   10500/100000): loss= 5507.23516172221\n",
      "Losgistic Regression(   10600/100000): loss= 5506.42303205762\n",
      "Losgistic Regression(   10700/100000): loss= 5505.64016583945\n",
      "Losgistic Regression(   10800/100000): loss= 5504.88858553373\n",
      "Losgistic Regression(   10900/100000): loss= 5504.17544005739\n",
      "Losgistic Regression(   11000/100000): loss= 5503.50012416932\n",
      "Losgistic Regression(   11100/100000): loss= 5502.84567707191\n",
      "Losgistic Regression(   11200/100000): loss= 5502.20786846475\n",
      "Losgistic Regression(   11300/100000): loss= 5501.60210248123\n",
      "Losgistic Regression(   11400/100000): loss= 5501.05982186157\n",
      "Losgistic Regression(   11500/100000): loss= 5500.45549177256\n",
      "Losgistic Regression(   11600/100000): loss= 5499.59817415762\n",
      "Losgistic Regression(   11700/100000): loss= 5498.79081573418\n",
      "Losgistic Regression(   11800/100000): loss= 5498.02629115607\n",
      "Losgistic Regression(   11900/100000): loss= 5497.3091165667\n",
      "Losgistic Regression(   12000/100000): loss= 5496.62878087498\n",
      "Losgistic Regression(   12100/100000): loss= 5495.97601418442\n",
      "Losgistic Regression(   12200/100000): loss= 5495.35697499503\n",
      "Losgistic Regression(   12300/100000): loss= 5494.76921641822\n",
      "Losgistic Regression(   12400/100000): loss= 5494.2428542026\n",
      "Losgistic Regression(   12500/100000): loss= 5493.53002657472\n",
      "Losgistic Regression(   12600/100000): loss= 5492.72952127158\n",
      "Losgistic Regression(   12700/100000): loss= 5492.00980337848\n",
      "Losgistic Regression(   12800/100000): loss= 5491.325995707\n",
      "Losgistic Regression(   12900/100000): loss= 5490.68489270183\n",
      "Losgistic Regression(   13000/100000): loss= 5490.08224042034\n",
      "Losgistic Regression(   13100/100000): loss= 5489.52004817367\n",
      "Losgistic Regression(   13200/100000): loss= 5489.011414236\n",
      "Losgistic Regression(   13300/100000): loss= 5488.2980801529\n",
      "Losgistic Regression(   13400/100000): loss= 5487.57156324623\n",
      "Losgistic Regression(   13500/100000): loss= 5486.8987038258\n",
      "Losgistic Regression(   13600/100000): loss= 5486.26718643836\n",
      "Losgistic Regression(   13700/100000): loss= 5485.67913986571\n",
      "Losgistic Regression(   13800/100000): loss= 5485.13221230525\n",
      "Losgistic Regression(   13900/100000): loss= 5484.62580920705\n",
      "Losgistic Regression(   14000/100000): loss= 5483.90751167447\n",
      "Losgistic Regression(   14100/100000): loss= 5483.03808067515\n",
      "Losgistic Regression(   14200/100000): loss= 5482.22733240992\n",
      "Losgistic Regression(   14300/100000): loss= 5481.48122856812\n",
      "Losgistic Regression(   14400/100000): loss= 5480.7871127802\n",
      "Losgistic Regression(   14500/100000): loss= 5480.13723707136\n",
      "Losgistic Regression(   14600/100000): loss= 5479.2900415124\n",
      "Losgistic Regression(   14700/100000): loss= 5478.46526880945\n",
      "Losgistic Regression(   14800/100000): loss= 5477.6947228974\n",
      "Losgistic Regression(   14900/100000): loss= 5476.98214956527\n",
      "Losgistic Regression(   15000/100000): loss= 5476.36130948395\n",
      "Losgistic Regression(   15100/100000): loss= 5475.66832117198\n",
      "Losgistic Regression(   15200/100000): loss= 5474.83064847409\n",
      "Losgistic Regression(   15300/100000): loss= 5474.07010564958\n",
      "Losgistic Regression(   15400/100000): loss= 5473.36269178611\n",
      "Losgistic Regression(   15500/100000): loss= 5472.7413802886\n",
      "Losgistic Regression(   15600/100000): loss= 5472.08778253234\n",
      "Losgistic Regression(   15700/100000): loss= 5471.30061066398\n",
      "Losgistic Regression(   15800/100000): loss= 5470.58924455943\n",
      "Losgistic Regression(   15900/100000): loss= 5469.9304088886\n",
      "Losgistic Regression(   16000/100000): loss= 5469.29335825475\n",
      "Losgistic Regression(   16100/100000): loss= 5468.58621520046\n",
      "Losgistic Regression(   16200/100000): loss= 5467.93292572775\n",
      "Losgistic Regression(   16300/100000): loss= 5467.30732093454\n",
      "Losgistic Regression(   16400/100000): loss= 5466.72801791811\n",
      "Losgistic Regression(   16500/100000): loss= 5466.00978013297\n",
      "Losgistic Regression(   16600/100000): loss= 5465.27623360792\n",
      "Losgistic Regression(   16700/100000): loss= 5464.67843660639\n",
      "Losgistic Regression(   16800/100000): loss= 5464.18513160678\n",
      "Losgistic Regression(   16900/100000): loss= 5463.53178458592\n",
      "Losgistic Regression(   17000/100000): loss= 5462.84758108584\n",
      "Losgistic Regression(   17100/100000): loss= 5462.24038847284\n",
      "Losgistic Regression(   17200/100000): loss= 5461.69561594346\n",
      "Losgistic Regression(   17300/100000): loss= 5461.05816075169\n",
      "Losgistic Regression(   17400/100000): loss= 5460.48822720874\n",
      "Losgistic Regression(   17500/100000): loss= 5459.96698087117\n",
      "Losgistic Regression(   17600/100000): loss= 5459.31511235488\n",
      "Losgistic Regression(   17700/100000): loss= 5458.71214125892\n",
      "Losgistic Regression(   17800/100000): loss= 5458.20314872269\n",
      "Losgistic Regression(   17900/100000): loss= 5457.625744967\n",
      "Losgistic Regression(   18000/100000): loss= 5457.06849608827\n",
      "Losgistic Regression(   18100/100000): loss= 5456.54301380383\n",
      "Losgistic Regression(   18200/100000): loss= 5455.95363957723\n",
      "Losgistic Regression(   18300/100000): loss= 5455.45109880843\n",
      "Losgistic Regression(   18400/100000): loss= 5454.928072419\n",
      "Losgistic Regression(   18500/100000): loss= 5454.36376239695\n",
      "Losgistic Regression(   18600/100000): loss= 5453.90506777163\n",
      "Losgistic Regression(   18700/100000): loss= 5453.32387332031\n",
      "Losgistic Regression(   18800/100000): loss= 5452.87688308764\n",
      "Losgistic Regression(   18900/100000): loss= 5452.39076550068\n",
      "Losgistic Regression(   19000/100000): loss= 5451.90605772973\n",
      "Losgistic Regression(   19100/100000): loss= 5451.40756362164\n",
      "Losgistic Regression(   19200/100000): loss= 5450.94162869991\n",
      "Losgistic Regression(   19300/100000): loss= 5450.44775571802\n",
      "Losgistic Regression(   19400/100000): loss= 5449.97108976042\n",
      "Losgistic Regression(   19500/100000): loss= 5449.52451204418\n",
      "Losgistic Regression(   19600/100000): loss= 5449.06233309833\n",
      "Losgistic Regression(   19700/100000): loss= 5448.60529507512\n",
      "Losgistic Regression(   19800/100000): loss= 5448.15871488234\n",
      "Losgistic Regression(   19900/100000): loss= 5447.73472524713\n",
      "Losgistic Regression(   20000/100000): loss= 5447.30192609267\n",
      "Losgistic Regression(   20100/100000): loss= 5446.89926029063\n",
      "Losgistic Regression(   20200/100000): loss= 5446.49957665853\n",
      "Losgistic Regression(   20300/100000): loss= 5446.06317913\n",
      "Losgistic Regression(   20400/100000): loss= 5445.63868069095\n",
      "Losgistic Regression(   20500/100000): loss= 5445.2073107274\n",
      "Losgistic Regression(   20600/100000): loss= 5444.81515958931\n",
      "Losgistic Regression(   20700/100000): loss= 5444.45464744083\n",
      "Losgistic Regression(   20800/100000): loss= 5444.08631193854\n",
      "Losgistic Regression(   20900/100000): loss= 5443.72545113849\n",
      "Losgistic Regression(   21000/100000): loss= 5443.3470035678\n",
      "Losgistic Regression(   21100/100000): loss= 5442.95857984612\n",
      "Losgistic Regression(   21200/100000): loss= 5442.57736093928\n",
      "Losgistic Regression(   21300/100000): loss= 5442.21751646783\n",
      "Losgistic Regression(   21400/100000): loss= 5441.894976955\n",
      "Losgistic Regression(   21500/100000): loss= 5441.5770093103\n",
      "Losgistic Regression(   21600/100000): loss= 5441.24443259131\n",
      "Losgistic Regression(   21700/100000): loss= 5440.8937862527\n",
      "Losgistic Regression(   21800/100000): loss= 5440.55351372247\n",
      "Losgistic Regression(   21900/100000): loss= 5440.22658917484\n",
      "Losgistic Regression(   22000/100000): loss= 5439.92030942567\n",
      "Losgistic Regression(   22100/100000): loss= 5439.62585180653\n",
      "Losgistic Regression(   22200/100000): loss= 5439.32531559103\n",
      "Losgistic Regression(   22300/100000): loss= 5439.00396032982\n",
      "Losgistic Regression(   22400/100000): loss= 5438.69286075977\n",
      "Losgistic Regression(   22500/100000): loss= 5438.40313616372\n",
      "Losgistic Regression(   22600/100000): loss= 5438.13022843492\n",
      "Losgistic Regression(   22700/100000): loss= 5437.87098243842\n",
      "Losgistic Regression(   22800/100000): loss= 5437.60326153099\n",
      "Losgistic Regression(   22900/100000): loss= 5437.32182142731\n",
      "Losgistic Regression(   23000/100000): loss= 5437.04738034934\n",
      "Losgistic Regression(   23100/100000): loss= 5436.79127550337\n",
      "Losgistic Regression(   23200/100000): loss= 5436.5410236222\n",
      "Losgistic Regression(   23300/100000): loss= 5436.29364721691\n",
      "Losgistic Regression(   23400/100000): loss= 5436.05610131404\n",
      "Losgistic Regression(   23500/100000): loss= 5435.82452963126\n",
      "Losgistic Regression(   23600/100000): loss= 5435.58704986346\n",
      "Losgistic Regression(   23700/100000): loss= 5435.35095200302\n",
      "Losgistic Regression(   23800/100000): loss= 5435.13068304116\n",
      "Losgistic Regression(   23900/100000): loss= 5434.9116005854\n",
      "Losgistic Regression(   24000/100000): loss= 5434.69996810045\n",
      "Losgistic Regression(   24100/100000): loss= 5434.49005123812\n",
      "Losgistic Regression(   24200/100000): loss= 5434.28357171052\n",
      "Losgistic Regression(   24300/100000): loss= 5434.0784951496\n",
      "Losgistic Regression(   24400/100000): loss= 5433.88310657817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-fb28b464b319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L),\n\u001b[0;32m----> 9\u001b[0;31m                        max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tX9\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_AGDR\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Restart if the loss of new weight is larger than the original one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "cols  = feature_columns\n",
    "\n",
    "L     = np.linalg.eigvals(train_tX9[idxes][:, cols].T @ train_tX9[idxes][:, cols]).max()\n",
    "print(np.real(1/L))\n",
    "\n",
    "lambda_ = 1\n",
    "w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L),\n",
    "                       max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso)\n",
    "\n",
    "y_pred = predict_labels(w, train_tX9[idxes][:, cols])\n",
    "tr_acc = accuracy(y_pred, transform_y_back(train_y9[idxes]))\n",
    "y_pred = predict_labels(w, cv_tX9[:, cols])\n",
    "te_acc = accuracy(y_pred, transform_y_back(cv_y9))\n",
    "y_pred = predict_labels(w, train_tX9[20000:][:, cols])\n",
    "acc = accuracy(y_pred, transform_y_back(train_y9[20000:]))\n",
    "print(lambda_, tr_acc, te_acc, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best lambda for a degree 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/150000): loss=6128.661958753694\n",
      "Losgistic Regression(1000/150000): loss=4382.793973805763\n",
      "Losgistic Regression(2000/150000): loss=4321.439300698346\n",
      "Losgistic Regression(3000/150000): loss=4291.208251980238\n",
      "Losgistic Regression(4000/150000): loss=4270.574245773539\n",
      "Losgistic Regression(5000/150000): loss=4255.255208797125\n",
      "Losgistic Regression(6000/150000): loss=4244.187346417512\n",
      "Losgistic Regression(7000/150000): loss=4236.342382593083\n",
      "Losgistic Regression(8000/150000): loss=4229.2149455352765\n",
      "Losgistic Regression(9000/150000): loss=4222.549848793302\n",
      "Losgistic Regression(10000/150000): loss=4216.256821112506\n",
      "Losgistic Regression(11000/150000): loss=4210.280363793595\n",
      "Losgistic Regression(12000/150000): loss=4204.581620909371\n",
      "Losgistic Regression(13000/150000): loss=4199.130612876477\n",
      "Losgistic Regression(14000/150000): loss=4193.902700925177\n",
      "Losgistic Regression(15000/150000): loss=4188.876869375156\n",
      "Losgistic Regression(16000/150000): loss=4184.034829564825\n",
      "Losgistic Regression(17000/150000): loss=4179.3605048153\n",
      "Losgistic Regression(18000/150000): loss=4175.203975960235\n",
      "Losgistic Regression(19000/150000): loss=4172.128890451348\n",
      "Losgistic Regression(20000/150000): loss=4169.404308525439\n",
      "Losgistic Regression(21000/150000): loss=4166.945582792079\n",
      "Losgistic Regression(22000/150000): loss=4164.697910753656\n",
      "Losgistic Regression(23000/150000): loss=4162.623370955874\n",
      "Losgistic Regression(24000/150000): loss=4160.694444672941\n",
      "Losgistic Regression(25000/150000): loss=4158.890417925157\n",
      "Losgistic Regression(26000/150000): loss=4157.195232293507\n",
      "Losgistic Regression(27000/150000): loss=4155.596121161573\n",
      "Losgistic Regression(28000/150000): loss=4154.082702688859\n",
      "Losgistic Regression(29000/150000): loss=4152.646368926147\n",
      "Losgistic Regression(30000/150000): loss=4151.279853515938\n",
      "Losgistic Regression(31000/150000): loss=4149.976926771299\n",
      "Losgistic Regression(32000/150000): loss=4148.732176552424\n",
      "Losgistic Regression(33000/150000): loss=4147.540849240619\n",
      "Losgistic Regression(34000/150000): loss=4146.398730789539\n",
      "Losgistic Regression(35000/150000): loss=4145.3020601081635\n",
      "Losgistic Regression(36000/150000): loss=4144.247460850778\n",
      "Losgistic Regression(37000/150000): loss=4143.231888176198\n",
      "Losgistic Regression(38000/150000): loss=4142.252586093812\n",
      "Losgistic Regression(39000/150000): loss=4141.307052495995\n",
      "Losgistic Regression(40000/150000): loss=4140.393009940427\n",
      "Losgistic Regression(41000/150000): loss=4139.50838085982\n",
      "Losgistic Regression(42000/150000): loss=4138.65126787957\n",
      "Losgistic Regression(43000/150000): loss=4137.819930481284\n",
      "Losgistic Regression(44000/150000): loss=4137.0127732559595\n",
      "Losgistic Regression(45000/150000): loss=4136.228330432692\n",
      "Losgistic Regression(46000/150000): loss=4135.465253085196\n",
      "Losgistic Regression(47000/150000): loss=4134.1443323593485\n",
      "Losgistic Regression(48000/150000): loss=4132.836213664394\n",
      "Losgistic Regression(49000/150000): loss=4131.548598167255\n",
      "Losgistic Regression(50000/150000): loss=4130.280447452421\n",
      "Losgistic Regression(51000/150000): loss=4129.030794438789\n",
      "Losgistic Regression(52000/150000): loss=4127.7987378407415\n",
      "Losgistic Regression(53000/150000): loss=4126.583436998648\n",
      "Losgistic Regression(54000/150000): loss=4125.3841070212775\n",
      "Losgistic Regression(55000/150000): loss=4124.200014284044\n",
      "Losgistic Regression(56000/150000): loss=4123.030472284158\n",
      "Losgistic Regression(57000/150000): loss=4121.87483784062\n",
      "Losgistic Regression(58000/150000): loss=4120.732507621012\n",
      "Losgistic Regression(59000/150000): loss=4119.602914974805\n",
      "Losgistic Regression(60000/150000): loss=4118.485527050165\n",
      "Losgistic Regression(61000/150000): loss=4117.379842171625\n",
      "Losgistic Regression(62000/150000): loss=4116.28538745605\n",
      "Losgistic Regression(63000/150000): loss=4115.201716646274\n",
      "Losgistic Regression(64000/150000): loss=4114.128408140856\n",
      "Losgistic Regression(65000/150000): loss=4113.065063202537\n",
      "Losgistic Regression(66000/150000): loss=4112.011304327397\n",
      "Losgistic Regression(67000/150000): loss=4110.966773759254\n",
      "Losgistic Regression(68000/150000): loss=4109.931132134869\n",
      "Losgistic Regression(69000/150000): loss=4108.9040572471185\n",
      "Losgistic Regression(70000/150000): loss=4107.88524291448\n",
      "Losgistic Regression(71000/150000): loss=4106.874397946057\n",
      "Losgistic Regression(72000/150000): loss=4105.8712451931\n",
      "Losgistic Regression(73000/150000): loss=4104.875520678365\n",
      "Losgistic Regression(74000/150000): loss=4103.886972795643\n",
      "Losgistic Regression(75000/150000): loss=4102.905361572792\n",
      "Losgistic Regression(76000/150000): loss=4101.930457992193\n",
      "Losgistic Regression(77000/150000): loss=4100.962043363106\n",
      "Losgistic Regression(78000/150000): loss=4099.999908741166\n",
      "Losgistic Regression(79000/150000): loss=4099.043854390546\n",
      "Losgistic Regression(80000/150000): loss=4098.093689285304\n",
      "Losgistic Regression(81000/150000): loss=4097.149230644885\n",
      "Losgistic Regression(82000/150000): loss=4096.2103035032105\n",
      "Losgistic Regression(83000/150000): loss=4095.2767403064836\n",
      "Losgistic Regression(84000/150000): loss=4094.348380641769\n",
      "Losgistic Regression(85000/150000): loss=4093.4250696102904\n",
      "Losgistic Regression(86000/150000): loss=4092.5066611711864\n",
      "Losgistic Regression(87000/150000): loss=4091.593013716083\n",
      "Losgistic Regression(88000/150000): loss=4090.6839912387345\n",
      "Losgistic Regression(89000/150000): loss=4089.779463475005\n",
      "Losgistic Regression(90000/150000): loss=4088.8793054909006\n",
      "Losgistic Regression(91000/150000): loss=4087.9833973753302\n",
      "Losgistic Regression(92000/150000): loss=4087.091623991697\n",
      "Losgistic Regression(93000/150000): loss=4086.2038746801322\n",
      "Losgistic Regression(94000/150000): loss=4085.3200435459976\n",
      "Losgistic Regression(95000/150000): loss=4084.4400271378972\n",
      "Losgistic Regression(96000/150000): loss=4083.563726604002\n",
      "Losgistic Regression(97000/150000): loss=4082.6910490395876\n",
      "Losgistic Regression(98000/150000): loss=4081.821904073697\n",
      "Losgistic Regression(99000/150000): loss=4080.9562047323307\n",
      "Losgistic Regression(100000/150000): loss=4080.0938673345763\n",
      "Losgistic Regression(101000/150000): loss=4079.234811363942\n",
      "Losgistic Regression(102000/150000): loss=4078.3789593505944\n",
      "Losgistic Regression(103000/150000): loss=4077.5262367569426\n",
      "Losgistic Regression(104000/150000): loss=4076.6765718659494\n",
      "Losgistic Regression(105000/150000): loss=4075.8298956724216\n",
      "Losgistic Regression(106000/150000): loss=4074.986141777644\n",
      "Losgistic Regression(107000/150000): loss=4074.145246287765\n",
      "Losgistic Regression(108000/150000): loss=4073.3071477161557\n",
      "Losgistic Regression(109000/150000): loss=4072.471786889442\n",
      "Losgistic Regression(110000/150000): loss=4071.639106857446\n",
      "Losgistic Regression(111000/150000): loss=4070.8090528066728\n",
      "Losgistic Regression(112000/150000): loss=4069.981571977565\n",
      "Losgistic Regression(113000/150000): loss=4069.1566135849607\n",
      "Losgistic Regression(114000/150000): loss=4068.334128741864\n",
      "Losgistic Regression(115000/150000): loss=4067.514070386298\n",
      "Losgistic Regression(116000/150000): loss=4066.6963932110834\n",
      "Losgistic Regression(117000/150000): loss=4065.8810555570626\n",
      "Losgistic Regression(118000/150000): loss=4065.068013712836\n",
      "Losgistic Regression(119000/150000): loss=4064.25722701785\n",
      "Losgistic Regression(120000/150000): loss=4063.44865651399\n",
      "Losgistic Regression(121000/150000): loss=4062.642264707486\n",
      "Losgistic Regression(122000/150000): loss=4061.83801549441\n",
      "Losgistic Regression(123000/150000): loss=4061.0358741047557\n",
      "Losgistic Regression(124000/150000): loss=4060.235807051358\n",
      "Losgistic Regression(125000/150000): loss=4059.437782534584\n",
      "Losgistic Regression(126000/150000): loss=4058.641769063937\n",
      "Losgistic Regression(127000/150000): loss=4057.8477366387892\n",
      "Losgistic Regression(128000/150000): loss=4057.055656489738\n",
      "Losgistic Regression(129000/150000): loss=4056.2655008337697\n",
      "Losgistic Regression(130000/150000): loss=4055.477242900723\n",
      "Losgistic Regression(131000/150000): loss=4054.6908568974927\n",
      "Losgistic Regression(132000/150000): loss=4053.906317968484\n",
      "Losgistic Regression(133000/150000): loss=4053.1236021583495\n",
      "Losgistic Regression(134000/150000): loss=4052.3426863762284\n",
      "Losgistic Regression(135000/150000): loss=4051.5635483610204\n",
      "Losgistic Regression(136000/150000): loss=4050.7861666489475\n",
      "Losgistic Regression(137000/150000): loss=4050.010520541866\n",
      "Losgistic Regression(138000/150000): loss=4049.236590076733\n",
      "Losgistic Regression(139000/150000): loss=4048.46435599626\n",
      "Losgistic Regression(140000/150000): loss=4047.6937997206537\n",
      "Losgistic Regression(141000/150000): loss=4046.9249033204223\n",
      "Losgistic Regression(142000/150000): loss=4046.1576494901965\n",
      "Losgistic Regression(143000/150000): loss=4045.3920215234975\n",
      "Losgistic Regression(144000/150000): loss=4044.62800328853\n",
      "Losgistic Regression(145000/150000): loss=4043.8655792049185\n",
      "Losgistic Regression(146000/150000): loss=4043.1047339495753\n",
      "Losgistic Regression(147000/150000): loss=4042.3454523613855\n",
      "Losgistic Regression(148000/150000): loss=4041.587721752981\n",
      "Losgistic Regression(149000/150000): loss=4040.8315283470806\n",
      "0.8207 0.81404\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.79002650416e-06\n",
      "Losgistic Regression(0/150000): loss=7981.108636380177\n",
      "Losgistic Regression(100/150000): loss=7981.110107054736\n",
      "0.00147067455873\n",
      "Totoal number of iterations =  100\n",
      "Loss =  7981.11010705\n",
      "0.8212 0.8158\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(20000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w_agdr = logistic_AGDR(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso, w0=w_agdr)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], \n",
    "                                         cv_tX7, cv_y7, w_agdr)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_validation_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-db02c5f6f0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m cross_validation_var(y9, tX9, \n\u001b[0m\u001b[1;32m      5\u001b[0m                      \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_up_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_validation_var' is not defined"
     ]
    }
   ],
   "source": [
    "set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "               max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n",
    "\n",
    "cross_validation_var(y9, tX9, \n",
    "                     K=25, \n",
    "                     func=set_up_f, \n",
    "                     seed=3, \n",
    "                     fpred=predict_cv,\n",
    "                     faccuracy=accuracy,\n",
    "                     max_fold=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filled_test_tX_median = fill_na(tX_test, np.median)\n",
    "#\n",
    "log_test_tX               = logs_of_features(filled_test_tX_median, Features_using_log)\n",
    "decomposed_test_tX        = decompose_categorical_features(tX_test)\n",
    "missing_indicator_test_tX = missing_indicator(tX_test)\n",
    "inverse_test_tX           = inver_terms(filled_test_tX_median, Features_using_log)\n",
    "mixed_test_tX             = mixed_features(filled_test_tX_median, Features_non_categorical)\n",
    "# mixed_invese_test_tX      = mixed_inverse_features(filled_test_tX_median, Features_denominate, Features_numerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Change\n",
    "# * filled_test_tX_median\n",
    "# * w\n",
    "# * mean_x, std_x\n",
    "# * degree\n",
    "##############################\n",
    "# For non categorical features, build polynomials\n",
    "poly_test_tX     = build_polynomial_without_mixed_term(filled_test_tX_median[:, Features_non_categorical], degree=degree)\n",
    "log_poly_test_tX = build_polynomial_without_mixed_term(log_test_tX, degree=degree)\n",
    "inv_poly_test_tX = build_polynomial_without_mixed_term(inverse_test_tX, degree=degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a design matrix\n",
    "design_matrix_test = np.c_[poly_test_tX, decomposed_test_tX, missing_indicator_test_tX, log_poly_test_tX, \n",
    "                           mixed_test_tX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 478)\n"
     ]
    }
   ],
   "source": [
    "test_tX, _, _ = standardize(design_matrix_test, mean_x, std_x)\n",
    "print(test_tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y0 = predict_labels(weights=w, data=test_tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = y0\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62168668762032808"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(y1 - y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/result12.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y0, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predt_11 = pd.read_csv('../results/result11.csv')\n",
    "predt_10 = pd.read_csv('../results/result10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predt_9  = pd.read_csv('../results/result9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     485635\n",
       "False     82603\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predt_11['Prediction'] == series).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series = pd.Series(y0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0702178367f34f9ebe15ab2ad5078a77": {
     "views": []
    },
    "104197bfe94d4c0d969b505d11b5c64f": {
     "views": []
    },
    "278b55cc2b5d470b914c860b737b13dd": {
     "views": []
    },
    "3bde44762a854e17913f8a1d09a4c769": {
     "views": []
    },
    "85d5f2ee705b4c90b7174e8404fecf2f": {
     "views": []
    },
    "c1bc1dc628504eb8a05c1d397345357a": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "e03e8d3520474092b8ccdcf44afab94a": {
     "views": []
    },
    "ff9fba9a21524b73b062f1f5b3ed2858": {
     "views": []
    }
   },
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
