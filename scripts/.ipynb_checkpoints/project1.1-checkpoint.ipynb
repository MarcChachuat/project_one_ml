{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from data_preprocessing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading of the data : done\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"loading of the data : done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio_of_training):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    p = np.random.permutation(np.arange(y.shape[0]))\n",
    "    n = int(y.shape[0] * ratio_of_training)\n",
    "    return  x[p][:n], x[p][n:], y[p][:n], y[p][n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoYkqZuhIUnqZmhIkrr9f9rzVfTtxFAnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a62e0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "\n",
    "from ipywidgets import IntSlider, interact\n",
    "from IPython.display import display\n",
    "\n",
    "def plot_hist(tx, i, transformation=None):\n",
    "    plt.figure()\n",
    "    if transformation is None:\n",
    "        plt.hist(tx[:, i], bins=100)\n",
    "    else:\n",
    "        plt.hist(transformation(tx[:, i]), bins=100)\n",
    "    plt.title(\"feature %i\" % i)\n",
    "    plt.show()\n",
    "\n",
    "interact(lambda x:plot_hist(tX, x), x=IntSlider(min=0, max=29))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Good Features                   : [7, 10, 14, 15, 17, 18, 20\n",
    "    Suffering from outlier          : [3, 8, 19, 23, 26, 29]\n",
    "    Suffering from outlier (skewed) : [1, 2, 5, 9, 13, 16, 21]\n",
    "    missing values                  : [0, 4, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "    categorical                     : [11, 12, 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devide Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Features_Good            = [7, 10, 14, 15, 17, 18, 20]\n",
    "Features_with_outlier    = [3, 8, 19, 23, 26, 29]\n",
    "Features_skewed          = [1, 2, 5, 9, 13, 16, 21]\n",
    "Features_missing_entry   = [0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "Features_categorical     = [11, 12, 22]\n",
    "Features_non_categorical = [x for x in range(30) if x not in Features_categorical]\n",
    "Features_using_log       = np.union1d(Features_with_outlier, Features_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devide Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completeness_status_tX = np.sum(tX == -999, axis=1)\n",
    "non_missing_tX         = tX[completeness_status_tX==0, :]\n",
    "missing_tX             = tX[completeness_status_tX!=0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logs_of_features(tx, feature_lists):\n",
    "    return np.log(tx[:, feature_lists] + 1e-8)\n",
    "\n",
    "def decompose_categorical_features(tx):\n",
    "    tmp11 = 1 * (tx[:, 11] > 0)    \n",
    "    tmp12 = 1 * (tx[:, 12] > 0.5)\n",
    "    \n",
    "    tmp22_0 = tx[:, 22].copy()\n",
    "    tmp22_0 = 1 * (tmp22_0 == 0)\n",
    "    \n",
    "    tmp22_1 = tx[:, 22].copy()\n",
    "    tmp22_1 = 1 * (tmp22_1 == 1)\n",
    "    \n",
    "    tmp22_2 = tx[:, 22].copy()\n",
    "    tmp22_2 = 1 * (tmp22_2 == 2)\n",
    "    \n",
    "    tmp22_3 = tx[:, 22].copy()\n",
    "    tmp22_3 = 1 * (tmp22_3 == 3)\n",
    "\n",
    "    m = np.c_[tmp11, tmp12, tmp22_0, tmp22_1, tmp22_2, tmp22_3]\n",
    "    if np.linalg.matrix_rank(m) < 6:\n",
    "        print (np.linalg.matrix_rank(m))\n",
    "        print (m)\n",
    "        print (\"Feature decomposition results in singularity\")\n",
    "    return m\n",
    "\n",
    "def missing_indicator(tx, features):\n",
    "    return 1 * (tx[:, features] == -999)\n",
    "\n",
    "def inver_terms(tx, features):\n",
    "    return 1/(tx[:, features]+1e-8)\n",
    "\n",
    "def mixed_features(tx, features):\n",
    "    foo = np.zeros(tx.shape[0])\n",
    "    for i, fi in enumerate(features):\n",
    "        for j in range(i+1, len(features)):\n",
    "             foo = np.c_[foo, tx[:, features[i]] * tx[:, features[j]]]\n",
    "    return foo[:, 1:]\n",
    "\n",
    "\n",
    "filled_tX_median     = fill_na(tX, np.median)\n",
    "filled_tX_mean       = fill_na(tX, np.mean)\n",
    "missing_indicator_tX = missing_indicator(tX, Features_missing_entry)\n",
    "log_tX               = logs_of_features(filled_tX_median, Features_using_log)\n",
    "decomposed_tX        = decompose_categorical_features(tX)\n",
    "inverse_tX           = inver_terms(filled_tX_median, Features_using_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(250000, 351)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_tX = mixed_features(filled_tX_median, Features_non_categorical)\n",
    "mixed_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tX9[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 271)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For non categorical features, build polynomials\n",
    "degree = 6\n",
    "poly_tX = build_polynomial_without_mixed_term(filled_tX_median[:, Features_non_categorical], degree=degree)\n",
    "log_poly_tX = build_polynomial_without_mixed_term(log_tX, degree=degree)\n",
    "\n",
    "# Build a design matrix\n",
    "design_matrix = np.c_[poly_tX, decomposed_tX, log_poly_tX, missing_indicator_tX, inverse_tX]\n",
    "\n",
    "tX8, mean_x, std_x = standardize(design_matrix)\n",
    "training_ratio = 0.9\n",
    "y8 = transform_y(y)\n",
    "train_tX8, cv_tX8, train_y8, cv_y8 = split_data(tX8, y8, training_ratio)\n",
    "cv_tX8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   7  27  28  29  34  37  54  55  56  59  64  81  82  83  85  88 108\n",
      " 110 115 135 136 137 139 142 149]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 14.64840786,   9.01130969, -38.00599868,  -7.60369266,\n",
       "       -10.25454996, -12.37340879,   5.42770314,  29.83063358,\n",
       "        18.06325981,  18.9255265 ,  -6.63364913,  -6.3437646 ,\n",
       "        21.8328975 , -18.1653926 ,  -7.4329066 ,  -6.06370522,\n",
       "         7.25364848, -15.41510176, -15.76396999,   5.12159616,\n",
       "       -36.22735198,  13.10069764, -11.83109089,   6.43516442,\n",
       "        -6.19027172,  -5.87337786])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = 5\n",
    "print(np.arange(len(w))[abs(w)>th])\n",
    "w[abs(w) > th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.07680083551e-06+0j)\n",
      "Losgistic Regression(       0/100000): loss= 13549.9736214758\n",
      "Losgistic Regression(     100/100000): loss= 9091.10586642711\n",
      "Losgistic Regression(     200/100000): loss= 8784.33403658442\n",
      "Losgistic Regression(     300/100000): loss= 8694.64805157422\n",
      "Losgistic Regression(     400/100000): loss= 8638.9324849029\n",
      "Losgistic Regression(     500/100000): loss= 8592.18731310545\n",
      "Losgistic Regression(     600/100000): loss= 8550.49448279567\n",
      "Losgistic Regression(     700/100000): loss= 8511.20518021099\n",
      "Losgistic Regression(     800/100000): loss= 8468.75263675766\n",
      "Losgistic Regression(     900/100000): loss= 8434.22193492045\n",
      "Losgistic Regression(    1000/100000): loss= 8404.46091760002\n",
      "Losgistic Regression(    1100/100000): loss= 8377.13971267559\n",
      "Losgistic Regression(    1200/100000): loss= 8352.38487730857\n",
      "Losgistic Regression(    1300/100000): loss= 8328.494203441\n",
      "Losgistic Regression(    1400/100000): loss= 8308.31339389056\n",
      "Losgistic Regression(    1500/100000): loss= 8292.11396590254\n",
      "Losgistic Regression(    1600/100000): loss= 8277.54529687386\n",
      "Losgistic Regression(    1700/100000): loss= 8265.33043476912\n",
      "Losgistic Regression(    1800/100000): loss= 8254.51008510371\n",
      "Losgistic Regression(    1900/100000): loss= 8244.75485935341\n",
      "Losgistic Regression(    2000/100000): loss= 8237.66774376054\n",
      "Losgistic Regression(    2100/100000): loss= 8231.29134174075\n",
      "Losgistic Regression(    2200/100000): loss= 8224.56178891797\n",
      "Losgistic Regression(    2300/100000): loss= 8217.98555345855\n",
      "Losgistic Regression(    2400/100000): loss= 8211.16355034519\n",
      "Losgistic Regression(    2500/100000): loss= 8203.94135381551\n",
      "Losgistic Regression(    2600/100000): loss= 8197.40501136637\n",
      "Losgistic Regression(    2700/100000): loss= 8191.20034142562\n",
      "Losgistic Regression(    2800/100000): loss= 8185.36048584205\n",
      "Losgistic Regression(    2900/100000): loss= 8179.87425276329\n",
      "Losgistic Regression(    3000/100000): loss= 8174.45323381174\n",
      "Losgistic Regression(    3100/100000): loss= 8169.49027593201\n",
      "Losgistic Regression(    3200/100000): loss= 8165.0306947494\n",
      "Losgistic Regression(    3300/100000): loss= 8163.34890645657\n",
      "Losgistic Regression(    3400/100000): loss= 8163.22927984263\n",
      "Losgistic Regression(    3500/100000): loss= 8163.07119816463\n",
      "Losgistic Regression(    3600/100000): loss= 8162.84312329524\n",
      "Losgistic Regression(    3700/100000): loss= 8162.51316366719\n",
      "Losgistic Regression(    3800/100000): loss= 8162.0910454205\n",
      "Losgistic Regression(    3900/100000): loss= 8161.64088409453\n",
      "Losgistic Regression(    4000/100000): loss= 8161.27740089755\n",
      "Losgistic Regression(    4100/100000): loss= 8161.03005123987\n",
      "Losgistic Regression(    4200/100000): loss= 8160.84861184907\n",
      "Losgistic Regression(    4300/100000): loss= 8160.7039499322\n",
      "Losgistic Regression(    4400/100000): loss= 8160.39917269137\n",
      "Losgistic Regression(    4500/100000): loss= 8159.12078164849\n",
      "Losgistic Regression(    4600/100000): loss= 8157.78971972209\n",
      "Losgistic Regression(    4700/100000): loss= 8156.42270463675\n",
      "Losgistic Regression(    4800/100000): loss= 8155.02450357542\n",
      "Losgistic Regression(    4900/100000): loss= 8153.6030451497\n",
      "Losgistic Regression(    5000/100000): loss= 8152.1650605914\n",
      "Losgistic Regression(    5100/100000): loss= 8150.71539235946\n",
      "Losgistic Regression(    5200/100000): loss= 8149.26038738486\n",
      "Losgistic Regression(    5300/100000): loss= 8147.80476304904\n",
      "Losgistic Regression(    5400/100000): loss= 8146.35373814914\n",
      "Losgistic Regression(    5500/100000): loss= 8144.91193925257\n",
      "Losgistic Regression(    5600/100000): loss= 8143.48151670063\n",
      "Losgistic Regression(    5700/100000): loss= 8142.06688159365\n",
      "Losgistic Regression(    5800/100000): loss= 8140.67097217023\n",
      "Losgistic Regression(    5900/100000): loss= 8139.29556864852\n",
      "Losgistic Regression(    6000/100000): loss= 8137.9434606933\n",
      "Losgistic Regression(    6100/100000): loss= 8136.61668187725\n",
      "Losgistic Regression(    6200/100000): loss= 8135.31609789463\n",
      "Losgistic Regression(    6300/100000): loss= 8134.04241708657\n",
      "Losgistic Regression(    6400/100000): loss= 8132.79571530159\n",
      "Losgistic Regression(    6500/100000): loss= 8131.5754984809\n",
      "Losgistic Regression(    6600/100000): loss= 8130.38100568538\n",
      "Losgistic Regression(    6700/100000): loss= 8129.21135150573\n",
      "Losgistic Regression(    6800/100000): loss= 8128.06493499452\n",
      "Losgistic Regression(    6900/100000): loss= 8126.94022098104\n",
      "Losgistic Regression(    7000/100000): loss= 8125.83507669826\n",
      "Losgistic Regression(    7100/100000): loss= 8124.74717757195\n",
      "Losgistic Regression(    7200/100000): loss= 8123.6759183125\n",
      "Losgistic Regression(    7300/100000): loss= 8122.61708383199\n",
      "Losgistic Regression(    7400/100000): loss= 8121.56834220958\n",
      "Losgistic Regression(    7500/100000): loss= 8120.52742084063\n",
      "Losgistic Regression(    7600/100000): loss= 8119.49205618911\n",
      "Losgistic Regression(    7700/100000): loss= 8118.46029063678\n",
      "Losgistic Regression(    7800/100000): loss= 8117.43059049149\n",
      "Losgistic Regression(    7900/100000): loss= 8116.40164856578\n",
      "Losgistic Regression(    8000/100000): loss= 8115.3724519735\n",
      "Losgistic Regression(    8100/100000): loss= 8114.34242878595\n",
      "Losgistic Regression(    8200/100000): loss= 8113.311722201\n",
      "Losgistic Regression(    8300/100000): loss= 8112.28105227171\n",
      "Losgistic Regression(    8400/100000): loss= 8111.2511694683\n",
      "Losgistic Regression(    8500/100000): loss= 8110.22141954538\n",
      "Losgistic Regression(    8600/100000): loss= 8109.19342359408\n",
      "Losgistic Regression(    8700/100000): loss= 8108.16845858176\n",
      "Losgistic Regression(    8800/100000): loss= 8107.14667162085\n",
      "Losgistic Regression(    8900/100000): loss= 8106.12677548902\n",
      "Losgistic Regression(    9000/100000): loss= 8105.10881639434\n",
      "Losgistic Regression(    9100/100000): loss= 8104.09288858157\n",
      "Losgistic Regression(    9200/100000): loss= 8103.07876251765\n",
      "Losgistic Regression(    9300/100000): loss= 8102.06657492085\n",
      "Losgistic Regression(    9400/100000): loss= 8101.05676394272\n",
      "Losgistic Regression(    9500/100000): loss= 8100.04824025806\n",
      "Losgistic Regression(    9600/100000): loss= 8099.04061487963\n",
      "Losgistic Regression(    9700/100000): loss= 8098.03455136369\n",
      "Losgistic Regression(    9800/100000): loss= 8097.02976677898\n",
      "Losgistic Regression(    9900/100000): loss= 8096.02565878831\n",
      "Losgistic Regression(   10000/100000): loss= 8095.02194653471\n",
      "Losgistic Regression(   10100/100000): loss= 8094.01867649055\n",
      "Losgistic Regression(   10200/100000): loss= 8093.01537255234\n",
      "Losgistic Regression(   10300/100000): loss= 8092.01164068442\n",
      "Losgistic Regression(   10400/100000): loss= 8091.00733032329\n",
      "Losgistic Regression(   10500/100000): loss= 8090.00264310685\n",
      "Losgistic Regression(   10600/100000): loss= 8088.99750975874\n",
      "Losgistic Regression(   10700/100000): loss= 8087.99213377465\n",
      "Losgistic Regression(   10800/100000): loss= 8086.98696687539\n",
      "Losgistic Regression(   10900/100000): loss= 8085.98376730539\n",
      "Losgistic Regression(   11000/100000): loss= 8084.98021571974\n",
      "Losgistic Regression(   11100/100000): loss= 8083.97621529406\n",
      "Losgistic Regression(   11200/100000): loss= 8082.97160136075\n",
      "Losgistic Regression(   11300/100000): loss= 8081.96614565931\n",
      "Losgistic Regression(   11400/100000): loss= 8080.95966476103\n",
      "Losgistic Regression(   11500/100000): loss= 8079.9520638506\n",
      "Losgistic Regression(   11600/100000): loss= 8078.9432504554\n",
      "Losgistic Regression(   11700/100000): loss= 8077.93318567795\n",
      "Losgistic Regression(   11800/100000): loss= 8076.92180203126\n",
      "Losgistic Regression(   11900/100000): loss= 8075.90912212455\n",
      "Losgistic Regression(   12000/100000): loss= 8074.89520018088\n",
      "Losgistic Regression(   12100/100000): loss= 8073.88013067485\n",
      "Losgistic Regression(   12200/100000): loss= 8072.86401629461\n",
      "Losgistic Regression(   12300/100000): loss= 8071.84684427925\n",
      "Losgistic Regression(   12400/100000): loss= 8070.82887004596\n",
      "Losgistic Regression(   12500/100000): loss= 8069.81007509616\n",
      "Losgistic Regression(   12600/100000): loss= 8068.79033968028\n",
      "Losgistic Regression(   12700/100000): loss= 8067.7697624994\n",
      "Losgistic Regression(   12800/100000): loss= 8066.7483055964\n",
      "Losgistic Regression(   12900/100000): loss= 8065.72572916531\n",
      "Losgistic Regression(   13000/100000): loss= 8064.70193631797\n",
      "Losgistic Regression(   13100/100000): loss= 8063.67704222003\n",
      "Losgistic Regression(   13200/100000): loss= 8062.65106681927\n",
      "Losgistic Regression(   13300/100000): loss= 8061.6238500936\n",
      "Losgistic Regression(   13400/100000): loss= 8060.59534550719\n",
      "Losgistic Regression(   13500/100000): loss= 8059.56566255451\n",
      "Losgistic Regression(   13600/100000): loss= 8058.53511832818\n",
      "Losgistic Regression(   13700/100000): loss= 8057.504203623\n",
      "Losgistic Regression(   13800/100000): loss= 8056.47240220213\n",
      "Losgistic Regression(   13900/100000): loss= 8055.43993084353\n",
      "Losgistic Regression(   14000/100000): loss= 8054.40703684062\n",
      "Losgistic Regression(   14100/100000): loss= 8053.37408388967\n",
      "Losgistic Regression(   14200/100000): loss= 8052.34165271403\n",
      "Losgistic Regression(   14300/100000): loss= 8051.31038516922\n",
      "Losgistic Regression(   14400/100000): loss= 8050.2807964475\n",
      "Losgistic Regression(   14500/100000): loss= 8049.25319873884\n",
      "Losgistic Regression(   14600/100000): loss= 8048.22770286264\n",
      "Losgistic Regression(   14700/100000): loss= 8047.20421434406\n",
      "Losgistic Regression(   14800/100000): loss= 8046.18248976986\n",
      "Losgistic Regression(   14900/100000): loss= 8045.16221400948\n",
      "Losgistic Regression(   15000/100000): loss= 8044.14323878842\n",
      "Losgistic Regression(   15100/100000): loss= 8043.12569619295\n",
      "Losgistic Regression(   15200/100000): loss= 8042.10948585603\n",
      "Losgistic Regression(   15300/100000): loss= 8041.09492026751\n",
      "Losgistic Regression(   15400/100000): loss= 8040.08218171383\n",
      "Losgistic Regression(   15500/100000): loss= 8039.07127856014\n",
      "Losgistic Regression(   15600/100000): loss= 8038.06203577219\n",
      "Losgistic Regression(   15700/100000): loss= 8037.05425162359\n",
      "Losgistic Regression(   15800/100000): loss= 8036.04777922873\n",
      "Losgistic Regression(   15900/100000): loss= 8035.04264931656\n",
      "Losgistic Regression(   16000/100000): loss= 8034.03909700544\n",
      "Losgistic Regression(   16100/100000): loss= 8033.03755227925\n",
      "Losgistic Regression(   16200/100000): loss= 8032.03858404085\n",
      "Losgistic Regression(   16300/100000): loss= 8031.04371028726\n",
      "Losgistic Regression(   16400/100000): loss= 8030.0531660003\n",
      "Losgistic Regression(   16500/100000): loss= 8029.06748818581\n",
      "Losgistic Regression(   16600/100000): loss= 8028.0862950201\n",
      "Losgistic Regression(   16700/100000): loss= 8027.1095231413\n",
      "Losgistic Regression(   16800/100000): loss= 8026.13699938277\n",
      "Losgistic Regression(   16900/100000): loss= 8025.16841575325\n",
      "Losgistic Regression(   17000/100000): loss= 8024.20365915355\n",
      "Losgistic Regression(   17100/100000): loss= 8023.24284622717\n",
      "Losgistic Regression(   17200/100000): loss= 8022.28605650473\n",
      "Losgistic Regression(   17300/100000): loss= 8021.33328541838\n",
      "Losgistic Regression(   17400/100000): loss= 8020.38516375026\n",
      "Losgistic Regression(   17500/100000): loss= 8019.44104377061\n",
      "Losgistic Regression(   17600/100000): loss= 8018.50072276382\n",
      "Losgistic Regression(   17700/100000): loss= 8017.56378354232\n",
      "Losgistic Regression(   17800/100000): loss= 8016.63000022411\n",
      "Losgistic Regression(   17900/100000): loss= 8015.69927425661\n",
      "Losgistic Regression(   18000/100000): loss= 8014.77155342716\n",
      "Losgistic Regression(   18100/100000): loss= 8013.84703670384\n",
      "Losgistic Regression(   18200/100000): loss= 8012.92604477916\n",
      "Losgistic Regression(   18300/100000): loss= 8012.0089669177\n",
      "Losgistic Regression(   18400/100000): loss= 8011.09620155042\n",
      "Losgistic Regression(   18500/100000): loss= 8010.18937253645\n",
      "Losgistic Regression(   18600/100000): loss= 8009.28851865477\n",
      "Losgistic Regression(   18700/100000): loss= 8008.3928860308\n",
      "Losgistic Regression(   18800/100000): loss= 8007.50360766752\n",
      "Losgistic Regression(   18900/100000): loss= 8006.62089649466\n",
      "Losgistic Regression(   19000/100000): loss= 8005.7441645232\n",
      "Losgistic Regression(   19100/100000): loss= 8004.87373231737\n",
      "Losgistic Regression(   19200/100000): loss= 8004.01000545394\n",
      "Losgistic Regression(   19300/100000): loss= 8003.15334542169\n",
      "Losgistic Regression(   19400/100000): loss= 8002.30399355566\n",
      "Losgistic Regression(   19500/100000): loss= 8001.46212303222\n",
      "Losgistic Regression(   19600/100000): loss= 8000.62785489664\n",
      "Losgistic Regression(   19700/100000): loss= 7999.80121108962\n",
      "Losgistic Regression(   19800/100000): loss= 7998.9822422046\n",
      "Losgistic Regression(   19900/100000): loss= 7998.17107333948\n",
      "Losgistic Regression(   20000/100000): loss= 7997.36788454696\n",
      "Losgistic Regression(   20100/100000): loss= 7996.57286296474\n",
      "Losgistic Regression(   20200/100000): loss= 7995.78622295076\n",
      "Losgistic Regression(   20300/100000): loss= 7995.00821023601\n",
      "Losgistic Regression(   20400/100000): loss= 7994.23907496112\n",
      "Losgistic Regression(   20500/100000): loss= 7993.47907376963\n",
      "Losgistic Regression(   20600/100000): loss= 7992.72840461698\n",
      "Losgistic Regression(   20700/100000): loss= 7991.98727154646\n",
      "Losgistic Regression(   20800/100000): loss= 7991.25691443072\n",
      "Losgistic Regression(   20900/100000): loss= 7990.53660978383\n",
      "Losgistic Regression(   21000/100000): loss= 7989.8263054503\n",
      "Losgistic Regression(   21100/100000): loss= 7989.12622252866\n",
      "Losgistic Regression(   21200/100000): loss= 7988.43651666899\n",
      "Losgistic Regression(   21300/100000): loss= 7987.75736182843\n",
      "Losgistic Regression(   21400/100000): loss= 7987.0888617636\n",
      "Losgistic Regression(   21500/100000): loss= 7986.43101787153\n",
      "Losgistic Regression(   21600/100000): loss= 7985.78376072092\n",
      "Losgistic Regression(   21700/100000): loss= 7985.14702179144\n",
      "Losgistic Regression(   21800/100000): loss= 7984.52073668793\n",
      "Losgistic Regression(   21900/100000): loss= 7983.90479033049\n",
      "Losgistic Regression(   22000/100000): loss= 7983.29910737923\n",
      "Losgistic Regression(   22100/100000): loss= 7982.703750428\n",
      "Losgistic Regression(   22200/100000): loss= 7982.11849659615\n",
      "Losgistic Regression(   22300/100000): loss= 7981.54322800921\n",
      "Losgistic Regression(   22400/100000): loss= 7980.97786704746\n",
      "Losgistic Regression(   22500/100000): loss= 7980.42209233411\n",
      "Losgistic Regression(   22600/100000): loss= 7979.87569015351\n",
      "Losgistic Regression(   22700/100000): loss= 7979.33844519832\n",
      "Losgistic Regression(   22800/100000): loss= 7978.81012910666\n",
      "Losgistic Regression(   22900/100000): loss= 7978.29052050063\n",
      "Losgistic Regression(   23000/100000): loss= 7977.77959683122\n",
      "Losgistic Regression(   23100/100000): loss= 7977.27702019007\n",
      "Losgistic Regression(   23200/100000): loss= 7976.78254337266\n",
      "Losgistic Regression(   23300/100000): loss= 7976.29595290394\n",
      "Losgistic Regression(   23400/100000): loss= 7975.81704046033\n",
      "Losgistic Regression(   23500/100000): loss= 7975.34559070326\n",
      "Losgistic Regression(   23600/100000): loss= 7974.881342585\n",
      "Losgistic Regression(   23700/100000): loss= 7974.4240205103\n",
      "Losgistic Regression(   23800/100000): loss= 7973.97338244615\n",
      "Losgistic Regression(   23900/100000): loss= 7973.52917752052\n",
      "Losgistic Regression(   24000/100000): loss= 7973.09120917027\n",
      "Losgistic Regression(   24100/100000): loss= 7972.65928014512\n",
      "Losgistic Regression(   24200/100000): loss= 7972.2332135039\n",
      "Losgistic Regression(   24300/100000): loss= 7971.81283377885\n",
      "Losgistic Regression(   24400/100000): loss= 7971.39797953122\n",
      "Losgistic Regression(   24500/100000): loss= 7970.98849588756\n",
      "Losgistic Regression(   24600/100000): loss= 7970.58453495654\n",
      "Losgistic Regression(   24700/100000): loss= 7970.18577562768\n",
      "Losgistic Regression(   24800/100000): loss= 7969.7919752336\n",
      "Losgistic Regression(   24900/100000): loss= 7969.40294018481\n",
      "Losgistic Regression(   25000/100000): loss= 7969.01859130443\n",
      "Losgistic Regression(   25100/100000): loss= 7968.63890293419\n",
      "Losgistic Regression(   25200/100000): loss= 7968.26379071589\n",
      "Losgistic Regression(   25300/100000): loss= 7967.8931733394\n",
      "Losgistic Regression(   25400/100000): loss= 7967.527016075\n",
      "Losgistic Regression(   25500/100000): loss= 7967.16538825437\n",
      "Losgistic Regression(   25600/100000): loss= 7966.80805255287\n",
      "Losgistic Regression(   25700/100000): loss= 7966.45492073881\n",
      "Losgistic Regression(   25800/100000): loss= 7966.10589407462\n",
      "Losgistic Regression(   25900/100000): loss= 7965.76094312367\n",
      "Losgistic Regression(   26000/100000): loss= 7965.4199830069\n",
      "Losgistic Regression(   26100/100000): loss= 7965.08294662508\n",
      "Losgistic Regression(   26200/100000): loss= 7964.74977206017\n",
      "Losgistic Regression(   26300/100000): loss= 7964.42036658393\n",
      "Losgistic Regression(   26400/100000): loss= 7964.09491943843\n",
      "Losgistic Regression(   26500/100000): loss= 7963.77348076384\n",
      "Losgistic Regression(   26600/100000): loss= 7963.45549768937\n",
      "Losgistic Regression(   26700/100000): loss= 7963.14085660174\n",
      "Losgistic Regression(   26800/100000): loss= 7962.82945956172\n",
      "Losgistic Regression(   26900/100000): loss= 7962.52118776906\n",
      "Losgistic Regression(   27000/100000): loss= 7962.21594716805\n",
      "Losgistic Regression(   27100/100000): loss= 7961.91367374264\n",
      "Losgistic Regression(   27200/100000): loss= 7961.61420084456\n",
      "Losgistic Regression(   27300/100000): loss= 7961.3173894507\n",
      "Losgistic Regression(   27400/100000): loss= 7961.02313808929\n",
      "Losgistic Regression(   27500/100000): loss= 7960.73139176419\n",
      "Losgistic Regression(   27600/100000): loss= 7960.44207031698\n",
      "Losgistic Regression(   27700/100000): loss= 7960.15511330613\n",
      "Losgistic Regression(   27800/100000): loss= 7959.87053542341\n",
      "Losgistic Regression(   27900/100000): loss= 7959.58822964287\n",
      "Losgistic Regression(   28000/100000): loss= 7959.30816447463\n",
      "Losgistic Regression(   28100/100000): loss= 7959.03029020286\n",
      "Losgistic Regression(   28200/100000): loss= 7958.75456057731\n",
      "Losgistic Regression(   28300/100000): loss= 7958.48094012177\n",
      "Losgistic Regression(   28400/100000): loss= 7958.20941884418\n",
      "Losgistic Regression(   28500/100000): loss= 7957.93997859126\n",
      "Losgistic Regression(   28600/100000): loss= 7957.67260970126\n",
      "Losgistic Regression(   28700/100000): loss= 7957.40733668749\n",
      "Losgistic Regression(   28800/100000): loss= 7957.14416557431\n",
      "Losgistic Regression(   28900/100000): loss= 7956.88317306894\n",
      "Losgistic Regression(   29000/100000): loss= 7956.62454843499\n",
      "Losgistic Regression(   29100/100000): loss= 7956.36797443341\n",
      "Losgistic Regression(   29200/100000): loss= 7956.11351883552\n",
      "Losgistic Regression(   29300/100000): loss= 7955.86145654323\n",
      "Losgistic Regression(   29400/100000): loss= 7955.61144844493\n",
      "Losgistic Regression(   29500/100000): loss= 7955.3637047993\n",
      "Losgistic Regression(   29600/100000): loss= 7955.11855077871\n",
      "Losgistic Regression(   29700/100000): loss= 7954.87542290267\n",
      "Losgistic Regression(   29800/100000): loss= 7954.63433115116\n",
      "Losgistic Regression(   29900/100000): loss= 7954.39516278258\n",
      "Losgistic Regression(   30000/100000): loss= 7954.15833563737\n",
      "Losgistic Regression(   30100/100000): loss= 7953.92337727166\n",
      "Losgistic Regression(   30200/100000): loss= 7953.69026315422\n",
      "Losgistic Regression(   30300/100000): loss= 7953.45899313884\n",
      "Losgistic Regression(   30400/100000): loss= 7953.22951864067\n",
      "Losgistic Regression(   30500/100000): loss= 7953.00180299465\n",
      "Losgistic Regression(   30600/100000): loss= 7952.77582137212\n",
      "Losgistic Regression(   30700/100000): loss= 7952.55170385104\n",
      "Losgistic Regression(   30800/100000): loss= 7952.32962080636\n",
      "Losgistic Regression(   30900/100000): loss= 7952.10995150887\n",
      "Losgistic Regression(   31000/100000): loss= 7951.89215872756\n",
      "Losgistic Regression(   31100/100000): loss= 7951.67607071881\n",
      "Losgistic Regression(   31200/100000): loss= 7951.46166593785\n",
      "Losgistic Regression(   31300/100000): loss= 7951.24892592507\n",
      "Losgistic Regression(   31400/100000): loss= 7951.03784185888\n",
      "Losgistic Regression(   31500/100000): loss= 7950.82840042162\n",
      "Losgistic Regression(   31600/100000): loss= 7950.62057591565\n",
      "Losgistic Regression(   31700/100000): loss= 7950.41438157431\n",
      "Losgistic Regression(   31800/100000): loss= 7950.20994281503\n",
      "Losgistic Regression(   31900/100000): loss= 7950.00709943499\n",
      "Losgistic Regression(   32000/100000): loss= 7949.80596014511\n",
      "Losgistic Regression(   32100/100000): loss= 7949.60648851502\n",
      "Losgistic Regression(   32200/100000): loss= 7949.40871518256\n",
      "Losgistic Regression(   32300/100000): loss= 7949.21260631894\n",
      "Losgistic Regression(   32400/100000): loss= 7949.01803438466\n",
      "Losgistic Regression(   32500/100000): loss= 7948.82544219579\n",
      "Losgistic Regression(   32600/100000): loss= 7948.63423087392\n",
      "Losgistic Regression(   32700/100000): loss= 7948.44436094436\n",
      "Losgistic Regression(   32800/100000): loss= 7948.25580812509\n",
      "Losgistic Regression(   32900/100000): loss= 7948.0685870136\n",
      "Losgistic Regression(   33000/100000): loss= 7947.88268995828\n",
      "Losgistic Regression(   33100/100000): loss= 7947.69805982086\n",
      "Losgistic Regression(   33200/100000): loss= 7947.51466486609\n",
      "Losgistic Regression(   33300/100000): loss= 7947.33252795855\n",
      "Losgistic Regression(   33400/100000): loss= 7947.15162397989\n",
      "Losgistic Regression(   33500/100000): loss= 7946.97190252217\n",
      "Losgistic Regression(   33600/100000): loss= 7946.79336314392\n",
      "Losgistic Regression(   33700/100000): loss= 7946.6160242363\n",
      "Losgistic Regression(   33800/100000): loss= 7946.43977804536\n",
      "Losgistic Regression(   33900/100000): loss= 7946.26461281206\n",
      "Losgistic Regression(   34000/100000): loss= 7946.09050208838\n",
      "Losgistic Regression(   34100/100000): loss= 7945.91739811567\n",
      "Losgistic Regression(   34200/100000): loss= 7945.7452772318\n",
      "Losgistic Regression(   34300/100000): loss= 7945.57427824031\n",
      "Losgistic Regression(   34400/100000): loss= 7945.40430751016\n",
      "Losgistic Regression(   34500/100000): loss= 7945.23524415865\n",
      "Losgistic Regression(   34600/100000): loss= 7945.06704924114\n",
      "Losgistic Regression(   34700/100000): loss= 7944.8996817323\n",
      "Losgistic Regression(   34800/100000): loss= 7944.73311559159\n",
      "Losgistic Regression(   34900/100000): loss= 7944.56745228455\n",
      "Losgistic Regression(   35000/100000): loss= 7944.40307359502\n",
      "Losgistic Regression(   35100/100000): loss= 7944.23943600595\n",
      "Losgistic Regression(   35200/100000): loss= 7944.07652228673\n",
      "Losgistic Regression(   35300/100000): loss= 7943.91431648018\n",
      "Losgistic Regression(   35400/100000): loss= 7943.75282637757\n",
      "Losgistic Regression(   35500/100000): loss= 7943.5920556703\n",
      "Losgistic Regression(   35600/100000): loss= 7943.43195463663\n",
      "Losgistic Regression(   35700/100000): loss= 7943.27258789357\n",
      "Losgistic Regression(   35800/100000): loss= 7943.05559479382\n",
      "Losgistic Regression(   35900/100000): loss= 7942.74331149824\n",
      "Losgistic Regression(   36000/100000): loss= 7942.43576333701\n",
      "Losgistic Regression(   36100/100000): loss= 7942.11970892865\n",
      "Losgistic Regression(   36200/100000): loss= 7941.79994874821\n",
      "Losgistic Regression(   36300/100000): loss= 7941.47423561622\n",
      "Losgistic Regression(   36400/100000): loss= 7941.16009894283\n",
      "Losgistic Regression(   36500/100000): loss= 7940.850240838\n",
      "Losgistic Regression(   36600/100000): loss= 7940.53454333969\n",
      "Losgistic Regression(   36700/100000): loss= 7940.21238144502\n",
      "Losgistic Regression(   36800/100000): loss= 7939.89175983406\n",
      "Losgistic Regression(   36900/100000): loss= 7939.5784645517\n",
      "Losgistic Regression(   37000/100000): loss= 7939.26854210356\n",
      "Losgistic Regression(   37100/100000): loss= 7938.95338948366\n",
      "Losgistic Regression(   37200/100000): loss= 7938.63900690095\n",
      "Losgistic Regression(   37300/100000): loss= 7938.33305937446\n",
      "Losgistic Regression(   37400/100000): loss= 7938.03635082218\n",
      "Losgistic Regression(   37500/100000): loss= 7937.74458166949\n",
      "Losgistic Regression(   37600/100000): loss= 7937.44879128831\n",
      "Losgistic Regression(   37700/100000): loss= 7937.14555556324\n",
      "Losgistic Regression(   37800/100000): loss= 7936.83746241357\n",
      "Losgistic Regression(   37900/100000): loss= 7936.52729450836\n",
      "Losgistic Regression(   38000/100000): loss= 7936.20432812416\n",
      "Losgistic Regression(   38100/100000): loss= 7935.87494329385\n",
      "Losgistic Regression(   38200/100000): loss= 7935.54858991498\n",
      "Losgistic Regression(   38300/100000): loss= 7935.22778746706\n",
      "Losgistic Regression(   38400/100000): loss= 7934.9052048969\n",
      "Losgistic Regression(   38500/100000): loss= 7934.57388335893\n",
      "Losgistic Regression(   38600/100000): loss= 7934.237706982\n",
      "Losgistic Regression(   38700/100000): loss= 7933.90442742291\n",
      "Losgistic Regression(   38800/100000): loss= 7933.57744617756\n",
      "Losgistic Regression(   38900/100000): loss= 7933.24314470046\n",
      "Losgistic Regression(   39000/100000): loss= 7932.90117076534\n",
      "Losgistic Regression(   39100/100000): loss= 7932.56048092231\n",
      "Losgistic Regression(   39200/100000): loss= 7932.23767847506\n",
      "Losgistic Regression(   39300/100000): loss= 7931.92374749077\n",
      "Losgistic Regression(   39400/100000): loss= 7931.60341372442\n",
      "Losgistic Regression(   39500/100000): loss= 7931.27290676337\n",
      "Losgistic Regression(   39600/100000): loss= 7930.93034627598\n",
      "Losgistic Regression(   39700/100000): loss= 7930.58472962655\n",
      "Losgistic Regression(   39800/100000): loss= 7930.23552975018\n",
      "Losgistic Regression(   39900/100000): loss= 7929.88181873296\n",
      "Losgistic Regression(   40000/100000): loss= 7929.52209481256\n",
      "Losgistic Regression(   40100/100000): loss= 7929.15906381631\n",
      "Losgistic Regression(   40200/100000): loss= 7928.79951194464\n",
      "Losgistic Regression(   40300/100000): loss= 7928.44470005688\n",
      "Losgistic Regression(   40400/100000): loss= 7928.0907699359\n",
      "Losgistic Regression(   40500/100000): loss= 7927.72544673248\n",
      "Losgistic Regression(   40600/100000): loss= 7927.3473003787\n",
      "Losgistic Regression(   40700/100000): loss= 7926.97285094038\n",
      "Losgistic Regression(   40800/100000): loss= 7926.60755911174\n",
      "Losgistic Regression(   40900/100000): loss= 7926.24934172915\n",
      "Losgistic Regression(   41000/100000): loss= 7925.88616288627\n",
      "Losgistic Regression(   41100/100000): loss= 7925.52491925566\n",
      "Losgistic Regression(   41200/100000): loss= 7925.17912912719\n",
      "Losgistic Regression(   41300/100000): loss= 7924.83759177816\n",
      "Losgistic Regression(   41400/100000): loss= 7924.48952123609\n",
      "Losgistic Regression(   41500/100000): loss= 7924.12175616102\n",
      "Losgistic Regression(   41600/100000): loss= 7923.74282118744\n",
      "Losgistic Regression(   41700/100000): loss= 7923.37196901545\n",
      "Losgistic Regression(   41800/100000): loss= 7923.00582879768\n",
      "Losgistic Regression(   41900/100000): loss= 7922.63195117604\n",
      "Losgistic Regression(   42000/100000): loss= 7922.25161066015\n",
      "Losgistic Regression(   42100/100000): loss= 7921.87651838399\n",
      "Losgistic Regression(   42200/100000): loss= 7921.50704456302\n",
      "Losgistic Regression(   42300/100000): loss= 7921.13062446508\n",
      "Losgistic Regression(   42400/100000): loss= 7920.74507599363\n",
      "Losgistic Regression(   42500/100000): loss= 7920.35143212826\n",
      "Losgistic Regression(   42600/100000): loss= 7919.9652208893\n",
      "Losgistic Regression(   42700/100000): loss= 7919.59416393801\n",
      "Losgistic Regression(   42800/100000): loss= 7919.22230652159\n",
      "Losgistic Regression(   42900/100000): loss= 7918.84987490793\n",
      "Losgistic Regression(   43000/100000): loss= 7918.48080386165\n",
      "Losgistic Regression(   43100/100000): loss= 7918.12599152756\n",
      "Losgistic Regression(   43200/100000): loss= 7917.77321499089\n",
      "Losgistic Regression(   43300/100000): loss= 7917.40598168068\n",
      "Losgistic Regression(   43400/100000): loss= 7917.02705161187\n",
      "Losgistic Regression(   43500/100000): loss= 7916.65014381631\n",
      "Losgistic Regression(   43600/100000): loss= 7916.25276762548\n",
      "Losgistic Regression(   43700/100000): loss= 7915.85328424718\n",
      "Losgistic Regression(   43800/100000): loss= 7915.44510418654\n",
      "Losgistic Regression(   43900/100000): loss= 7915.03136363584\n",
      "Losgistic Regression(   44000/100000): loss= 7914.63028549423\n",
      "Losgistic Regression(   44100/100000): loss= 7914.23360765517\n",
      "Losgistic Regression(   44200/100000): loss= 7913.83474733667\n",
      "Losgistic Regression(   44300/100000): loss= 7913.4353996736\n",
      "Losgistic Regression(   44400/100000): loss= 7913.0408205032\n",
      "Losgistic Regression(   44500/100000): loss= 7912.66066522204\n",
      "Losgistic Regression(   44600/100000): loss= 7912.29082979331\n",
      "Losgistic Regression(   44700/100000): loss= 7911.92503651638\n",
      "Losgistic Regression(   44800/100000): loss= 7911.56092597011\n",
      "Losgistic Regression(   44900/100000): loss= 7911.20398376643\n",
      "Losgistic Regression(   45000/100000): loss= 7910.85483743577\n",
      "Losgistic Regression(   45100/100000): loss= 7910.50657397056\n",
      "Losgistic Regression(   45200/100000): loss= 7910.15476009656\n",
      "Losgistic Regression(   45300/100000): loss= 7909.79786145269\n",
      "Losgistic Regression(   45400/100000): loss= 7909.44364542085\n",
      "Losgistic Regression(   45500/100000): loss= 7909.09121462789\n",
      "Losgistic Regression(   45600/100000): loss= 7908.73977560017\n",
      "Losgistic Regression(   45700/100000): loss= 7908.39047001278\n",
      "Losgistic Regression(   45800/100000): loss= 7908.03327813743\n",
      "Losgistic Regression(   45900/100000): loss= 7907.67370586215\n",
      "Losgistic Regression(   46000/100000): loss= 7907.32168184858\n",
      "Losgistic Regression(   46100/100000): loss= 7906.97692230914\n",
      "Losgistic Regression(   46200/100000): loss= 7906.62926820785\n",
      "Losgistic Regression(   46300/100000): loss= 7906.28525950949\n",
      "Losgistic Regression(   46400/100000): loss= 7905.9460859547\n",
      "Losgistic Regression(   46500/100000): loss= 7905.61072667191\n",
      "Losgistic Regression(   46600/100000): loss= 7905.2696691526\n",
      "Losgistic Regression(   46700/100000): loss= 7904.91517299176\n",
      "Losgistic Regression(   46800/100000): loss= 7904.54710704037\n",
      "Losgistic Regression(   46900/100000): loss= 7904.16620697908\n",
      "Losgistic Regression(   47000/100000): loss= 7903.77692598109\n",
      "Losgistic Regression(   47100/100000): loss= 7903.37957802581\n",
      "Losgistic Regression(   47200/100000): loss= 7902.97094986749\n",
      "Losgistic Regression(   47300/100000): loss= 7902.56906305712\n",
      "Losgistic Regression(   47400/100000): loss= 7902.16853560092\n",
      "Losgistic Regression(   47500/100000): loss= 7901.75130147439\n",
      "Losgistic Regression(   47600/100000): loss= 7901.31467625833\n",
      "Losgistic Regression(   47700/100000): loss= 7900.8621499261\n",
      "Losgistic Regression(   47800/100000): loss= 7900.39790683667\n",
      "Losgistic Regression(   47900/100000): loss= 7899.92312554944\n",
      "Losgistic Regression(   48000/100000): loss= 7899.45294697378\n",
      "Losgistic Regression(   48100/100000): loss= 7898.98955081751\n",
      "Losgistic Regression(   48200/100000): loss= 7898.53105587877\n",
      "Losgistic Regression(   48300/100000): loss= 7898.08805559116\n",
      "Losgistic Regression(   48400/100000): loss= 7897.65352039076\n",
      "Losgistic Regression(   48500/100000): loss= 7897.21598554603\n",
      "Losgistic Regression(   48600/100000): loss= 7896.77130693089\n",
      "Losgistic Regression(   48700/100000): loss= 7896.31569897553\n",
      "Losgistic Regression(   48800/100000): loss= 7895.85136459806\n",
      "Losgistic Regression(   48900/100000): loss= 7895.38112193463\n",
      "Losgistic Regression(   49000/100000): loss= 7894.91776947685\n",
      "Losgistic Regression(   49100/100000): loss= 7894.46048076719\n",
      "Losgistic Regression(   49200/100000): loss= 7893.99893325263\n",
      "Losgistic Regression(   49300/100000): loss= 7893.53937492584\n",
      "Losgistic Regression(   49400/100000): loss= 7893.0784988065\n",
      "Losgistic Regression(   49500/100000): loss= 7892.60505988353\n",
      "Losgistic Regression(   49600/100000): loss= 7892.11787408168\n",
      "Losgistic Regression(   49700/100000): loss= 7891.61527009133\n",
      "Losgistic Regression(   49800/100000): loss= 7891.09852705138\n",
      "Losgistic Regression(   49900/100000): loss= 7890.57317776916\n",
      "Losgistic Regression(   50000/100000): loss= 7890.05284207852\n",
      "Losgistic Regression(   50100/100000): loss= 7889.54160098174\n",
      "Losgistic Regression(   50200/100000): loss= 7889.03958346232\n",
      "Losgistic Regression(   50300/100000): loss= 7888.54402230247\n",
      "Losgistic Regression(   50400/100000): loss= 7888.05003148095\n",
      "Losgistic Regression(   50500/100000): loss= 7887.55193506077\n",
      "Losgistic Regression(   50600/100000): loss= 7887.04805613736\n",
      "Losgistic Regression(   50700/100000): loss= 7886.54139591272\n",
      "Losgistic Regression(   50800/100000): loss= 7886.03205593706\n",
      "Losgistic Regression(   50900/100000): loss= 7885.52096745812\n",
      "Losgistic Regression(   51000/100000): loss= 7885.01802800907\n",
      "Losgistic Regression(   51100/100000): loss= 7884.52375354071\n",
      "Losgistic Regression(   51200/100000): loss= 7884.0381288868\n",
      "Losgistic Regression(   51300/100000): loss= 7883.56576005236\n",
      "Losgistic Regression(   51400/100000): loss= 7883.09684314157\n",
      "Losgistic Regression(   51500/100000): loss= 7882.61627174444\n",
      "Losgistic Regression(   51600/100000): loss= 7882.12540313915\n",
      "Losgistic Regression(   51700/100000): loss= 7881.62433015765\n",
      "Losgistic Regression(   51800/100000): loss= 7881.11241477864\n",
      "Losgistic Regression(   51900/100000): loss= 7880.59717176308\n",
      "Losgistic Regression(   52000/100000): loss= 7880.08217686307\n",
      "Losgistic Regression(   52100/100000): loss= 7879.56991433681\n",
      "Losgistic Regression(   52200/100000): loss= 7879.06045343959\n",
      "Losgistic Regression(   52300/100000): loss= 7878.54914877802\n",
      "Losgistic Regression(   52400/100000): loss= 7878.03373680401\n",
      "Losgistic Regression(   52500/100000): loss= 7877.51106989016\n",
      "Losgistic Regression(   52600/100000): loss= 7876.98230714506\n",
      "Losgistic Regression(   52700/100000): loss= 7876.44570266238\n",
      "Losgistic Regression(   52800/100000): loss= 7875.90372734705\n",
      "Losgistic Regression(   52900/100000): loss= 7875.36660804755\n",
      "Losgistic Regression(   53000/100000): loss= 7874.8426550757\n",
      "Losgistic Regression(   53100/100000): loss= 7874.33543716714\n",
      "Losgistic Regression(   53200/100000): loss= 7873.83885153486\n",
      "Losgistic Regression(   53300/100000): loss= 7873.34571025661\n",
      "Losgistic Regression(   53400/100000): loss= 7872.85774460742\n",
      "Losgistic Regression(   53500/100000): loss= 7872.37186790257\n",
      "Losgistic Regression(   53600/100000): loss= 7871.89252803529\n",
      "Losgistic Regression(   53700/100000): loss= 7871.41358770962\n",
      "Losgistic Regression(   53800/100000): loss= 7870.92943941699\n",
      "Losgistic Regression(   53900/100000): loss= 7870.4508478652\n",
      "Losgistic Regression(   54000/100000): loss= 7869.98318615736\n",
      "Losgistic Regression(   54100/100000): loss= 7869.5264269972\n",
      "Losgistic Regression(   54200/100000): loss= 7869.07562080902\n",
      "Losgistic Regression(   54300/100000): loss= 7868.62768214441\n",
      "Losgistic Regression(   54400/100000): loss= 7868.17776059821\n",
      "Losgistic Regression(   54500/100000): loss= 7867.72145771268\n",
      "Losgistic Regression(   54600/100000): loss= 7867.25595692772\n",
      "Losgistic Regression(   54700/100000): loss= 7866.77645347431\n",
      "Losgistic Regression(   54800/100000): loss= 7866.28790603169\n",
      "Losgistic Regression(   54900/100000): loss= 7865.7996286759\n",
      "Losgistic Regression(   55000/100000): loss= 7865.31428836253\n",
      "Losgistic Regression(   55100/100000): loss= 7864.83033942557\n",
      "Losgistic Regression(   55200/100000): loss= 7864.34806485164\n",
      "Losgistic Regression(   55300/100000): loss= 7863.86762564117\n",
      "Losgistic Regression(   55400/100000): loss= 7863.3875124861\n",
      "Losgistic Regression(   55500/100000): loss= 7862.90531690665\n",
      "Losgistic Regression(   55600/100000): loss= 7862.4125471803\n",
      "Losgistic Regression(   55700/100000): loss= 7861.91073606727\n",
      "Losgistic Regression(   55800/100000): loss= 7861.40702475184\n",
      "Losgistic Regression(   55900/100000): loss= 7860.90682618094\n",
      "Losgistic Regression(   56000/100000): loss= 7860.41361477601\n",
      "Losgistic Regression(   56100/100000): loss= 7859.92402027646\n",
      "Losgistic Regression(   56200/100000): loss= 7859.4423926571\n",
      "Losgistic Regression(   56300/100000): loss= 7858.96679651915\n",
      "Losgistic Regression(   56400/100000): loss= 7858.48832118721\n",
      "Losgistic Regression(   56500/100000): loss= 7858.00079683981\n",
      "Losgistic Regression(   56600/100000): loss= 7857.5046835328\n",
      "Losgistic Regression(   56700/100000): loss= 7857.00805234742\n",
      "Losgistic Regression(   56800/100000): loss= 7856.50745603902\n",
      "Losgistic Regression(   56900/100000): loss= 7855.99754900442\n",
      "Losgistic Regression(   57000/100000): loss= 7855.48064862233\n",
      "Losgistic Regression(   57100/100000): loss= 7854.96184806672\n",
      "Losgistic Regression(   57200/100000): loss= 7854.45040414615\n",
      "Losgistic Regression(   57300/100000): loss= 7853.93992603954\n",
      "Losgistic Regression(   57400/100000): loss= 7853.41567210894\n",
      "Losgistic Regression(   57500/100000): loss= 7852.87612153919\n",
      "Losgistic Regression(   57600/100000): loss= 7852.32824205529\n",
      "Losgistic Regression(   57700/100000): loss= 7851.77900788039\n",
      "Losgistic Regression(   57800/100000): loss= 7851.22729122333\n",
      "Losgistic Regression(   57900/100000): loss= 7850.66951192519\n",
      "Losgistic Regression(   58000/100000): loss= 7850.10602464021\n",
      "Losgistic Regression(   58100/100000): loss= 7849.53502632417\n",
      "Losgistic Regression(   58200/100000): loss= 7848.96322352492\n",
      "Losgistic Regression(   58300/100000): loss= 7848.40482942337\n",
      "Losgistic Regression(   58400/100000): loss= 7847.85221976209\n",
      "Losgistic Regression(   58500/100000): loss= 7847.29126583823\n",
      "Losgistic Regression(   58600/100000): loss= 7846.72096527825\n",
      "Losgistic Regression(   58700/100000): loss= 7846.1449497319\n",
      "Losgistic Regression(   58800/100000): loss= 7845.56736892405\n",
      "Losgistic Regression(   58900/100000): loss= 7844.99410520061\n",
      "Losgistic Regression(   59000/100000): loss= 7844.4224916135\n",
      "Losgistic Regression(   59100/100000): loss= 7843.84157523242\n",
      "Losgistic Regression(   59200/100000): loss= 7843.26526306433\n",
      "Losgistic Regression(   59300/100000): loss= 7842.69802710163\n",
      "Losgistic Regression(   59400/100000): loss= 7842.1373189407\n",
      "Losgistic Regression(   59500/100000): loss= 7841.56826590214\n",
      "Losgistic Regression(   59600/100000): loss= 7840.98321490992\n",
      "Losgistic Regression(   59700/100000): loss= 7840.39186620242\n",
      "Losgistic Regression(   59800/100000): loss= 7839.79377592329\n",
      "Losgistic Regression(   59900/100000): loss= 7839.19521556873\n",
      "Losgistic Regression(   60000/100000): loss= 7838.58950956971\n",
      "Losgistic Regression(   60100/100000): loss= 7837.9773637457\n",
      "Losgistic Regression(   60200/100000): loss= 7837.36437276779\n",
      "Losgistic Regression(   60300/100000): loss= 7836.75658460104\n",
      "Losgistic Regression(   60400/100000): loss= 7836.14727220508\n",
      "Losgistic Regression(   60500/100000): loss= 7835.52289216949\n",
      "Losgistic Regression(   60600/100000): loss= 7834.88809352325\n",
      "Losgistic Regression(   60700/100000): loss= 7834.25429690903\n",
      "Losgistic Regression(   60800/100000): loss= 7833.61749886328\n",
      "Losgistic Regression(   60900/100000): loss= 7832.97762984425\n",
      "Losgistic Regression(   61000/100000): loss= 7832.33496929466\n",
      "Losgistic Regression(   61100/100000): loss= 7831.688140234\n",
      "Losgistic Regression(   61200/100000): loss= 7831.04394893201\n",
      "Losgistic Regression(   61300/100000): loss= 7830.40577530337\n",
      "Losgistic Regression(   61400/100000): loss= 7829.7695670046\n",
      "Losgistic Regression(   61500/100000): loss= 7829.12886114175\n",
      "Losgistic Regression(   61600/100000): loss= 7828.48585673837\n",
      "Losgistic Regression(   61700/100000): loss= 7827.83934311535\n",
      "Losgistic Regression(   61800/100000): loss= 7827.1867706047\n",
      "Losgistic Regression(   61900/100000): loss= 7826.52814607173\n",
      "Losgistic Regression(   62000/100000): loss= 7825.87081959718\n",
      "Losgistic Regression(   62100/100000): loss= 7825.22567093152\n",
      "Losgistic Regression(   62200/100000): loss= 7824.58593336847\n",
      "Losgistic Regression(   62300/100000): loss= 7823.94479387865\n",
      "Losgistic Regression(   62400/100000): loss= 7823.30125563041\n",
      "Losgistic Regression(   62500/100000): loss= 7822.66077037468\n",
      "Losgistic Regression(   62600/100000): loss= 7822.03046587745\n",
      "Losgistic Regression(   62700/100000): loss= 7821.39977284936\n",
      "Losgistic Regression(   62800/100000): loss= 7820.75127013829\n",
      "Losgistic Regression(   62900/100000): loss= 7820.09112209534\n",
      "Losgistic Regression(   63000/100000): loss= 7819.43954995535\n",
      "Losgistic Regression(   63100/100000): loss= 7818.80400345884\n",
      "Losgistic Regression(   63200/100000): loss= 7818.17807384913\n",
      "Losgistic Regression(   63300/100000): loss= 7817.55098067444\n",
      "Losgistic Regression(   63400/100000): loss= 7816.92257681229\n",
      "Losgistic Regression(   63500/100000): loss= 7816.29633971431\n",
      "Losgistic Regression(   63600/100000): loss= 7815.67739469394\n",
      "Losgistic Regression(   63700/100000): loss= 7815.05849789153\n",
      "Losgistic Regression(   63800/100000): loss= 7814.42544825032\n",
      "Losgistic Regression(   63900/100000): loss= 7813.78460147702\n",
      "Losgistic Regression(   64000/100000): loss= 7813.15390784597\n",
      "Losgistic Regression(   64100/100000): loss= 7812.53466703413\n",
      "Losgistic Regression(   64200/100000): loss= 7811.91979427248\n",
      "Losgistic Regression(   64300/100000): loss= 7811.30465929893\n",
      "Losgistic Regression(   64400/100000): loss= 7810.69468484706\n",
      "Losgistic Regression(   64500/100000): loss= 7810.08906709484\n",
      "Losgistic Regression(   64600/100000): loss= 7809.48088832771\n",
      "Losgistic Regression(   64700/100000): loss= 7808.86484868575\n",
      "Losgistic Regression(   64800/100000): loss= 7808.23768130853\n",
      "Losgistic Regression(   64900/100000): loss= 7807.60761690242\n",
      "Losgistic Regression(   65000/100000): loss= 7806.9833340047\n",
      "Losgistic Regression(   65100/100000): loss= 7806.36113546982\n",
      "Losgistic Regression(   65200/100000): loss= 7805.74099041774\n",
      "Losgistic Regression(   65300/100000): loss= 7805.12298470305\n",
      "Losgistic Regression(   65400/100000): loss= 7804.50092043304\n",
      "Losgistic Regression(   65500/100000): loss= 7803.86656710275\n",
      "Losgistic Regression(   65600/100000): loss= 7803.22428328514\n",
      "Losgistic Regression(   65700/100000): loss= 7802.60039457141\n",
      "Losgistic Regression(   65800/100000): loss= 7801.99583403754\n",
      "Losgistic Regression(   65900/100000): loss= 7801.38278025628\n",
      "Losgistic Regression(   66000/100000): loss= 7800.75197321073\n",
      "Losgistic Regression(   66100/100000): loss= 7800.09890614325\n",
      "Losgistic Regression(   66200/100000): loss= 7799.42992946087\n",
      "Losgistic Regression(   66300/100000): loss= 7798.77077834551\n",
      "Losgistic Regression(   66400/100000): loss= 7798.14584371814\n",
      "Losgistic Regression(   66500/100000): loss= 7797.53945049239\n",
      "Losgistic Regression(   66600/100000): loss= 7796.93137131785\n",
      "Losgistic Regression(   66700/100000): loss= 7796.32322691207\n",
      "Losgistic Regression(   66800/100000): loss= 7795.70338491584\n",
      "Losgistic Regression(   66900/100000): loss= 7795.06715375491\n",
      "Losgistic Regression(   67000/100000): loss= 7794.41953839648\n",
      "Losgistic Regression(   67100/100000): loss= 7793.7720260382\n",
      "Losgistic Regression(   67200/100000): loss= 7793.13034299055\n",
      "Losgistic Regression(   67300/100000): loss= 7792.49547515293\n",
      "Losgistic Regression(   67400/100000): loss= 7791.87768492527\n",
      "Losgistic Regression(   67500/100000): loss= 7791.2768005421\n",
      "Losgistic Regression(   67600/100000): loss= 7790.67526584706\n",
      "Losgistic Regression(   67700/100000): loss= 7790.06112559134\n",
      "Losgistic Regression(   67800/100000): loss= 7789.41860939677\n",
      "Losgistic Regression(   67900/100000): loss= 7788.73704751756\n",
      "Losgistic Regression(   68000/100000): loss= 7788.04281269899\n",
      "Losgistic Regression(   68100/100000): loss= 7787.35509772436\n",
      "Losgistic Regression(   68200/100000): loss= 7786.67783508883\n",
      "Losgistic Regression(   68300/100000): loss= 7785.99328611769\n",
      "Losgistic Regression(   68400/100000): loss= 7785.29004002351\n",
      "Losgistic Regression(   68500/100000): loss= 7784.58216281099\n",
      "Losgistic Regression(   68600/100000): loss= 7783.88318950269\n",
      "Losgistic Regression(   68700/100000): loss= 7783.20534553064\n",
      "Losgistic Regression(   68800/100000): loss= 7782.51977842341\n",
      "Losgistic Regression(   68900/100000): loss= 7781.79518497472\n",
      "Losgistic Regression(   69000/100000): loss= 7780.97157409984\n",
      "Losgistic Regression(   69100/100000): loss= 7780.09023012269\n",
      "Losgistic Regression(   69200/100000): loss= 7779.24061011504\n",
      "Losgistic Regression(   69300/100000): loss= 7778.43952613596\n",
      "Losgistic Regression(   69400/100000): loss= 7777.67829971268\n",
      "Losgistic Regression(   69500/100000): loss= 7776.93239615428\n",
      "Losgistic Regression(   69600/100000): loss= 7776.1650097903\n",
      "Losgistic Regression(   69700/100000): loss= 7775.37700346079\n",
      "Losgistic Regression(   69800/100000): loss= 7774.57301796543\n",
      "Losgistic Regression(   69900/100000): loss= 7773.76308397635\n",
      "Losgistic Regression(   70000/100000): loss= 7772.95428723435\n",
      "Losgistic Regression(   70100/100000): loss= 7772.14354594836\n",
      "Losgistic Regression(   70200/100000): loss= 7771.33998552741\n",
      "Losgistic Regression(   70300/100000): loss= 7770.54233351146\n",
      "Losgistic Regression(   70400/100000): loss= 7769.75308024512\n",
      "Losgistic Regression(   70500/100000): loss= 7768.96139820127\n",
      "Losgistic Regression(   70600/100000): loss= 7768.14990519595\n",
      "Losgistic Regression(   70700/100000): loss= 7767.31089857783\n",
      "Losgistic Regression(   70800/100000): loss= 7766.43391700356\n",
      "Losgistic Regression(   70900/100000): loss= 7765.53458103839\n",
      "Losgistic Regression(   71000/100000): loss= 7764.64468718522\n",
      "Losgistic Regression(   71100/100000): loss= 7763.77632314023\n",
      "Losgistic Regression(   71200/100000): loss= 7762.91500516514\n",
      "Losgistic Regression(   71300/100000): loss= 7762.03448103711\n",
      "Losgistic Regression(   71400/100000): loss= 7761.13018250726\n",
      "Losgistic Regression(   71500/100000): loss= 7760.21797580869\n",
      "Losgistic Regression(   71600/100000): loss= 7759.31968129603\n",
      "Losgistic Regression(   71700/100000): loss= 7758.43711101774\n",
      "Losgistic Regression(   71800/100000): loss= 7757.55921607662\n",
      "Losgistic Regression(   71900/100000): loss= 7756.68235892858\n",
      "Losgistic Regression(   72000/100000): loss= 7755.80270035518\n",
      "Losgistic Regression(   72100/100000): loss= 7754.9268911372\n",
      "Losgistic Regression(   72200/100000): loss= 7754.06478376715\n",
      "Losgistic Regression(   72300/100000): loss= 7753.22374711749\n",
      "Losgistic Regression(   72400/100000): loss= 7752.40049470896\n",
      "Losgistic Regression(   72500/100000): loss= 7751.58334766082\n",
      "Losgistic Regression(   72600/100000): loss= 7750.77256315137\n",
      "Losgistic Regression(   72700/100000): loss= 7749.98121031509\n",
      "Losgistic Regression(   72800/100000): loss= 7749.22488044115\n",
      "Losgistic Regression(   72900/100000): loss= 7748.48999995565\n",
      "Losgistic Regression(   73000/100000): loss= 7747.73630825814\n",
      "Losgistic Regression(   73100/100000): loss= 7746.94971394664\n",
      "Losgistic Regression(   73200/100000): loss= 7746.14488611422\n",
      "Losgistic Regression(   73300/100000): loss= 7745.33816954953\n",
      "Losgistic Regression(   73400/100000): loss= 7744.53819051026\n",
      "Losgistic Regression(   73500/100000): loss= 7743.74780205361\n",
      "Losgistic Regression(   73600/100000): loss= 7742.94490899124\n",
      "Losgistic Regression(   73700/100000): loss= 7742.10154997381\n",
      "Losgistic Regression(   73800/100000): loss= 7741.22598876233\n",
      "Losgistic Regression(   73900/100000): loss= 7740.3381022995\n",
      "Losgistic Regression(   74000/100000): loss= 7739.44575958522\n",
      "Losgistic Regression(   74100/100000): loss= 7738.55900881735\n",
      "Losgistic Regression(   74200/100000): loss= 7737.66753154425\n",
      "Losgistic Regression(   74300/100000): loss= 7736.76103593038\n",
      "Losgistic Regression(   74400/100000): loss= 7735.85229927266\n",
      "Losgistic Regression(   74500/100000): loss= 7734.95682499939\n",
      "Losgistic Regression(   74600/100000): loss= 7734.08680026546\n",
      "Losgistic Regression(   74700/100000): loss= 7733.23924227841\n",
      "Losgistic Regression(   74800/100000): loss= 7732.41249688203\n",
      "Losgistic Regression(   74900/100000): loss= 7731.60811748657\n",
      "Losgistic Regression(   75000/100000): loss= 7730.83087809242\n",
      "Losgistic Regression(   75100/100000): loss= 7730.07891452635\n",
      "Losgistic Regression(   75200/100000): loss= 7729.34404342202\n",
      "Losgistic Regression(   75300/100000): loss= 7728.61345711725\n",
      "Losgistic Regression(   75400/100000): loss= 7727.87478482927\n",
      "Losgistic Regression(   75500/100000): loss= 7727.13458063737\n",
      "Losgistic Regression(   75600/100000): loss= 7726.42043354629\n",
      "Losgistic Regression(   75700/100000): loss= 7725.73751286637\n",
      "Losgistic Regression(   75800/100000): loss= 7725.07068903216\n",
      "Losgistic Regression(   75900/100000): loss= 7724.39204732196\n",
      "Losgistic Regression(   76000/100000): loss= 7723.67387235121\n",
      "Losgistic Regression(   76100/100000): loss= 7722.92248192632\n",
      "Losgistic Regression(   76200/100000): loss= 7722.16362070828\n",
      "Losgistic Regression(   76300/100000): loss= 7721.40525446672\n",
      "Losgistic Regression(   76400/100000): loss= 7720.64188150835\n",
      "Losgistic Regression(   76500/100000): loss= 7719.84982935182\n",
      "Losgistic Regression(   76600/100000): loss= 7719.03192145317\n",
      "Losgistic Regression(   76700/100000): loss= 7718.19317996992\n",
      "Losgistic Regression(   76800/100000): loss= 7717.35271909507\n",
      "Losgistic Regression(   76900/100000): loss= 7716.53354883142\n",
      "Losgistic Regression(   77000/100000): loss= 7715.73085333734\n",
      "Losgistic Regression(   77100/100000): loss= 7714.92093836736\n",
      "Losgistic Regression(   77200/100000): loss= 7714.0843979891\n",
      "Losgistic Regression(   77300/100000): loss= 7713.22860399204\n",
      "Losgistic Regression(   77400/100000): loss= 7712.37035253771\n",
      "Losgistic Regression(   77500/100000): loss= 7711.52524541881\n",
      "Losgistic Regression(   77600/100000): loss= 7710.71046565678\n",
      "Losgistic Regression(   77700/100000): loss= 7709.91509427592\n",
      "Losgistic Regression(   77800/100000): loss= 7709.13803049693\n",
      "Losgistic Regression(   77900/100000): loss= 7708.38277911397\n",
      "Losgistic Regression(   78000/100000): loss= 7707.64339821906\n",
      "Losgistic Regression(   78100/100000): loss= 7706.91845360178\n",
      "Losgistic Regression(   78200/100000): loss= 7706.17628566108\n",
      "Losgistic Regression(   78300/100000): loss= 7705.41190615105\n",
      "Losgistic Regression(   78400/100000): loss= 7704.63570270734\n",
      "Losgistic Regression(   78500/100000): loss= 7703.87119289783\n",
      "Losgistic Regression(   78600/100000): loss= 7703.15224360284\n",
      "Losgistic Regression(   78700/100000): loss= 7702.46369098004\n",
      "Losgistic Regression(   78800/100000): loss= 7701.77036610738\n",
      "Losgistic Regression(   78900/100000): loss= 7701.04024370209\n",
      "Losgistic Regression(   79000/100000): loss= 7700.25694474832\n",
      "Losgistic Regression(   79100/100000): loss= 7699.43761409995\n",
      "Losgistic Regression(   79200/100000): loss= 7698.59605411461\n",
      "Losgistic Regression(   79300/100000): loss= 7697.7512510533\n",
      "Losgistic Regression(   79400/100000): loss= 7696.90667091964\n",
      "Losgistic Regression(   79500/100000): loss= 7696.04013325678\n",
      "Losgistic Regression(   79600/100000): loss= 7695.15109998187\n",
      "Losgistic Regression(   79700/100000): loss= 7694.25159445596\n",
      "Losgistic Regression(   79800/100000): loss= 7693.34948295456\n",
      "Losgistic Regression(   79900/100000): loss= 7692.45350117793\n",
      "Losgistic Regression(   80000/100000): loss= 7691.55438224658\n",
      "Losgistic Regression(   80100/100000): loss= 7690.63672026368\n",
      "Losgistic Regression(   80200/100000): loss= 7689.71146642383\n",
      "Losgistic Regression(   80300/100000): loss= 7688.81901248697\n",
      "Losgistic Regression(   80400/100000): loss= 7687.97670063677\n",
      "Losgistic Regression(   80500/100000): loss= 7687.17625933892\n",
      "Losgistic Regression(   80600/100000): loss= 7686.39931359793\n",
      "Losgistic Regression(   80700/100000): loss= 7685.62279075856\n",
      "Losgistic Regression(   80800/100000): loss= 7684.84510064596\n",
      "Losgistic Regression(   80900/100000): loss= 7684.08387011783\n",
      "Losgistic Regression(   81000/100000): loss= 7683.34662729118\n",
      "Losgistic Regression(   81100/100000): loss= 7682.63564279104\n",
      "Losgistic Regression(   81200/100000): loss= 7681.93580754945\n",
      "Losgistic Regression(   81300/100000): loss= 7681.23370377731\n",
      "Losgistic Regression(   81400/100000): loss= 7680.5158381873\n",
      "Losgistic Regression(   81500/100000): loss= 7679.78108926371\n",
      "Losgistic Regression(   81600/100000): loss= 7679.03986946929\n",
      "Losgistic Regression(   81700/100000): loss= 7678.29145345987\n",
      "Losgistic Regression(   81800/100000): loss= 7677.51871778688\n",
      "Losgistic Regression(   81900/100000): loss= 7676.71163582302\n",
      "Losgistic Regression(   82000/100000): loss= 7675.88789399107\n",
      "Losgistic Regression(   82100/100000): loss= 7675.04869013509\n",
      "Losgistic Regression(   82200/100000): loss= 7674.20033391185\n",
      "Losgistic Regression(   82300/100000): loss= 7673.34490134528\n",
      "Losgistic Regression(   82400/100000): loss= 7672.46659627959\n",
      "Losgistic Regression(   82500/100000): loss= 7671.56815400894\n",
      "Losgistic Regression(   82600/100000): loss= 7670.66655790986\n",
      "Losgistic Regression(   82700/100000): loss= 7669.7736095204\n",
      "Losgistic Regression(   82800/100000): loss= 7668.89121523843\n",
      "Losgistic Regression(   82900/100000): loss= 7668.01651767742\n",
      "Losgistic Regression(   83000/100000): loss= 7667.14523561198\n",
      "Losgistic Regression(   83100/100000): loss= 7666.281305593\n",
      "Losgistic Regression(   83200/100000): loss= 7665.43789149203\n",
      "Losgistic Regression(   83300/100000): loss= 7664.62439804022\n",
      "Losgistic Regression(   83400/100000): loss= 7663.84146160457\n",
      "Losgistic Regression(   83500/100000): loss= 7663.06874100157\n",
      "Losgistic Regression(   83600/100000): loss= 7662.29778382954\n",
      "Losgistic Regression(   83700/100000): loss= 7661.52846682226\n",
      "Losgistic Regression(   83800/100000): loss= 7660.76708008045\n",
      "Losgistic Regression(   83900/100000): loss= 7660.03616179152\n",
      "Losgistic Regression(   84000/100000): loss= 7659.34476983062\n",
      "Losgistic Regression(   84100/100000): loss= 7658.67858905009\n",
      "Losgistic Regression(   84200/100000): loss= 7658.01210066234\n",
      "Losgistic Regression(   84300/100000): loss= 7657.32556570933\n",
      "Losgistic Regression(   84400/100000): loss= 7656.62257186143\n",
      "Losgistic Regression(   84500/100000): loss= 7655.90158801798\n",
      "Losgistic Regression(   84600/100000): loss= 7655.16615013196\n",
      "Losgistic Regression(   84700/100000): loss= 7654.41918547951\n",
      "Losgistic Regression(   84800/100000): loss= 7653.66540087797\n",
      "Losgistic Regression(   84900/100000): loss= 7652.90952105624\n",
      "Losgistic Regression(   85000/100000): loss= 7652.15346040918\n",
      "Losgistic Regression(   85100/100000): loss= 7651.39311225985\n",
      "Losgistic Regression(   85200/100000): loss= 7650.61123818309\n",
      "Losgistic Regression(   85300/100000): loss= 7649.80343819015\n",
      "Losgistic Regression(   85400/100000): loss= 7648.9682563958\n",
      "Losgistic Regression(   85500/100000): loss= 7648.12056981809\n",
      "Losgistic Regression(   85600/100000): loss= 7647.28583885034\n",
      "Losgistic Regression(   85700/100000): loss= 7646.46973503335\n",
      "Losgistic Regression(   85800/100000): loss= 7645.66738882533\n",
      "Losgistic Regression(   85900/100000): loss= 7644.87928023541\n",
      "Losgistic Regression(   86000/100000): loss= 7644.09244932458\n",
      "Losgistic Regression(   86100/100000): loss= 7643.31986649999\n",
      "Losgistic Regression(   86200/100000): loss= 7642.57445462928\n",
      "Losgistic Regression(   86300/100000): loss= 7641.85614748111\n",
      "Losgistic Regression(   86400/100000): loss= 7641.16866696241\n",
      "Losgistic Regression(   86500/100000): loss= 7640.49858612377\n",
      "Losgistic Regression(   86600/100000): loss= 7639.83702137477\n",
      "Losgistic Regression(   86700/100000): loss= 7639.18006254211\n",
      "Losgistic Regression(   86800/100000): loss= 7638.53223438258\n",
      "Losgistic Regression(   86900/100000): loss= 7637.89055273897\n",
      "Losgistic Regression(   87000/100000): loss= 7637.25896683439\n",
      "Losgistic Regression(   87100/100000): loss= 7636.63323245334\n",
      "Losgistic Regression(   87200/100000): loss= 7635.99455302984\n",
      "Losgistic Regression(   87300/100000): loss= 7635.34051373334\n",
      "Losgistic Regression(   87400/100000): loss= 7634.67706373168\n",
      "Losgistic Regression(   87500/100000): loss= 7634.01261985702\n",
      "Losgistic Regression(   87600/100000): loss= 7633.33991030242\n",
      "Losgistic Regression(   87700/100000): loss= 7632.63614842741\n",
      "Losgistic Regression(   87800/100000): loss= 7631.89351749768\n",
      "Losgistic Regression(   87900/100000): loss= 7631.12764024772\n",
      "Losgistic Regression(   88000/100000): loss= 7630.36821207421\n",
      "Losgistic Regression(   88100/100000): loss= 7629.62427477549\n",
      "Losgistic Regression(   88200/100000): loss= 7628.87091578805\n",
      "Losgistic Regression(   88300/100000): loss= 7628.07726159177\n",
      "Losgistic Regression(   88400/100000): loss= 7627.25002373345\n",
      "Losgistic Regression(   88500/100000): loss= 7626.40518965735\n",
      "Losgistic Regression(   88600/100000): loss= 7625.56628358947\n",
      "Losgistic Regression(   88700/100000): loss= 7624.73590737935\n",
      "Losgistic Regression(   88800/100000): loss= 7623.91953675952\n",
      "Losgistic Regression(   88900/100000): loss= 7623.1258584771\n",
      "Losgistic Regression(   89000/100000): loss= 7622.35831783743\n",
      "Losgistic Regression(   89100/100000): loss= 7621.61201254772\n",
      "Losgistic Regression(   89200/100000): loss= 7620.88886167428\n",
      "Losgistic Regression(   89300/100000): loss= 7620.1875982548\n",
      "Losgistic Regression(   89400/100000): loss= 7619.51056760192\n",
      "Losgistic Regression(   89500/100000): loss= 7618.85067938834\n",
      "Losgistic Regression(   89600/100000): loss= 7618.19797181968\n",
      "Losgistic Regression(   89700/100000): loss= 7617.55066487367\n",
      "Losgistic Regression(   89800/100000): loss= 7616.9197582042\n",
      "Losgistic Regression(   89900/100000): loss= 7616.31158333808\n",
      "Losgistic Regression(   90000/100000): loss= 7615.71901755785\n",
      "Losgistic Regression(   90100/100000): loss= 7615.11963227686\n",
      "Losgistic Regression(   90200/100000): loss= 7614.50931500656\n",
      "Losgistic Regression(   90300/100000): loss= 7613.88993802764\n",
      "Losgistic Regression(   90400/100000): loss= 7613.26279689885\n",
      "Losgistic Regression(   90500/100000): loss= 7612.6199291957\n",
      "Losgistic Regression(   90600/100000): loss= 7611.94775382423\n",
      "Losgistic Regression(   90700/100000): loss= 7611.24142642724\n",
      "Losgistic Regression(   90800/100000): loss= 7610.50782946882\n",
      "Losgistic Regression(   90900/100000): loss= 7609.74078928096\n",
      "Losgistic Regression(   91000/100000): loss= 7608.96490037533\n",
      "Losgistic Regression(   91100/100000): loss= 7608.19074862167\n",
      "Losgistic Regression(   91200/100000): loss= 7607.40441207934\n",
      "Losgistic Regression(   91300/100000): loss= 7606.59639103458\n",
      "Losgistic Regression(   91400/100000): loss= 7605.77336409327\n",
      "Losgistic Regression(   91500/100000): loss= 7604.94376333707\n",
      "Losgistic Regression(   91600/100000): loss= 7604.1054871665\n",
      "Losgistic Regression(   91700/100000): loss= 7603.26523477659\n",
      "Losgistic Regression(   91800/100000): loss= 7602.44925517118\n",
      "Losgistic Regression(   91900/100000): loss= 7601.65905322752\n",
      "Losgistic Regression(   92000/100000): loss= 7600.88369669682\n",
      "Losgistic Regression(   92100/100000): loss= 7600.11080649514\n",
      "Losgistic Regression(   92200/100000): loss= 7599.34291347542\n",
      "Losgistic Regression(   92300/100000): loss= 7598.59805948285\n",
      "Losgistic Regression(   92400/100000): loss= 7597.87651675208\n",
      "Losgistic Regression(   92500/100000): loss= 7597.17970760183\n",
      "Losgistic Regression(   92600/100000): loss= 7596.49404824731\n",
      "Losgistic Regression(   92700/100000): loss= 7595.82341443976\n",
      "Losgistic Regression(   92800/100000): loss= 7595.17615259942\n",
      "Losgistic Regression(   92900/100000): loss= 7594.53706148492\n",
      "Losgistic Regression(   93000/100000): loss= 7593.90437250082\n",
      "Losgistic Regression(   93100/100000): loss= 7593.26095607807\n",
      "Losgistic Regression(   93200/100000): loss= 7592.6099062029\n",
      "Losgistic Regression(   93300/100000): loss= 7591.95962954667\n",
      "Losgistic Regression(   93400/100000): loss= 7591.30919746478\n",
      "Losgistic Regression(   93500/100000): loss= 7590.66808027117\n",
      "Losgistic Regression(   93600/100000): loss= 7590.02908562082\n",
      "Losgistic Regression(   93700/100000): loss= 7589.36637252196\n",
      "Losgistic Regression(   93800/100000): loss= 7588.67513309098\n",
      "Losgistic Regression(   93900/100000): loss= 7587.95318384196\n",
      "Losgistic Regression(   94000/100000): loss= 7587.20426119537\n",
      "Losgistic Regression(   94100/100000): loss= 7586.43167915234\n",
      "Losgistic Regression(   94200/100000): loss= 7585.64792257371\n",
      "Losgistic Regression(   94300/100000): loss= 7584.85753148535\n",
      "Losgistic Regression(   94400/100000): loss= 7584.05794278679\n",
      "Losgistic Regression(   94500/100000): loss= 7583.25115056776\n",
      "Losgistic Regression(   94600/100000): loss= 7582.43781587684\n",
      "Losgistic Regression(   94700/100000): loss= 7581.62947040806\n",
      "Losgistic Regression(   94800/100000): loss= 7580.82716373454\n",
      "Losgistic Regression(   94900/100000): loss= 7580.03155622197\n",
      "Losgistic Regression(   95000/100000): loss= 7579.25267774669\n",
      "Losgistic Regression(   95100/100000): loss= 7578.49203521505\n",
      "Losgistic Regression(   95200/100000): loss= 7577.76352734165\n",
      "Losgistic Regression(   95300/100000): loss= 7577.07615407096\n",
      "Losgistic Regression(   95400/100000): loss= 7576.42011602211\n",
      "Losgistic Regression(   95500/100000): loss= 7575.79199018811\n",
      "Losgistic Regression(   95600/100000): loss= 7575.20509368232\n",
      "Losgistic Regression(   95700/100000): loss= 7574.64447066204\n",
      "Losgistic Regression(   95800/100000): loss= 7574.09363750785\n",
      "Losgistic Regression(   95900/100000): loss= 7573.54110245297\n",
      "Losgistic Regression(   96000/100000): loss= 7572.97532950296\n",
      "Losgistic Regression(   96100/100000): loss= 7572.40584710815\n",
      "Losgistic Regression(   96200/100000): loss= 7571.83705926082\n",
      "Losgistic Regression(   96300/100000): loss= 7571.26603792482\n",
      "Losgistic Regression(   96400/100000): loss= 7570.68406342738\n",
      "Losgistic Regression(   96500/100000): loss= 7570.09099075053\n",
      "Losgistic Regression(   96600/100000): loss= 7569.50115058116\n",
      "Losgistic Regression(   96700/100000): loss= 7568.90597014215\n",
      "Losgistic Regression(   96800/100000): loss= 7568.27090712225\n",
      "Losgistic Regression(   96900/100000): loss= 7567.58853588077\n",
      "Losgistic Regression(   97000/100000): loss= 7566.87041417103\n",
      "Losgistic Regression(   97100/100000): loss= 7566.14107998129\n",
      "Losgistic Regression(   97200/100000): loss= 7565.40665205685\n",
      "Losgistic Regression(   97300/100000): loss= 7564.6738899952\n",
      "Losgistic Regression(   97400/100000): loss= 7563.93576843411\n",
      "Losgistic Regression(   97500/100000): loss= 7563.19196202714\n",
      "Losgistic Regression(   97600/100000): loss= 7562.46513613325\n",
      "Losgistic Regression(   97700/100000): loss= 7561.75183035364\n",
      "Losgistic Regression(   97800/100000): loss= 7561.03079552981\n",
      "Losgistic Regression(   97900/100000): loss= 7560.30013768971\n",
      "Losgistic Regression(   98000/100000): loss= 7559.58133759356\n",
      "Losgistic Regression(   98100/100000): loss= 7558.88411672454\n",
      "Losgistic Regression(   98200/100000): loss= 7558.20340316616\n",
      "Losgistic Regression(   98300/100000): loss= 7557.53217894148\n",
      "Losgistic Regression(   98400/100000): loss= 7556.87079128583\n",
      "Losgistic Regression(   98500/100000): loss= 7556.23287294737\n",
      "Losgistic Regression(   98600/100000): loss= 7555.61738897468\n",
      "Losgistic Regression(   98700/100000): loss= 7555.02633782265\n",
      "Losgistic Regression(   98800/100000): loss= 7554.44882762335\n",
      "Losgistic Regression(   98900/100000): loss= 7553.87045575705\n",
      "Losgistic Regression(   99000/100000): loss= 7553.30281144657\n",
      "Losgistic Regression(   99100/100000): loss= 7552.75491987563\n",
      "Losgistic Regression(   99200/100000): loss= 7552.20940862504\n",
      "Losgistic Regression(   99300/100000): loss= 7551.65560271828\n",
      "Losgistic Regression(   99400/100000): loss= 7551.08172993656\n",
      "Losgistic Regression(   99500/100000): loss= 7550.49517684192\n",
      "Losgistic Regression(   99600/100000): loss= 7549.91076326463\n",
      "Losgistic Regression(   99700/100000): loss= 7549.32673523122\n",
      "Losgistic Regression(   99800/100000): loss= 7548.72643301202\n",
      "Losgistic Regression(   99900/100000): loss= 7548.09385892156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.82520000000000004, 0.81759999999999999)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxes = np.arange(20000)\n",
    "cols = np.arange(train_tX8.shape[1])\n",
    "\n",
    "L = np.linalg.eigvals(train_tX8[idxes][:, cols].T @ train_tX8[idxes][:, cols]).max()\n",
    "print(1/L)\n",
    "\n",
    "w,cost = logistic_AGDR(train_y8[idxes], train_tX8[idxes][:, cols], gamma=np.real(1/L), \n",
    "                   max_iters = 100000, lambda_=0.01, regularizor=regularizor_lasso)\n",
    "\n",
    "y_pred = predict_labels(w, train_tX8[idxes][:, cols])\n",
    "tr_acc = accuracy(y_pred, transform_y_back(train_y8[idxes]))\n",
    "y_pred = predict_labels(w, cv_tX8[:, cols])\n",
    "te_acc = accuracy(y_pred, transform_y_back(cv_y8))\n",
    "tr_acc, te_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82091555555555551"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, train_tX8[:, cols])\n",
    "acc = accuracy(y_pred, transform_y_back(train_y8))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.99130204, -22.60515521, -54.68483643,  -0.12276102,  -1.12390906])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_cache = w\n",
    "w_cache[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# low degree but more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 528)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For non categorical features, build polynomials\n",
    "degree = 3\n",
    "poly_tX = build_polynomial_without_mixed_term(filled_tX_median[:, Features_non_categorical], degree=degree)\n",
    "log_poly_tX = build_polynomial_without_mixed_term(log_tX, degree=degree)\n",
    "inv_poly_tX = build_polynomial_without_mixed_term(inverse_tX, degree=degree)\n",
    "\n",
    "# Build a design matrix\n",
    "design_matrix = np.c_[poly_tX, decomposed_tX, log_poly_tX, missing_indicator_tX, inv_poly_tX, mixed_tX]\n",
    "\n",
    "tX9, mean_x, std_x = standardize(design_matrix)\n",
    "training_ratio = 0.9\n",
    "y9 = transform_y(y)\n",
    "train_tX9, cv_tX9, train_y9, cv_y9 = split_data(tX9, y9, training_ratio)\n",
    "cv_tX9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3738395527164696e-06\n",
      "Losgistic Regression(       0/100000): loss= 3768.74511212999\n",
      "Losgistic Regression(     100/100000): loss= 3768.62540795822\n",
      "Losgistic Regression(     200/100000): loss= 3768.35755694892\n",
      "Losgistic Regression(     300/100000): loss= 3767.99336296835\n",
      "Losgistic Regression(     400/100000): loss= 3767.56825872288\n",
      "Losgistic Regression(     500/100000): loss= 3767.10818173239\n",
      "Losgistic Regression(     600/100000): loss= 3766.63190397685\n",
      "Losgistic Regression(     700/100000): loss= 3766.1522033618\n",
      "Losgistic Regression(     800/100000): loss= 3765.6790834391\n",
      "Losgistic Regression(     900/100000): loss= 3765.21935259467\n",
      "Losgistic Regression(    1000/100000): loss= 3764.77788466784\n",
      "Losgistic Regression(    1100/100000): loss= 3764.35761212578\n",
      "Losgistic Regression(    1200/100000): loss= 3763.96023731049\n",
      "Losgistic Regression(    1300/100000): loss= 3763.58625980676\n",
      "Losgistic Regression(    1400/100000): loss= 3763.23501219288\n",
      "Losgistic Regression(    1500/100000): loss= 3762.90533258951\n",
      "Losgistic Regression(    1600/100000): loss= 3762.59705120327\n",
      "Losgistic Regression(    1700/100000): loss= 3762.30750572353\n",
      "Losgistic Regression(    1800/100000): loss= 3762.04070086285\n",
      "Losgistic Regression(    1900/100000): loss= 3761.76315005666\n",
      "Losgistic Regression(    2000/100000): loss= 3761.49563580292\n",
      "Losgistic Regression(    2100/100000): loss= 3761.24511582287\n",
      "Losgistic Regression(    2200/100000): loss= 3761.01269517017\n",
      "Losgistic Regression(    2300/100000): loss= 3760.78452831032\n",
      "Losgistic Regression(    2400/100000): loss= 3760.56423526591\n",
      "Losgistic Regression(    2500/100000): loss= 3760.35194324647\n",
      "Losgistic Regression(    2600/100000): loss= 3760.17451626437\n",
      "Losgistic Regression(    2700/100000): loss= 3760.01486462592\n",
      "Losgistic Regression(    2800/100000): loss= 3759.85398791685\n",
      "Losgistic Regression(    2900/100000): loss= 3759.67835030749\n",
      "Losgistic Regression(    3000/100000): loss= 3759.5440534124\n",
      "Losgistic Regression(    3100/100000): loss= 3759.39472263595\n",
      "Losgistic Regression(    3200/100000): loss= 3759.04504653062\n",
      "Losgistic Regression(    3300/100000): loss= 3758.61618302087\n",
      "Losgistic Regression(    3400/100000): loss= 3758.24314592555\n",
      "Losgistic Regression(    3500/100000): loss= 3757.80760970751\n",
      "Losgistic Regression(    3600/100000): loss= 3757.35564362869\n",
      "Losgistic Regression(    3700/100000): loss= 3756.93311534536\n",
      "Losgistic Regression(    3800/100000): loss= 3756.54298350925\n",
      "Losgistic Regression(    3900/100000): loss= 3756.14765877409\n",
      "Losgistic Regression(    4000/100000): loss= 3755.7809538925\n",
      "Losgistic Regression(    4100/100000): loss= 3755.41114178293\n",
      "Losgistic Regression(    4200/100000): loss= 3755.07148626035\n",
      "Losgistic Regression(    4300/100000): loss= 3754.74928157029\n",
      "Losgistic Regression(    4400/100000): loss= 3754.51245532445\n",
      "Losgistic Regression(    4500/100000): loss= 3754.25776197434\n",
      "Losgistic Regression(    4600/100000): loss= 3753.98682633276\n",
      "Losgistic Regression(    4700/100000): loss= 3753.70296053164\n",
      "Losgistic Regression(    4800/100000): loss= 3753.46282796832\n",
      "Losgistic Regression(    4900/100000): loss= 3753.32181326149\n",
      "Losgistic Regression(    5000/100000): loss= 3753.15017194804\n",
      "Losgistic Regression(    5100/100000): loss= 3753.0044886958\n",
      "Losgistic Regression(    5200/100000): loss= 3752.91102154196\n",
      "Losgistic Regression(    5300/100000): loss= 3752.91358739981\n",
      "Totoal number of iterations =  5300\n",
      "Loss                        =  3752.9135874\n",
      "0.5 0.8389 0.81624\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "cols = np.arange(train_tX9.shape[1])\n",
    "\n",
    "L = np.linalg.eigvals(train_tX9[idxes][:, cols].T @ train_tX9[idxes][:, cols]).max()\n",
    "print(np.real(1/L))\n",
    "\n",
    "lambda_ = 0.5\n",
    "w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L), \n",
    "               max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso, w0=w)\n",
    "\n",
    "y_pred = predict_labels(w, train_tX9[idxes][:, cols])\n",
    "tr_acc = accuracy(y_pred, transform_y_back(train_y9[idxes]))\n",
    "y_pred = predict_labels(w, cv_tX9[:, cols])\n",
    "te_acc = accuracy(y_pred, transform_y_back(cv_y9))\n",
    "print(lambda_, tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.37383955272e-06+0j)\n",
      "Losgistic Regression(       0/100000): loss= 3836.42974291578\n",
      "Losgistic Regression(     100/100000): loss= 3836.42971201905\n",
      "Totoal number of iterations =  100\n",
      "Loss                        =  3836.42971202\n",
      "1 0.8362 0.81764\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "cols  = np.arange(train_tX9.shape[1])\n",
    "\n",
    "L = np.linalg.eigvals(train_tX9[idxes][:, cols].T @ train_tX9[idxes][:, cols]).max()\n",
    "print(1/L)\n",
    "\n",
    "lambda_ = 1\n",
    "w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L), \n",
    "               max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso, w0=w)\n",
    "\n",
    "y_pred = predict_labels(w, train_tX9[idxes][:, cols])\n",
    "tr_acc = accuracy(y_pred, transform_y_back(train_y9[idxes]))\n",
    "y_pred = predict_labels(w, cv_tX9[:, cols])\n",
    "te_acc = accuracy(y_pred, transform_y_back(cv_y9))\n",
    "print(lambda_, tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82065333333333335"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, train_tX9[:][:, cols])\n",
    "acc = accuracy(y_pred, transform_y_back(train_y9[:]))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cf2d2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w)\n",
    "\n",
    "plt.plot([poly_tX.shape[1], poly_tX.shape[1]], [-5, 5], 'k-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1], poly_tX.shape[1]+decomposed_tX.shape[1]], [-5, 5], 'r-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1], \n",
    "          poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1]], [-5, 5], 'g-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1], \n",
    "          poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1]],\n",
    "         [-5, 5], 'r--')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1], \n",
    "          poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1]],\n",
    "         [-5, 5], 'k--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best lambda for a degree 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/150000): loss=6128.661958753694\n",
      "Losgistic Regression(1000/150000): loss=4382.793973805763\n",
      "Losgistic Regression(2000/150000): loss=4321.439300698346\n",
      "Losgistic Regression(3000/150000): loss=4291.208251980238\n",
      "Losgistic Regression(4000/150000): loss=4270.574245773539\n",
      "Losgistic Regression(5000/150000): loss=4255.255208797125\n",
      "Losgistic Regression(6000/150000): loss=4244.187346417512\n",
      "Losgistic Regression(7000/150000): loss=4236.342382593083\n",
      "Losgistic Regression(8000/150000): loss=4229.2149455352765\n",
      "Losgistic Regression(9000/150000): loss=4222.549848793302\n",
      "Losgistic Regression(10000/150000): loss=4216.256821112506\n",
      "Losgistic Regression(11000/150000): loss=4210.280363793595\n",
      "Losgistic Regression(12000/150000): loss=4204.581620909371\n",
      "Losgistic Regression(13000/150000): loss=4199.130612876477\n",
      "Losgistic Regression(14000/150000): loss=4193.902700925177\n",
      "Losgistic Regression(15000/150000): loss=4188.876869375156\n",
      "Losgistic Regression(16000/150000): loss=4184.034829564825\n",
      "Losgistic Regression(17000/150000): loss=4179.3605048153\n",
      "Losgistic Regression(18000/150000): loss=4175.203975960235\n",
      "Losgistic Regression(19000/150000): loss=4172.128890451348\n",
      "Losgistic Regression(20000/150000): loss=4169.404308525439\n",
      "Losgistic Regression(21000/150000): loss=4166.945582792079\n",
      "Losgistic Regression(22000/150000): loss=4164.697910753656\n",
      "Losgistic Regression(23000/150000): loss=4162.623370955874\n",
      "Losgistic Regression(24000/150000): loss=4160.694444672941\n",
      "Losgistic Regression(25000/150000): loss=4158.890417925157\n",
      "Losgistic Regression(26000/150000): loss=4157.195232293507\n",
      "Losgistic Regression(27000/150000): loss=4155.596121161573\n",
      "Losgistic Regression(28000/150000): loss=4154.082702688859\n",
      "Losgistic Regression(29000/150000): loss=4152.646368926147\n",
      "Losgistic Regression(30000/150000): loss=4151.279853515938\n",
      "Losgistic Regression(31000/150000): loss=4149.976926771299\n",
      "Losgistic Regression(32000/150000): loss=4148.732176552424\n",
      "Losgistic Regression(33000/150000): loss=4147.540849240619\n",
      "Losgistic Regression(34000/150000): loss=4146.398730789539\n",
      "Losgistic Regression(35000/150000): loss=4145.3020601081635\n",
      "Losgistic Regression(36000/150000): loss=4144.247460850778\n",
      "Losgistic Regression(37000/150000): loss=4143.231888176198\n",
      "Losgistic Regression(38000/150000): loss=4142.252586093812\n",
      "Losgistic Regression(39000/150000): loss=4141.307052495995\n",
      "Losgistic Regression(40000/150000): loss=4140.393009940427\n",
      "Losgistic Regression(41000/150000): loss=4139.50838085982\n",
      "Losgistic Regression(42000/150000): loss=4138.65126787957\n",
      "Losgistic Regression(43000/150000): loss=4137.819930481284\n",
      "Losgistic Regression(44000/150000): loss=4137.0127732559595\n",
      "Losgistic Regression(45000/150000): loss=4136.228330432692\n",
      "Losgistic Regression(46000/150000): loss=4135.465253085196\n",
      "Losgistic Regression(47000/150000): loss=4134.1443323593485\n",
      "Losgistic Regression(48000/150000): loss=4132.836213664394\n",
      "Losgistic Regression(49000/150000): loss=4131.548598167255\n",
      "Losgistic Regression(50000/150000): loss=4130.280447452421\n",
      "Losgistic Regression(51000/150000): loss=4129.030794438789\n",
      "Losgistic Regression(52000/150000): loss=4127.7987378407415\n",
      "Losgistic Regression(53000/150000): loss=4126.583436998648\n",
      "Losgistic Regression(54000/150000): loss=4125.3841070212775\n",
      "Losgistic Regression(55000/150000): loss=4124.200014284044\n",
      "Losgistic Regression(56000/150000): loss=4123.030472284158\n",
      "Losgistic Regression(57000/150000): loss=4121.87483784062\n",
      "Losgistic Regression(58000/150000): loss=4120.732507621012\n",
      "Losgistic Regression(59000/150000): loss=4119.602914974805\n",
      "Losgistic Regression(60000/150000): loss=4118.485527050165\n",
      "Losgistic Regression(61000/150000): loss=4117.379842171625\n",
      "Losgistic Regression(62000/150000): loss=4116.28538745605\n",
      "Losgistic Regression(63000/150000): loss=4115.201716646274\n",
      "Losgistic Regression(64000/150000): loss=4114.128408140856\n",
      "Losgistic Regression(65000/150000): loss=4113.065063202537\n",
      "Losgistic Regression(66000/150000): loss=4112.011304327397\n",
      "Losgistic Regression(67000/150000): loss=4110.966773759254\n",
      "Losgistic Regression(68000/150000): loss=4109.931132134869\n",
      "Losgistic Regression(69000/150000): loss=4108.9040572471185\n",
      "Losgistic Regression(70000/150000): loss=4107.88524291448\n",
      "Losgistic Regression(71000/150000): loss=4106.874397946057\n",
      "Losgistic Regression(72000/150000): loss=4105.8712451931\n",
      "Losgistic Regression(73000/150000): loss=4104.875520678365\n",
      "Losgistic Regression(74000/150000): loss=4103.886972795643\n",
      "Losgistic Regression(75000/150000): loss=4102.905361572792\n",
      "Losgistic Regression(76000/150000): loss=4101.930457992193\n",
      "Losgistic Regression(77000/150000): loss=4100.962043363106\n",
      "Losgistic Regression(78000/150000): loss=4099.999908741166\n",
      "Losgistic Regression(79000/150000): loss=4099.043854390546\n",
      "Losgistic Regression(80000/150000): loss=4098.093689285304\n",
      "Losgistic Regression(81000/150000): loss=4097.149230644885\n",
      "Losgistic Regression(82000/150000): loss=4096.2103035032105\n",
      "Losgistic Regression(83000/150000): loss=4095.2767403064836\n",
      "Losgistic Regression(84000/150000): loss=4094.348380641769\n",
      "Losgistic Regression(85000/150000): loss=4093.4250696102904\n",
      "Losgistic Regression(86000/150000): loss=4092.5066611711864\n",
      "Losgistic Regression(87000/150000): loss=4091.593013716083\n",
      "Losgistic Regression(88000/150000): loss=4090.6839912387345\n",
      "Losgistic Regression(89000/150000): loss=4089.779463475005\n",
      "Losgistic Regression(90000/150000): loss=4088.8793054909006\n",
      "Losgistic Regression(91000/150000): loss=4087.9833973753302\n",
      "Losgistic Regression(92000/150000): loss=4087.091623991697\n",
      "Losgistic Regression(93000/150000): loss=4086.2038746801322\n",
      "Losgistic Regression(94000/150000): loss=4085.3200435459976\n",
      "Losgistic Regression(95000/150000): loss=4084.4400271378972\n",
      "Losgistic Regression(96000/150000): loss=4083.563726604002\n",
      "Losgistic Regression(97000/150000): loss=4082.6910490395876\n",
      "Losgistic Regression(98000/150000): loss=4081.821904073697\n",
      "Losgistic Regression(99000/150000): loss=4080.9562047323307\n",
      "Losgistic Regression(100000/150000): loss=4080.0938673345763\n",
      "Losgistic Regression(101000/150000): loss=4079.234811363942\n",
      "Losgistic Regression(102000/150000): loss=4078.3789593505944\n",
      "Losgistic Regression(103000/150000): loss=4077.5262367569426\n",
      "Losgistic Regression(104000/150000): loss=4076.6765718659494\n",
      "Losgistic Regression(105000/150000): loss=4075.8298956724216\n",
      "Losgistic Regression(106000/150000): loss=4074.986141777644\n",
      "Losgistic Regression(107000/150000): loss=4074.145246287765\n",
      "Losgistic Regression(108000/150000): loss=4073.3071477161557\n",
      "Losgistic Regression(109000/150000): loss=4072.471786889442\n",
      "Losgistic Regression(110000/150000): loss=4071.639106857446\n",
      "Losgistic Regression(111000/150000): loss=4070.8090528066728\n",
      "Losgistic Regression(112000/150000): loss=4069.981571977565\n",
      "Losgistic Regression(113000/150000): loss=4069.1566135849607\n",
      "Losgistic Regression(114000/150000): loss=4068.334128741864\n",
      "Losgistic Regression(115000/150000): loss=4067.514070386298\n",
      "Losgistic Regression(116000/150000): loss=4066.6963932110834\n",
      "Losgistic Regression(117000/150000): loss=4065.8810555570626\n",
      "Losgistic Regression(118000/150000): loss=4065.068013712836\n",
      "Losgistic Regression(119000/150000): loss=4064.25722701785\n",
      "Losgistic Regression(120000/150000): loss=4063.44865651399\n",
      "Losgistic Regression(121000/150000): loss=4062.642264707486\n",
      "Losgistic Regression(122000/150000): loss=4061.83801549441\n",
      "Losgistic Regression(123000/150000): loss=4061.0358741047557\n",
      "Losgistic Regression(124000/150000): loss=4060.235807051358\n",
      "Losgistic Regression(125000/150000): loss=4059.437782534584\n",
      "Losgistic Regression(126000/150000): loss=4058.641769063937\n",
      "Losgistic Regression(127000/150000): loss=4057.8477366387892\n",
      "Losgistic Regression(128000/150000): loss=4057.055656489738\n",
      "Losgistic Regression(129000/150000): loss=4056.2655008337697\n",
      "Losgistic Regression(130000/150000): loss=4055.477242900723\n",
      "Losgistic Regression(131000/150000): loss=4054.6908568974927\n",
      "Losgistic Regression(132000/150000): loss=4053.906317968484\n",
      "Losgistic Regression(133000/150000): loss=4053.1236021583495\n",
      "Losgistic Regression(134000/150000): loss=4052.3426863762284\n",
      "Losgistic Regression(135000/150000): loss=4051.5635483610204\n",
      "Losgistic Regression(136000/150000): loss=4050.7861666489475\n",
      "Losgistic Regression(137000/150000): loss=4050.010520541866\n",
      "Losgistic Regression(138000/150000): loss=4049.236590076733\n",
      "Losgistic Regression(139000/150000): loss=4048.46435599626\n",
      "Losgistic Regression(140000/150000): loss=4047.6937997206537\n",
      "Losgistic Regression(141000/150000): loss=4046.9249033204223\n",
      "Losgistic Regression(142000/150000): loss=4046.1576494901965\n",
      "Losgistic Regression(143000/150000): loss=4045.3920215234975\n",
      "Losgistic Regression(144000/150000): loss=4044.62800328853\n",
      "Losgistic Regression(145000/150000): loss=4043.8655792049185\n",
      "Losgistic Regression(146000/150000): loss=4043.1047339495753\n",
      "Losgistic Regression(147000/150000): loss=4042.3454523613855\n",
      "Losgistic Regression(148000/150000): loss=4041.587721752981\n",
      "Losgistic Regression(149000/150000): loss=4040.8315283470806\n",
      "0.8207 0.81404\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.79002650416e-06\n",
      "Losgistic Regression(0/150000): loss=7981.108636380177\n",
      "Losgistic Regression(100/150000): loss=7981.110107054736\n",
      "0.00147067455873\n",
      "Totoal number of iterations =  100\n",
      "Loss =  7981.11010705\n",
      "0.8212 0.8158\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(20000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w_agdr = logistic_AGDR(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso, w0=w_agdr)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], \n",
    "                                         cv_tX7, cv_y7, w_agdr)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_var(y, tx, K, func, seed, fpred, faccuracy, max_fold):\n",
    "    \"\"\"Cross validation (varied version)\n",
    "    \n",
    "    When the datasets are large and we want to train on a small subset of data,\n",
    "    and validate them on the rest of datasets. We will first divide the the data\n",
    "    into K parts and train on only one of them, then test them on the rest K-1\n",
    "    parts.\n",
    "    \n",
    "    input:\n",
    "        y   : 1-D array (processed).\n",
    "        tx  : 2-D design matrix (processed).\n",
    "        K   : K-folded cross validation\n",
    "        func: function which takes (y, tx) as argument\n",
    "        seed: random seed\n",
    "        \n",
    "        fpred: prediction function\n",
    "        faccuracy: function for calculating accuracy\n",
    "        \n",
    "    return:\n",
    "        ave_acc : averaged accuracy\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    n = len(y)\n",
    "    tr_size = n // K\n",
    "    perm  = np.random.permutation(np.arange(n)).reshape(K, tr_size)\n",
    "    idx = np.ma.array(np.arange(K), mask=False)\n",
    "                                    \n",
    "    def get_index():\n",
    "        i = 0\n",
    "        while i < K:\n",
    "            idx.mask[i] = True\n",
    "            yield perm[i], perm[idx].flatten()\n",
    "            idx.mask[i] = False\n",
    "            i = i + 1\n",
    "    \n",
    "    accs = 0\n",
    "    for i, (tr_idx, cv_idx) in zip(np.arange(max_fold), get_index()):\n",
    "        start   = time.time()\n",
    "\n",
    "        w, cost = func(y[tr_idx], tx[tr_idx, :])\n",
    "        end     = time.time()\n",
    "        print (\"Time for {i:2}th cross validation = {t:3.6}s\".format(i=i, t=end-start))\n",
    "        \n",
    "        pred_y  = fpred(tx[cv_idx], w)\n",
    "        cv_acc  = faccuracy(pred_y, y[cv_idx])\n",
    "        pred_y  = fpred(tx[tr_idx], w)\n",
    "        tr_acc  = faccuracy(pred_y, y[tr_idx])\n",
    "        \n",
    "        print (\"Training Accuracy         = {a:6.10}\".format(a=tr_acc))\n",
    "        print (\"Cross Validation Accuracy = {a:6.10}\".format(a=cv_acc))\n",
    "        accs    = accs + cv_acc\n",
    "        \n",
    "    return accs/min(max_fold,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_cv(tx, w):\n",
    "    return 1*((tx @ w) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 5284.41690074827\n",
      "Losgistic Regression(     100/10000): loss= 3663.02806010826\n",
      "Losgistic Regression(     200/10000): loss= 2393.9420278097\n",
      "Losgistic Regression(     300/10000): loss= 1880.59494727767\n",
      "Losgistic Regression(     400/10000): loss= 1477.97573908367\n",
      "Losgistic Regression(     500/10000): loss= 1144.44978311721\n",
      "Losgistic Regression(     600/10000): loss= 916.734019199147\n",
      "Losgistic Regression(     700/10000): loss= 763.123374497964\n",
      "Losgistic Regression(     800/10000): loss= 663.432905908736\n",
      "Losgistic Regression(     900/10000): loss= 589.87385537189\n",
      "Losgistic Regression(    1000/10000): loss= 534.514104660792\n",
      "Losgistic Regression(    1100/10000): loss= 480.65569547533\n",
      "Losgistic Regression(    1200/10000): loss= 432.966960621097\n",
      "Losgistic Regression(    1300/10000): loss= 402.693641638441\n",
      "Losgistic Regression(    1400/10000): loss= 385.600234718755\n",
      "Losgistic Regression(    1500/10000): loss= 376.775679275437\n",
      "Losgistic Regression(    1600/10000): loss= 371.026400710226\n",
      "Losgistic Regression(    1700/10000): loss= 364.289777840195\n",
      "Losgistic Regression(    1800/10000): loss= 360.491298664682\n",
      "Losgistic Regression(    1900/10000): loss= 355.911861619698\n",
      "Losgistic Regression(    2000/10000): loss= 352.037915952763\n",
      "Losgistic Regression(    2100/10000): loss= 348.015504841296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-501-7f5005e617ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                      \u001b[0mfpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_cv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                      \u001b[0mfaccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      \u001b[0mmax_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                     )\n",
      "\u001b[0;32m<ipython-input-497-1131cd0dd7c2>\u001b[0m in \u001b[0;36mcross_validation_var\u001b[0;34m(y, tx, K, func, seed, fpred, faccuracy, max_fold)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mstart\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mend\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Time for {i:2}th cross validation = {t:3.6}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-501-7f5005e617ba>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      1\u001b[0m set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n\u001b[0;32m----> 2\u001b[0;31m                max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m cross_validation_var(y9, tX9, \n\u001b[1;32m      5\u001b[0m                      \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_AGDR\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlast_cost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mw_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Restart if the loss of new weight is larger than the original one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mregularizor_lasso\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregularizor_lasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# return loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregularizor_ridge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "               max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n",
    "\n",
    "cross_validation_var(y9, tX9, \n",
    "                     K=25, \n",
    "                     func=set_up_f, \n",
    "                     seed=3, \n",
    "                     fpred=predict_cv,\n",
    "                     faccuracy=accuracy,\n",
    "                     max_fold=5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "filled_test_tX_median = fill_na(tX_test, np.median)\n",
    "filled_test_tX_mean   = fill_na(tX_test, np.mean)\n",
    "#\n",
    "log_test_tX               = logs_of_features(filled_test_tX_median, Features_using_log)\n",
    "decomposed_test_tX        = decompose_categorical_features(tX_test)\n",
    "missing_indicator_test_tX = missing_indicator(tX_test, Features_missing_entry)\n",
    "inverse_test_tX           = inver_terms(filled_test_tX_median, Features_using_log)\n",
    "mixed_test_tX = mixed_features(filled_test_tX_median, Features_non_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 528)\n"
     ]
    }
   ],
   "source": [
    "## Change\n",
    "# * filled_test_tX_median\n",
    "# * w\n",
    "# * mean_x, std_x\n",
    "# * degree\n",
    "##############################\n",
    "# For non categorical features, build polynomials\n",
    "poly_test_tX = build_polynomial_without_mixed_term(filled_test_tX_median[:, Features_non_categorical], degree=degree)\n",
    "log_poly_test_tX = build_polynomial_without_mixed_term(log_test_tX, degree=degree)\n",
    "inv_poly_test_tX = build_polynomial_without_mixed_term(inverse_test_tX, degree=degree)\n",
    "\n",
    "# Build a design matrix\n",
    "design_matrix_test = np.c_[poly_test_tX, decomposed_test_tX, log_poly_test_tX, missing_indicator_test_tX, \n",
    "                           inv_poly_test_tX, mixed_test_tX]\n",
    "\n",
    "test_tX, _, _ = standardize(design_matrix_test, mean_x, std_x)\n",
    "print(test_tX.shape)\n",
    "y0 = predict_labels(weights=w, data=test_tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/result9.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y0, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0702178367f34f9ebe15ab2ad5078a77": {
     "views": []
    },
    "104197bfe94d4c0d969b505d11b5c64f": {
     "views": []
    },
    "278b55cc2b5d470b914c860b737b13dd": {
     "views": []
    },
    "3bde44762a854e17913f8a1d09a4c769": {
     "views": []
    },
    "85d5f2ee705b4c90b7174e8404fecf2f": {
     "views": []
    },
    "c1bc1dc628504eb8a05c1d397345357a": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "e03e8d3520474092b8ccdcf44afab94a": {
     "views": []
    },
    "ff9fba9a21524b73b062f1f5b3ed2858": {
     "views": []
    }
   },
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
