{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from data_preprocessing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading of the data : done\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"loading of the data : done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functions import logistic_regression_GD\n",
    "\n",
    "def prediction(tx, w):\n",
    "    y = tx @ w\n",
    "    y[y > 0] = 1\n",
    "    y[y <= 0] = 0\n",
    "    return y\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)\n",
    "\n",
    "log_features = [1, 3, 4, 5, 8, 9, 13, 16, 19, 23, 26, 29]\n",
    "normal_features = [0, 2, 6, 7, 10, 14, 17, 21, 24, 27]\n",
    "uniform_feature = [15, 18, 20, 25, 28]\n",
    "# two side are large\n",
    "bernoulli_feature = [11, 12]\n",
    "categorical_features = [22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"loading of the data : done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_total_training_samples = tX.shape[0]\n",
    "n_total_features         = tX.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_missing_values = []\n",
    "for i in range(n_total_features):\n",
    "    if -999 in tX[:, i]:\n",
    "        columns_with_missing_values.append(i)\n",
    "\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_features = [1, 3, 4, 5, 8, 9, 13, 16, 19, 23, 26, 29]\n",
    "normal_features = [0, 2, 6, 7, 10, 14, 17, 21, 24, 27]\n",
    "uniform_feature = [15, 18, 20, 25, 28]\n",
    "# two side are large (perhaps beta distribution)\n",
    "bernoulli_feature = [11, 12]\n",
    "categorical_features = [22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio_of_training):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    p = np.random.permutation(np.arange(y.shape[0]))\n",
    "    n = int(y.shape[0] * ratio_of_training)\n",
    "    return  x[p][:n], x[p][n:], y[p][:n], y[p][n:]\n",
    "\n",
    "def prediction(tx, w):\n",
    "    y = tx @ w\n",
    "    y[y > 0] = 1\n",
    "    y[y <= 0] = 0\n",
    "    return y\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_y(y):\n",
    "    tmp = y.copy()\n",
    "    tmp[tmp == -1]=0\n",
    "    return tmp\n",
    "\n",
    "def transform_y_back(y):\n",
    "    tmp = y.copy()\n",
    "    tmp[tmp==0]=-1  \n",
    "    return tmp\n",
    "\n",
    "def prediction_and_accuracy(tr_tx, tr_y, te_tx, te_y, w):\n",
    "    return accuracy(tr_y, prediction(tr_tx, w)), accuracy(te_y, prediction(te_tx, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill missing values with their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoYkqZuhIUnqZmhIkrr9f9rzVfTtxFAnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1286ea6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "from IPython.display import display\n",
    "\n",
    "def fill_na(tx=tX, method=np.mean):\n",
    "    filled = tx.copy()\n",
    "    for col in columns_with_missing_values:\n",
    "        tmp = filled[:, col]\n",
    "        tmp[tmp == -999] = method(tmp[tmp != -999])\n",
    "        filled[:, col] = tmp\n",
    "    return filled\n",
    "\n",
    "def plot_hist(tx, i, transformation=None):\n",
    "    plt.figure()\n",
    "    if transformation is None:\n",
    "        plt.hist(tx[:, i], bins=100)\n",
    "    else:\n",
    "        plt.hist(transformation(tx[:, i]), bins=100)\n",
    "    plt.title(\"feature %i\" % i)\n",
    "    plt.show()\n",
    "    \n",
    "# before processing \n",
    "interactive(lambda x:plot_hist(tX, x), x=IntSlider(min=0, max=29))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand features to reduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   0,   1,   0,   1],\n",
       "       [  2,   3,   4,   9,   8,  27],\n",
       "       [  4,   5,  16,  25,  64, 125],\n",
       "       [  6,   7,  36,  49, 216, 343],\n",
       "       [  8,   9,  64,  81, 512, 729]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_polynomial_without_mixed_term(tx, degree=2):\n",
    "    n = tx.shape[0]\n",
    "    tmp = tx\n",
    "    for i in range(2, degree+1):\n",
    "        tmp = np.c_[tmp, tx**i]\n",
    "    # The function standardize will add a column of 1s in the first column\n",
    "    return tmp\n",
    "\n",
    "build_polynomial_without_mixed_term(np.arange(10).reshape(5,2), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier 1\n",
    "\n",
    "specifications\n",
    "    - polynomial of degree 2\n",
    "    - didin't apply log to some features\n",
    "    - validation (not cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill -999 with mean/median/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 16, 19, 21, 22, 23, 26, 29]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_tX = fill_na()\n",
    "\n",
    "columns_non_negative = []\n",
    "for i in range(n_total_features):\n",
    "    if len(filled_tX[filled_tX[:, i] < 0, i]) == 0:\n",
    "        columns_non_negative.append(i)\n",
    "\n",
    "columns_non_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data used for interactive data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105f6fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive(lambda x:plot_hist(filled_tX, x), x=IntSlider(value=1, min=0, max=n_total_features-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some of the data is not allowed to perform log\n",
    "def plot_function(idx):\n",
    "    if idx in columns_non_negative:\n",
    "        plot_hist(filled_tX, idx, lambda y: np.log(y+1e-6))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "interactive(plot_function, idx=IntSlider(value=1, min=0, max=n_total_features-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 61)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(filled_tX, degree = 2)\n",
    "## We can take logs of each column Here *******************************\n",
    "tX1, mean_x1, std_x1 = standardize(tmp)\n",
    "y1 = transform_y(y)\n",
    "tX1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sepearte training sets and cross validation sets and Predict w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 61)"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX1, cv_tX1, train_y1, cv_y1 = split_data(tX1, y1, training_ratio)\n",
    "cv_tX1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_info = np.array(info)\n",
    "sizes = plot_info[:, 2]\n",
    "tr_error = plot_info[:, 0]\n",
    "te_error = plot_info[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEACAYAAACd2SCPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSimlGuX/AXK3XDI07Vs5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1244c95f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sizes, tr_error, 'r*-', sizes, te_error, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For how large the size, we can train it very accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma=0.1\n",
    "lambda=0.01\n",
    "\n",
    "|training size|iteration| acc train| acc test|\n",
    "|:--:|:---:|:---:|\n",
    "|100|1000| 0.98|0.70|\n",
    "|150|1000|0.88|0.7|\n",
    "|150|5000|0.93|0.71|\n",
    "|150|10000|0.96|0.72464|\n",
    "|150|20000|0.947|0.707|\n",
    "|150|20000|0.953333333333| 0.70664|\n",
    "|200|10000|0.885| 0.70876|\n",
    "|10000|1000|0.6934| 0.69128|\n",
    "\n",
    "we can see that with the grow of training size, the same classifier performs bad on traing set very quickly. For the same training sets, the grows of iterations can improve training performance, but can hardly influence test error.\n",
    "\n",
    "Our model suffers from bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(0/2000): loss=-4257.306958500683\n",
      "Losgistic Regression(1000/2000): loss=-16823.984342794996\n",
      "0.845 0.73416\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(200)\n",
    "w, losses = reg_logistic_regression_GD(train_y1[idxes], train_tX1[idxes], gamma=0.001, \n",
    "                       max_iters = 2000, lambda_=0, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX1[idxes], train_y1[idxes], cv_tX1, cv_y1, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the (original) feature 2, 4, 15, 18 has small feature in in it self, but their square can be large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classfier 2: Higher Order Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 16, 19, 21, 22, 23, 26, 29]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_tX = fill_na()\n",
    "\n",
    "columns_non_negative = []\n",
    "for i in range(n_total_features):\n",
    "    if len(filled_tX[filled_tX[:, i] < 0, i]) == 0:\n",
    "        columns_non_negative.append(i)\n",
    "\n",
    "columns_non_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 91)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(filled_tX, degree = 3)\n",
    "## We can take logs of each column Here *******************************\n",
    "tX2, mean_x2, std_x2 = standardize(tmp)\n",
    "y2 = transform_y(y)\n",
    "tX2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sepearte training sets and cross validation sets and Predict w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 91)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX2, cv_tX2, train_y2, cv_y2 = split_data(tX2, y2, training_ratio)\n",
    "cv_tX2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31858812606e-05\n",
      "Losgistic Regression(0/100000): loss=-1576381.4131746194\n",
      "Losgistic Regression(1000/100000): loss=-2609588.730715125\n",
      "Losgistic Regression(2000/100000): loss=-3167813.2138219867\n",
      "Losgistic Regression(3000/100000): loss=-3472116.763273852\n",
      "Losgistic Regression(4000/100000): loss=-3685713.505443715\n",
      "Losgistic Regression(5000/100000): loss=-3849144.4587358832\n",
      "Losgistic Regression(6000/100000): loss=-3981252.780542511\n",
      "Losgistic Regression(7000/100000): loss=-4091986.6941092336\n",
      "Losgistic Regression(8000/100000): loss=-4187103.3739277883\n",
      "Losgistic Regression(9000/100000): loss=-4270247.950210498\n",
      "Losgistic Regression(10000/100000): loss=-4343888.490349865\n",
      "Losgistic Regression(11000/100000): loss=-4409771.966527175\n",
      "Losgistic Regression(12000/100000): loss=-4469173.647771348\n",
      "Losgistic Regression(13000/100000): loss=-4523053.768381426\n",
      "Losgistic Regression(14000/100000): loss=-4572165.075756724\n",
      "Losgistic Regression(15000/100000): loss=-4617120.198719306\n",
      "Losgistic Regression(16000/100000): loss=-4658431.460440949\n",
      "Losgistic Regression(17000/100000): loss=-4696533.776438372\n",
      "Losgistic Regression(18000/100000): loss=-4731798.785862885\n",
      "Losgistic Regression(19000/100000): loss=-4764546.182064939\n",
      "Losgistic Regression(20000/100000): loss=-4795051.625150841\n",
      "Losgistic Regression(21000/100000): loss=-4823553.743449943\n",
      "Losgistic Regression(22000/100000): loss=-4850259.862277383\n",
      "Losgistic Regression(23000/100000): loss=-4875350.531390548\n",
      "Losgistic Regression(24000/100000): loss=-4898983.454344895\n",
      "Losgistic Regression(25000/100000): loss=-4921297.340273586\n",
      "Losgistic Regression(26000/100000): loss=-4942413.462051473\n",
      "Losgistic Regression(27000/100000): loss=-4962438.986907133\n",
      "Losgistic Regression(28000/100000): loss=-4981468.558170063\n",
      "Losgistic Regression(29000/100000): loss=-4999585.841008508\n",
      "Losgistic Regression(30000/100000): loss=-5016864.965058875\n",
      "Losgistic Regression(31000/100000): loss=-5033371.745346901\n",
      "Losgistic Regression(32000/100000): loss=-5049164.741337445\n",
      "Losgistic Regression(33000/100000): loss=-5064296.186086943\n",
      "Losgistic Regression(34000/100000): loss=-5078812.804623643\n",
      "Losgistic Regression(35000/100000): loss=-5092756.393514927\n",
      "Losgistic Regression(36000/100000): loss=-5106165.058016837\n",
      "Losgistic Regression(37000/100000): loss=-5119072.781526758\n",
      "Losgistic Regression(38000/100000): loss=-5131511.020404617\n",
      "Losgistic Regression(39000/100000): loss=-5143507.664347334\n",
      "Losgistic Regression(40000/100000): loss=-5155088.63508416\n",
      "Losgistic Regression(41000/100000): loss=-5166277.743503345\n",
      "Losgistic Regression(42000/100000): loss=-5177096.9263597075\n",
      "Losgistic Regression(43000/100000): loss=-5187566.4487711685\n",
      "Losgistic Regression(44000/100000): loss=-5197705.050099795\n",
      "Losgistic Regression(45000/100000): loss=-5207530.119119958\n",
      "Losgistic Regression(46000/100000): loss=-5217057.79366191\n",
      "Losgistic Regression(47000/100000): loss=-5226303.074583019\n",
      "Losgistic Regression(48000/100000): loss=-5235279.921821131\n",
      "Losgistic Regression(49000/100000): loss=-5244001.340726925\n",
      "Losgistic Regression(50000/100000): loss=-5252479.46022717\n",
      "Losgistic Regression(51000/100000): loss=-5260725.603928999\n",
      "Losgistic Regression(52000/100000): loss=-5268750.354984655\n",
      "Losgistic Regression(53000/100000): loss=-5276563.615347583\n",
      "Losgistic Regression(54000/100000): loss=-5284174.659932345\n",
      "Losgistic Regression(55000/100000): loss=-5291592.18611666\n",
      "Losgistic Regression(56000/100000): loss=-5298824.358974835\n",
      "Losgistic Regression(57000/100000): loss=-5305878.852597846\n",
      "Losgistic Regression(58000/100000): loss=-5312762.887828574\n",
      "Losgistic Regression(59000/100000): loss=-5319483.252422188\n",
      "Losgistic Regression(60000/100000): loss=-5326046.3873747615\n",
      "Losgistic Regression(61000/100000): loss=-5332458.353504039\n",
      "Losgistic Regression(62000/100000): loss=-5338724.852569162\n",
      "Losgistic Regression(63000/100000): loss=-5344851.300808033\n",
      "Losgistic Regression(64000/100000): loss=-5350842.828958583\n",
      "Losgistic Regression(65000/100000): loss=-5356704.300721952\n",
      "Losgistic Regression(66000/100000): loss=-5362440.331261646\n",
      "Losgistic Regression(67000/100000): loss=-5368055.304237758\n",
      "Losgistic Regression(68000/100000): loss=-5373553.387373273\n",
      "Losgistic Regression(69000/100000): loss=-5378938.546697936\n",
      "Losgistic Regression(70000/100000): loss=-5384214.559610502\n",
      "Losgistic Regression(71000/100000): loss=-5389385.026880057\n",
      "Losgistic Regression(72000/100000): loss=-5394453.383690101\n",
      "Losgistic Regression(73000/100000): loss=-5399422.909817011\n",
      "Losgistic Regression(74000/100000): loss=-5404296.739022105\n",
      "Losgistic Regression(75000/100000): loss=-5409077.867729876\n",
      "Losgistic Regression(76000/100000): loss=-5413769.163055664\n",
      "Losgistic Regression(77000/100000): loss=-5418373.370240863\n",
      "Losgistic Regression(78000/100000): loss=-5422893.119547225\n",
      "Losgistic Regression(79000/100000): loss=-5427330.9326570975\n",
      "Losgistic Regression(80000/100000): loss=-5431689.228621795\n",
      "Losgistic Regression(81000/100000): loss=-5435970.329396363\n",
      "Losgistic Regression(82000/100000): loss=-5440176.464994871\n",
      "Losgistic Regression(83000/100000): loss=-5444309.778297924\n",
      "Losgistic Regression(84000/100000): loss=-5448372.329540389\n",
      "Losgistic Regression(85000/100000): loss=-5452366.100504982\n",
      "Losgistic Regression(86000/100000): loss=-5456293.185446158\n",
      "Losgistic Regression(87000/100000): loss=-5460155.1175794825\n",
      "Losgistic Regression(88000/100000): loss=-5463953.779596244\n",
      "Losgistic Regression(89000/100000): loss=-5467690.875100382\n",
      "Losgistic Regression(90000/100000): loss=-5471368.045793549\n",
      "Losgistic Regression(91000/100000): loss=-5474986.876479005\n",
      "Losgistic Regression(92000/100000): loss=-5478548.896940535\n",
      "Losgistic Regression(93000/100000): loss=-5482055.584147429\n",
      "Losgistic Regression(94000/100000): loss=-5485508.364485841\n",
      "Losgistic Regression(95000/100000): loss=-5488908.615921628\n",
      "Losgistic Regression(96000/100000): loss=-5492257.670073381\n",
      "Losgistic Regression(97000/100000): loss=-5495556.814190101\n",
      "Losgistic Regression(98000/100000): loss=-5498807.293032587\n",
      "Losgistic Regression(99000/100000): loss=-5502010.310660532\n",
      "0.8172 0.80656\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L=np.linalg.eigvals(train_tX2[idxes].T @ train_tX2[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y2[idxes], train_tX2[idxes], gamma=1/L, \n",
    "                       max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX2[idxes], train_y2[idxes], cv_tX2, cv_y2, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.6932772662e-06\n",
      "Losgistic Regression(0/100000): loss=434457.21842224855\n",
      "Losgistic Regression(1000/100000): loss=-10234622.776920963\n",
      "Losgistic Regression(2000/100000): loss=-12230873.927219868\n",
      "Losgistic Regression(3000/100000): loss=-13370184.385959959\n",
      "Losgistic Regression(4000/100000): loss=-14177368.686676051\n",
      "Losgistic Regression(5000/100000): loss=-14804224.395295477\n",
      "Losgistic Regression(6000/100000): loss=-15313039.706620015\n",
      "Losgistic Regression(7000/100000): loss=-15737437.263241317\n",
      "Losgistic Regression(8000/100000): loss=-16098504.39866479\n",
      "Losgistic Regression(9000/100000): loss=-16410311.024095053\n",
      "Losgistic Regression(10000/100000): loss=-16682700.350381702\n",
      "Losgistic Regression(11000/100000): loss=-16922849.827691004\n",
      "Losgistic Regression(12000/100000): loss=-17136175.98913687\n",
      "Losgistic Regression(13000/100000): loss=-17326879.368117437\n",
      "Losgistic Regression(14000/100000): loss=-17498293.407778494\n",
      "Losgistic Regression(15000/100000): loss=-17653107.48084559\n",
      "Losgistic Regression(16000/100000): loss=-17793529.65548029\n",
      "Losgistic Regression(17000/100000): loss=-17921393.395987857\n",
      "Losgistic Regression(18000/100000): loss=-18038237.25761377\n",
      "Losgistic Regression(19000/100000): loss=-18145364.019332215\n",
      "Losgistic Regression(20000/100000): loss=-18243884.099737927\n",
      "Losgistic Regression(21000/100000): loss=-18334751.18453073\n",
      "Losgistic Regression(22000/100000): loss=-18418786.030906536\n",
      "Losgistic Regression(23000/100000): loss=-18496700.57322757\n",
      "Losgistic Regression(24000/100000): loss=-18569113.75459297\n",
      "Losgistic Regression(25000/100000): loss=-18636565.985747747\n",
      "Losgistic Regression(26000/100000): loss=-18699530.91251867\n",
      "Losgistic Regression(27000/100000): loss=-18758425.16131391\n",
      "Losgistic Regression(28000/100000): loss=-18813617.131521203\n",
      "Losgistic Regression(29000/100000): loss=-18865433.07428224\n",
      "Losgistic Regression(30000/100000): loss=-18914163.557746664\n",
      "Losgistic Regression(31000/100000): loss=-18960068.0583519\n",
      "Losgistic Regression(32000/100000): loss=-19003378.944174524\n",
      "Losgistic Regression(33000/100000): loss=-19044304.78913666\n",
      "Losgistic Regression(34000/100000): loss=-19083033.163536843\n",
      "Losgistic Regression(35000/100000): loss=-19119733.020641763\n",
      "Losgistic Regression(36000/100000): loss=-19154556.770490967\n",
      "Losgistic Regression(37000/100000): loss=-19187642.10459073\n",
      "Losgistic Regression(38000/100000): loss=-19219113.613513943\n",
      "Losgistic Regression(39000/100000): loss=-19249083.985745035\n",
      "Losgistic Regression(40000/100000): loss=-19277656.17267111\n",
      "Losgistic Regression(41000/100000): loss=-19304923.26221315\n",
      "Losgistic Regression(42000/100000): loss=-19330970.35926909\n",
      "Losgistic Regression(43000/100000): loss=-19355875.180184305\n",
      "Losgistic Regression(44000/100000): loss=-19379708.859251298\n",
      "Losgistic Regression(45000/100000): loss=-19402536.647963177\n",
      "Losgistic Regression(46000/100000): loss=-19424418.52547581\n",
      "Losgistic Regression(47000/100000): loss=-19445409.734691165\n",
      "Losgistic Regression(48000/100000): loss=-19465561.254464686\n",
      "Losgistic Regression(49000/100000): loss=-19484920.21633419\n",
      "Losgistic Regression(50000/100000): loss=-19503530.27297856\n",
      "Losgistic Regression(51000/100000): loss=-19521431.924608763\n",
      "Losgistic Regression(52000/100000): loss=-19538662.808669314\n",
      "Losgistic Regression(53000/100000): loss=-19555257.95751001\n",
      "Losgistic Regression(54000/100000): loss=-19571250.028062366\n",
      "Losgistic Regression(55000/100000): loss=-19586669.621781625\n",
      "Losgistic Regression(56000/100000): loss=-19601544.96572731\n",
      "Losgistic Regression(57000/100000): loss=-19615902.92292718\n",
      "Losgistic Regression(58000/100000): loss=-19629768.48523869\n",
      "Losgistic Regression(59000/100000): loss=-19643165.110032324\n",
      "Losgistic Regression(60000/100000): loss=-19656115.52071395\n",
      "Losgistic Regression(61000/100000): loss=-19668639.440809503\n",
      "Losgistic Regression(62000/100000): loss=-19680756.558475815\n",
      "Losgistic Regression(63000/100000): loss=-19692485.377999134\n",
      "Losgistic Regression(64000/100000): loss=-19703843.2780626\n",
      "Losgistic Regression(65000/100000): loss=-19714846.658731826\n",
      "Losgistic Regression(66000/100000): loss=-19725511.017401554\n",
      "Losgistic Regression(67000/100000): loss=-19735851.0127058\n",
      "Losgistic Regression(68000/100000): loss=-19745880.52210366\n",
      "Losgistic Regression(69000/100000): loss=-19755612.69436495\n",
      "Losgistic Regression(70000/100000): loss=-19765059.997607972\n",
      "Losgistic Regression(71000/100000): loss=-19774234.26339828\n",
      "Losgistic Regression(72000/100000): loss=-19783146.727319214\n",
      "Losgistic Regression(73000/100000): loss=-19791808.066357877\n",
      "Losgistic Regression(74000/100000): loss=-19800228.433412656\n",
      "Losgistic Regression(75000/100000): loss=-19808417.489186414\n",
      "Losgistic Regression(76000/100000): loss=-19816384.43170213\n",
      "Losgistic Regression(77000/100000): loss=-19824138.023650773\n",
      "Losgistic Regression(78000/100000): loss=-19831686.617763985\n",
      "Losgistic Regression(79000/100000): loss=-19839038.180371005\n",
      "Losgistic Regression(80000/100000): loss=-19846200.40784303\n",
      "Losgistic Regression(81000/100000): loss=-19853180.31290167\n",
      "Losgistic Regression(82000/100000): loss=-19859985.01820347\n",
      "Losgistic Regression(83000/100000): loss=-19866621.119001914\n",
      "Losgistic Regression(84000/100000): loss=-19873094.94309165\n",
      "Losgistic Regression(85000/100000): loss=-19879412.547330413\n",
      "Losgistic Regression(86000/100000): loss=-19885579.728835315\n",
      "Losgistic Regression(87000/100000): loss=-19891602.03770426\n",
      "Losgistic Regression(88000/100000): loss=-19897484.78940485\n",
      "Losgistic Regression(89000/100000): loss=-19903233.07653867\n",
      "Losgistic Regression(90000/100000): loss=-19908851.77993583\n",
      "Losgistic Regression(91000/100000): loss=-19914345.57909794\n",
      "Losgistic Regression(92000/100000): loss=-19919718.96202289\n",
      "Losgistic Regression(93000/100000): loss=-19924976.2344513\n",
      "Losgistic Regression(94000/100000): loss=-19930121.528572276\n",
      "Losgistic Regression(95000/100000): loss=-19935158.811225545\n",
      "Losgistic Regression(96000/100000): loss=-19940091.891634226\n",
      "Losgistic Regression(97000/100000): loss=-19944924.428700306\n",
      "Losgistic Regression(98000/100000): loss=-19949659.93789192\n",
      "Losgistic Regression(99000/100000): loss=-19954301.79774978\n",
      "0.8108 0.8062\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L=np.linalg.eigvals(train_tX2[idxes].T @ train_tX2[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y2[idxes], train_tX2[idxes], gamma=1/L, \n",
    "                       max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX2[idxes], train_y2[idxes], cv_tX2, cv_y2, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 10, 13, 16,  0,  2,  8,  9, 13, 19,  0,  1,  3,  9, 13, 19,\n",
       "        0])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(91)[abs(w)>1]%30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   9.99999500e-07,   9.99999000e-13,\n",
       "          9.99998500e-19],\n",
       "       [  2.00000000e+00,   3.00000000e+00,   4.00000000e+00,\n",
       "          9.00000000e+00,   1.09861262e+00,   1.20694969e+00,\n",
       "          1.32597017e+00],\n",
       "       [  4.00000000e+00,   5.00000000e+00,   1.60000000e+01,\n",
       "          2.50000000e+01,   1.60943811e+00,   2.59029104e+00,\n",
       "          4.16891312e+00],\n",
       "       [  6.00000000e+00,   7.00000000e+00,   3.60000000e+01,\n",
       "          4.90000000e+01,   1.94591029e+00,   3.78656686e+00,\n",
       "          7.36831943e+00],\n",
       "       [  8.00000000e+00,   9.00000000e+00,   6.40000000e+01,\n",
       "          8.10000000e+01,   2.19722469e+00,   4.82779633e+00,\n",
       "          1.06077533e+01]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_polynomial_without_mixed_term_with_log(tx, log_cols, degree=2):\n",
    "    n = tx.shape[0]\n",
    "    tmp = tx\n",
    "    for i in range(2, degree+1):\n",
    "        tmp = np.c_[tmp, tx**i]\n",
    "        \n",
    "    tmp = np.c_[tmp, np.log(tx[:, log_cols] + 1e-6)]\n",
    "    tmp = np.c_[tmp, (np.log(tx[:, log_cols] + 1e-6))**2]\n",
    "    tmp = np.c_[tmp, (np.log(tx[:, log_cols] + 1e-6))**3]\n",
    "    return tmp\n",
    "\n",
    "build_polynomial_without_mixed_term_with_log(np.arange(10).reshape(5,2), [1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 133)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term_with_log(filled_tX, log_features, degree = 4)\n",
    "tX3, mean_x3, std_x3 = standardize(tmp)\n",
    "y3 = transform_y(y)\n",
    "tX3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 133)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX3, cv_tX3, train_y3, cv_y3 = split_data(tX3, y3, training_ratio)\n",
    "cv_tX3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18430305201e-05\n",
      "Losgistic Regression(0/10000): loss=-2611163.012880917\n",
      "Losgistic Regression(1000/10000): loss=-3936990.4564067796\n",
      "Losgistic Regression(2000/10000): loss=-4313944.990193981\n",
      "Losgistic Regression(3000/10000): loss=-4530935.028052804\n",
      "Losgistic Regression(4000/10000): loss=-4686759.265161883\n",
      "Losgistic Regression(5000/10000): loss=-4806968.908878522\n",
      "Losgistic Regression(6000/10000): loss=-4904122.84751413\n",
      "Losgistic Regression(7000/10000): loss=-4985372.565744881\n",
      "Losgistic Regression(8000/10000): loss=-5055214.850281095\n",
      "Losgistic Regression(9000/10000): loss=-5116816.102477511\n",
      "0.813 0.80572\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L=np.linalg.eigvals(train_tX3[idxes].T @ train_tX3[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y3[idxes], train_tX3[idxes], gamma=0.0001, \n",
    "                       max_iters = 10000, lambda_=0.00, regularizor=regularizor_ridge)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX3[idxes], train_y3[idxes], cv_tX3, cv_y3, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For higher dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 115)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term_with_log(filled_tX, log_features, degree = 3)\n",
    "tX4, mean_x4, std_x4 = standardize(tmp)\n",
    "y4 = transform_y(y)\n",
    "tX4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 115)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX4, cv_tX4, train_y4, cv_y4 = split_data(tX4, y4, training_ratio)\n",
    "cv_tX4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025368797602\n",
      "Losgistic Regression(0/10000): loss=-893711.0623600627\n",
      "Losgistic Regression(1000/10000): loss=-944014.4622320133\n",
      "Losgistic Regression(2000/10000): loss=-1033972.7961426789\n",
      "Losgistic Regression(3000/10000): loss=-1083628.4283881453\n",
      "Losgistic Regression(4000/10000): loss=-1114844.76407547\n",
      "Losgistic Regression(5000/10000): loss=-1136101.0571259223\n",
      "Losgistic Regression(6000/10000): loss=-1151420.8972547455\n",
      "Losgistic Regression(7000/10000): loss=-1162947.157179648\n",
      "Losgistic Regression(8000/10000): loss=-1171919.5317238276\n",
      "Losgistic Regression(9000/10000): loss=-1179101.937071516\n",
      "0.83 0.80236\n",
      "Losgistic Regression(0/10000): loss=-888225.356995657\n",
      "Losgistic Regression(1000/10000): loss=-914899.2770507383\n",
      "Losgistic Regression(2000/10000): loss=-951666.3112675188\n",
      "Losgistic Regression(3000/10000): loss=-972020.5028958153\n",
      "Losgistic Regression(4000/10000): loss=-986281.9179660012\n",
      "Losgistic Regression(5000/10000): loss=-997211.5103137465\n",
      "Losgistic Regression(6000/10000): loss=-1006039.1417347568\n",
      "Losgistic Regression(7000/10000): loss=-1013453.2140185822\n",
      "Losgistic Regression(8000/10000): loss=-1019863.7194260703\n",
      "Losgistic Regression(9000/10000): loss=-1025526.9867476765\n",
      "0.8255 0.79928\n",
      "Losgistic Regression(0/10000): loss=-187414.56318102175\n",
      "Losgistic Regression(1000/10000): loss=-910476.2669747613\n",
      "Losgistic Regression(2000/10000): loss=-996468.4181271823\n",
      "Losgistic Regression(3000/10000): loss=-1026000.330118845\n",
      "Losgistic Regression(4000/10000): loss=-1043636.8692616977\n",
      "Losgistic Regression(5000/10000): loss=-1056548.6702058925\n",
      "Losgistic Regression(6000/10000): loss=-1067113.3076216786\n",
      "Losgistic Regression(7000/10000): loss=-1076316.6124512162\n",
      "Losgistic Regression(8000/10000): loss=-1084619.7870569746\n",
      "Losgistic Regression(9000/10000): loss=-1092261.81118189\n",
      "0.8165 0.79644\n",
      "Losgistic Regression(0/10000): loss=-842663.2328824318\n",
      "Losgistic Regression(1000/10000): loss=-1047295.7661965976\n",
      "Losgistic Regression(2000/10000): loss=-1079830.0264446877\n",
      "Losgistic Regression(3000/10000): loss=-1097823.3417883508\n",
      "Losgistic Regression(4000/10000): loss=-1109819.1619287694\n",
      "Losgistic Regression(5000/10000): loss=-1118843.2123723465\n",
      "Losgistic Regression(6000/10000): loss=-1126232.0238309514\n",
      "Losgistic Regression(7000/10000): loss=-1132621.3044755156\n",
      "Losgistic Regression(8000/10000): loss=-1138341.6483244142\n",
      "Losgistic Regression(9000/10000): loss=-1143581.6129379468\n",
      "0.8255 0.79868\n",
      "Losgistic Regression(0/10000): loss=-1117242.3208371364\n",
      "Losgistic Regression(1000/10000): loss=-1098595.7163759307\n",
      "Losgistic Regression(2000/10000): loss=-1125809.8024460913\n",
      "Losgistic Regression(3000/10000): loss=-1145343.612964509\n",
      "Losgistic Regression(4000/10000): loss=-1161280.2159424936\n",
      "Losgistic Regression(5000/10000): loss=-1174947.0665290602\n",
      "Losgistic Regression(6000/10000): loss=-1186946.4629059574\n",
      "Losgistic Regression(7000/10000): loss=-1197634.9089258444\n",
      "Losgistic Regression(8000/10000): loss=-1207261.5873182365\n",
      "Losgistic Regression(9000/10000): loss=-1216015.444156253\n",
      "0.831 0.80144\n",
      "Losgistic Regression(0/10000): loss=-643959.0156982732\n",
      "Losgistic Regression(1000/10000): loss=-848835.6779847938\n",
      "Losgistic Regression(2000/10000): loss=-855704.3382310825\n",
      "Losgistic Regression(3000/10000): loss=-851982.8594892009\n",
      "Losgistic Regression(4000/10000): loss=-863694.2728537429\n",
      "Losgistic Regression(5000/10000): loss=-871568.5971944006\n",
      "Losgistic Regression(6000/10000): loss=-876234.9026945467\n",
      "Losgistic Regression(7000/10000): loss=-879035.2786650405\n",
      "Losgistic Regression(8000/10000): loss=-880842.3302220242\n",
      "Losgistic Regression(9000/10000): loss=-882136.956801291\n",
      "0.8355 0.79776\n",
      "Losgistic Regression(0/10000): loss=-611656.6876125119\n",
      "Losgistic Regression(1000/10000): loss=-942485.5925478501\n",
      "Losgistic Regression(2000/10000): loss=-949216.2162596553\n",
      "Losgistic Regression(3000/10000): loss=-959263.9295640655\n",
      "Losgistic Regression(4000/10000): loss=-969699.1326666051\n",
      "Losgistic Regression(5000/10000): loss=-979632.7007027564\n",
      "Losgistic Regression(6000/10000): loss=-988748.808773844\n",
      "Losgistic Regression(7000/10000): loss=-997175.1088148319\n",
      "Losgistic Regression(8000/10000): loss=-1005486.9257729569\n",
      "Losgistic Regression(9000/10000): loss=-1013761.0200689401\n",
      "0.8275 0.79768\n",
      "Losgistic Regression(0/10000): loss=-1140339.7282607036\n",
      "Losgistic Regression(1000/10000): loss=-1369592.4545054086\n",
      "Losgistic Regression(2000/10000): loss=-1369830.5020148335\n",
      "Losgistic Regression(3000/10000): loss=-1373978.547302951\n",
      "Losgistic Regression(4000/10000): loss=-1380831.6398379337\n",
      "Losgistic Regression(5000/10000): loss=-1388917.1492501264\n",
      "Losgistic Regression(6000/10000): loss=-1397399.8462677891\n",
      "Losgistic Regression(7000/10000): loss=-1405842.2714541412\n",
      "Losgistic Regression(8000/10000): loss=-1414019.987908485\n",
      "Losgistic Regression(9000/10000): loss=-1421820.6718661443\n",
      "0.837 0.80384\n",
      "Losgistic Regression(0/10000): loss=-852138.2869366638\n",
      "Losgistic Regression(1000/10000): loss=-1140690.0437953426\n",
      "Losgistic Regression(2000/10000): loss=-1165826.4970581883\n",
      "Losgistic Regression(3000/10000): loss=-1181301.0602005483\n",
      "Losgistic Regression(4000/10000): loss=-1191602.564569472\n",
      "Losgistic Regression(5000/10000): loss=-1201917.7019693856\n",
      "Losgistic Regression(6000/10000): loss=-1209903.2126902395\n",
      "Losgistic Regression(7000/10000): loss=-1215820.7919976818\n",
      "Losgistic Regression(8000/10000): loss=-1220097.5045952047\n",
      "Losgistic Regression(9000/10000): loss=-1222974.4896819138\n",
      "0.843 0.7962\n",
      "Losgistic Regression(0/10000): loss=-901449.4085884681\n",
      "Losgistic Regression(1000/10000): loss=-849347.3003788169\n",
      "Losgistic Regression(2000/10000): loss=-850472.9386638028\n",
      "Losgistic Regression(3000/10000): loss=-853298.5861760261\n",
      "Losgistic Regression(4000/10000): loss=-855070.8852689307\n",
      "Losgistic Regression(5000/10000): loss=-856211.637571377\n",
      "Losgistic Regression(6000/10000): loss=-857233.9460180368\n",
      "Losgistic Regression(7000/10000): loss=-858392.121052195\n",
      "Losgistic Regression(8000/10000): loss=-859730.2400875996\n",
      "Losgistic Regression(9000/10000): loss=-861192.9838514352\n",
      "0.8205 0.80228\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(200)\n",
    "L=np.linalg.eigvals(train_tX4[idxes].T @ train_tX4[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w = np.random.randn(train_tX4.shape[1]) * 0.1\n",
    "maximum_size = train_y4.shape[0]\n",
    "\n",
    "for i in range(10):\n",
    "    idxes = np.random.randint(0, maximum_size, 2000)\n",
    "    w, losses = reg_logistic_regression_SGD(train_y4[idxes], train_tX4[idxes], gamma=1/L, \n",
    "                       max_iters = 10000, lambda_=0.0001, regularizor=regularizor_lasso, w0=w)\n",
    "\n",
    "    tr_acc, te_acc = prediction_and_accuracy(train_tX4[idxes], train_y4[idxes], cv_tX4, cv_y4, w)\n",
    "    print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205 0.80228\n"
     ]
    }
   ],
   "source": [
    "tr_acc, te_acc = prediction_and_accuracy(train_tX4[idxes], train_y4[idxes], cv_tX4, cv_y4, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Higher Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 187)"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term_with_log(filled_tX, log_features, degree = 5)\n",
    "tX5, mean_x5, std_x5 = standardize(tmp)\n",
    "y5 = transform_y(y)\n",
    "tX5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 187)"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX5, cv_tX5, train_y5, cv_y5 = split_data(tX5, y5, training_ratio)\n",
    "cv_tX5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23143465589e-05\n",
      "Losgistic Regression(0/20000): loss=-152948.4985915193\n",
      "Losgistic Regression(1000/20000): loss=-558222.8949306096\n",
      "Losgistic Regression(2000/20000): loss=-641129.1642222997\n",
      "Losgistic Regression(3000/20000): loss=-694583.4965614841\n",
      "Losgistic Regression(4000/20000): loss=-733162.1216511249\n",
      "Losgistic Regression(5000/20000): loss=-763211.2116422988\n",
      "Losgistic Regression(6000/20000): loss=-787804.3128164957\n",
      "Losgistic Regression(7000/20000): loss=-808619.5440130396\n",
      "Losgistic Regression(8000/20000): loss=-826656.972967146\n",
      "Losgistic Regression(9000/20000): loss=-842556.2018722652\n",
      "Losgistic Regression(10000/20000): loss=-856751.7948597504\n",
      "Losgistic Regression(11000/20000): loss=-869553.7814276543\n",
      "Losgistic Regression(12000/20000): loss=-881193.2501136299\n",
      "Losgistic Regression(13000/20000): loss=-891847.882410691\n",
      "Losgistic Regression(14000/20000): loss=-901657.8873610676\n",
      "Losgistic Regression(15000/20000): loss=-910736.4680695792\n",
      "Losgistic Regression(16000/20000): loss=-919176.2815885241\n",
      "Losgistic Regression(17000/20000): loss=-927054.5826593324\n",
      "Losgistic Regression(18000/20000): loss=-934436.5140949821\n",
      "Losgistic Regression(19000/20000): loss=-941377.1463648106\n",
      "0.816 0.78864\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(2000)\n",
    "L = np.linalg.eigvals(train_tX5[idxes].T @ train_tX5[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y5[idxes], train_tX5[idxes], gamma=1/L, \n",
    "                   max_iters = 20000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX5[idxes], train_y5[idxes], cv_tX5, cv_y5, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we only use those features not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will lose important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_missing_featuers = [i for i in range(30) if i not in columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 58)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(tX[:, no_missing_featuers], degree = 3)\n",
    "tX6, mean_x6, std_x6 = standardize(tmp)\n",
    "y6 = transform_y(y)\n",
    "tX6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 58)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX6, cv_tX6, train_y6, cv_y6 = split_data(tX6, y6, training_ratio)\n",
    "cv_tX6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5626812771e-05\n",
      "Losgistic Regression(0/10000): loss=56635.91876811694\n",
      "Losgistic Regression(1000/10000): loss=-70010.60927836965\n",
      "Losgistic Regression(2000/10000): loss=-85440.47538549885\n",
      "Losgistic Regression(3000/10000): loss=-94429.26704526579\n",
      "Losgistic Regression(4000/10000): loss=-100765.83908769637\n",
      "Losgistic Regression(5000/10000): loss=-105695.19605704182\n",
      "Losgistic Regression(6000/10000): loss=-109808.0258851113\n",
      "Losgistic Regression(7000/10000): loss=-113411.64391236744\n",
      "Losgistic Regression(8000/10000): loss=-116671.54858691993\n",
      "Losgistic Regression(9000/10000): loss=-119669.28184539902\n",
      "0.798 0.76268\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L = np.linalg.eigvals(train_tX6[idxes].T @ train_tX6[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y6[idxes], train_tX6[idxes], gamma=1/L, \n",
    "                   max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX6[idxes], train_y6[idxes], cv_tX6, cv_y6, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_missing_featuers = [i for i in range(30) if i not in columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 61)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(tX[:, 0:12], degree = 5)\n",
    "tX7, mean_x7, std_x7 = standardize(tmp)\n",
    "y7 = transform_y(y)\n",
    "tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 61)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX7, cv_tX7, train_y7, cv_y7 = split_data(tX7, y7, training_ratio)\n",
    "cv_tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.366584075e-05\n",
      "Losgistic Regression(0/100000): loss=80551.96217051383\n",
      "Losgistic Regression(1000/100000): loss=-630513.4147324018\n",
      "Losgistic Regression(2000/100000): loss=-795592.0324176315\n",
      "Losgistic Regression(3000/100000): loss=-774859.9636654159\n",
      "Losgistic Regression(4000/100000): loss=-903662.8565081253\n",
      "Losgistic Regression(5000/100000): loss=-996484.1774392383\n",
      "Losgistic Regression(6000/100000): loss=-1067880.9798726423\n",
      "Losgistic Regression(7000/100000): loss=-1128515.1268494313\n",
      "Losgistic Regression(8000/100000): loss=-1182350.0825128625\n",
      "Losgistic Regression(9000/100000): loss=-1231311.037993663\n",
      "Losgistic Regression(10000/100000): loss=-1276433.9699305983\n",
      "Losgistic Regression(11000/100000): loss=-1318312.417875254\n",
      "Losgistic Regression(12000/100000): loss=-1357323.755029252\n",
      "Losgistic Regression(13000/100000): loss=-1393752.8738484322\n",
      "Losgistic Regression(14000/100000): loss=-1427857.195925385\n",
      "Losgistic Regression(15000/100000): loss=-1459893.3792936816\n",
      "Losgistic Regression(16000/100000): loss=-1490115.1517191748\n",
      "Losgistic Regression(17000/100000): loss=-1518748.420602009\n",
      "Losgistic Regression(18000/100000): loss=-1545957.9547523828\n",
      "Losgistic Regression(19000/100000): loss=-1571840.3672107547\n",
      "Losgistic Regression(20000/100000): loss=-1596452.094921439\n",
      "Losgistic Regression(21000/100000): loss=-1619847.5363121359\n",
      "Losgistic Regression(22000/100000): loss=-1642095.4107520522\n",
      "Losgistic Regression(23000/100000): loss=-1663276.1870252134\n",
      "Losgistic Regression(24000/100000): loss=-1683473.1284398849\n",
      "Losgistic Regression(25000/100000): loss=-1702766.0946204553\n",
      "Losgistic Regression(26000/100000): loss=-1721228.6074398377\n",
      "Losgistic Regression(27000/100000): loss=-1738927.0343240583\n",
      "Losgistic Regression(28000/100000): loss=-1755920.8043789673\n",
      "Losgistic Regression(29000/100000): loss=-1772263.0517694037\n",
      "Losgistic Regression(30000/100000): loss=-1788001.222470149\n",
      "Losgistic Regression(31000/100000): loss=-1803177.9632087452\n",
      "Losgistic Regression(32000/100000): loss=-1817831.6284656336\n",
      "Losgistic Regression(33000/100000): loss=-1831996.8616005434\n",
      "Losgistic Regression(34000/100000): loss=-1845705.8533768144\n",
      "Losgistic Regression(35000/100000): loss=-1858986.4307749649\n",
      "Losgistic Regression(36000/100000): loss=-1871864.5216737315\n",
      "Losgistic Regression(37000/100000): loss=-1884364.0254758184\n",
      "Losgistic Regression(38000/100000): loss=-1896506.8643760344\n",
      "Losgistic Regression(39000/100000): loss=-1908313.1986635604\n",
      "Losgistic Regression(40000/100000): loss=-1919801.663623177\n",
      "Losgistic Regression(41000/100000): loss=-1930989.44138767\n",
      "Losgistic Regression(42000/100000): loss=-1941892.4699810178\n",
      "Losgistic Regression(43000/100000): loss=-1952525.5487590118\n",
      "Losgistic Regression(44000/100000): loss=-1962902.444637683\n",
      "Losgistic Regression(45000/100000): loss=-1973035.9857846184\n",
      "Losgistic Regression(46000/100000): loss=-1982938.1447510433\n",
      "Losgistic Regression(47000/100000): loss=-1992620.1591715952\n",
      "Losgistic Regression(48000/100000): loss=-2002092.5544720117\n",
      "Losgistic Regression(49000/100000): loss=-2011364.949927667\n",
      "Losgistic Regression(50000/100000): loss=-2020446.6446341656\n",
      "Losgistic Regression(51000/100000): loss=-2029346.3097112968\n",
      "Losgistic Regression(52000/100000): loss=-2038072.0923009838\n",
      "Losgistic Regression(53000/100000): loss=-2046631.663355307\n",
      "Losgistic Regression(54000/100000): loss=-2055032.254138727\n",
      "Losgistic Regression(55000/100000): loss=-2063280.5613327862\n",
      "Losgistic Regression(56000/100000): loss=-2071383.2265596623\n",
      "Losgistic Regression(57000/100000): loss=-2079346.3052253923\n",
      "Losgistic Regression(58000/100000): loss=-2087175.5417391164\n",
      "Losgistic Regression(59000/100000): loss=-2094876.3929295891\n",
      "Losgistic Regression(60000/100000): loss=-2102454.0332399197\n",
      "Losgistic Regression(61000/100000): loss=-2109913.3714683354\n",
      "Losgistic Regression(62000/100000): loss=-2117259.067587272\n",
      "Losgistic Regression(63000/100000): loss=-2124495.5484359856\n",
      "Losgistic Regression(64000/100000): loss=-2131627.0222418676\n",
      "Losgistic Regression(65000/100000): loss=-2138657.4920485998\n",
      "Losgistic Regression(66000/100000): loss=-2145590.7616337333\n",
      "Losgistic Regression(67000/100000): loss=-2152430.489025854\n",
      "Losgistic Regression(68000/100000): loss=-2159180.1127063455\n",
      "Losgistic Regression(69000/100000): loss=-2165842.9276861367\n",
      "Losgistic Regression(70000/100000): loss=-2172422.079083604\n",
      "Losgistic Regression(71000/100000): loss=-2178920.5698814546\n",
      "Losgistic Regression(72000/100000): loss=-2185341.268380006\n",
      "Losgistic Regression(73000/100000): loss=-2191686.9154275027\n",
      "Losgistic Regression(74000/100000): loss=-2197960.131275927\n",
      "Losgistic Regression(75000/100000): loss=-2204163.42202057\n",
      "Losgistic Regression(76000/100000): loss=-2210299.1856329823\n",
      "Losgistic Regression(77000/100000): loss=-2216369.717609268\n",
      "Losgistic Regression(78000/100000): loss=-2222377.2162602777\n",
      "Losgistic Regression(79000/100000): loss=-2228323.7876699865\n",
      "Losgistic Regression(80000/100000): loss=-2234211.450346511\n",
      "Losgistic Regression(81000/100000): loss=-2240042.1395883067\n",
      "Losgistic Regression(82000/100000): loss=-2245817.746295438\n",
      "Losgistic Regression(83000/100000): loss=-2251540.1891373326\n",
      "Losgistic Regression(84000/100000): loss=-2257210.8779892563\n",
      "Losgistic Regression(85000/100000): loss=-2262831.537805881\n",
      "Losgistic Regression(86000/100000): loss=-2268403.7680877578\n",
      "Losgistic Regression(87000/100000): loss=-2273929.093281213\n",
      "Losgistic Regression(88000/100000): loss=-2279408.9731722907\n",
      "Losgistic Regression(89000/100000): loss=-2284844.8087720615\n",
      "Losgistic Regression(90000/100000): loss=-2290237.9461814817\n",
      "Losgistic Regression(91000/100000): loss=-2295589.679575813\n",
      "Losgistic Regression(92000/100000): loss=-2300901.2537544477\n",
      "Losgistic Regression(93000/100000): loss=-2306173.8664365956\n",
      "Losgistic Regression(94000/100000): loss=-2311408.6703808685\n",
      "Losgistic Regression(95000/100000): loss=-2316606.7753674327\n",
      "Losgistic Regression(96000/100000): loss=-2321769.2500646585\n",
      "Losgistic Regression(97000/100000): loss=-2326897.123793159\n",
      "Losgistic Regression(98000/100000): loss=-2331991.388196933\n",
      "Losgistic Regression(99000/100000): loss=-2337053.0767164757\n",
      "0.7932 0.79168\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "                   max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_missing_featuers = [i for i in range(30) if i not in columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (69982,) (11,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-655-9f6d918c1b66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstatus_tX\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_with_missing_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (69982,) (11,) "
     ]
    }
   ],
   "source": [
    "tX[status_tX==7, columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_tX =np.sum(tX == -999, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_missing_tX = tX[status_tX == 0, :]\n",
    "missing_tX = tX[status_tX != 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 121)"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(fill_na(tX, np.median), degree=4)\n",
    "tX7, mean_x7, std_x7 = standardize(tmp)\n",
    "y7 = transform_y(y)\n",
    "tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 121)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX7, cv_tX7, train_y7, cv_y7 = split_data(tX7, y7, training_ratio)\n",
    "cv_tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29134200029e-05\n",
      "Losgistic Regression(0/100000): loss=-1717512.0003571624\n",
      "Losgistic Regression(1000/100000): loss=-3648780.826639854\n",
      "Losgistic Regression(2000/100000): loss=-4081218.460205971\n",
      "Losgistic Regression(3000/100000): loss=-4294065.902958807\n",
      "Losgistic Regression(4000/100000): loss=-4428942.877889703\n",
      "Losgistic Regression(5000/100000): loss=-4526784.783047925\n",
      "Losgistic Regression(6000/100000): loss=-4603639.278624557\n",
      "Losgistic Regression(7000/100000): loss=-4667203.4450635025\n",
      "Losgistic Regression(8000/100000): loss=-4721732.211992899\n",
      "Losgistic Regression(9000/100000): loss=-4769965.168966695\n",
      "Losgistic Regression(10000/100000): loss=-4814218.630067779\n",
      "Losgistic Regression(11000/100000): loss=-4856063.867183341\n",
      "Losgistic Regression(12000/100000): loss=-4895444.936642361\n",
      "Losgistic Regression(13000/100000): loss=-4931965.253089373\n",
      "Losgistic Regression(14000/100000): loss=-4965607.549755028\n",
      "Losgistic Regression(15000/100000): loss=-4996576.564565537\n",
      "Losgistic Regression(16000/100000): loss=-5025131.551641333\n",
      "Losgistic Regression(17000/100000): loss=-5051518.641098386\n",
      "Losgistic Regression(18000/100000): loss=-5075956.812618833\n",
      "Losgistic Regression(19000/100000): loss=-5098642.835348515\n",
      "Losgistic Regression(20000/100000): loss=-5119758.619210919\n",
      "Losgistic Regression(21000/100000): loss=-5139474.947577568\n",
      "Losgistic Regression(22000/100000): loss=-5157951.555844031\n",
      "Losgistic Regression(23000/100000): loss=-5175335.880489707\n",
      "Losgistic Regression(24000/100000): loss=-5191761.215012589\n",
      "Losgistic Regression(25000/100000): loss=-5207345.573022047\n",
      "Losgistic Regression(26000/100000): loss=-5222190.9801878445\n",
      "Losgistic Regression(27000/100000): loss=-5236384.085826428\n",
      "Losgistic Regression(28000/100000): loss=-5249997.2832159735\n",
      "Losgistic Regression(29000/100000): loss=-5263090.565476814\n",
      "Losgistic Regression(30000/100000): loss=-5275713.68756587\n",
      "Losgistic Regression(31000/100000): loss=-5287908.302626834\n",
      "Losgistic Regression(32000/100000): loss=-5299709.8187843645\n",
      "Losgistic Regression(33000/100000): loss=-5311148.859704677\n",
      "Losgistic Regression(34000/100000): loss=-5322252.332743386\n",
      "Losgistic Regression(35000/100000): loss=-5333044.176485074\n",
      "Losgistic Regression(36000/100000): loss=-5343545.87754277\n",
      "Losgistic Regression(37000/100000): loss=-5353776.860461733\n",
      "Losgistic Regression(38000/100000): loss=-5363754.681026568\n",
      "Losgistic Regression(39000/100000): loss=-5373495.3078898\n",
      "Losgistic Regression(40000/100000): loss=-5383013.280572444\n",
      "Losgistic Regression(41000/100000): loss=-5392321.853211723\n",
      "Losgistic Regression(42000/100000): loss=-5401433.125625127\n",
      "Losgistic Regression(43000/100000): loss=-5410358.159828562\n",
      "Losgistic Regression(44000/100000): loss=-5419107.084262135\n",
      "Losgistic Regression(45000/100000): loss=-5427689.176481941\n",
      "Losgistic Regression(46000/100000): loss=-5436113.00536876\n",
      "Losgistic Regression(47000/100000): loss=-5444386.386776669\n",
      "Losgistic Regression(48000/100000): loss=-5452516.559538267\n",
      "Losgistic Regression(49000/100000): loss=-5460510.2053415105\n",
      "Losgistic Regression(50000/100000): loss=-5468373.503756234\n",
      "Losgistic Regression(51000/100000): loss=-5476112.17992159\n",
      "Losgistic Regression(52000/100000): loss=-5483731.547446015\n",
      "Losgistic Regression(53000/100000): loss=-5491236.959222174\n",
      "Losgistic Regression(54000/100000): loss=-5498632.665388892\n",
      "Losgistic Regression(55000/100000): loss=-5505922.895334726\n",
      "Losgistic Regression(56000/100000): loss=-5513111.647577949\n",
      "Losgistic Regression(57000/100000): loss=-5520202.847643162\n",
      "Losgistic Regression(58000/100000): loss=-5527199.767281473\n",
      "Losgistic Regression(59000/100000): loss=-5534105.7840347625\n",
      "Losgistic Regression(60000/100000): loss=-5540924.018326043\n",
      "Losgistic Regression(61000/100000): loss=-5547657.405585693\n",
      "Losgistic Regression(62000/100000): loss=-5554308.712905005\n",
      "Losgistic Regression(63000/100000): loss=-5560880.553616889\n",
      "Losgistic Regression(64000/100000): loss=-5567375.399870251\n",
      "Losgistic Regression(65000/100000): loss=-5573795.582807619\n",
      "Losgistic Regression(66000/100000): loss=-5580143.355259051\n",
      "Losgistic Regression(67000/100000): loss=-5586420.808684804\n",
      "Losgistic Regression(68000/100000): loss=-5592629.947348569\n",
      "Losgistic Regression(69000/100000): loss=-5598772.68309413\n",
      "Losgistic Regression(70000/100000): loss=-5604850.8392861625\n",
      "Losgistic Regression(71000/100000): loss=-5610866.156332824\n",
      "Losgistic Regression(72000/100000): loss=-5616820.297182732\n",
      "Losgistic Regression(73000/100000): loss=-5622714.852451766\n",
      "Losgistic Regression(74000/100000): loss=-5628551.34510517\n",
      "Losgistic Regression(75000/100000): loss=-5634331.234739348\n",
      "Losgistic Regression(76000/100000): loss=-5640055.921550685\n",
      "Losgistic Regression(77000/100000): loss=-5645726.749918227\n",
      "Losgistic Regression(78000/100000): loss=-5651345.011722134\n",
      "Losgistic Regression(79000/100000): loss=-5656911.949403531\n",
      "Losgistic Regression(80000/100000): loss=-5662428.758780659\n",
      "Losgistic Regression(81000/100000): loss=-5667896.591648945\n",
      "Losgistic Regression(82000/100000): loss=-5673316.558185232\n",
      "Losgistic Regression(83000/100000): loss=-5678689.7291736435\n",
      "Losgistic Regression(84000/100000): loss=-5684017.138069497\n",
      "Losgistic Regression(85000/100000): loss=-5689299.782915568\n",
      "Losgistic Regression(86000/100000): loss=-5694538.628123628\n",
      "Losgistic Regression(87000/100000): loss=-5699734.606133248\n",
      "Losgistic Regression(88000/100000): loss=-5704888.61895814\n",
      "Losgistic Regression(89000/100000): loss=-5710001.539629852\n",
      "Losgistic Regression(90000/100000): loss=-5715074.213547263\n",
      "Losgistic Regression(91000/100000): loss=-5720107.459739791\n",
      "Losgistic Regression(92000/100000): loss=-5725102.072051273\n",
      "Losgistic Regression(93000/100000): loss=-5730058.82024467\n",
      "Losgistic Regression(94000/100000): loss=-5734978.451064634\n",
      "Losgistic Regression(95000/100000): loss=-5739861.710068867\n",
      "Losgistic Regression(96000/100000): loss=-5744709.2714858465\n",
      "Losgistic Regression(97000/100000): loss=-5749521.85774167\n",
      "Losgistic Regression(98000/100000): loss=-5754300.097403789\n",
      "Losgistic Regression(99000/100000): loss=-5759044.635053105\n",
      "0.8196 0.81232\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 100000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/150000): loss=-15963144.592339689\n",
      "Losgistic Regression(1000/150000): loss=-16667671.693541316\n",
      "Losgistic Regression(2000/150000): loss=-17212372.60830468\n",
      "Losgistic Regression(3000/150000): loss=-17408288.015940994\n",
      "Losgistic Regression(4000/150000): loss=-17499791.72486209\n",
      "Losgistic Regression(5000/150000): loss=-17548372.77054372\n",
      "Losgistic Regression(6000/150000): loss=-17424324.785498414\n",
      "Losgistic Regression(7000/150000): loss=-17536282.58854017\n",
      "Losgistic Regression(8000/150000): loss=-17665225.62948474\n",
      "Losgistic Regression(9000/150000): loss=-17771234.220502492\n",
      "Losgistic Regression(10000/150000): loss=-17862328.549905375\n",
      "Losgistic Regression(11000/150000): loss=-17943540.53694835\n",
      "Losgistic Regression(12000/150000): loss=-18018073.32970965\n",
      "Losgistic Regression(13000/150000): loss=-18087990.19267849\n",
      "Losgistic Regression(14000/150000): loss=-18154630.56449576\n",
      "Losgistic Regression(15000/150000): loss=-18218867.233228095\n",
      "Losgistic Regression(16000/150000): loss=-18281270.823078956\n",
      "Losgistic Regression(17000/150000): loss=-18342218.39320433\n",
      "Losgistic Regression(18000/150000): loss=-18435383.798773292\n",
      "Losgistic Regression(19000/150000): loss=-18553707.95708682\n",
      "Losgistic Regression(20000/150000): loss=-18654152.412033368\n",
      "Losgistic Regression(21000/150000): loss=-18741290.129997093\n",
      "Losgistic Regression(22000/150000): loss=-18818513.39220994\n",
      "Losgistic Regression(23000/150000): loss=-18888196.613676947\n",
      "Losgistic Regression(24000/150000): loss=-18952020.617195252\n",
      "Losgistic Regression(25000/150000): loss=-19011189.104808155\n",
      "Losgistic Regression(26000/150000): loss=-19066568.07505603\n",
      "Losgistic Regression(27000/150000): loss=-19118779.827511385\n",
      "Losgistic Regression(28000/150000): loss=-19168273.037237417\n",
      "Losgistic Regression(29000/150000): loss=-19215378.210000932\n",
      "Losgistic Regression(30000/150000): loss=-19260349.026626144\n",
      "Losgistic Regression(31000/150000): loss=-19303390.558237266\n",
      "Losgistic Regression(32000/150000): loss=-19344675.952670265\n",
      "Losgistic Regression(33000/150000): loss=-19384356.21492956\n",
      "Losgistic Regression(34000/150000): loss=-19422564.267713998\n",
      "Losgistic Regression(35000/150000): loss=-19459419.053055372\n",
      "Losgistic Regression(36000/150000): loss=-19495026.476087008\n",
      "Losgistic Regression(37000/150000): loss=-19529481.40428285\n",
      "Losgistic Regression(38000/150000): loss=-19562868.14263374\n",
      "Losgistic Regression(39000/150000): loss=-19595262.362413235\n",
      "Losgistic Regression(40000/150000): loss=-19626731.761524867\n",
      "Losgistic Regression(41000/150000): loss=-19657337.192728862\n",
      "Losgistic Regression(42000/150000): loss=-19687133.269414235\n",
      "Losgistic Regression(43000/150000): loss=-19716169.367877137\n",
      "Losgistic Regression(44000/150000): loss=-19744490.193191063\n",
      "Losgistic Regression(45000/150000): loss=-19772136.363503594\n",
      "Losgistic Regression(46000/150000): loss=-19799145.01460443\n",
      "Losgistic Regression(47000/150000): loss=-19825550.424517505\n",
      "Losgistic Regression(48000/150000): loss=-19851383.366509672\n",
      "Losgistic Regression(49000/150000): loss=-19876672.383162055\n",
      "Losgistic Regression(50000/150000): loss=-19901444.01593583\n",
      "Losgistic Regression(51000/150000): loss=-19925722.879678182\n",
      "Losgistic Regression(52000/150000): loss=-19949531.869924173\n",
      "Losgistic Regression(53000/150000): loss=-19972892.34129317\n",
      "Losgistic Regression(54000/150000): loss=-19995824.261182\n",
      "Losgistic Regression(55000/150000): loss=-20018346.342671063\n",
      "Losgistic Regression(56000/150000): loss=-20040476.159898415\n",
      "Losgistic Regression(57000/150000): loss=-20062230.24861914\n",
      "Losgistic Regression(58000/150000): loss=-20083624.19420613\n",
      "Losgistic Regression(59000/150000): loss=-20104672.709029183\n",
      "Losgistic Regression(60000/150000): loss=-20125389.700790152\n",
      "Losgistic Regression(61000/150000): loss=-20145788.333171863\n",
      "Losgistic Regression(62000/150000): loss=-20165881.0799047\n",
      "Losgistic Regression(63000/150000): loss=-20185679.773185827\n",
      "Losgistic Regression(64000/150000): loss=-20205195.64727371\n",
      "Losgistic Regression(65000/150000): loss=-20224439.37788066\n",
      "Losgistic Regression(66000/150000): loss=-20243421.11795405\n",
      "Losgistic Regression(67000/150000): loss=-20262150.53028276\n",
      "Losgistic Regression(68000/150000): loss=-20280636.81733525\n",
      "Losgistic Regression(69000/150000): loss=-20298888.748660617\n",
      "Losgistic Regression(70000/150000): loss=-20316914.686135832\n",
      "Losgistic Regression(71000/150000): loss=-20334722.607293468\n",
      "Losgistic Regression(72000/150000): loss=-20352320.12693773\n",
      "Losgistic Regression(73000/150000): loss=-20369714.517228875\n",
      "Losgistic Regression(74000/150000): loss=-20386912.726378668\n",
      "Losgistic Regression(75000/150000): loss=-20403921.396091286\n",
      "Losgistic Regression(76000/150000): loss=-20420746.87786263\n",
      "Losgistic Regression(77000/150000): loss=-20437395.248236492\n",
      "Losgistic Regression(78000/150000): loss=-20453872.323106408\n",
      "Losgistic Regression(79000/150000): loss=-20470183.6711375\n",
      "Losgistic Regression(80000/150000): loss=-20486334.626384888\n",
      "Losgistic Regression(81000/150000): loss=-20502330.347674042\n",
      "Losgistic Regression(82000/150000): loss=-20518175.697866544\n",
      "Losgistic Regression(83000/150000): loss=-20533875.326734703\n",
      "Losgistic Regression(84000/150000): loss=-20549433.761970814\n",
      "Losgistic Regression(85000/150000): loss=-20564855.336505868\n",
      "Losgistic Regression(86000/150000): loss=-20580144.206985194\n",
      "Losgistic Regression(87000/150000): loss=-20595304.364065837\n",
      "Losgistic Regression(88000/150000): loss=-20610339.640901018\n",
      "Losgistic Regression(89000/150000): loss=-20625253.720875375\n",
      "Losgistic Regression(90000/150000): loss=-20640050.144822266\n",
      "Losgistic Regression(91000/150000): loss=-20654732.317781053\n",
      "Losgistic Regression(92000/150000): loss=-20669303.51539274\n",
      "Losgistic Regression(93000/150000): loss=-20683766.88993064\n",
      "Losgistic Regression(94000/150000): loss=-20698125.604815446\n",
      "Losgistic Regression(95000/150000): loss=-20712382.362203814\n",
      "Losgistic Regression(96000/150000): loss=-20726540.029994026\n",
      "Losgistic Regression(97000/150000): loss=-20740601.357217215\n",
      "Losgistic Regression(98000/150000): loss=-20754568.961751632\n",
      "Losgistic Regression(99000/150000): loss=-20768445.364826187\n",
      "Losgistic Regression(100000/150000): loss=-20782232.99834265\n",
      "Losgistic Regression(101000/150000): loss=-20795934.20895085\n",
      "Losgistic Regression(102000/150000): loss=-20809551.261651605\n",
      "Losgistic Regression(103000/150000): loss=-20823086.34324159\n",
      "Losgistic Regression(104000/150000): loss=-20836541.565597717\n",
      "Losgistic Regression(105000/150000): loss=-20849918.968823846\n",
      "Losgistic Regression(106000/150000): loss=-20863220.524239197\n",
      "Losgistic Regression(107000/150000): loss=-20876448.137219135\n",
      "Losgistic Regression(108000/150000): loss=-20889603.649893805\n",
      "Losgistic Regression(109000/150000): loss=-20902688.84371179\n",
      "Losgistic Regression(110000/150000): loss=-20915705.441875268\n",
      "Losgistic Regression(111000/150000): loss=-20928655.111657858\n",
      "Losgistic Regression(112000/150000): loss=-20941539.466597144\n",
      "Losgistic Regression(113000/150000): loss=-20954360.06858956\n",
      "Losgistic Regression(114000/150000): loss=-20967118.42988034\n",
      "Losgistic Regression(115000/150000): loss=-20979816.014955983\n",
      "Losgistic Regression(116000/150000): loss=-20992454.242345512\n",
      "Losgistic Regression(117000/150000): loss=-21005034.55788459\n",
      "Losgistic Regression(118000/150000): loss=-21017558.36034714\n",
      "Losgistic Regression(119000/150000): loss=-21030026.76326061\n",
      "Losgistic Regression(120000/150000): loss=-21042441.025808834\n",
      "Losgistic Regression(121000/150000): loss=-21054802.367445927\n",
      "Losgistic Regression(122000/150000): loss=-21067111.968994368\n",
      "Losgistic Regression(123000/150000): loss=-21079370.974605225\n",
      "Losgistic Regression(124000/150000): loss=-21091580.519189302\n",
      "Losgistic Regression(125000/150000): loss=-21103741.575371515\n",
      "Losgistic Regression(126000/150000): loss=-21115855.274776522\n",
      "Losgistic Regression(127000/150000): loss=-21127922.63539414\n",
      "Losgistic Regression(128000/150000): loss=-21139944.634014986\n",
      "Losgistic Regression(129000/150000): loss=-21151922.219391104\n",
      "Losgistic Regression(130000/150000): loss=-21163856.313511338\n",
      "Losgistic Regression(131000/150000): loss=-21175747.812185638\n",
      "Losgistic Regression(132000/150000): loss=-21187597.585676435\n",
      "Losgistic Regression(133000/150000): loss=-21199406.479365725\n",
      "Losgistic Regression(134000/150000): loss=-21211175.314480532\n",
      "Losgistic Regression(135000/150000): loss=-21222904.88881692\n",
      "Losgistic Regression(136000/150000): loss=-21234595.97745332\n",
      "Losgistic Regression(137000/150000): loss=-21246249.333463915\n",
      "Losgistic Regression(138000/150000): loss=-21257865.68862294\n",
      "Losgistic Regression(139000/150000): loss=-21269445.754085775\n",
      "Losgistic Regression(140000/150000): loss=-21280990.221053425\n",
      "Losgistic Regression(141000/150000): loss=-21292499.761419725\n",
      "Losgistic Regression(142000/150000): loss=-21303975.028401177\n",
      "Losgistic Regression(143000/150000): loss=-21315416.657151002\n",
      "Losgistic Regression(144000/150000): loss=-21326825.265357062\n",
      "Losgistic Regression(145000/150000): loss=-21338201.45382456\n",
      "Losgistic Regression(146000/150000): loss=-21349545.807045463\n",
      "Losgistic Regression(147000/150000): loss=-21360858.893753592\n",
      "Losgistic Regression(148000/150000): loss=-21372141.261900645\n",
      "Losgistic Regression(149000/150000): loss=-21383393.60608778\n",
      "0.8206 0.81404\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/200000): loss=9221233.193410587\n",
      "Losgistic Regression(1000/200000): loss=-8296709.870189696\n",
      "Losgistic Regression(2000/200000): loss=-9862373.335854245\n",
      "Losgistic Regression(3000/200000): loss=-10831655.627667965\n",
      "Losgistic Regression(4000/200000): loss=-11536115.326674325\n",
      "Losgistic Regression(5000/200000): loss=-12075983.19691701\n",
      "Losgistic Regression(6000/200000): loss=-12511679.921146508\n",
      "Losgistic Regression(7000/200000): loss=-12874634.530713344\n",
      "Losgistic Regression(8000/200000): loss=-13183781.813886058\n",
      "Losgistic Regression(9000/200000): loss=-13451475.683294827\n",
      "Losgistic Regression(10000/200000): loss=-13686264.729243582\n",
      "Losgistic Regression(11000/200000): loss=-13894353.282241577\n",
      "Losgistic Regression(12000/200000): loss=-14080409.165111527\n",
      "Losgistic Regression(13000/200000): loss=-14248028.215456463\n",
      "Losgistic Regression(14000/200000): loss=-14400034.902295291\n",
      "Losgistic Regression(15000/200000): loss=-14538685.367201654\n",
      "Losgistic Regression(16000/200000): loss=-14665810.422242574\n",
      "Losgistic Regression(17000/200000): loss=-14782916.78769217\n",
      "Losgistic Regression(18000/200000): loss=-14891254.333433822\n",
      "Losgistic Regression(19000/200000): loss=-14991873.216922691\n",
      "Losgistic Regression(20000/200000): loss=-15085661.818622943\n",
      "Losgistic Regression(21000/200000): loss=-15173374.494184973\n",
      "Losgistic Regression(22000/200000): loss=-15255657.076266274\n",
      "Losgistic Regression(23000/200000): loss=-15333064.691913279\n",
      "Losgistic Regression(24000/200000): loss=-15406077.250464492\n",
      "Losgistic Regression(25000/200000): loss=-15475111.46468787\n",
      "Losgistic Regression(26000/200000): loss=-15540530.164464993\n",
      "Losgistic Regression(27000/200000): loss=-15602652.144422477\n",
      "Losgistic Regression(28000/200000): loss=-15661756.59855738\n",
      "Losgistic Regression(29000/200000): loss=-15718090.681211257\n",
      "Losgistic Regression(30000/200000): loss=-15771873.42853335\n",
      "Losgistic Regression(31000/200000): loss=-15823299.724603375\n",
      "Losgistic Regression(32000/200000): loss=-15872544.122707773\n",
      "Losgistic Regression(33000/200000): loss=-15919762.216177683\n",
      "Losgistic Regression(34000/200000): loss=-15965094.412630633\n",
      "Losgistic Regression(35000/200000): loss=-16008667.626215851\n",
      "Losgistic Regression(36000/200000): loss=-16050596.762161136\n",
      "Losgistic Regression(37000/200000): loss=-16090986.244844718\n",
      "Losgistic Regression(38000/200000): loss=-16129931.068899903\n",
      "Losgistic Regression(39000/200000): loss=-16167518.469641412\n",
      "Losgistic Regression(40000/200000): loss=-16203826.843156993\n",
      "Losgistic Regression(41000/200000): loss=-16238932.811184576\n",
      "Losgistic Regression(42000/200000): loss=-16272902.172215093\n",
      "Losgistic Regression(43000/200000): loss=-16305797.218551645\n",
      "Losgistic Regression(44000/200000): loss=-16337675.812302783\n",
      "Losgistic Regression(45000/200000): loss=-16368591.519090205\n",
      "Losgistic Regression(46000/200000): loss=-16398594.604502432\n",
      "Losgistic Regression(47000/200000): loss=-16427731.442229554\n",
      "Losgistic Regression(48000/200000): loss=-16456045.163175305\n",
      "Losgistic Regression(49000/200000): loss=-16483576.566281792\n",
      "Losgistic Regression(50000/200000): loss=-16510363.634348368\n",
      "Losgistic Regression(51000/200000): loss=-16536442.011038754\n",
      "Losgistic Regression(52000/200000): loss=-16561845.182608223\n",
      "Losgistic Regression(53000/200000): loss=-16586604.646016343\n",
      "Losgistic Regression(54000/200000): loss=-16610750.062645478\n",
      "Losgistic Regression(55000/200000): loss=-16634309.397241011\n",
      "Losgistic Regression(56000/200000): loss=-16657309.043296032\n",
      "Losgistic Regression(57000/200000): loss=-16679774.237536144\n",
      "Losgistic Regression(58000/200000): loss=-16701727.997607468\n",
      "Losgistic Regression(59000/200000): loss=-16723192.803961642\n",
      "Losgistic Regression(60000/200000): loss=-16744190.123113343\n",
      "Losgistic Regression(61000/200000): loss=-16764739.620601697\n",
      "Losgistic Regression(62000/200000): loss=-16784860.445874166\n",
      "Losgistic Regression(63000/200000): loss=-16804570.634906154\n",
      "Losgistic Regression(64000/200000): loss=-16823887.316555038\n",
      "Losgistic Regression(65000/200000): loss=-16842826.765116077\n",
      "Losgistic Regression(66000/200000): loss=-16861404.44877828\n",
      "Losgistic Regression(67000/200000): loss=-16879635.062913246\n",
      "Losgistic Regression(68000/200000): loss=-16897532.499491137\n",
      "Losgistic Regression(69000/200000): loss=-16915109.627233636\n",
      "Losgistic Regression(70000/200000): loss=-16932375.62675917\n",
      "Losgistic Regression(71000/200000): loss=-16949321.877258256\n",
      "Losgistic Regression(72000/200000): loss=-16965873.97368649\n",
      "Losgistic Regression(73000/200000): loss=-16982136.042267747\n",
      "Losgistic Regression(74000/200000): loss=-17000237.152673308\n",
      "Losgistic Regression(75000/200000): loss=-17021583.65598896\n",
      "Losgistic Regression(76000/200000): loss=-17043970.591341488\n",
      "Losgistic Regression(77000/200000): loss=-17065941.128483687\n",
      "Losgistic Regression(78000/200000): loss=-17087095.663208783\n",
      "Losgistic Regression(79000/200000): loss=-17107384.282479513\n",
      "Losgistic Regression(80000/200000): loss=-17126848.869228657\n",
      "Losgistic Regression(81000/200000): loss=-17145552.507693302\n",
      "Losgistic Regression(82000/200000): loss=-17163559.630797774\n",
      "Losgistic Regression(83000/200000): loss=-17180930.142480195\n",
      "Losgistic Regression(84000/200000): loss=-17197717.83383353\n",
      "Losgistic Regression(85000/200000): loss=-17213970.2926287\n",
      "Losgistic Regression(86000/200000): loss=-17229729.377108026\n",
      "Losgistic Regression(87000/200000): loss=-17245031.749546144\n",
      "Losgistic Regression(88000/200000): loss=-17259909.73608141\n",
      "Losgistic Regression(89000/200000): loss=-17274391.761022363\n",
      "Losgistic Regression(90000/200000): loss=-17288502.964557145\n",
      "Losgistic Regression(91000/200000): loss=-17302265.49544166\n",
      "Losgistic Regression(92000/200000): loss=-17315699.62697328\n",
      "Losgistic Regression(93000/200000): loss=-17328822.959768597\n",
      "Losgistic Regression(94000/200000): loss=-17341651.570914235\n",
      "Losgistic Regression(95000/200000): loss=-17354199.99790428\n",
      "Losgistic Regression(96000/200000): loss=-17366481.44938831\n",
      "Losgistic Regression(97000/200000): loss=-17378507.98603965\n",
      "Losgistic Regression(98000/200000): loss=-17390290.671500623\n",
      "Losgistic Regression(99000/200000): loss=-17401839.699547835\n",
      "Losgistic Regression(100000/200000): loss=-17413164.50190193\n",
      "Losgistic Regression(101000/200000): loss=-17424273.83997349\n",
      "Losgistic Regression(102000/200000): loss=-17435175.883154728\n",
      "Losgistic Regression(103000/200000): loss=-17445878.27579568\n",
      "Losgistic Regression(104000/200000): loss=-17456388.19463346\n",
      "Losgistic Regression(105000/200000): loss=-17466712.39815364\n",
      "Losgistic Regression(106000/200000): loss=-17476857.269118346\n",
      "Losgistic Regression(107000/200000): loss=-17486828.851301502\n",
      "Losgistic Regression(108000/200000): loss=-17496632.881289374\n",
      "Losgistic Regression(109000/200000): loss=-17506274.816098414\n",
      "Losgistic Regression(110000/200000): loss=-17515759.857220694\n",
      "Losgistic Regression(111000/200000): loss=-17525092.97162176\n",
      "Losgistic Regression(112000/200000): loss=-17534278.910133857\n",
      "Losgistic Regression(113000/200000): loss=-17543322.22362305\n",
      "Losgistic Regression(114000/200000): loss=-17552227.27724787\n",
      "Losgistic Regression(115000/200000): loss=-17560998.263080467\n",
      "Losgistic Regression(116000/200000): loss=-17569639.21132063\n",
      "Losgistic Regression(117000/200000): loss=-17578154.000299282\n",
      "Losgistic Regression(118000/200000): loss=-17586546.36543788\n",
      "Losgistic Regression(119000/200000): loss=-17594819.907306068\n",
      "Losgistic Regression(120000/200000): loss=-17602978.098898266\n",
      "Losgistic Regression(121000/200000): loss=-17611024.292234577\n",
      "Losgistic Regression(122000/200000): loss=-17618961.724371083\n",
      "Losgistic Regression(123000/200000): loss=-17626793.5228987\n",
      "Losgistic Regression(124000/200000): loss=-17634522.71099164\n",
      "Losgistic Regression(125000/200000): loss=-17642152.212063603\n",
      "Losgistic Regression(126000/200000): loss=-17649684.85407687\n",
      "Losgistic Regression(127000/200000): loss=-17657123.37354593\n",
      "Losgistic Regression(128000/200000): loss=-17664470.41927034\n",
      "Losgistic Regression(129000/200000): loss=-17671728.555826336\n",
      "Losgistic Regression(130000/200000): loss=-17678900.26684325\n",
      "Losgistic Regression(131000/200000): loss=-17685987.958087157\n",
      "Losgistic Regression(132000/200000): loss=-17692993.96037038\n",
      "Losgistic Regression(133000/200000): loss=-17699920.532303743\n",
      "Losgistic Regression(134000/200000): loss=-17706769.86290609\n",
      "Losgistic Regression(135000/200000): loss=-17713544.074083492\n",
      "Losgistic Regression(136000/200000): loss=-17720245.222988892\n",
      "Losgistic Regression(137000/200000): loss=-17726875.304272033\n",
      "Losgistic Regression(138000/200000): loss=-17733436.25222772\n",
      "Losgistic Regression(139000/200000): loss=-17739929.94285017\n",
      "Losgistic Regression(140000/200000): loss=-17746358.1958\n",
      "Losgistic Regression(141000/200000): loss=-17752722.776288506\n",
      "Losgistic Regression(142000/200000): loss=-17759025.396886405\n",
      "Losgistic Regression(143000/200000): loss=-17765267.71926001\n",
      "Losgistic Regression(144000/200000): loss=-17771451.35583935\n",
      "Losgistic Regression(145000/200000): loss=-17777577.87142259\n",
      "Losgistic Regression(146000/200000): loss=-17783648.78471932\n",
      "Losgistic Regression(147000/200000): loss=-17789665.569836583\n",
      "Losgistic Regression(148000/200000): loss=-17795629.6577095\n",
      "Losgistic Regression(149000/200000): loss=-17801542.437480263\n",
      "Losgistic Regression(150000/200000): loss=-17807405.257826913\n",
      "Losgistic Regression(151000/200000): loss=-17813219.42824462\n",
      "Losgistic Regression(152000/200000): loss=-17818986.22028161\n",
      "Losgistic Regression(153000/200000): loss=-17824706.868730955\n",
      "Losgistic Regression(154000/200000): loss=-17830382.57278132\n",
      "Losgistic Regression(155000/200000): loss=-17836014.49712678\n",
      "Losgistic Regression(156000/200000): loss=-17841603.773038916\n",
      "Losgistic Regression(157000/200000): loss=-17847151.499401823\n",
      "Losgistic Regression(158000/200000): loss=-17852658.743710753\n",
      "Losgistic Regression(159000/200000): loss=-17858126.54303678\n",
      "Losgistic Regression(160000/200000): loss=-17863555.90496046\n",
      "Losgistic Regression(161000/200000): loss=-17868947.808470927\n",
      "Losgistic Regression(162000/200000): loss=-17874303.20483662\n",
      "Losgistic Regression(163000/200000): loss=-17879623.018446036\n",
      "Losgistic Regression(164000/200000): loss=-17884908.147619877\n",
      "Losgistic Regression(165000/200000): loss=-17890159.465396725\n",
      "Losgistic Regression(166000/200000): loss=-17895377.82029177\n",
      "Losgistic Regression(167000/200000): loss=-17900564.037030887\n",
      "Losgistic Regression(168000/200000): loss=-17905718.91725986\n",
      "Losgistic Regression(169000/200000): loss=-17910843.24023039\n",
      "Losgistic Regression(170000/200000): loss=-17915938.003268752\n",
      "Losgistic Regression(171000/200000): loss=-17921003.317106325\n",
      "Losgistic Regression(172000/200000): loss=-17926040.29152501\n",
      "Losgistic Regression(173000/200000): loss=-17931049.655804064\n",
      "Losgistic Regression(174000/200000): loss=-17936032.079917077\n",
      "Losgistic Regression(175000/200000): loss=-17940988.21335437\n",
      "Losgistic Regression(176000/200000): loss=-17945918.689526442\n",
      "Losgistic Regression(177000/200000): loss=-17950824.126050126\n",
      "Losgistic Regression(178000/200000): loss=-17955705.12467692\n",
      "Losgistic Regression(179000/200000): loss=-17960562.27135848\n",
      "Losgistic Regression(180000/200000): loss=-17965396.13645041\n",
      "Losgistic Regression(181000/200000): loss=-17970207.275002968\n",
      "Losgistic Regression(182000/200000): loss=-17974996.227101076\n",
      "Losgistic Regression(183000/200000): loss=-17979763.51822546\n",
      "Losgistic Regression(184000/200000): loss=-17984509.659622066\n",
      "Losgistic Regression(185000/200000): loss=-17989235.148673024\n",
      "Losgistic Regression(186000/200000): loss=-17993940.46926314\n",
      "Losgistic Regression(187000/200000): loss=-17998626.09214151\n",
      "Losgistic Regression(188000/200000): loss=-18003292.475274738\n",
      "Losgistic Regression(189000/200000): loss=-18007940.06419316\n",
      "Losgistic Regression(190000/200000): loss=-18012569.292327724\n",
      "Losgistic Regression(191000/200000): loss=-18017180.581338838\n",
      "Losgistic Regression(192000/200000): loss=-18021774.341436442\n",
      "Losgistic Regression(193000/200000): loss=-18026350.971691515\n",
      "Losgistic Regression(194000/200000): loss=-18030910.86033916\n",
      "Losgistic Regression(195000/200000): loss=-18035454.38507335\n",
      "Losgistic Regression(196000/200000): loss=-18039981.91333365\n",
      "Losgistic Regression(197000/200000): loss=-18044493.802584063\n",
      "Losgistic Regression(198000/200000): loss=-18048990.40058408\n",
      "Losgistic Regression(199000/200000): loss=-18053472.0456523\n",
      "0.8153 0.81116\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "                   max_iters = 200000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29134200029e-05\n",
      "Losgistic Regression(0/500000): loss=723067.1099047789\n",
      "Losgistic Regression(1000/500000): loss=-2180644.3801299524\n",
      "Losgistic Regression(2000/500000): loss=-2683202.9590092166\n",
      "Losgistic Regression(3000/500000): loss=-2967470.1614839495\n",
      "Losgistic Regression(4000/500000): loss=-3169324.4327838887\n",
      "Losgistic Regression(5000/500000): loss=-3326573.8785344507\n",
      "Losgistic Regression(6000/500000): loss=-3454658.275956527\n",
      "Losgistic Regression(7000/500000): loss=-3561873.782498788\n",
      "Losgistic Regression(8000/500000): loss=-3653327.650025917\n",
      "Losgistic Regression(9000/500000): loss=-3732460.9580051103\n",
      "Losgistic Regression(10000/500000): loss=-3801728.694845286\n",
      "Losgistic Regression(11000/500000): loss=-3862951.477844607\n",
      "Losgistic Regression(12000/500000): loss=-3917519.0733853476\n",
      "Losgistic Regression(13000/500000): loss=-3966516.5337259974\n",
      "Losgistic Regression(14000/500000): loss=-4010806.198426675\n",
      "Losgistic Regression(15000/500000): loss=-4051083.1900321273\n",
      "Losgistic Regression(16000/500000): loss=-4087914.3312534806\n",
      "Losgistic Regression(17000/500000): loss=-4121766.240937528\n",
      "Losgistic Regression(18000/500000): loss=-4153026.2153896713\n",
      "Losgistic Regression(19000/500000): loss=-4182018.034277366\n",
      "Losgistic Regression(20000/500000): loss=-4209014.127197313\n",
      "Losgistic Regression(21000/500000): loss=-4234245.106475307\n",
      "Losgistic Regression(22000/500000): loss=-4257907.288591299\n",
      "Losgistic Regression(23000/500000): loss=-4280168.700814427\n",
      "Losgistic Regression(24000/500000): loss=-4301173.88648389\n",
      "Losgistic Regression(25000/500000): loss=-4321047.806489049\n",
      "Losgistic Regression(26000/500000): loss=-4339899.003728052\n",
      "Losgistic Regression(27000/500000): loss=-4357822.1885899715\n",
      "Losgistic Regression(28000/500000): loss=-4374900.365378018\n",
      "Losgistic Regression(29000/500000): loss=-4391206.584557362\n",
      "Losgistic Regression(30000/500000): loss=-4406805.395609202\n",
      "Losgistic Regression(31000/500000): loss=-4421754.051138049\n",
      "Losgistic Regression(32000/500000): loss=-4436103.537358008\n",
      "Losgistic Regression(33000/500000): loss=-4449899.392406212\n",
      "Losgistic Regression(34000/500000): loss=-4463182.442867774\n",
      "Losgistic Regression(35000/500000): loss=-4475989.40369084\n",
      "Losgistic Regression(36000/500000): loss=-4488353.394622344\n",
      "Losgistic Regression(37000/500000): loss=-4500304.377877786\n",
      "Losgistic Regression(38000/500000): loss=-4511869.534107016\n",
      "Losgistic Regression(39000/500000): loss=-4523073.587723554\n",
      "Losgistic Regression(40000/500000): loss=-4533939.081495126\n",
      "Losgistic Regression(41000/500000): loss=-4544486.6220708685\n",
      "Losgistic Regression(42000/500000): loss=-4554735.088164329\n",
      "Losgistic Regression(43000/500000): loss=-4564701.813395436\n",
      "Losgistic Regression(44000/500000): loss=-4574402.74617158\n",
      "Losgistic Regression(45000/500000): loss=-4583852.5899133375\n",
      "Losgistic Regression(46000/500000): loss=-4593064.926488221\n",
      "Losgistic Regression(47000/500000): loss=-4602052.325234697\n",
      "Losgistic Regression(48000/500000): loss=-4610826.439166844\n",
      "Losgistic Regression(49000/500000): loss=-4619398.08982726\n",
      "Losgistic Regression(50000/500000): loss=-4627777.347872448\n",
      "Losgistic Regression(51000/500000): loss=-4635973.596788974\n",
      "Losgistic Regression(52000/500000): loss=-4643995.595899976\n",
      "Losgistic Regression(53000/500000): loss=-4651851.5396739\n",
      "Losgistic Regression(54000/500000): loss=-4659549.1022393005\n",
      "Losgistic Regression(55000/500000): loss=-4667095.48952058\n",
      "Losgistic Regression(56000/500000): loss=-4674497.481045966\n",
      "Losgistic Regression(57000/500000): loss=-4681761.470903401\n",
      "Losgistic Regression(58000/500000): loss=-4688893.50685117\n",
      "Losgistic Regression(59000/500000): loss=-4695899.328338545\n",
      "Losgistic Regression(60000/500000): loss=-4702784.404182171\n",
      "Losgistic Regression(61000/500000): loss=-4709553.97062216\n",
      "Losgistic Regression(62000/500000): loss=-4716213.070411673\n",
      "Losgistic Regression(63000/500000): loss=-4722766.5934309345\n",
      "Losgistic Regression(64000/500000): loss=-4729219.3189989105\n",
      "Losgistic Regression(65000/500000): loss=-4735575.959512292\n",
      "Losgistic Regression(66000/500000): loss=-4741841.204202947\n",
      "Losgistic Regression(67000/500000): loss=-4748019.760640682\n",
      "Losgistic Regression(68000/500000): loss=-4754116.390171781\n",
      "Losgistic Regression(69000/500000): loss=-4760135.931974847\n",
      "Losgistic Regression(70000/500000): loss=-4766083.309232959\n",
      "Losgistic Regression(71000/500000): loss=-4771963.51063777\n",
      "Losgistic Regression(72000/500000): loss=-4777781.5416936185\n",
      "Losgistic Regression(73000/500000): loss=-4783542.343509977\n",
      "Losgistic Regression(74000/500000): loss=-4789250.681869558\n",
      "Losgistic Regression(75000/500000): loss=-4794911.0154838925\n",
      "Losgistic Regression(76000/500000): loss=-4800527.357945695\n",
      "Losgistic Regression(77000/500000): loss=-4806103.151157398\n",
      "Losgistic Regression(78000/500000): loss=-4811641.16764254\n",
      "Losgistic Regression(79000/500000): loss=-4817143.45492656\n",
      "Losgistic Regression(80000/500000): loss=-4822611.328188204\n",
      "Losgistic Regression(81000/500000): loss=-4828045.409571202\n",
      "Losgistic Regression(82000/500000): loss=-4833445.705922283\n",
      "Losgistic Regression(83000/500000): loss=-4838811.712651135\n",
      "Losgistic Regression(84000/500000): loss=-4844142.530273123\n",
      "Losgistic Regression(85000/500000): loss=-4849436.981519532\n",
      "Losgistic Regression(86000/500000): loss=-4854693.719736413\n",
      "Losgistic Regression(87000/500000): loss=-4859911.3241189355\n",
      "Losgistic Regression(88000/500000): loss=-4865088.370809215\n",
      "Losgistic Regression(89000/500000): loss=-4870223.497789293\n",
      "Losgistic Regression(90000/500000): loss=-4875315.4445481105\n",
      "Losgistic Regression(91000/500000): loss=-4880363.079203542\n",
      "Losgistic Regression(92000/500000): loss=-4885365.414990594\n",
      "Losgistic Regression(93000/500000): loss=-4890321.617268409\n",
      "Losgistic Regression(94000/500000): loss=-4895231.003687872\n",
      "Losgistic Regression(95000/500000): loss=-4900093.039699834\n",
      "Losgistic Regression(96000/500000): loss=-4904907.330189868\n",
      "Losgistic Regression(97000/500000): loss=-4909673.609734801\n",
      "Losgistic Regression(98000/500000): loss=-4914391.731493369\n",
      "Losgistic Regression(99000/500000): loss=-4919061.655988845\n",
      "Losgistic Regression(100000/500000): loss=-4923683.439580283\n",
      "Losgistic Regression(101000/500000): loss=-4928257.22344216\n",
      "Losgistic Regression(102000/500000): loss=-4932783.223149362\n",
      "Losgistic Regression(103000/500000): loss=-4937261.718973616\n",
      "Losgistic Regression(104000/500000): loss=-4941693.046947147\n",
      "Losgistic Regression(105000/500000): loss=-4946077.590708932\n",
      "Losgistic Regression(106000/500000): loss=-4950415.774122467\n",
      "Losgistic Regression(107000/500000): loss=-4954708.054634659\n",
      "Losgistic Regression(108000/500000): loss=-4958954.917328729\n",
      "Losgistic Regression(109000/500000): loss=-4963156.869631513\n",
      "Losgistic Regression(110000/500000): loss=-4967314.436623803\n",
      "Losgistic Regression(111000/500000): loss=-4971428.15689951\n",
      "Losgistic Regression(112000/500000): loss=-4975498.5789318485\n",
      "Losgistic Regression(113000/500000): loss=-4979526.257898483\n",
      "Losgistic Regression(114000/500000): loss=-4983511.752923942\n",
      "Losgistic Regression(115000/500000): loss=-4987455.624699542\n",
      "Losgistic Regression(116000/500000): loss=-4991358.433445085\n",
      "Losgistic Regression(117000/500000): loss=-4995220.737180036\n",
      "Losgistic Regression(118000/500000): loss=-4999043.090273722\n",
      "Losgistic Regression(119000/500000): loss=-5002826.0421087425\n",
      "Losgistic Regression(120000/500000): loss=-5006570.135390457\n",
      "Losgistic Regression(121000/500000): loss=-5010275.909433288\n",
      "Losgistic Regression(122000/500000): loss=-5013943.892984651\n",
      "Losgistic Regression(123000/500000): loss=-5017574.608235628\n",
      "Losgistic Regression(124000/500000): loss=-5021168.569431578\n",
      "Losgistic Regression(125000/500000): loss=-5024726.282680361\n",
      "Losgistic Regression(126000/500000): loss=-5028248.245876235\n",
      "Losgistic Regression(127000/500000): loss=-5031734.948711895\n",
      "Losgistic Regression(128000/500000): loss=-5035186.872766759\n",
      "Losgistic Regression(129000/500000): loss=-5038604.491658745\n",
      "Losgistic Regression(130000/500000): loss=-5041988.272486318\n",
      "Losgistic Regression(131000/500000): loss=-5045338.671151696\n",
      "Losgistic Regression(132000/500000): loss=-5048656.139939734\n",
      "Losgistic Regression(133000/500000): loss=-5051941.123082718\n",
      "Losgistic Regression(134000/500000): loss=-5055194.058121244\n",
      "Losgistic Regression(135000/500000): loss=-5058415.3762912955\n",
      "Losgistic Regression(136000/500000): loss=-5061605.5028505055\n",
      "Losgistic Regression(137000/500000): loss=-5064764.857397711\n",
      "Losgistic Regression(138000/500000): loss=-5067893.854182896\n",
      "Losgistic Regression(139000/500000): loss=-5070992.902406025\n",
      "Losgistic Regression(140000/500000): loss=-5074062.406501734\n",
      "Losgistic Regression(141000/500000): loss=-5077102.766407182\n",
      "Losgistic Regression(142000/500000): loss=-5080114.377810989\n",
      "Losgistic Regression(143000/500000): loss=-5083097.6333496\n",
      "Losgistic Regression(144000/500000): loss=-5086052.918757148\n",
      "Losgistic Regression(145000/500000): loss=-5088980.619676971\n",
      "Losgistic Regression(146000/500000): loss=-5091881.1166287\n",
      "Losgistic Regression(147000/500000): loss=-5094754.787032206\n",
      "Losgistic Regression(148000/500000): loss=-5097602.005170539\n",
      "Losgistic Regression(149000/500000): loss=-5100423.1422403315\n",
      "Losgistic Regression(150000/500000): loss=-5103218.566432135\n",
      "Losgistic Regression(151000/500000): loss=-5105988.642999252\n",
      "Losgistic Regression(152000/500000): loss=-5108733.734310178\n",
      "Losgistic Regression(153000/500000): loss=-5111454.199885232\n",
      "Losgistic Regression(154000/500000): loss=-5114150.396418492\n",
      "Losgistic Regression(155000/500000): loss=-5116822.677786869\n",
      "Losgistic Regression(156000/500000): loss=-5119471.395047281\n",
      "Losgistic Regression(157000/500000): loss=-5122096.8964233305\n",
      "Losgistic Regression(158000/500000): loss=-5124699.527282863\n",
      "Losgistic Regression(159000/500000): loss=-5127279.630107385\n",
      "Losgistic Regression(160000/500000): loss=-5129837.54445444\n",
      "Losgistic Regression(161000/500000): loss=-5132373.606913818\n",
      "Losgistic Regression(162000/500000): loss=-5134888.151060818\n",
      "Losgistic Regression(163000/500000): loss=-5137381.507402091\n",
      "Losgistic Regression(164000/500000): loss=-5139854.003320612\n",
      "Losgistic Regression(165000/500000): loss=-5142305.963017718\n",
      "Losgistic Regression(166000/500000): loss=-5144737.709814864\n",
      "Losgistic Regression(167000/500000): loss=-5147149.557332932\n",
      "Losgistic Regression(168000/500000): loss=-5149541.8212630795\n",
      "Losgistic Regression(169000/500000): loss=-5151914.812616281\n",
      "Losgistic Regression(170000/500000): loss=-5154268.838844615\n",
      "Losgistic Regression(171000/500000): loss=-5156604.203818138\n",
      "Losgistic Regression(172000/500000): loss=-5158921.20778631\n",
      "Losgistic Regression(173000/500000): loss=-5161220.147330709\n",
      "Losgistic Regression(174000/500000): loss=-5163501.315313945\n",
      "Losgistic Regression(175000/500000): loss=-5165765.000828249\n",
      "Losgistic Regression(176000/500000): loss=-5168011.489145511\n",
      "Losgistic Regression(177000/500000): loss=-5170241.061669484\n",
      "Losgistic Regression(178000/500000): loss=-5172453.995890125\n",
      "Losgistic Regression(179000/500000): loss=-5174650.565342264\n",
      "Losgistic Regression(180000/500000): loss=-5176831.039565721\n",
      "Losgistic Regression(181000/500000): loss=-5178995.684068845\n",
      "Losgistic Regression(182000/500000): loss=-5181144.76029568\n",
      "Losgistic Regression(183000/500000): loss=-5183278.525596347\n",
      "Losgistic Regression(184000/500000): loss=-5185397.233200829\n",
      "Losgistic Regression(185000/500000): loss=-5187501.132196256\n",
      "Losgistic Regression(186000/500000): loss=-5189590.466890962\n",
      "Losgistic Regression(187000/500000): loss=-5191665.4794835625\n",
      "Losgistic Regression(188000/500000): loss=-5193726.406194506\n",
      "Losgistic Regression(189000/500000): loss=-5195773.478961814\n",
      "Losgistic Regression(190000/500000): loss=-5197806.92588245\n",
      "Losgistic Regression(191000/500000): loss=-5199826.970870211\n",
      "Losgistic Regression(192000/500000): loss=-5201833.833665327\n",
      "Losgistic Regression(193000/500000): loss=-5203827.729828254\n",
      "Losgistic Regression(194000/500000): loss=-5205808.868077606\n",
      "Losgistic Regression(195000/500000): loss=-5207777.46150124\n",
      "Losgistic Regression(196000/500000): loss=-5209733.710085019\n",
      "Losgistic Regression(197000/500000): loss=-5211677.812482581\n",
      "Losgistic Regression(198000/500000): loss=-5213609.963591999\n",
      "Losgistic Regression(199000/500000): loss=-5215530.354218584\n",
      "Losgistic Regression(200000/500000): loss=-5217439.171113903\n",
      "Losgistic Regression(201000/500000): loss=-5219336.59701911\n",
      "Losgistic Regression(202000/500000): loss=-5221222.81070808\n",
      "Losgistic Regression(203000/500000): loss=-5223097.987031868\n",
      "Losgistic Regression(204000/500000): loss=-5224962.2969662\n",
      "Losgistic Regression(205000/500000): loss=-5226815.907661279\n",
      "Losgistic Regression(206000/500000): loss=-5228658.982494424\n",
      "Losgistic Regression(207000/500000): loss=-5230491.681125513\n",
      "Losgistic Regression(208000/500000): loss=-5232314.1595546715\n",
      "Losgistic Regression(209000/500000): loss=-5234126.570182516\n",
      "Losgistic Regression(210000/500000): loss=-5235929.061872623\n",
      "Losgistic Regression(211000/500000): loss=-5237721.78001604\n",
      "Losgistic Regression(212000/500000): loss=-5239504.866597656\n",
      "Losgistic Regression(213000/500000): loss=-5241278.4602643605\n",
      "Losgistic Regression(214000/500000): loss=-5243042.696394535\n",
      "Losgistic Regression(215000/500000): loss=-5244797.70716943\n",
      "Losgistic Regression(216000/500000): loss=-5246543.621644594\n",
      "Losgistic Regression(217000/500000): loss=-5248280.565823079\n",
      "Losgistic Regression(218000/500000): loss=-5250008.662729035\n",
      "Losgistic Regression(219000/500000): loss=-5251728.032481968\n",
      "Losgistic Regression(220000/500000): loss=-5253438.792371041\n",
      "Losgistic Regression(221000/500000): loss=-5255141.0569297895\n",
      "Losgistic Regression(222000/500000): loss=-5256834.938010631\n",
      "Losgistic Regression(223000/500000): loss=-5258520.544859048\n",
      "Losgistic Regression(224000/500000): loss=-5260197.984187567\n",
      "Losgistic Regression(225000/500000): loss=-5261867.360248965\n",
      "Losgistic Regression(226000/500000): loss=-5263528.774908854\n",
      "Losgistic Regression(227000/500000): loss=-5265182.327717395\n",
      "Losgistic Regression(228000/500000): loss=-5266828.115979917\n",
      "Losgistic Regression(229000/500000): loss=-5268466.234826519\n",
      "Losgistic Regression(230000/500000): loss=-5270096.777280342\n",
      "Losgistic Regression(231000/500000): loss=-5271719.834324515\n",
      "Losgistic Regression(232000/500000): loss=-5273335.4949677205\n",
      "Losgistic Regression(233000/500000): loss=-5274943.846308157\n",
      "Losgistic Regression(234000/500000): loss=-5276544.973595924\n",
      "Losgistic Regression(235000/500000): loss=-5278138.960293917\n",
      "Losgistic Regression(236000/500000): loss=-5279725.88813676\n",
      "Losgistic Regression(237000/500000): loss=-5281305.837188262\n",
      "Losgistic Regression(238000/500000): loss=-5282878.885896915\n",
      "Losgistic Regression(239000/500000): loss=-5284445.111149808\n",
      "Losgistic Regression(240000/500000): loss=-5286004.588324539\n",
      "Losgistic Regression(241000/500000): loss=-5287557.391339483\n",
      "Losgistic Regression(242000/500000): loss=-5289103.592702221\n",
      "Losgistic Regression(243000/500000): loss=-5290643.26355614\n",
      "Losgistic Regression(244000/500000): loss=-5292176.4737253785\n",
      "Losgistic Regression(245000/500000): loss=-5293703.29175794\n",
      "Losgistic Regression(246000/500000): loss=-5295223.784967085\n",
      "Losgistic Regression(247000/500000): loss=-5296738.01947114\n",
      "Losgistic Regression(248000/500000): loss=-5298246.060231563\n",
      "Losgistic Regression(249000/500000): loss=-5299747.971089458\n",
      "Losgistic Regression(250000/500000): loss=-5301243.814800529\n",
      "Losgistic Regression(251000/500000): loss=-5302733.653068418\n",
      "Losgistic Regression(252000/500000): loss=-5304217.546576832\n",
      "Losgistic Regression(253000/500000): loss=-5305695.555019934\n",
      "Losgistic Regression(254000/500000): loss=-5307167.737131539\n",
      "Losgistic Regression(255000/500000): loss=-5308634.150713043\n",
      "Losgistic Regression(256000/500000): loss=-5310094.852659942\n",
      "Losgistic Regression(257000/500000): loss=-5311549.898987272\n",
      "Losgistic Regression(258000/500000): loss=-5312999.344853725\n",
      "Losgistic Regression(259000/500000): loss=-5314443.244584869\n",
      "Losgistic Regression(260000/500000): loss=-5315881.65169512\n",
      "Losgistic Regression(261000/500000): loss=-5317314.618908745\n",
      "Losgistic Regression(262000/500000): loss=-5318742.19818007\n",
      "Losgistic Regression(263000/500000): loss=-5320164.440712485\n",
      "Losgistic Regression(264000/500000): loss=-5321581.396976831\n",
      "Losgistic Regression(265000/500000): loss=-5322993.116728793\n",
      "Losgistic Regression(266000/500000): loss=-5324399.649025649\n",
      "Losgistic Regression(267000/500000): loss=-5325801.042242188\n",
      "Losgistic Regression(268000/500000): loss=-5327197.344085934\n",
      "Losgistic Regression(269000/500000): loss=-5328588.601611817\n",
      "Losgistic Regression(270000/500000): loss=-5329974.86123605\n",
      "Losgistic Regression(271000/500000): loss=-5331356.16874951\n",
      "Losgistic Regression(272000/500000): loss=-5332732.569330682\n",
      "Losgistic Regression(273000/500000): loss=-5334104.107557806\n",
      "Losgistic Regression(274000/500000): loss=-5335470.8274208065\n",
      "Losgistic Regression(275000/500000): loss=-5336832.772332604\n",
      "Losgistic Regression(276000/500000): loss=-5338189.985140007\n",
      "Losgistic Regression(277000/500000): loss=-5339542.508134259\n",
      "Losgistic Regression(278000/500000): loss=-5340890.383060843\n",
      "Losgistic Regression(279000/500000): loss=-5342233.651130175\n",
      "Losgistic Regression(280000/500000): loss=-5343572.353026214\n",
      "Losgistic Regression(281000/500000): loss=-5344906.5289156325\n",
      "Losgistic Regression(282000/500000): loss=-5346236.218456785\n",
      "Losgistic Regression(283000/500000): loss=-5347561.460808205\n",
      "Losgistic Regression(284000/500000): loss=-5348882.294636846\n",
      "Losgistic Regression(285000/500000): loss=-5350198.758126088\n",
      "Losgistic Regression(286000/500000): loss=-5351510.888983582\n",
      "Losgistic Regression(287000/500000): loss=-5352818.724448639\n",
      "Losgistic Regression(288000/500000): loss=-5354122.3013731865\n",
      "Losgistic Regression(289000/500000): loss=-5355421.657322712\n",
      "Losgistic Regression(290000/500000): loss=-5356716.825800815\n",
      "Losgistic Regression(291000/500000): loss=-5358007.843337541\n",
      "Losgistic Regression(292000/500000): loss=-5359294.744909305\n",
      "Losgistic Regression(293000/500000): loss=-5360577.565085147\n",
      "Losgistic Regression(294000/500000): loss=-5361856.338019515\n",
      "Losgistic Regression(295000/500000): loss=-5363131.097454447\n",
      "Losgistic Regression(296000/500000): loss=-5364401.876724125\n",
      "Losgistic Regression(297000/500000): loss=-5365668.708760426\n",
      "Losgistic Regression(298000/500000): loss=-5366931.62609854\n",
      "Losgistic Regression(299000/500000): loss=-5368190.66088267\n",
      "Losgistic Regression(300000/500000): loss=-5369445.844871439\n",
      "Losgistic Regression(301000/500000): loss=-5370697.209443482\n",
      "Losgistic Regression(302000/500000): loss=-5371944.785602701\n",
      "Losgistic Regression(303000/500000): loss=-5373188.603983613\n",
      "Losgistic Regression(304000/500000): loss=-5374428.694856441\n",
      "Losgistic Regression(305000/500000): loss=-5375665.088132271\n",
      "Losgistic Regression(306000/500000): loss=-5376897.813367985\n",
      "Losgistic Regression(307000/500000): loss=-5378126.899771165\n",
      "Losgistic Regression(308000/500000): loss=-5379352.376204952\n",
      "Losgistic Regression(309000/500000): loss=-5380574.2711927965\n",
      "Losgistic Regression(310000/500000): loss=-5381792.612923041\n",
      "Losgistic Regression(311000/500000): loss=-5383007.429253722\n",
      "Losgistic Regression(312000/500000): loss=-5384218.747716799\n",
      "Losgistic Regression(313000/500000): loss=-5385426.595522891\n",
      "Losgistic Regression(314000/500000): loss=-5386630.99956546\n",
      "Losgistic Regression(315000/500000): loss=-5387831.986425325\n",
      "Losgistic Regression(316000/500000): loss=-5389029.5823746985\n",
      "Losgistic Regression(317000/500000): loss=-5390223.813381582\n",
      "Losgistic Regression(318000/500000): loss=-5391414.7051137695\n",
      "Losgistic Regression(319000/500000): loss=-5392602.282943005\n",
      "Losgistic Regression(320000/500000): loss=-5393786.57194895\n",
      "Losgistic Regression(321000/500000): loss=-5394967.596923152\n",
      "Losgistic Regression(322000/500000): loss=-5396145.382372918\n",
      "Losgistic Regression(323000/500000): loss=-5397319.952525262\n",
      "Losgistic Regression(324000/500000): loss=-5398491.3313305415\n",
      "Losgistic Regression(325000/500000): loss=-5399659.54246634\n",
      "Losgistic Regression(326000/500000): loss=-5400824.60934104\n",
      "Losgistic Regression(327000/500000): loss=-5401986.5550975185\n",
      "Losgistic Regression(328000/500000): loss=-5403145.402616714\n",
      "Losgistic Regression(329000/500000): loss=-5404301.174521159\n",
      "Losgistic Regression(330000/500000): loss=-5405453.89317846\n",
      "Losgistic Regression(331000/500000): loss=-5406603.580704746\n",
      "Losgistic Regression(332000/500000): loss=-5407750.2589679705\n",
      "Losgistic Regression(333000/500000): loss=-5408893.949591351\n",
      "Losgistic Regression(334000/500000): loss=-5410034.673956671\n",
      "Losgistic Regression(335000/500000): loss=-5411172.453207416\n",
      "Losgistic Regression(336000/500000): loss=-5412307.308252039\n",
      "Losgistic Regression(337000/500000): loss=-5413439.259767134\n",
      "Losgistic Regression(338000/500000): loss=-5414568.328200502\n",
      "Losgistic Regression(339000/500000): loss=-5415694.533774191\n",
      "Losgistic Regression(340000/500000): loss=-5416817.896487651\n",
      "Losgistic Regression(341000/500000): loss=-5417938.4361205455\n",
      "Losgistic Regression(342000/500000): loss=-5419056.172235827\n",
      "Losgistic Regression(343000/500000): loss=-5420171.12418255\n",
      "Losgistic Regression(344000/500000): loss=-5421283.311098747\n",
      "Losgistic Regression(345000/500000): loss=-5422392.751914257\n",
      "Losgistic Regression(346000/500000): loss=-5423499.465353522\n",
      "Losgistic Regression(347000/500000): loss=-5424603.469938247\n",
      "Losgistic Regression(348000/500000): loss=-5425704.7839901885\n",
      "Losgistic Regression(349000/500000): loss=-5426803.425633773\n",
      "Losgistic Regression(350000/500000): loss=-5427899.410921073\n",
      "Losgistic Regression(351000/500000): loss=-5428992.761920775\n",
      "Losgistic Regression(352000/500000): loss=-5430083.493670885\n",
      "Losgistic Regression(353000/500000): loss=-5431171.62338346\n",
      "Losgistic Regression(354000/500000): loss=-5432257.168337032\n",
      "Losgistic Regression(355000/500000): loss=-5433340.145586931\n",
      "Losgistic Regression(356000/500000): loss=-5434420.571973787\n",
      "Losgistic Regression(357000/500000): loss=-5435498.464143886\n",
      "Losgistic Regression(358000/500000): loss=-5436573.83856144\n",
      "Losgistic Regression(359000/500000): loss=-5437646.711515604\n",
      "Losgistic Regression(360000/500000): loss=-5438717.099124337\n",
      "Losgistic Regression(361000/500000): loss=-5439785.0173375355\n",
      "Losgistic Regression(362000/500000): loss=-5440850.481939331\n",
      "Losgistic Regression(363000/500000): loss=-5441913.508550309\n",
      "Losgistic Regression(364000/500000): loss=-5442974.1126295915\n",
      "Losgistic Regression(365000/500000): loss=-5444032.309476934\n",
      "Losgistic Regression(366000/500000): loss=-5445088.1142346775\n",
      "Losgistic Regression(367000/500000): loss=-5446141.541889864\n",
      "Losgistic Regression(368000/500000): loss=-5447192.607276088\n",
      "Losgistic Regression(369000/500000): loss=-5448241.325075626\n",
      "Losgistic Regression(370000/500000): loss=-5449287.709821252\n",
      "Losgistic Regression(371000/500000): loss=-5450331.775898214\n",
      "Losgistic Regression(372000/500000): loss=-5451373.5375462035\n",
      "Losgistic Regression(373000/500000): loss=-5452413.008861102\n",
      "Losgistic Regression(374000/500000): loss=-5453450.203796907\n",
      "Losgistic Regression(375000/500000): loss=-5454485.13616758\n",
      "Losgistic Regression(376000/500000): loss=-5455517.819648775\n",
      "Losgistic Regression(377000/500000): loss=-5456548.267779699\n",
      "Losgistic Regression(378000/500000): loss=-5457576.493964802\n",
      "Losgistic Regression(379000/500000): loss=-5458602.511475585\n",
      "Losgistic Regression(380000/500000): loss=-5459626.333452141\n",
      "Losgistic Regression(381000/500000): loss=-5460647.972904976\n",
      "Losgistic Regression(382000/500000): loss=-5461667.442716689\n",
      "Losgistic Regression(383000/500000): loss=-5462684.7556433985\n",
      "Losgistic Regression(384000/500000): loss=-5463699.924316639\n",
      "Losgistic Regression(385000/500000): loss=-5464712.961244702\n",
      "Losgistic Regression(386000/500000): loss=-5465723.878814332\n",
      "Losgistic Regression(387000/500000): loss=-5466732.689292163\n",
      "Losgistic Regression(388000/500000): loss=-5467739.404826394\n",
      "Losgistic Regression(389000/500000): loss=-5468744.037447976\n",
      "Losgistic Regression(390000/500000): loss=-5469746.599072574\n",
      "Losgistic Regression(391000/500000): loss=-5470747.101501542\n",
      "Losgistic Regression(392000/500000): loss=-5471745.556423592\n",
      "Losgistic Regression(393000/500000): loss=-5472741.975416178\n",
      "Losgistic Regression(394000/500000): loss=-5473736.369946723\n",
      "Losgistic Regression(395000/500000): loss=-5474728.751374472\n",
      "Losgistic Regression(396000/500000): loss=-5475719.130951092\n",
      "Losgistic Regression(397000/500000): loss=-5476707.519823006\n",
      "Losgistic Regression(398000/500000): loss=-5477693.929031743\n",
      "Losgistic Regression(399000/500000): loss=-5478678.369515611\n",
      "Losgistic Regression(400000/500000): loss=-5479660.852111053\n",
      "Losgistic Regression(401000/500000): loss=-5480641.387553704\n",
      "Losgistic Regression(402000/500000): loss=-5481619.986479864\n",
      "Losgistic Regression(403000/500000): loss=-5482596.659427717\n",
      "Losgistic Regression(404000/500000): loss=-5483571.416838061\n",
      "Losgistic Regression(405000/500000): loss=-5484544.269056184\n",
      "Losgistic Regression(406000/500000): loss=-5485515.22633221\n",
      "Losgistic Regression(407000/500000): loss=-5486484.298823635\n",
      "Losgistic Regression(408000/500000): loss=-5487451.496595343\n",
      "Losgistic Regression(409000/500000): loss=-5488416.833735521\n",
      "Losgistic Regression(410000/500000): loss=-5489380.372571278\n",
      "Losgistic Regression(411000/500000): loss=-5490342.066392267\n",
      "Losgistic Regression(412000/500000): loss=-5491301.924854935\n",
      "Losgistic Regression(413000/500000): loss=-5492259.957566102\n",
      "Losgistic Regression(414000/500000): loss=-5493216.17405231\n",
      "Losgistic Regression(415000/500000): loss=-5494170.583757488\n",
      "Losgistic Regression(416000/500000): loss=-5495123.1960430965\n",
      "Losgistic Regression(417000/500000): loss=-5496074.020188733\n",
      "Losgistic Regression(418000/500000): loss=-5497023.065392886\n",
      "Losgistic Regression(419000/500000): loss=-5497970.340773681\n",
      "Losgistic Regression(420000/500000): loss=-5498915.855369744\n",
      "Losgistic Regression(421000/500000): loss=-5499859.618141066\n",
      "Losgistic Regression(422000/500000): loss=-5500801.637969815\n",
      "Losgistic Regression(423000/500000): loss=-5501741.923661232\n",
      "Losgistic Regression(424000/500000): loss=-5502680.483944566\n",
      "Losgistic Regression(425000/500000): loss=-5503617.327473818\n",
      "Losgistic Regression(426000/500000): loss=-5504552.462828737\n",
      "Losgistic Regression(427000/500000): loss=-5505485.898515566\n",
      "Losgistic Regression(428000/500000): loss=-5506417.642967977\n",
      "Losgistic Regression(429000/500000): loss=-5507347.704547847\n",
      "Losgistic Regression(430000/500000): loss=-5508276.091818226\n",
      "Losgistic Regression(431000/500000): loss=-5509202.810890352\n",
      "Losgistic Regression(432000/500000): loss=-5510127.872892584\n",
      "Losgistic Regression(433000/500000): loss=-5511051.284965328\n",
      "Losgistic Regression(434000/500000): loss=-5511973.055012846\n",
      "Losgistic Regression(435000/500000): loss=-5512893.190962466\n",
      "Losgistic Regression(436000/500000): loss=-5513811.700686116\n",
      "Losgistic Regression(437000/500000): loss=-5514728.591991506\n",
      "Losgistic Regression(438000/500000): loss=-5515643.872622233\n",
      "Losgistic Regression(439000/500000): loss=-5516557.550258868\n",
      "Losgistic Regression(440000/500000): loss=-5517469.632519799\n",
      "Losgistic Regression(441000/500000): loss=-5518380.135705911\n",
      "Losgistic Regression(442000/500000): loss=-5519289.052216836\n",
      "Losgistic Regression(443000/500000): loss=-5520196.394110484\n",
      "Losgistic Regression(444000/500000): loss=-5521102.170228001\n",
      "Losgistic Regression(445000/500000): loss=-5522006.388090479\n",
      "Losgistic Regression(446000/500000): loss=-5522909.054991781\n",
      "Losgistic Regression(447000/500000): loss=-5523810.178139538\n",
      "Losgistic Regression(448000/500000): loss=-5524709.764678759\n",
      "Losgistic Regression(449000/500000): loss=-5525607.821696754\n",
      "Losgistic Regression(450000/500000): loss=-5526504.356224666\n",
      "Losgistic Regression(451000/500000): loss=-5527399.375238159\n",
      "Losgistic Regression(452000/500000): loss=-5528292.885658083\n",
      "Losgistic Regression(453000/500000): loss=-5529184.894350968\n",
      "Losgistic Regression(454000/500000): loss=-5530075.408129566\n",
      "Losgistic Regression(455000/500000): loss=-5530964.433753395\n",
      "Losgistic Regression(456000/500000): loss=-5531851.977929311\n",
      "Losgistic Regression(457000/500000): loss=-5532738.047311949\n",
      "Losgistic Regression(458000/500000): loss=-5533622.648504494\n",
      "Losgistic Regression(459000/500000): loss=-5534505.788058957\n",
      "Losgistic Regression(460000/500000): loss=-5535387.472485771\n",
      "Losgistic Regression(461000/500000): loss=-5536267.70883831\n",
      "Losgistic Regression(462000/500000): loss=-5537146.502932459\n",
      "Losgistic Regression(463000/500000): loss=-5538023.861041679\n",
      "Losgistic Regression(464000/500000): loss=-5538899.789472172\n",
      "Losgistic Regression(465000/500000): loss=-5539774.294489398\n",
      "Losgistic Regression(466000/500000): loss=-5540647.38230979\n",
      "Losgistic Regression(467000/500000): loss=-5541519.059101129\n",
      "Losgistic Regression(468000/500000): loss=-5542389.330983888\n",
      "Losgistic Regression(469000/500000): loss=-5543258.2040321175\n",
      "Losgistic Regression(470000/500000): loss=-5544125.684274107\n",
      "Losgistic Regression(471000/500000): loss=-5544991.777693026\n",
      "Losgistic Regression(472000/500000): loss=-5545856.490227454\n",
      "Losgistic Regression(473000/500000): loss=-5546719.8277719\n",
      "Losgistic Regression(474000/500000): loss=-5547581.796177202\n",
      "Losgistic Regression(475000/500000): loss=-5548442.401251117\n",
      "Losgistic Regression(476000/500000): loss=-5549301.648758754\n",
      "Losgistic Regression(477000/500000): loss=-5550159.544423012\n",
      "Losgistic Regression(478000/500000): loss=-5551016.093925104\n",
      "Losgistic Regression(479000/500000): loss=-5551871.302904922\n",
      "Losgistic Regression(480000/500000): loss=-5552725.176961578\n",
      "Losgistic Regression(481000/500000): loss=-5553577.721653771\n",
      "Losgistic Regression(482000/500000): loss=-5554428.942500199\n",
      "Losgistic Regression(483000/500000): loss=-5555278.844980117\n",
      "Losgistic Regression(484000/500000): loss=-5556127.434533585\n",
      "Losgistic Regression(485000/500000): loss=-5556974.716561985\n",
      "Losgistic Regression(486000/500000): loss=-5557820.69642838\n",
      "Losgistic Regression(487000/500000): loss=-5558665.3794580065\n",
      "Losgistic Regression(488000/500000): loss=-5559508.770938483\n",
      "Losgistic Regression(489000/500000): loss=-5560350.876120429\n",
      "Losgistic Regression(490000/500000): loss=-5561191.7002176205\n",
      "Losgistic Regression(491000/500000): loss=-5562031.2484075595\n",
      "Losgistic Regression(492000/500000): loss=-5562869.525831739\n",
      "Losgistic Regression(493000/500000): loss=-5563706.537596019\n",
      "Losgistic Regression(494000/500000): loss=-5564542.28877102\n",
      "Losgistic Regression(495000/500000): loss=-5565376.784392488\n",
      "Losgistic Regression(496000/500000): loss=-5566210.029461578\n",
      "Losgistic Regression(497000/500000): loss=-5567042.028945317\n",
      "Losgistic Regression(498000/500000): loss=-5567872.787776835\n",
      "Losgistic Regression(499000/500000): loss=-5568702.310855737\n",
      "1e-06 0.8182 0.81204\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "               max_iters = 500000, lambda_= 1e-6, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(i, tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "for i in range(len(w)-1):\n",
    "    correlation_list.append(np.corrcoef(train_y7, train_tX7[:, i])[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.65129855e-02,  -3.51612919e-01,  -1.38712350e-02,\n",
       "         1.91876052e-01,   2.03718015e-01,   2.13023438e-01,\n",
       "        -1.83174366e-01,   1.31601540e-02,  -1.51333685e-02,\n",
       "         1.52662286e-01,  -1.96198138e-01,   2.72353631e-01,\n",
       "         1.75387841e-01,   2.34864606e-01,  -2.94158012e-04,\n",
       "        -4.79748618e-03,  -3.29888084e-02,   2.48140191e-03,\n",
       "         4.86870767e-03,   2.12883597e-02,   5.62790946e-03,\n",
       "         1.35090525e-01,   1.33094440e-01,   1.15036848e-01,\n",
       "        -2.56697681e-04,   1.39365659e-03,   2.32211127e-02,\n",
       "         1.37242072e-03,  -4.50596689e-03,   1.33843107e-01,\n",
       "        -4.22503370e-02,  -2.09894679e-01,  -6.31467775e-02,\n",
       "         1.06434966e-01,   2.33094856e-01,   1.75096532e-01,\n",
       "         1.81317012e-01,  -1.37815655e-02,   2.06173286e-03,\n",
       "         8.84978519e-02,  -1.00459724e-01,   4.62967153e-02,\n",
       "         2.13143137e-01,   1.26363568e-01,  -1.12019572e-01,\n",
       "        -1.44671666e-03,  -1.92796971e-02,  -1.46377790e-01,\n",
       "        -2.47121748e-03,   2.22883334e-02,   3.23624906e-04,\n",
       "         8.30113265e-02,   8.62348200e-02,   8.07217753e-02,\n",
       "         1.95750276e-01,   8.85521759e-02,   8.77029235e-03,\n",
       "         1.86384871e-01,   9.52482953e-02,   5.11589268e-02,\n",
       "        -4.90664985e-02,  -6.30038984e-02,  -3.63508109e-02,\n",
       "         1.63276303e-02,   2.27360557e-01,   1.18168301e-01,\n",
       "        -1.33579922e-01,  -3.81670787e-02,   2.93492027e-03,\n",
       "         3.91113254e-02,  -3.36458940e-02,   2.50089338e-01,\n",
       "         2.17023486e-01,   3.74269855e-02,   1.29428493e-03,\n",
       "        -2.43513983e-03,  -7.84938556e-03,   2.89080197e-03,\n",
       "         2.86141122e-03,   4.57132011e-03,   2.39176022e-03,\n",
       "         3.90604196e-02,   4.98079693e-02,   4.44785511e-02,\n",
       "         1.97998449e-03,   1.51299647e-03,  -4.17043612e-03,\n",
       "         3.91378005e-04,  -3.77494031e-03,   8.12723929e-03,\n",
       "        -3.44925065e-02,  -1.38334072e-02,  -1.32643427e-02,\n",
       "         4.37465398e-03,   2.10426051e-01,   7.36977091e-02,\n",
       "         1.12715032e-01,  -5.77959765e-02,   2.92614511e-03,\n",
       "         1.38871506e-02,  -1.16356262e-02,   2.98185903e-02,\n",
       "         2.14309616e-01,   7.91169191e-03,  -1.07215211e-01,\n",
       "        -9.44478783e-04,  -3.61987122e-03,  -1.36937512e-01,\n",
       "        -2.15002143e-03,   3.04912762e-03,  -3.29993414e-04,\n",
       "         1.36936533e-02,   2.46375152e-02,   1.97294368e-02,\n",
       "         1.56738089e-01,   6.60647683e-02,  -6.78762277e-03,\n",
       "         1.59555743e-01,   7.45483074e-02,  -4.56185279e-03])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array(correlation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14,  15,  17,  18,  20,  24,  25,  27,  28,  38,  45,  48,  50,\n",
       "        56,  68,  74,  75,  76,  77,  78,  79,  80,  84,  85,  86,  87,\n",
       "        88,  89,  93,  98, 103, 105, 106, 108, 109, 110, 116, 119])"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(c))[abs(c) < 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   6,   9,  11,  14,  15,  20,  24,  25,  27,  36,  41,  47,\n",
       "        48,  50,  55,  56,  68,  72,  74,  75,  77,  80,  82,  84,  85,\n",
       "        87, 101, 104, 108, 110, 115, 117])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(w))[abs(w) < 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_missing_featuers = [i for i in range(30) if i not in columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_tX =np.sum(tX == -999, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_missing_tX = tX[status_tX == 0, :]\n",
    "missing_tX = tX[status_tX != 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 301)"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(fill_na(tX, np.median), degree=10)\n",
    "tX7, mean_x7, std_x7 = standardize(tmp)\n",
    "y7 = transform_y(y)\n",
    "tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 301)"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX7, cv_tX7, train_y7, cv_y7 = split_data(tX7, y7, training_ratio)\n",
    "cv_tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.49473614186e-05+0j)\n",
      "Losgistic Regression(0/100000): loss=-44619.0435651786\n",
      "Losgistic Regression(1000/100000): loss=-154856.83025499398\n",
      "Losgistic Regression(2000/100000): loss=-142309.8661505646\n",
      "Losgistic Regression(3000/100000): loss=-145717.54056625118\n",
      "Losgistic Regression(4000/100000): loss=-152960.1357161893\n",
      "Losgistic Regression(5000/100000): loss=-159643.1937239881\n",
      "Losgistic Regression(6000/100000): loss=-165739.41687550774\n",
      "Losgistic Regression(7000/100000): loss=-171345.22204597376\n",
      "Losgistic Regression(8000/100000): loss=-176512.10742339757\n",
      "Losgistic Regression(9000/100000): loss=-181282.82337985822\n",
      "Losgistic Regression(10000/100000): loss=-185699.22739621307\n",
      "Losgistic Regression(11000/100000): loss=-189801.72317867383\n",
      "Losgistic Regression(12000/100000): loss=-193628.15191199194\n",
      "Losgistic Regression(13000/100000): loss=-197213.15342974968\n",
      "Losgistic Regression(14000/100000): loss=-200587.66586620157\n",
      "Losgistic Regression(15000/100000): loss=-203778.91895909168\n",
      "Losgistic Regression(16000/100000): loss=-206810.53935261403\n",
      "Losgistic Regression(17000/100000): loss=-209702.7837786889\n",
      "Losgistic Regression(18000/100000): loss=-212472.59171053203\n",
      "Losgistic Regression(19000/100000): loss=-215134.17057606805\n",
      "Losgistic Regression(20000/100000): loss=-217699.28307508634\n",
      "Losgistic Regression(21000/100000): loss=-220177.58871101835\n",
      "Losgistic Regression(22000/100000): loss=-222576.9659365387\n",
      "Losgistic Regression(23000/100000): loss=-224903.9611453428\n",
      "Losgistic Regression(24000/100000): loss=-227164.09602443394\n",
      "Losgistic Regression(25000/100000): loss=-229362.0411662442\n",
      "Losgistic Regression(26000/100000): loss=-231501.88559864002\n",
      "Losgistic Regression(27000/100000): loss=-233587.2209028704\n",
      "Losgistic Regression(28000/100000): loss=-235621.2218698786\n",
      "Losgistic Regression(29000/100000): loss=-237606.78000594256\n",
      "Losgistic Regression(30000/100000): loss=-239546.52944255868\n",
      "Losgistic Regression(31000/100000): loss=-241442.8217958809\n",
      "Losgistic Regression(32000/100000): loss=-243297.82236092674\n",
      "Losgistic Regression(33000/100000): loss=-245113.5152853879\n",
      "Losgistic Regression(34000/100000): loss=-246891.67364572638\n",
      "Losgistic Regression(35000/100000): loss=-248633.95649644628\n",
      "Losgistic Regression(36000/100000): loss=-250341.88141672823\n",
      "Losgistic Regression(37000/100000): loss=-252016.84054350836\n",
      "Losgistic Regression(38000/100000): loss=-253660.11825247837\n",
      "Losgistic Regression(39000/100000): loss=-255272.90634234785\n",
      "Losgistic Regression(40000/100000): loss=-256856.29370029861\n",
      "Losgistic Regression(41000/100000): loss=-258411.31548304757\n",
      "Losgistic Regression(42000/100000): loss=-259938.95295832277\n",
      "Losgistic Regression(43000/100000): loss=-261440.0928176191\n",
      "Losgistic Regression(44000/100000): loss=-262915.6105595232\n",
      "Losgistic Regression(45000/100000): loss=-264366.3212414645\n",
      "Losgistic Regression(46000/100000): loss=-265793.00051039056\n",
      "Losgistic Regression(47000/100000): loss=-267196.4036268392\n",
      "Losgistic Regression(48000/100000): loss=-268577.2756116806\n",
      "Losgistic Regression(49000/100000): loss=-269936.2498388919\n",
      "Losgistic Regression(50000/100000): loss=-271274.0023760741\n",
      "Losgistic Regression(51000/100000): loss=-272591.1756603653\n",
      "Losgistic Regression(52000/100000): loss=-273888.3871307957\n",
      "Losgistic Regression(53000/100000): loss=-275166.2319600392\n",
      "Losgistic Regression(54000/100000): loss=-276425.2845359411\n",
      "Losgistic Regression(55000/100000): loss=-277666.10064163036\n",
      "Losgistic Regression(56000/100000): loss=-278889.21423312277\n",
      "Losgistic Regression(57000/100000): loss=-280095.13163128117\n",
      "Losgistic Regression(58000/100000): loss=-281284.3803962245\n",
      "Losgistic Regression(59000/100000): loss=-282457.4308686122\n",
      "Losgistic Regression(60000/100000): loss=-283614.73871331936\n",
      "Losgistic Regression(61000/100000): loss=-284756.7512317457\n",
      "Losgistic Regression(62000/100000): loss=-285883.90928247094\n",
      "Losgistic Regression(63000/100000): loss=-286996.6375226465\n",
      "Losgistic Regression(64000/100000): loss=-288095.35342889436\n",
      "Losgistic Regression(65000/100000): loss=-289180.4231357479\n",
      "Losgistic Regression(66000/100000): loss=-290252.2157900949\n",
      "Losgistic Regression(67000/100000): loss=-291311.09705059323\n",
      "Losgistic Regression(68000/100000): loss=-292357.421185898\n",
      "Losgistic Regression(69000/100000): loss=-293391.5282476685\n",
      "Losgistic Regression(70000/100000): loss=-294413.74384509516\n",
      "Losgistic Regression(71000/100000): loss=-295424.3814324547\n",
      "Losgistic Regression(72000/100000): loss=-296423.7389413081\n",
      "Losgistic Regression(73000/100000): loss=-297412.15044429246\n",
      "Losgistic Regression(74000/100000): loss=-298389.82647354365\n",
      "Losgistic Regression(75000/100000): loss=-299357.0674870768\n",
      "Losgistic Regression(76000/100000): loss=-300314.1338254353\n",
      "Losgistic Regression(77000/100000): loss=-301261.27851260814\n",
      "Losgistic Regression(78000/100000): loss=-302198.7446867059\n",
      "Losgistic Regression(79000/100000): loss=-303126.7666095449\n",
      "Losgistic Regression(80000/100000): loss=-304045.5655279103\n",
      "Losgistic Regression(81000/100000): loss=-304955.3565329858\n",
      "Losgistic Regression(82000/100000): loss=-305856.3484860978\n",
      "Losgistic Regression(83000/100000): loss=-306748.75882233016\n",
      "Losgistic Regression(84000/100000): loss=-307632.77709619916\n",
      "Losgistic Regression(85000/100000): loss=-308508.6003767585\n",
      "Losgistic Regression(86000/100000): loss=-309376.48164036963\n",
      "Losgistic Regression(87000/100000): loss=-310236.55093015207\n",
      "Losgistic Regression(88000/100000): loss=-311088.9346418826\n",
      "Losgistic Regression(89000/100000): loss=-311933.8037545129\n",
      "Losgistic Regression(90000/100000): loss=-312771.31804656103\n",
      "Losgistic Regression(91000/100000): loss=-313601.6334482724\n",
      "Losgistic Regression(92000/100000): loss=-314424.8991449588\n",
      "Losgistic Regression(93000/100000): loss=-315241.25893033994\n",
      "Losgistic Regression(94000/100000): loss=-316050.8514492733\n",
      "Losgistic Regression(95000/100000): loss=-316853.808615489\n",
      "Losgistic Regression(96000/100000): loss=-317650.2647809219\n",
      "Losgistic Regression(97000/100000): loss=-318440.3446491304\n",
      "Losgistic Regression(98000/100000): loss=-319224.1700612231\n",
      "Losgistic Regression(99000/100000): loss=-320001.8591833757\n",
      "0.843 0.77476\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(1000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1e-5, \n",
    "                   max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 16, 19, 21, 22, 23, 26, 29]"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fill_na_test(method=np.mean):\n",
    "    filled = tX_test.copy()\n",
    "    for col in columns_with_missing_values:\n",
    "        tmp = filled[:, col]\n",
    "        tmp[tmp == -999] = method(tmp[tmp != -999])\n",
    "        filled[:, col] = tmp\n",
    "    return filled\n",
    "\n",
    "\n",
    "filled_test_tX = fill_na_test(np.median)\n",
    "\n",
    "columns_non_negative = []\n",
    "for i in range(n_total_features):\n",
    "    if len(filled_test_tX[filled_test_tX[:, i] < 0, i]) == 0:\n",
    "        columns_non_negative.append(i)\n",
    "\n",
    "columns_non_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 121)"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(filled_test_tX, degree=4)\n",
    "## We can take logs of each column Here *******************************\n",
    "test_tX, _, _ = standardize(tmp, mean_x7, std_x7)\n",
    "test_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y0=transform_y_back(prediction(test_tX, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176977,)"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0[y0==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238,)"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/result4.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = transform_y_back(y0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "023cb81475a945fe97d29be3cb3600e9": {
     "views": []
    },
    "0656d01812174c589117e89a7f476eeb": {
     "views": []
    },
    "06e08db57e4c4440a3ad1b3951854893": {
     "views": []
    },
    "09d8152e0e4842f281b6992d966f84ff": {
     "views": []
    },
    "0b17fa30a11f499bbbb6b8402e16c6c0": {
     "views": []
    },
    "0e1159612a454425bbac65a45bf46ae4": {
     "views": []
    },
    "0e2681dfe3854507b8e382172c8d24a9": {
     "views": []
    },
    "0e3a8a0386ac4eeabbe0d75c720fe8ca": {
     "views": []
    },
    "12028e93822d44fb98bff43ebb5ae85f": {
     "views": []
    },
    "12c1d615bd694bcea1fd9ab1124e2733": {
     "views": []
    },
    "17693f1c9ae243fba5f88f6bd0563d5c": {
     "views": []
    },
    "19e769fc042640babb5295d1974c62b8": {
     "views": []
    },
    "19ee98066002482e9fc35c5c1daf61ba": {
     "views": []
    },
    "1a632485f2d8478193df72588650d618": {
     "views": []
    },
    "1a86c3fa0e2d4cd0b5251249d12134bc": {
     "views": []
    },
    "1ee72dc401884de7b2faf7aa90592e2e": {
     "views": []
    },
    "1f1d9fd36c85413abd59a0c5d2f6afac": {
     "views": []
    },
    "218b8bbbb96c44759803d1d7bb93518b": {
     "views": []
    },
    "22ac1bf8dc1a42ec9f6bf037c2b351e0": {
     "views": []
    },
    "2691c30802414c95b1e293e0281cb59c": {
     "views": []
    },
    "2802b13066f649798d7bf253e4bb2b91": {
     "views": []
    },
    "2f8de774e24d4bfbbe7fc7a6c589165b": {
     "views": []
    },
    "33a6f920af1a414eb26969a29ca0e9f5": {
     "views": []
    },
    "33d5f66f8db64bb29aee64e9e6f4b00a": {
     "views": []
    },
    "358ab636ff8949958e22afe4df4c92f5": {
     "views": []
    },
    "360a79441c40473eac4ed7aa38bc332e": {
     "views": []
    },
    "3732e557ec5f4420ae4be9ba1b0c6950": {
     "views": []
    },
    "3891a8618a4e46e0a33e3c1fc96e927f": {
     "views": []
    },
    "3b1570fddde04f0caf52f4ff45d29edb": {
     "views": []
    },
    "3d44ad00ad504e909c2c5165179b38f3": {
     "views": []
    },
    "3ff3abd2ef184017be0348ce847f1a3b": {
     "views": []
    },
    "4104a7b34dea4cac9f83200068a87e4e": {
     "views": []
    },
    "411967c7a0484176b05d585607f58205": {
     "views": []
    },
    "42283aa269b24b079eba746badb6b732": {
     "views": []
    },
    "42a2771cc4de461b8e1f7930eb1a5b05": {
     "views": []
    },
    "43533ec6a6424e228d82dedae9d2e033": {
     "views": []
    },
    "44341b3861874225ac2c4a39d9e9b20a": {
     "views": []
    },
    "4514ac6a980d4c1cb0b2865b036f78b4": {
     "views": []
    },
    "462a940358654d2b84aa86fb2ddb7fa5": {
     "views": []
    },
    "467d39db9951416ca21632ab9321781c": {
     "views": []
    },
    "48db65e31eb143489065ea6f27cdafee": {
     "views": []
    },
    "4a47e66ec8f145b2a4ce40452beb05eb": {
     "views": []
    },
    "4e8c9deaad524bda8b1da06a0ec346ca": {
     "views": []
    },
    "514aac3eeee24c6d950fdbc506e2c983": {
     "views": []
    },
    "51600ba1f25f46f28712de210eecac48": {
     "views": []
    },
    "53b9a6a0056f4d3eb3d049a6b79902e6": {
     "views": []
    },
    "54f2df5f3b374a7b92906e2c6f9ec3b6": {
     "views": []
    },
    "55a0f08bc43a45199474b57bfab46e44": {
     "views": []
    },
    "5608d42ea9e94999ab42b8ad0c206f85": {
     "views": []
    },
    "58b9d8f8c94d4706bfa0fb67ecbbd1c8": {
     "views": []
    },
    "59566267562941e28c581d183adfb31b": {
     "views": []
    },
    "59b6af240339412ca33740de32807e72": {
     "views": []
    },
    "5a53fd2cbfcc483e84a2d66c3874dcd4": {
     "views": []
    },
    "5c9a253daa9641bdb41bb1e907995314": {
     "views": []
    },
    "5da1672732bf4560a2f195fbfa18fd2f": {
     "views": []
    },
    "5f61c6f48bc6407f808b26dfd3ff650e": {
     "views": []
    },
    "60025b85d7d7483cbd0389d15fa40b24": {
     "views": []
    },
    "6290e48913ff4b9785efd7c0fc29e29a": {
     "views": []
    },
    "64aa7fe3a5964b7289a6e95b667b7697": {
     "views": []
    },
    "68319ce5b3a74f10b5004a4a7e6e9bde": {
     "views": []
    },
    "69b3066124164f6e9fee59423542e6ce": {
     "views": []
    },
    "6f2439baf7df4a6ba620e3c431601ec5": {
     "views": []
    },
    "712bf0da5abb466daed7cb112ac4be9f": {
     "views": []
    },
    "737b5fb46e5647d58c0167b33056f32d": {
     "views": []
    },
    "74e7c1ac51d44ba99eb3190482e44c9f": {
     "views": []
    },
    "74f46c0299024dbd9fa900a91a41e71c": {
     "views": []
    },
    "7646e6b27e164eb4a922f3369c0d1359": {
     "views": []
    },
    "76716f91d3974e5188e3ce2031b8edc6": {
     "views": []
    },
    "7856af6c59bf4b02ad553e5de1667e6e": {
     "views": []
    },
    "84cc99bad75e48c79fd29af3b3b6cf23": {
     "views": []
    },
    "8554ffcd85804c4480cb9568679288e6": {
     "views": []
    },
    "86d8b3ca74394075a5040754a7d0f112": {
     "views": []
    },
    "8712c2d770b044e580b70be8ff47a70c": {
     "views": []
    },
    "89e5d5a410f148b3b9f038d07b985826": {
     "views": []
    },
    "8bc31e9d2e244f859b9944277b10685e": {
     "views": []
    },
    "8c8280c2e5e843ea912ce840c1f9a815": {
     "views": []
    },
    "8de53ca2548b46ffa4709418cdc5e47a": {
     "views": []
    },
    "8f571f7019b648d6982443a00678ca09": {
     "views": []
    },
    "8fd8e55f342e4cd2854b16ae86e8431b": {
     "views": []
    },
    "905c684e215f46b9851f87a98a19cbe8": {
     "views": []
    },
    "910792a394e442a2b3537417187c38f0": {
     "views": [
      {
       "cell_index": 137
      }
     ]
    },
    "92c8926847d9495698e180d643661298": {
     "views": []
    },
    "957d7a710a95407ba7de2bc66a90cb87": {
     "views": []
    },
    "95b6733c17ac4871b980ba45f329b81e": {
     "views": []
    },
    "971324d43edb41bc906914d34cac4e84": {
     "views": []
    },
    "97aa3c9800f04dff94ded3f655bfda84": {
     "views": []
    },
    "9873d25dd712418e87df9d8e7d1140d3": {
     "views": [
      {
       "cell_index": 137
      }
     ]
    },
    "9cdc11d2dbb545308ed63a064fc63ece": {
     "views": []
    },
    "9f070cfb9678487d907255f42842b0fb": {
     "views": []
    },
    "9fedd7eee0504abbad19cbda3aebb022": {
     "views": []
    },
    "9ffc35d8432c40ee9895c565f3c4abda": {
     "views": []
    },
    "a0a41254ad204744aa98206b576555d0": {
     "views": []
    },
    "a72d857b82d54d389cc8a371a4b03d39": {
     "views": []
    },
    "a74ee4818b264fbb95d165877824da55": {
     "views": []
    },
    "a9f0e192ef384cabb84c130bb60cfaea": {
     "views": []
    },
    "aa47af235b6e4335a9843f236ab8fef9": {
     "views": []
    },
    "aafafbbeae1c40cebeb269ac6833d061": {
     "views": []
    },
    "af20bec9e7a54c8ea66dc4787a3a09ea": {
     "views": []
    },
    "b1b641006f214f70a713be0f41831858": {
     "views": []
    },
    "b4c1f164f05b4f3bb3a437b454a9e499": {
     "views": []
    },
    "ba3ea59619e4415b9aa586a7af6b945a": {
     "views": []
    },
    "bbd68922ec024005a35d8f6c24577f3c": {
     "views": []
    },
    "bc3a26e14c1e4962a46cf7287e8991b6": {
     "views": []
    },
    "bf54fb7095864dcba467fe64fb76a32d": {
     "views": []
    },
    "bfa043dfddd741abb68307bd0bff96b2": {
     "views": []
    },
    "c0f3489352ed4aa3b0b6ab1abb9777d8": {
     "views": []
    },
    "c107053cbde24768a5c39237e591246f": {
     "views": []
    },
    "c2853ffd303b4614aeeac08b8f09141a": {
     "views": []
    },
    "c7dc57c0cd694805bac3f843578ae253": {
     "views": []
    },
    "ca45a1359d0a47bf93208f7941df740a": {
     "views": []
    },
    "cc59f3ba01b04abfa8cd8b84857ed625": {
     "views": []
    },
    "cc6eb0825e3b4e6991f36100c530879f": {
     "views": []
    },
    "cde680dc31024d48b4812e5fefc39266": {
     "views": []
    },
    "ce5f660489ae4eb68bec161d9d3afce6": {
     "views": []
    },
    "cf32fbdee557427e8a9d06c5e36c7d24": {
     "views": []
    },
    "d3c5139772f14218afa6d3cf87edefb9": {
     "views": []
    },
    "d5d8edac095d4c858774ee96236435c0": {
     "views": []
    },
    "d76b311a962d49bd9af777ff5f3f6132": {
     "views": []
    },
    "da3ca8bd2ea84ef39e5e1280c6bebc25": {
     "views": []
    },
    "de3928afe68841acb45bd27c88dcb8f5": {
     "views": []
    },
    "df04152de8aa4696b0ffd12180b89e9c": {
     "views": []
    },
    "df0bf6fbe6514083a3c8dc0d19a23b2d": {
     "views": []
    },
    "e0ea1699749a41e687b5683624033244": {
     "views": []
    },
    "e2645b8789c242e18c5db1e0997cf20d": {
     "views": []
    },
    "e3e681e437c3484cb7f49bd2f0655669": {
     "views": []
    },
    "e841538f7d8742289247f2aa9626b7da": {
     "views": []
    },
    "e972522844ca40e1b9378bcffb5ce313": {
     "views": []
    },
    "e9a93b10f7e84f6d9247f21115304861": {
     "views": []
    },
    "eb932c58dcb649c784bd9ed1aef50a74": {
     "views": []
    },
    "ebf85c945d094c6c94f279c3522cf276": {
     "views": []
    },
    "edd161163b4f4188964b434d9a316a75": {
     "views": []
    },
    "ee2859b43dbe405db03026763ff27e6b": {
     "views": []
    },
    "ee4ecf624eb24702a501c36235e105d7": {
     "views": []
    },
    "f140365f0cef4c11b5c5390d1a64edc6": {
     "views": []
    },
    "f16dce26e29447fb92414f4a56c3d000": {
     "views": []
    },
    "f4888ce60df2468389ab39f32b9585b4": {
     "views": []
    },
    "f6930490a4964d05abfd130fcad37587": {
     "views": []
    },
    "f6aa17847f7a45d4bd2d512633e13fe4": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "f6d9425b6be14c40b50c15979e35be99": {
     "views": []
    },
    "fca522c4fd7a47be86a1aab9fa26494c": {
     "views": []
    },
    "fd7dab3919ab4f0f838fe25f216d2d81": {
     "views": []
    },
    "fec936e794b94e74be9eed7a0c172ba7": {
     "views": []
    }
   },
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
