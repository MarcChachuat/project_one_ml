{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "# from data_preprocessing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading of the data : done\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"loading of the data : done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_y = '../data/y'\n",
    "save_tX = '../data/tX'\n",
    "np.save(save_y, y)\n",
    "np.save(save_tX, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functions import logistic_regression_GD\n",
    "\n",
    "def prediction(tx, w):\n",
    "    y = tx @ w\n",
    "    y[y > 0] = 1\n",
    "    y[y <= 0] = 0\n",
    "    return y\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)\n",
    "\n",
    "log_features = [1, 3, 4, 5, 8, 9, 13, 16, 19, 23, 26, 29]\n",
    "normal_features = [0, 2, 6, 7, 10, 14, 17, 21, 24, 27]\n",
    "uniform_feature = [15, 18, 20, 25, 28]\n",
    "# two side are large\n",
    "bernoulli_feature = [11, 12]\n",
    "categorical_features = [22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"loading of the data : done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_total_training_samples = tX.shape[0]\n",
    "n_total_features         = tX.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_missing_values = []\n",
    "for i in range(n_total_features):\n",
    "    if -999 in tX[:, i]:\n",
    "        columns_with_missing_values.append(i)\n",
    "\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_features = [1, 3, 4, 5, 8, 9, 13, 16, 19, 23, 26, 29]\n",
    "normal_features = [0, 2, 6, 7, 10, 14, 17, 21, 24, 27]\n",
    "uniform_feature = [15, 18, 20, 25, 28]\n",
    "# two side are large (perhaps beta distribution)\n",
    "bernoulli_feature = [11, 12]\n",
    "categorical_features = [22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio_of_training):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    p = np.random.permutation(np.arange(y.shape[0]))\n",
    "    n = int(y.shape[0] * ratio_of_training)\n",
    "    return  x[p][:n], x[p][n:], y[p][:n], y[p][n:]\n",
    "\n",
    "def prediction(tx, w):\n",
    "    y = tx @ w\n",
    "    y[y > 0] = 1\n",
    "    y[y <= 0] = 0\n",
    "    return y\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_y(y):\n",
    "    tmp = y.copy()\n",
    "    tmp[tmp == -1]=0\n",
    "    return tmp\n",
    "\n",
    "def transform_y_back(y):\n",
    "    tmp = y.copy()\n",
    "    tmp[tmp==0]=-1  \n",
    "    return tmp\n",
    "\n",
    "def prediction_and_accuracy(tr_tx, tr_y, te_tx, te_y, w):\n",
    "    return accuracy(tr_y, prediction(tr_tx, w)), accuracy(te_y, prediction(te_tx, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill missing values with their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.52758308455e-07\n",
      "Losgistic Regression(0/100000): loss=262923.85734496365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1138-ffe48f9c358e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m w = logistic_AGDR(y, logistic_tX, gamma=1/L, \n\u001b[0;32m---> 11\u001b[0;31m                    max_iters = iterations, lambda_=0.00001, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprediction_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_AGDR\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mw_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degree = 3\n",
    "iterations = 100000\n",
    "\n",
    "tmp = build_polynomial_without_mixed_term(filled_tX, degree=degree)\n",
    "logistic_tX, mean_x, std_x = standardize(tmp)\n",
    "\n",
    "L = abs(np.linalg.eigvals(logistic_tX.T @ logistic_tX)).max()\n",
    "print(1/L)\n",
    "\n",
    "w = logistic_AGDR(y, logistic_tX, gamma=1/L, \n",
    "                   max_iters = iterations, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "def prediction_accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)\n",
    "\n",
    "logistic_pred_y = predict_labels(w, logistic_tX)\n",
    "err_log = prediction_accuracy(ytrain, logistic_pred_y)\n",
    "print (\"logistic regression of degree\", 4)\n",
    "print (err_log)\n",
    "\n",
    "tmp = build_polynomial_without_mixed_term(xvalid1bis, degree=degree)\n",
    "validation_tx = standardize(tmp)\n",
    "\n",
    "print (\"validation accuracy: \", prediction_accuracy(yvalid, predict_labels(w, validation_tx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEKCAYAAADXdbjqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10875a5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "from IPython.display import display\n",
    "\n",
    "def fill_na(tx=tX, method=np.mean):\n",
    "    filled = tx.copy()\n",
    "    for col in columns_with_missing_values:\n",
    "        tmp = filled[:, col]\n",
    "        tmp[tmp == -999] = method(tmp[tmp != -999])\n",
    "        filled[:, col] = tmp\n",
    "    return filled\n",
    "\n",
    "def plot_hist(tx, i, transformation=None):\n",
    "    plt.figure()\n",
    "    if transformation is None:\n",
    "        plt.hist(tx[:, i], bins=100)\n",
    "    else:\n",
    "        plt.hist(transformation(tx[:, i]), bins=100)\n",
    "    plt.title(\"feature %i\" % i)\n",
    "    plt.show()\n",
    "    \n",
    "# before processing \n",
    "interactive(lambda x:plot_hist(tX, x), x=IntSlider(min=0, max=29))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Good Features                   : [7, 10, 14, 15, 17, 18, 20\n",
    "    Suffering from outlier          : [3, 8, 19, 23, 26, 29]\n",
    "    Suffering from outlier (skewed) : [1, 2, 5, 9, 13, 16, 21]\n",
    "    missing values                  : [0, 4, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "    categorical                     : [11, 12, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Good_Features                   = [7, 10, 14, 15, 17, 18, 20]\n",
    "Suffering_from_outlier          = [3, 8, 19, 23, 26, 29]\n",
    "Suffering_from_outlier_skewed   = [1, 2, 5, 9, 13, 16, 21]\n",
    "missing_values                  = [0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "categorical                     = [11, 12, 22]\n",
    "non_categorical                 = [x for x in range(30) if x not in categorical]\n",
    "Features_with_outlier           = np.union1d(Suffering_from_outlier_skewed, Suffering_from_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completeness_status_tX = np.sum(tX == -999, axis=1)\n",
    "non_missing_tX         = tX[completeness_status_tX==0, :]\n",
    "missing_tX             = tX[completeness_status_tX!=0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logs_of_features(tx, feature_lists):\n",
    "    return np.log(tx[:, feature_lists] + 1e-8)\n",
    "\n",
    "def decompose_categorical_features(tx):\n",
    "    # For feature tx{:, 11}\n",
    "    tmp11 = 1 * (tx[:, 11] > 0)    \n",
    "    tmp12 = 1 * (tx[:, 12] > 0.5)\n",
    "    \n",
    "    tmp22_0 = tx[:, 22].copy()\n",
    "    tmp22_0 = 1 * (tmp22_0 == 0)\n",
    "    \n",
    "    tmp22_1 = tx[:, 22].copy()\n",
    "    tmp22_1 = 1 * (tmp22_1 == 1)\n",
    "    \n",
    "    tmp22_2 = tx[:, 22].copy()\n",
    "    tmp22_2 = 1 * (tmp22_2 == 2)\n",
    "    \n",
    "    tmp22_3 = tx[:, 22].copy()\n",
    "    tmp22_3 = 1 * (tmp22_3 == 3)\n",
    "\n",
    "    m = np.c_[tmp11, tmp12, tmp22_0, tmp22_1, tmp22_2, tmp22_3]\n",
    "    if np.linalg.matrix_rank(m) < 6:\n",
    "        print (np.linalg.matrix_rank(m))\n",
    "        print (m)\n",
    "        print (\"Feature decomposition results in singularity\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_tX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposed_tX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15116186,  0.21826215, -5.04049425,  3.73928037,  3.21179471,\n",
       "        2.19237402])"
      ]
     },
     "execution_count": 1119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[108:114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  7 31 37 47 54 55 56 61 63 73 84 91 98]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 19.08872793,  14.30756479,  16.4090869 , -61.42722856,\n",
       "       -37.28161281,  12.12797737, -38.44387495, -31.61727634,\n",
       "        11.0479459 ,  10.02961668,  94.00667723,  11.43962656,\n",
       "        71.9602947 , -18.22266129,  35.28852604, -24.39112541,\n",
       "        36.87640824, -16.33589521, -28.63099965,  11.11530519,\n",
       "       -11.30043654, -59.94103878, -10.06137713, -73.97941383,\n",
       "        24.33272386, -11.76771908, -21.95944171,  13.00005561,  10.54535122])"
      ]
     },
     "execution_count": 1113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th=10\n",
    "print(np.arange(len(w))[abs(w > th)])\n",
    "w[abs(w)> th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 128)"
      ]
     },
     "execution_count": 1103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# engineered_tX = \n",
    "no_missing_featuers = [i for i in range(30) if i not in missing_values]\n",
    "\n",
    "filled_tX           = fill_na(tX, np.median)\n",
    "log_tX              = logs_of_features(filled_tX, Features_with_outlier)\n",
    "decomposed_tX       = decompose_categorical_features(filled_tX)\n",
    "poly_tX             = build_polynomial_without_mixed_term(filled_tX[:, non_categorical], degree=4)\n",
    "\n",
    "tX8, mean_x, std_x = standardize(np.c_[poly_tX, decomposed_tX, log_tX])\n",
    "\n",
    "training_ratio = 0.9\n",
    "y8 = transform_y(y)\n",
    "train_tX8, cv_tX8, train_y8, cv_y8 = split_data(tX8, y8, training_ratio)\n",
    "cv_tX8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000175127525558\n",
      "Losgistic Regression(0/100000): loss=1048.668438268728\n",
      "Losgistic Regression(100/100000): loss=137.74977623127228\n",
      "Losgistic Regression(200/100000): loss=107.67354656163215\n",
      "Losgistic Regression(300/100000): loss=100.1708774298158\n",
      "Losgistic Regression(400/100000): loss=96.11517199548318\n",
      "Losgistic Regression(500/100000): loss=92.55801068925695\n",
      "Losgistic Regression(600/100000): loss=85.80416093288075\n",
      "Losgistic Regression(700/100000): loss=79.90494207214722\n",
      "Losgistic Regression(800/100000): loss=74.61366508674054\n",
      "Losgistic Regression(900/100000): loss=69.03141771862849\n",
      "Losgistic Regression(1000/100000): loss=62.9531181591113\n",
      "Losgistic Regression(1100/100000): loss=57.958126857889404\n",
      "Losgistic Regression(1200/100000): loss=53.887712060175396\n",
      "Losgistic Regression(1300/100000): loss=50.21579936837567\n",
      "Losgistic Regression(1400/100000): loss=46.9277166190745\n",
      "Losgistic Regression(1500/100000): loss=43.915171597247095\n",
      "Losgistic Regression(1600/100000): loss=41.04227775427714\n",
      "Losgistic Regression(1700/100000): loss=38.21614858484615\n",
      "Losgistic Regression(1800/100000): loss=35.55198979137834\n",
      "Losgistic Regression(1900/100000): loss=33.10561762033622\n",
      "Losgistic Regression(2000/100000): loss=30.852833824293477\n",
      "Losgistic Regression(2100/100000): loss=28.81935216988032\n",
      "Losgistic Regression(2200/100000): loss=26.95934856707307\n",
      "Losgistic Regression(2300/100000): loss=25.33088228824996\n",
      "Losgistic Regression(2400/100000): loss=23.92655702061115\n",
      "Losgistic Regression(2500/100000): loss=22.685520548285425\n",
      "Losgistic Regression(2600/100000): loss=21.598220249612968\n",
      "Losgistic Regression(2700/100000): loss=20.647923121845004\n",
      "Losgistic Regression(2800/100000): loss=19.826691741064423\n",
      "Losgistic Regression(2900/100000): loss=19.06959254054511\n",
      "Losgistic Regression(3000/100000): loss=18.301177383530913\n",
      "Losgistic Regression(3100/100000): loss=17.475413829230984\n",
      "Losgistic Regression(3200/100000): loss=16.551415618952273\n",
      "Losgistic Regression(3300/100000): loss=15.522290966929365\n",
      "Losgistic Regression(3400/100000): loss=14.381496084675392\n",
      "Losgistic Regression(3500/100000): loss=13.133829220571636\n",
      "Losgistic Regression(3600/100000): loss=11.804341448104715\n",
      "Losgistic Regression(3700/100000): loss=10.412765448577954\n",
      "Losgistic Regression(3800/100000): loss=8.987342961407517\n",
      "Losgistic Regression(3900/100000): loss=7.526849325163201\n",
      "Losgistic Regression(4000/100000): loss=6.006172053752169\n",
      "Losgistic Regression(4100/100000): loss=4.4180714813263675\n",
      "Losgistic Regression(4200/100000): loss=2.758816055573928\n",
      "Losgistic Regression(4300/100000): loss=1.0288371123609905\n",
      "Losgistic Regression(4400/100000): loss=-0.7674991852592393\n",
      "Losgistic Regression(4500/100000): loss=-2.6348521384330033\n",
      "Losgistic Regression(4600/100000): loss=-4.568808206037468\n",
      "Losgistic Regression(4700/100000): loss=-6.557225509601554\n",
      "Losgistic Regression(4800/100000): loss=-8.592796903920386\n",
      "Losgistic Regression(4900/100000): loss=-10.665434004173262\n",
      "Losgistic Regression(5000/100000): loss=-12.763937728612683\n",
      "Losgistic Regression(5100/100000): loss=-14.884883780084225\n",
      "Losgistic Regression(5200/100000): loss=-17.030737313056445\n",
      "Losgistic Regression(5300/100000): loss=-19.20531808045158\n",
      "Losgistic Regression(5400/100000): loss=-21.407593927512714\n",
      "Losgistic Regression(5500/100000): loss=-23.630049247003566\n",
      "Losgistic Regression(5600/100000): loss=-25.863607785618115\n",
      "Losgistic Regression(5700/100000): loss=-28.09744178146933\n",
      "Losgistic Regression(5800/100000): loss=-30.320043503642836\n",
      "Losgistic Regression(5900/100000): loss=-32.526675597550145\n",
      "Losgistic Regression(6000/100000): loss=-34.719831844486144\n",
      "Losgistic Regression(6100/100000): loss=-36.904202047019325\n",
      "Losgistic Regression(6200/100000): loss=-39.0836778716773\n",
      "Losgistic Regression(6300/100000): loss=-41.261643197018074\n",
      "Losgistic Regression(6400/100000): loss=-43.44105361329896\n",
      "Losgistic Regression(6500/100000): loss=-45.62556812534967\n",
      "Losgistic Regression(6600/100000): loss=-47.8179744354213\n",
      "Losgistic Regression(6700/100000): loss=-50.01909047744706\n",
      "Losgistic Regression(6800/100000): loss=-52.491254826529676\n",
      "Losgistic Regression(6900/100000): loss=-55.242120752759234\n",
      "Losgistic Regression(7000/100000): loss=-57.999478646925304\n",
      "Losgistic Regression(7100/100000): loss=-60.76288945389442\n",
      "Losgistic Regression(7200/100000): loss=-63.53261065536898\n",
      "Losgistic Regression(7300/100000): loss=-66.31032555059245\n",
      "Losgistic Regression(7400/100000): loss=-69.10057408024996\n",
      "Losgistic Regression(7500/100000): loss=-71.95510280964875\n",
      "Losgistic Regression(7600/100000): loss=-74.98256351140307\n",
      "Losgistic Regression(7700/100000): loss=-78.02286631055905\n",
      "Losgistic Regression(7800/100000): loss=-81.07649533713844\n",
      "Losgistic Regression(7900/100000): loss=-84.14585060022374\n",
      "Losgistic Regression(8000/100000): loss=-87.2323046628291\n",
      "Losgistic Regression(8100/100000): loss=-90.33587398355597\n",
      "Losgistic Regression(8200/100000): loss=-93.45435991516729\n",
      "Losgistic Regression(8300/100000): loss=-96.58324560313686\n",
      "Losgistic Regression(8400/100000): loss=-99.71621869685316\n",
      "Losgistic Regression(8500/100000): loss=-102.84582957080363\n",
      "Losgistic Regression(8600/100000): loss=-105.96397883388205\n",
      "Losgistic Regression(8700/100000): loss=-109.06439737880319\n",
      "Losgistic Regression(8800/100000): loss=-112.14356114584551\n",
      "Losgistic Regression(8900/100000): loss=-115.19980879626664\n",
      "Losgistic Regression(9000/100000): loss=-118.23296732030589\n",
      "Losgistic Regression(9100/100000): loss=-121.2448553112153\n",
      "Losgistic Regression(9200/100000): loss=-124.23717763859565\n",
      "Losgistic Regression(9300/100000): loss=-127.21171972276727\n",
      "Losgistic Regression(9400/100000): loss=-130.16937210059112\n",
      "Losgistic Regression(9500/100000): loss=-133.1524915723778\n",
      "Losgistic Regression(9600/100000): loss=-136.23344604962304\n",
      "Losgistic Regression(9700/100000): loss=-139.29058719026094\n",
      "Losgistic Regression(9800/100000): loss=-142.31960906314615\n",
      "Losgistic Regression(9900/100000): loss=-145.3170267884727\n",
      "Losgistic Regression(10000/100000): loss=-148.28061506218583\n",
      "Losgistic Regression(10100/100000): loss=-151.20927068417623\n",
      "Losgistic Regression(10200/100000): loss=-154.16016989118754\n",
      "Losgistic Regression(10300/100000): loss=-157.15558636803905\n",
      "Losgistic Regression(10400/100000): loss=-160.11720983438232\n",
      "Losgistic Regression(10500/100000): loss=-163.04616073127718\n",
      "Losgistic Regression(10600/100000): loss=-165.9433177870134\n",
      "Losgistic Regression(10700/100000): loss=-168.80937514179337\n",
      "Losgistic Regression(10800/100000): loss=-171.64444440287699\n",
      "Losgistic Regression(10900/100000): loss=-174.44915265222917\n",
      "Losgistic Regression(11000/100000): loss=-177.22440142005485\n",
      "Losgistic Regression(11100/100000): loss=-179.97084396643555\n",
      "Losgistic Regression(11200/100000): loss=-182.68962638832753\n",
      "Losgistic Regression(11300/100000): loss=-185.38186986139726\n",
      "Losgistic Regression(11400/100000): loss=-188.04840954907476\n",
      "Losgistic Regression(11500/100000): loss=-190.68958743558255\n",
      "Losgistic Regression(11600/100000): loss=-193.30507327496852\n",
      "Losgistic Regression(11700/100000): loss=-195.89456945213166\n",
      "Losgistic Regression(11800/100000): loss=-198.45867057134484\n",
      "Losgistic Regression(11900/100000): loss=-200.99861063214692\n",
      "Losgistic Regression(12000/100000): loss=-203.5152392910597\n",
      "Losgistic Regression(12100/100000): loss=-206.0100819027642\n",
      "Losgistic Regression(12200/100000): loss=-208.48377774665428\n",
      "Losgistic Regression(12300/100000): loss=-210.9363700058455\n",
      "Losgistic Regression(12400/100000): loss=-213.3688387632592\n",
      "Losgistic Regression(12500/100000): loss=-215.7801819968183\n",
      "Losgistic Regression(12600/100000): loss=-218.17059187707014\n",
      "Losgistic Regression(12700/100000): loss=-220.54102006627772\n",
      "Losgistic Regression(12800/100000): loss=-222.89133936522123\n",
      "Losgistic Regression(12900/100000): loss=-225.22215811742862\n",
      "Losgistic Regression(13000/100000): loss=-227.53324066743394\n",
      "Losgistic Regression(13100/100000): loss=-229.82411996316335\n",
      "Losgistic Regression(13200/100000): loss=-232.09411498829633\n",
      "Losgistic Regression(13300/100000): loss=-234.3435414415833\n",
      "Losgistic Regression(13400/100000): loss=-236.57357015696155\n",
      "Losgistic Regression(13500/100000): loss=-238.78380410682638\n",
      "Losgistic Regression(13600/100000): loss=-240.97524647943604\n",
      "Losgistic Regression(13700/100000): loss=-243.1655039698269\n",
      "Losgistic Regression(13800/100000): loss=-245.40357053908215\n",
      "Losgistic Regression(13900/100000): loss=-247.6256480273219\n",
      "Losgistic Regression(14000/100000): loss=-249.832705413328\n",
      "Losgistic Regression(14100/100000): loss=-252.0313888628121\n",
      "Losgistic Regression(14200/100000): loss=-254.28593150089392\n",
      "Losgistic Regression(14300/100000): loss=-256.5271335772955\n",
      "Losgistic Regression(14400/100000): loss=-258.75317295420376\n",
      "Losgistic Regression(14500/100000): loss=-260.9639276179949\n",
      "Losgistic Regression(14600/100000): loss=-263.16004045665113\n",
      "Losgistic Regression(14700/100000): loss=-265.34003579183747\n",
      "Losgistic Regression(14800/100000): loss=-267.5027352487026\n",
      "Losgistic Regression(14900/100000): loss=-269.64892717941973\n",
      "Losgistic Regression(15000/100000): loss=-271.7778667035937\n",
      "Losgistic Regression(15100/100000): loss=-273.8882051035366\n",
      "Losgistic Regression(15200/100000): loss=-275.98000950109264\n",
      "Losgistic Regression(15300/100000): loss=-278.05362056928624\n",
      "Losgistic Regression(15400/100000): loss=-280.1085879269893\n",
      "Losgistic Regression(15500/100000): loss=-282.1449614938927\n",
      "Losgistic Regression(15600/100000): loss=-284.1643067188541\n",
      "Losgistic Regression(15700/100000): loss=-286.1673014443434\n",
      "Losgistic Regression(15800/100000): loss=-288.1541532270073\n",
      "Losgistic Regression(15900/100000): loss=-290.1261892856546\n",
      "Losgistic Regression(16000/100000): loss=-292.08485282139577\n",
      "Losgistic Regression(16100/100000): loss=-294.03010315651375\n",
      "Losgistic Regression(16200/100000): loss=-295.9617682228971\n",
      "Losgistic Regression(16300/100000): loss=-297.8801148482043\n",
      "Losgistic Regression(16400/100000): loss=-299.7847500127763\n",
      "Losgistic Regression(16500/100000): loss=-301.67520622486825\n",
      "Losgistic Regression(16600/100000): loss=-303.55156744053795\n",
      "Losgistic Regression(16700/100000): loss=-305.41407747914263\n",
      "Losgistic Regression(16800/100000): loss=-307.2626602612771\n",
      "Losgistic Regression(16900/100000): loss=-309.0972478149129\n",
      "Losgistic Regression(17000/100000): loss=-310.91825442412573\n",
      "Losgistic Regression(17100/100000): loss=-312.7263164353936\n",
      "Losgistic Regression(17200/100000): loss=-314.5213957564004\n",
      "Losgistic Regression(17300/100000): loss=-316.3033342875438\n",
      "Losgistic Regression(17400/100000): loss=-318.0726395401679\n",
      "Losgistic Regression(17500/100000): loss=-319.82968960200674\n",
      "Losgistic Regression(17600/100000): loss=-321.5743416255216\n",
      "Losgistic Regression(17700/100000): loss=-323.3069238750122\n",
      "Losgistic Regression(17800/100000): loss=-325.02825693384153\n",
      "Losgistic Regression(17900/100000): loss=-326.738569412126\n",
      "Losgistic Regression(18000/100000): loss=-328.43778404860217\n",
      "Losgistic Regression(18100/100000): loss=-330.12635431441936\n",
      "Losgistic Regression(18200/100000): loss=-331.8046677715197\n",
      "Losgistic Regression(18300/100000): loss=-333.47257660037417\n",
      "Losgistic Regression(18400/100000): loss=-335.1301256352461\n",
      "Losgistic Regression(18500/100000): loss=-336.7775483529911\n",
      "Losgistic Regression(18600/100000): loss=-338.41539679728106\n",
      "Losgistic Regression(18700/100000): loss=-340.0432986809556\n",
      "Losgistic Regression(18800/100000): loss=-341.66169215345354\n",
      "Losgistic Regression(18900/100000): loss=-343.27122594884185\n",
      "Losgistic Regression(19000/100000): loss=-344.87252380085175\n",
      "Losgistic Regression(19100/100000): loss=-346.4656483347508\n",
      "Losgistic Regression(19200/100000): loss=-348.05039859302866\n",
      "Losgistic Regression(19300/100000): loss=-349.6274715989946\n",
      "Losgistic Regression(19400/100000): loss=-351.197553032867\n",
      "Losgistic Regression(19500/100000): loss=-352.76076211707806\n",
      "Losgistic Regression(19600/100000): loss=-354.317225235094\n",
      "Losgistic Regression(19700/100000): loss=-355.86740746084536\n",
      "Losgistic Regression(19800/100000): loss=-357.4120852829315\n",
      "Losgistic Regression(19900/100000): loss=-358.95111423468217\n",
      "Losgistic Regression(20000/100000): loss=-360.48426830574175\n",
      "Losgistic Regression(20100/100000): loss=-362.0119448043722\n",
      "Losgistic Regression(20200/100000): loss=-363.53427782180876\n",
      "Losgistic Regression(20300/100000): loss=-365.0506658031387\n",
      "Losgistic Regression(20400/100000): loss=-366.56097921299903\n",
      "Losgistic Regression(20500/100000): loss=-368.06498461243723\n",
      "Losgistic Regression(20600/100000): loss=-369.5626717212602\n",
      "Losgistic Regression(20700/100000): loss=-371.05355717387147\n",
      "Losgistic Regression(20800/100000): loss=-372.53718606927316\n",
      "Losgistic Regression(20900/100000): loss=-374.0144171286929\n",
      "Losgistic Regression(21000/100000): loss=-375.48564715430376\n",
      "Losgistic Regression(21100/100000): loss=-376.95106056595165\n",
      "Losgistic Regression(21200/100000): loss=-378.4106294095854\n",
      "Losgistic Regression(21300/100000): loss=-379.8650022749274\n",
      "Losgistic Regression(21400/100000): loss=-381.31443027458255\n",
      "Losgistic Regression(21500/100000): loss=-382.75916171667535\n",
      "Losgistic Regression(21600/100000): loss=-384.1995471345085\n",
      "Losgistic Regression(21700/100000): loss=-385.63568394009855\n",
      "Losgistic Regression(21800/100000): loss=-387.0676433084891\n",
      "Losgistic Regression(21900/100000): loss=-388.49552753809104\n",
      "Losgistic Regression(22000/100000): loss=-389.91952442380654\n",
      "Losgistic Regression(22100/100000): loss=-391.33977541971103\n",
      "Losgistic Regression(22200/100000): loss=-392.756386592773\n",
      "Losgistic Regression(22300/100000): loss=-394.16965426670373\n",
      "Losgistic Regression(22400/100000): loss=-395.57944498413224\n",
      "Losgistic Regression(22500/100000): loss=-396.9855029341151\n",
      "Losgistic Regression(22600/100000): loss=-398.38784565626173\n",
      "Losgistic Regression(22700/100000): loss=-399.7864990645093\n",
      "Losgistic Regression(22800/100000): loss=-401.1811903053316\n",
      "Losgistic Regression(22900/100000): loss=-402.5717150367665\n",
      "Losgistic Regression(23000/100000): loss=-403.95809044309055\n",
      "Losgistic Regression(23100/100000): loss=-405.340297366244\n",
      "Losgistic Regression(23200/100000): loss=-406.7182611324963\n",
      "Losgistic Regression(23300/100000): loss=-408.09192684678663\n",
      "Losgistic Regression(23400/100000): loss=-409.46126765228666\n",
      "Losgistic Regression(23500/100000): loss=-410.82628720063275\n",
      "Losgistic Regression(23600/100000): loss=-412.23167892656295\n",
      "Losgistic Regression(23700/100000): loss=-413.6566666819037\n",
      "Losgistic Regression(23800/100000): loss=-415.077871212705\n",
      "Losgistic Regression(23900/100000): loss=-416.49501001394657\n",
      "Losgistic Regression(24000/100000): loss=-417.90830996275923\n",
      "Losgistic Regression(24100/100000): loss=-419.31809993799163\n",
      "Losgistic Regression(24200/100000): loss=-420.72436668632554\n",
      "Losgistic Regression(24300/100000): loss=-422.1269980641207\n",
      "Losgistic Regression(24400/100000): loss=-423.52591878353405\n",
      "Losgistic Regression(24500/100000): loss=-424.9212112261048\n",
      "Losgistic Regression(24600/100000): loss=-426.31258427414014\n",
      "Losgistic Regression(24700/100000): loss=-427.7003542844649\n",
      "Losgistic Regression(24800/100000): loss=-429.0847360130417\n",
      "Losgistic Regression(24900/100000): loss=-430.46564733626036\n",
      "Losgistic Regression(25000/100000): loss=-431.8427050282197\n",
      "Losgistic Regression(25100/100000): loss=-433.21583090194184\n",
      "Losgistic Regression(25200/100000): loss=-434.5852754101093\n",
      "Losgistic Regression(25300/100000): loss=-435.9509375338133\n",
      "Losgistic Regression(25400/100000): loss=-437.312466626151\n",
      "Losgistic Regression(25500/100000): loss=-438.66968315908554\n",
      "Losgistic Regression(25600/100000): loss=-440.02270248281707\n",
      "Losgistic Regression(25700/100000): loss=-441.3714516972829\n",
      "Losgistic Regression(25800/100000): loss=-442.71578479983503\n",
      "Losgistic Regression(25900/100000): loss=-444.05571748621935\n",
      "Losgistic Regression(26000/100000): loss=-445.39128795172616\n",
      "Losgistic Regression(26100/100000): loss=-446.722565474426\n",
      "Losgistic Regression(26200/100000): loss=-448.0495672732155\n",
      "Losgistic Regression(26300/100000): loss=-449.372362632392\n",
      "Losgistic Regression(26400/100000): loss=-450.69107455741744\n",
      "Losgistic Regression(26500/100000): loss=-452.00581109135373\n",
      "Losgistic Regression(26600/100000): loss=-453.3164336261824\n",
      "Losgistic Regression(26700/100000): loss=-454.62266852711565\n",
      "Losgistic Regression(26800/100000): loss=-455.9243112883063\n",
      "Losgistic Regression(26900/100000): loss=-457.2212740826369\n",
      "Losgistic Regression(27000/100000): loss=-458.51354929360133\n",
      "Losgistic Regression(27100/100000): loss=-459.8012129609606\n",
      "Losgistic Regression(27200/100000): loss=-461.0841939177743\n",
      "Losgistic Regression(27300/100000): loss=-462.362346428772\n",
      "Losgistic Regression(27400/100000): loss=-463.63562971728595\n",
      "Losgistic Regression(27500/100000): loss=-464.90419043315165\n",
      "Losgistic Regression(27600/100000): loss=-466.1682467124724\n",
      "Losgistic Regression(27700/100000): loss=-467.4278183685724\n",
      "Losgistic Regression(27800/100000): loss=-468.6828267932904\n",
      "Losgistic Regression(27900/100000): loss=-469.9333353526901\n",
      "Losgistic Regression(28000/100000): loss=-471.1794603311704\n",
      "Losgistic Regression(28100/100000): loss=-472.42131161168624\n",
      "Losgistic Regression(28200/100000): loss=-473.65881382959617\n",
      "Losgistic Regression(28300/100000): loss=-474.89199671796496\n",
      "Losgistic Regression(28400/100000): loss=-476.12091899997256\n",
      "Losgistic Regression(28500/100000): loss=-477.3455726133253\n",
      "Losgistic Regression(28600/100000): loss=-478.5660200616348\n",
      "Losgistic Regression(28700/100000): loss=-479.78235035837184\n",
      "Losgistic Regression(28800/100000): loss=-480.9946080899812\n",
      "Losgistic Regression(28900/100000): loss=-482.2027424378341\n",
      "Losgistic Regression(29000/100000): loss=-483.40682045412865\n",
      "Losgistic Regression(29100/100000): loss=-484.6069543116321\n",
      "Losgistic Regression(29200/100000): loss=-485.80308772208883\n",
      "Losgistic Regression(29300/100000): loss=-486.9951241328083\n",
      "Losgistic Regression(29400/100000): loss=-488.1831364520319\n",
      "Losgistic Regression(29500/100000): loss=-489.3673456149588\n",
      "Losgistic Regression(29600/100000): loss=-490.54781985700794\n",
      "Losgistic Regression(29700/100000): loss=-491.7244302443689\n",
      "Losgistic Regression(29800/100000): loss=-492.89711563316115\n",
      "Losgistic Regression(29900/100000): loss=-494.0659799699114\n",
      "Losgistic Regression(30000/100000): loss=-495.23114400676377\n",
      "Losgistic Regression(30100/100000): loss=-496.39252770233657\n",
      "Losgistic Regression(30200/100000): loss=-497.5500037990474\n",
      "Losgistic Regression(30300/100000): loss=-498.70355971030335\n",
      "Losgistic Regression(30400/100000): loss=-499.8532054917997\n",
      "Losgistic Regression(30500/100000): loss=-500.9989193269476\n",
      "Losgistic Regression(30600/100000): loss=-502.14073691429326\n",
      "Losgistic Regression(30700/100000): loss=-503.27875685856566\n",
      "Losgistic Regression(30800/100000): loss=-504.41303410170076\n",
      "Losgistic Regression(30900/100000): loss=-505.54355319539803\n",
      "Losgistic Regression(31000/100000): loss=-506.6701827530849\n",
      "Losgistic Regression(31100/100000): loss=-507.79300309419705\n",
      "Losgistic Regression(31200/100000): loss=-508.91318495686477\n",
      "Losgistic Regression(31300/100000): loss=-510.03039924700334\n",
      "Losgistic Regression(31400/100000): loss=-511.1445592835074\n",
      "Losgistic Regression(31500/100000): loss=-512.2549280128228\n",
      "Losgistic Regression(31600/100000): loss=-513.361568796061\n",
      "Losgistic Regression(31700/100000): loss=-514.464885458226\n",
      "Losgistic Regression(31800/100000): loss=-515.5647990818829\n",
      "Losgistic Regression(31900/100000): loss=-516.6618309389428\n",
      "Losgistic Regression(32000/100000): loss=-517.7550616005263\n",
      "Losgistic Regression(32100/100000): loss=-518.8443481731438\n",
      "Losgistic Regression(32200/100000): loss=-519.9300686770058\n",
      "Losgistic Regression(32300/100000): loss=-521.0120515839122\n",
      "Losgistic Regression(32400/100000): loss=-522.0908546846496\n",
      "Losgistic Regression(32500/100000): loss=-523.1661745073368\n",
      "Losgistic Regression(32600/100000): loss=-524.2377887913698\n",
      "Losgistic Regression(32700/100000): loss=-525.3055874095385\n",
      "Losgistic Regression(32800/100000): loss=-526.3695250553897\n",
      "Losgistic Regression(32900/100000): loss=-527.4301879867958\n",
      "Losgistic Regression(33000/100000): loss=-528.4871969901978\n",
      "Losgistic Regression(33100/100000): loss=-529.540266730559\n",
      "Losgistic Regression(33200/100000): loss=-530.5888312308906\n",
      "Losgistic Regression(33300/100000): loss=-531.6332793753651\n",
      "Losgistic Regression(33400/100000): loss=-532.6744568283103\n",
      "Losgistic Regression(33500/100000): loss=-533.7115000896043\n",
      "Losgistic Regression(33600/100000): loss=-534.7447294914567\n",
      "Losgistic Regression(33700/100000): loss=-535.7742613130105\n",
      "Losgistic Regression(33800/100000): loss=-536.7996993695103\n",
      "Losgistic Regression(33900/100000): loss=-537.8210213081851\n",
      "Losgistic Regression(34000/100000): loss=-538.8386466958098\n",
      "Losgistic Regression(34100/100000): loss=-539.8524297489475\n",
      "Losgistic Regression(34200/100000): loss=-540.8629852398005\n",
      "Losgistic Regression(34300/100000): loss=-541.8703422069461\n",
      "Losgistic Regression(34400/100000): loss=-542.8733995667561\n",
      "Losgistic Regression(34500/100000): loss=-543.8728345851389\n",
      "Losgistic Regression(34600/100000): loss=-544.8695086697454\n",
      "Losgistic Regression(34700/100000): loss=-545.8629557876282\n",
      "Losgistic Regression(34800/100000): loss=-546.8535797255628\n",
      "Losgistic Regression(34900/100000): loss=-547.8412162577858\n",
      "Losgistic Regression(35000/100000): loss=-548.8255467689398\n",
      "Losgistic Regression(35100/100000): loss=-549.8068084991528\n",
      "Losgistic Regression(35200/100000): loss=-550.7849876685409\n",
      "Losgistic Regression(35300/100000): loss=-551.7604489914152\n",
      "Losgistic Regression(35400/100000): loss=-552.7330200860066\n",
      "Losgistic Regression(35500/100000): loss=-553.7019374130635\n",
      "Losgistic Regression(35600/100000): loss=-554.6673195443153\n",
      "Losgistic Regression(35700/100000): loss=-555.6296543803305\n",
      "Losgistic Regression(35800/100000): loss=-556.5886828826789\n",
      "Losgistic Regression(35900/100000): loss=-557.5440506370654\n",
      "Losgistic Regression(36000/100000): loss=-558.4955758469541\n",
      "Losgistic Regression(36100/100000): loss=-559.4431387124387\n",
      "Losgistic Regression(36200/100000): loss=-560.3867890079874\n",
      "Losgistic Regression(36300/100000): loss=-561.3271077992516\n",
      "Losgistic Regression(36400/100000): loss=-562.2642201373816\n",
      "Losgistic Regression(36500/100000): loss=-563.1977345501997\n",
      "Losgistic Regression(36600/100000): loss=-564.1275396606112\n",
      "Losgistic Regression(36700/100000): loss=-565.0534773123086\n",
      "Losgistic Regression(36800/100000): loss=-565.9755442561834\n",
      "Losgistic Regression(36900/100000): loss=-566.894174936435\n",
      "Losgistic Regression(37000/100000): loss=-567.8095146225171\n",
      "Losgistic Regression(37100/100000): loss=-568.7214572471948\n",
      "Losgistic Regression(37200/100000): loss=-569.6299015530308\n",
      "Losgistic Regression(37300/100000): loss=-570.5344194001075\n",
      "Losgistic Regression(37400/100000): loss=-571.4352366408807\n",
      "Losgistic Regression(37500/100000): loss=-572.3330628541523\n",
      "Losgistic Regression(37600/100000): loss=-573.2279596621695\n",
      "Losgistic Regression(37700/100000): loss=-574.1194937131665\n",
      "Losgistic Regression(37800/100000): loss=-575.0076269823977\n",
      "Losgistic Regression(37900/100000): loss=-575.8925436740373\n",
      "Losgistic Regression(38000/100000): loss=-576.774310580537\n",
      "Losgistic Regression(38100/100000): loss=-577.653064651497\n",
      "Losgistic Regression(38200/100000): loss=-578.5286854960933\n",
      "Losgistic Regression(38300/100000): loss=-579.4008555018085\n",
      "Losgistic Regression(38400/100000): loss=-580.2693987698743\n",
      "Losgistic Regression(38500/100000): loss=-581.1345308504807\n",
      "Losgistic Regression(38600/100000): loss=-581.9964724294778\n",
      "Losgistic Regression(38700/100000): loss=-582.8553942384458\n",
      "Losgistic Regression(38800/100000): loss=-583.7113284721738\n",
      "Losgistic Regression(38900/100000): loss=-584.5640851412294\n",
      "Losgistic Regression(39000/100000): loss=-585.4136205414417\n",
      "Losgistic Regression(39100/100000): loss=-586.2599250173206\n",
      "Losgistic Regression(39200/100000): loss=-587.1031202470969\n",
      "Losgistic Regression(39300/100000): loss=-587.9433558191058\n",
      "Losgistic Regression(39400/100000): loss=-588.7812302168833\n",
      "Losgistic Regression(39500/100000): loss=-589.6161340343098\n",
      "Losgistic Regression(39600/100000): loss=-590.4477374150515\n",
      "Losgistic Regression(39700/100000): loss=-591.2761272845938\n",
      "Losgistic Regression(39800/100000): loss=-592.1013019705117\n",
      "Losgistic Regression(39900/100000): loss=-592.9237324760792\n",
      "Losgistic Regression(40000/100000): loss=-593.7469293902747\n",
      "Losgistic Regression(40100/100000): loss=-594.5688492265446\n",
      "Losgistic Regression(40200/100000): loss=-595.3876833679959\n",
      "Losgistic Regression(40300/100000): loss=-596.203257059725\n",
      "Losgistic Regression(40400/100000): loss=-597.015527330575\n",
      "Losgistic Regression(40500/100000): loss=-597.8250962675716\n",
      "Losgistic Regression(40600/100000): loss=-598.631972089547\n",
      "Losgistic Regression(40700/100000): loss=-599.4361500894973\n",
      "Losgistic Regression(40800/100000): loss=-600.2377377869454\n",
      "Losgistic Regression(40900/100000): loss=-601.0364162793921\n",
      "Losgistic Regression(41000/100000): loss=-601.8326056057429\n",
      "Losgistic Regression(41100/100000): loss=-602.6264961622032\n",
      "Losgistic Regression(41200/100000): loss=-603.4179883769834\n",
      "Losgistic Regression(41300/100000): loss=-604.2070563805313\n",
      "Losgistic Regression(41400/100000): loss=-604.9937260547117\n",
      "Losgistic Regression(41500/100000): loss=-605.7777801781322\n",
      "Losgistic Regression(41600/100000): loss=-606.5596435919704\n",
      "Losgistic Regression(41700/100000): loss=-607.3392901536749\n",
      "Losgistic Regression(41800/100000): loss=-608.1162578487312\n",
      "Losgistic Regression(41900/100000): loss=-608.8908487767148\n",
      "Losgistic Regression(42000/100000): loss=-609.6631430955061\n",
      "Losgistic Regression(42100/100000): loss=-610.4329305644276\n",
      "Losgistic Regression(42200/100000): loss=-611.200542110957\n",
      "Losgistic Regression(42300/100000): loss=-611.965922483923\n",
      "Losgistic Regression(42400/100000): loss=-612.7289249159248\n",
      "Losgistic Regression(42500/100000): loss=-613.4898591990792\n",
      "Losgistic Regression(42600/100000): loss=-614.248763968668\n",
      "Losgistic Regression(42700/100000): loss=-615.0054966315806\n",
      "Losgistic Regression(42800/100000): loss=-615.760369103917\n",
      "Losgistic Regression(42900/100000): loss=-616.5135786825373\n",
      "Losgistic Regression(43000/100000): loss=-617.2652014719037\n",
      "Losgistic Regression(43100/100000): loss=-618.0155503825749\n",
      "Losgistic Regression(43200/100000): loss=-618.7646849339378\n",
      "Losgistic Regression(43300/100000): loss=-619.5123975824666\n",
      "Losgistic Regression(43400/100000): loss=-620.2585303434494\n",
      "Losgistic Regression(43500/100000): loss=-621.0032121861636\n",
      "Losgistic Regression(43600/100000): loss=-621.7464795796186\n",
      "Losgistic Regression(43700/100000): loss=-622.4882614666136\n",
      "Losgistic Regression(43800/100000): loss=-623.228445770503\n",
      "Losgistic Regression(43900/100000): loss=-623.9670867752388\n",
      "Losgistic Regression(44000/100000): loss=-624.704045831006\n",
      "Losgistic Regression(44100/100000): loss=-625.4391624182497\n",
      "Losgistic Regression(44200/100000): loss=-626.1726903264524\n",
      "Losgistic Regression(44300/100000): loss=-626.9048936426941\n",
      "Losgistic Regression(44400/100000): loss=-627.6357569570624\n",
      "Losgistic Regression(44500/100000): loss=-628.3654101305525\n",
      "Losgistic Regression(44600/100000): loss=-629.0938224675522\n",
      "Losgistic Regression(44700/100000): loss=-629.8209277487008\n",
      "Losgistic Regression(44800/100000): loss=-630.5469849171884\n",
      "Losgistic Regression(44900/100000): loss=-631.272200249032\n",
      "Losgistic Regression(45000/100000): loss=-631.9963724571285\n",
      "Losgistic Regression(45100/100000): loss=-632.7194459924389\n",
      "Losgistic Regression(45200/100000): loss=-633.4415553358302\n",
      "Losgistic Regression(45300/100000): loss=-634.1626050070213\n",
      "Losgistic Regression(45400/100000): loss=-634.8825651940534\n",
      "Losgistic Regression(45500/100000): loss=-635.6014699331147\n",
      "Losgistic Regression(45600/100000): loss=-636.3191486734441\n",
      "Losgistic Regression(45700/100000): loss=-637.0354434011815\n",
      "Losgistic Regression(45800/100000): loss=-637.7504727178015\n",
      "Losgistic Regression(45900/100000): loss=-638.4644334053888\n",
      "Losgistic Regression(46000/100000): loss=-639.1774870400177\n",
      "Losgistic Regression(46100/100000): loss=-639.8896941596597\n",
      "Losgistic Regression(46200/100000): loss=-640.6010429798914\n",
      "Losgistic Regression(46300/100000): loss=-641.3116365110344\n",
      "Losgistic Regression(46400/100000): loss=-642.0216633532749\n",
      "Losgistic Regression(46500/100000): loss=-642.7311950351564\n",
      "Losgistic Regression(46600/100000): loss=-643.4402224912956\n",
      "Losgistic Regression(46700/100000): loss=-644.1486481279084\n",
      "Losgistic Regression(46800/100000): loss=-644.8563875228855\n",
      "Losgistic Regression(46900/100000): loss=-645.5634467478485\n",
      "Losgistic Regression(47000/100000): loss=-646.2697291658592\n",
      "Losgistic Regression(47100/100000): loss=-646.9751105453734\n",
      "Losgistic Regression(47200/100000): loss=-647.6795252914122\n",
      "Losgistic Regression(47300/100000): loss=-648.3828649176353\n",
      "Losgistic Regression(47400/100000): loss=-649.0849563798602\n",
      "Losgistic Regression(47500/100000): loss=-649.7858207520186\n",
      "Losgistic Regression(47600/100000): loss=-650.4855299800722\n",
      "Losgistic Regression(47700/100000): loss=-651.1840125642753\n",
      "Losgistic Regression(47800/100000): loss=-651.8812979436583\n",
      "Losgistic Regression(47900/100000): loss=-652.5775121715782\n",
      "Losgistic Regression(48000/100000): loss=-653.2726506882912\n",
      "Losgistic Regression(48100/100000): loss=-653.9668176526295\n",
      "Losgistic Regression(48200/100000): loss=-654.6602313365413\n",
      "Losgistic Regression(48300/100000): loss=-655.3528097992809\n",
      "Losgistic Regression(48400/100000): loss=-656.0443015075346\n",
      "Losgistic Regression(48500/100000): loss=-656.7345894535334\n",
      "Losgistic Regression(48600/100000): loss=-657.4237755659892\n",
      "Losgistic Regression(48700/100000): loss=-658.1121610135641\n",
      "Losgistic Regression(48800/100000): loss=-658.8000860149268\n",
      "Losgistic Regression(48900/100000): loss=-659.4872149443055\n",
      "Losgistic Regression(49000/100000): loss=-660.1727746132518\n",
      "Losgistic Regression(49100/100000): loss=-660.8565051790719\n",
      "Losgistic Regression(49200/100000): loss=-661.5385488380822\n",
      "Losgistic Regression(49300/100000): loss=-662.2194129056583\n",
      "Losgistic Regression(49400/100000): loss=-662.8990228942539\n",
      "Losgistic Regression(49500/100000): loss=-663.577214408973\n",
      "Losgistic Regression(49600/100000): loss=-664.2537924106557\n",
      "Losgistic Regression(49700/100000): loss=-664.9287041812005\n",
      "Losgistic Regression(49800/100000): loss=-665.6020854272496\n",
      "Losgistic Regression(49900/100000): loss=-666.2739866111774\n",
      "Losgistic Regression(50000/100000): loss=-666.9443063992346\n",
      "Losgistic Regression(50100/100000): loss=-667.6130719703888\n",
      "Losgistic Regression(50200/100000): loss=-668.2802055426533\n",
      "Losgistic Regression(50300/100000): loss=-668.9460142420799\n",
      "Losgistic Regression(50400/100000): loss=-669.6104020601958\n",
      "Losgistic Regression(50500/100000): loss=-670.2731102748644\n",
      "Losgistic Regression(50600/100000): loss=-670.9341312513657\n",
      "Losgistic Regression(50700/100000): loss=-671.5936157339639\n",
      "Losgistic Regression(50800/100000): loss=-672.2517205050916\n",
      "Losgistic Regression(50900/100000): loss=-672.9084642569997\n",
      "Losgistic Regression(51000/100000): loss=-673.5636808123342\n",
      "Losgistic Regression(51100/100000): loss=-674.2170654711233\n",
      "Losgistic Regression(51200/100000): loss=-674.868401585104\n",
      "Losgistic Regression(51300/100000): loss=-675.5178990146582\n",
      "Losgistic Regression(51400/100000): loss=-676.1658606009347\n",
      "Losgistic Regression(51500/100000): loss=-676.8122436064468\n",
      "Losgistic Regression(51600/100000): loss=-677.4567690269735\n",
      "Losgistic Regression(51700/100000): loss=-678.0993086560424\n",
      "Losgistic Regression(51800/100000): loss=-678.7399141791908\n",
      "Losgistic Regression(51900/100000): loss=-679.378596810834\n",
      "Losgistic Regression(52000/100000): loss=-680.0151899609841\n",
      "Losgistic Regression(52100/100000): loss=-680.6499113459149\n",
      "Losgistic Regression(52200/100000): loss=-681.2831161882544\n",
      "Losgistic Regression(52300/100000): loss=-681.9145845081938\n",
      "Losgistic Regression(52400/100000): loss=-682.5439148649411\n",
      "Losgistic Regression(52500/100000): loss=-683.1716297388034\n",
      "Losgistic Regression(52600/100000): loss=-683.7972805025136\n",
      "Losgistic Regression(52700/100000): loss=-684.420704277071\n",
      "Losgistic Regression(52800/100000): loss=-685.0420003041545\n",
      "Losgistic Regression(52900/100000): loss=-685.6617440976617\n",
      "Losgistic Regression(53000/100000): loss=-686.2799764588381\n",
      "Losgistic Regression(53100/100000): loss=-686.8964487823971\n",
      "Losgistic Regression(53200/100000): loss=-687.5108907730363\n",
      "Losgistic Regression(53300/100000): loss=-688.1234757316308\n",
      "Losgistic Regression(53400/100000): loss=-688.7339144273129\n",
      "Losgistic Regression(53500/100000): loss=-689.3424662845987\n",
      "Losgistic Regression(53600/100000): loss=-689.9493222573674\n",
      "Losgistic Regression(53700/100000): loss=-690.5545622575879\n",
      "Losgistic Regression(53800/100000): loss=-691.1581139400846\n",
      "Losgistic Regression(53900/100000): loss=-691.7598541514388\n",
      "Losgistic Regression(54000/100000): loss=-692.3601413879531\n",
      "Losgistic Regression(54100/100000): loss=-692.9588658577874\n",
      "Losgistic Regression(54200/100000): loss=-693.555838368761\n",
      "Losgistic Regression(54300/100000): loss=-694.1512500451951\n",
      "Losgistic Regression(54400/100000): loss=-694.745237661904\n",
      "Losgistic Regression(54500/100000): loss=-695.3377223585086\n",
      "Losgistic Regression(54600/100000): loss=-695.9287541062527\n",
      "Losgistic Regression(54700/100000): loss=-696.5182287532188\n",
      "Losgistic Regression(54800/100000): loss=-697.1061566601836\n",
      "Losgistic Regression(54900/100000): loss=-697.6923810731273\n",
      "Losgistic Regression(55000/100000): loss=-698.2854606246659\n",
      "Losgistic Regression(55100/100000): loss=-698.8855159233756\n",
      "Losgistic Regression(55200/100000): loss=-699.484210820183\n",
      "Losgistic Regression(55300/100000): loss=-700.0814798541664\n",
      "Losgistic Regression(55400/100000): loss=-700.6774282508455\n",
      "Losgistic Regression(55500/100000): loss=-701.2720583644734\n",
      "Losgistic Regression(55600/100000): loss=-701.8653756096592\n",
      "Losgistic Regression(55700/100000): loss=-702.4575420915212\n",
      "Losgistic Regression(55800/100000): loss=-703.0487782338181\n",
      "Losgistic Regression(55900/100000): loss=-703.6392140239782\n",
      "Losgistic Regression(56000/100000): loss=-704.2287132690227\n",
      "Losgistic Regression(56100/100000): loss=-704.8172648927195\n",
      "Losgistic Regression(56200/100000): loss=-705.4048069969147\n",
      "Losgistic Regression(56300/100000): loss=-705.9911876821557\n",
      "Losgistic Regression(56400/100000): loss=-706.5764007459965\n",
      "Losgistic Regression(56500/100000): loss=-707.1605484642971\n",
      "Losgistic Regression(56600/100000): loss=-707.7435444693938\n",
      "Losgistic Regression(56700/100000): loss=-708.3253978996781\n",
      "Losgistic Regression(56800/100000): loss=-708.9060532904127\n",
      "Losgistic Regression(56900/100000): loss=-709.4855577381303\n",
      "Losgistic Regression(57000/100000): loss=-710.0638189013786\n",
      "Losgistic Regression(57100/100000): loss=-710.6407434683825\n",
      "Losgistic Regression(57200/100000): loss=-711.216485675017\n",
      "Losgistic Regression(57300/100000): loss=-711.7910496363849\n",
      "Losgistic Regression(57400/100000): loss=-712.364712022724\n",
      "Losgistic Regression(57500/100000): loss=-712.9377419218555\n",
      "Losgistic Regression(57600/100000): loss=-713.5103620171818\n",
      "Losgistic Regression(57700/100000): loss=-714.0823888241822\n",
      "Losgistic Regression(57800/100000): loss=-714.6535811054077\n",
      "Losgistic Regression(57900/100000): loss=-715.2239588044126\n",
      "Losgistic Regression(58000/100000): loss=-715.7938949461657\n",
      "Losgistic Regression(58100/100000): loss=-716.3633538961651\n",
      "Losgistic Regression(58200/100000): loss=-716.9322442558282\n",
      "Losgistic Regression(58300/100000): loss=-717.5006869378049\n",
      "Losgistic Regression(58400/100000): loss=-718.0687104038113\n",
      "Losgistic Regression(58500/100000): loss=-718.6362316544079\n",
      "Losgistic Regression(58600/100000): loss=-719.2031382816541\n",
      "Losgistic Regression(58700/100000): loss=-719.7694504413138\n",
      "Losgistic Regression(58800/100000): loss=-720.3352280775731\n",
      "Losgistic Regression(58900/100000): loss=-720.9003695408139\n",
      "Losgistic Regression(59000/100000): loss=-721.4650028673136\n",
      "Losgistic Regression(59100/100000): loss=-722.0293468073867\n",
      "Losgistic Regression(59200/100000): loss=-722.5933633964331\n",
      "Losgistic Regression(59300/100000): loss=-723.15696387366\n",
      "Losgistic Regression(59400/100000): loss=-723.7202778457176\n",
      "Losgistic Regression(59500/100000): loss=-724.2831891623363\n",
      "Losgistic Regression(59600/100000): loss=-724.8455787497719\n",
      "Losgistic Regression(59700/100000): loss=-725.4074559471721\n",
      "Losgistic Regression(59800/100000): loss=-725.9691840801711\n",
      "Losgistic Regression(59900/100000): loss=-726.5309490743093\n",
      "Losgistic Regression(60000/100000): loss=-727.0924914821993\n",
      "Losgistic Regression(60100/100000): loss=-727.653442855139\n",
      "Losgistic Regression(60200/100000): loss=-728.213777633998\n",
      "Losgistic Regression(60300/100000): loss=-728.7736255067462\n",
      "Losgistic Regression(60400/100000): loss=-729.3329613168776\n",
      "Losgistic Regression(60500/100000): loss=-729.8916982024173\n",
      "Losgistic Regression(60600/100000): loss=-730.4497987657306\n",
      "Losgistic Regression(60700/100000): loss=-731.0073781490279\n",
      "Losgistic Regression(60800/100000): loss=-731.5645275443848\n",
      "Losgistic Regression(60900/100000): loss=-732.1212449001358\n",
      "Losgistic Regression(61000/100000): loss=-732.6774750744786\n",
      "Losgistic Regression(61100/100000): loss=-733.2330458725966\n",
      "Losgistic Regression(61200/100000): loss=-733.7878481179421\n",
      "Losgistic Regression(61300/100000): loss=-734.3420192841422\n",
      "Losgistic Regression(61400/100000): loss=-734.8957953353774\n",
      "Losgistic Regression(61500/100000): loss=-735.4492525214863\n",
      "Losgistic Regression(61600/100000): loss=-736.0023237439218\n",
      "Losgistic Regression(61700/100000): loss=-736.5549702981916\n",
      "Losgistic Regression(61800/100000): loss=-737.1071969006341\n",
      "Losgistic Regression(61900/100000): loss=-737.6589329294234\n",
      "Losgistic Regression(62000/100000): loss=-738.2100779052951\n",
      "Losgistic Regression(62100/100000): loss=-738.7607332153686\n",
      "Losgistic Regression(62200/100000): loss=-739.3110326348345\n",
      "Losgistic Regression(62300/100000): loss=-739.8609682516958\n",
      "Losgistic Regression(62400/100000): loss=-740.4104310165517\n",
      "Losgistic Regression(62500/100000): loss=-740.9593519194258\n",
      "Losgistic Regression(62600/100000): loss=-741.5076728815906\n",
      "Losgistic Regression(62700/100000): loss=-742.0552690357767\n",
      "Losgistic Regression(62800/100000): loss=-742.6020030328646\n",
      "Losgistic Regression(62900/100000): loss=-743.1478691133957\n",
      "Losgistic Regression(63000/100000): loss=-743.6929789856035\n",
      "Losgistic Regression(63100/100000): loss=-744.2374318365859\n",
      "Losgistic Regression(63200/100000): loss=-744.7813803326967\n",
      "Losgistic Regression(63300/100000): loss=-745.3249202909205\n",
      "Losgistic Regression(63400/100000): loss=-745.8679531841625\n",
      "Losgistic Regression(63500/100000): loss=-746.4102926707684\n",
      "Losgistic Regression(63600/100000): loss=-746.951841828034\n",
      "Losgistic Regression(63700/100000): loss=-747.492682468852\n",
      "Losgistic Regression(63800/100000): loss=-748.0329511620968\n",
      "Losgistic Regression(63900/100000): loss=-748.5727371998898\n",
      "Losgistic Regression(64000/100000): loss=-749.1120976496758\n",
      "Losgistic Regression(64100/100000): loss=-749.6510295526446\n",
      "Losgistic Regression(64200/100000): loss=-750.1894117278126\n",
      "Losgistic Regression(64300/100000): loss=-750.727096632306\n",
      "Losgistic Regression(64400/100000): loss=-751.2641608869012\n",
      "Losgistic Regression(64500/100000): loss=-751.8005338956573\n",
      "Losgistic Regression(64600/100000): loss=-752.336093540855\n",
      "Losgistic Regression(64700/100000): loss=-752.8708220356393\n",
      "Losgistic Regression(64800/100000): loss=-753.4046417572118\n",
      "Losgistic Regression(64900/100000): loss=-753.937654491334\n",
      "Losgistic Regression(65000/100000): loss=-754.4698011353986\n",
      "Losgistic Regression(65100/100000): loss=-755.00101814901\n",
      "Losgistic Regression(65200/100000): loss=-755.531418967687\n",
      "Losgistic Regression(65300/100000): loss=-756.0609226985754\n",
      "Losgistic Regression(65400/100000): loss=-756.589520668057\n",
      "Losgistic Regression(65500/100000): loss=-757.1171841129213\n",
      "Losgistic Regression(65600/100000): loss=-757.6440661409787\n",
      "Losgistic Regression(65700/100000): loss=-758.1704099634572\n",
      "Losgistic Regression(65800/100000): loss=-758.6961949068116\n",
      "Losgistic Regression(65900/100000): loss=-759.221106629152\n",
      "Losgistic Regression(66000/100000): loss=-759.7450874519243\n",
      "Losgistic Regression(66100/100000): loss=-760.268391801723\n",
      "Losgistic Regression(66200/100000): loss=-760.7910794385509\n",
      "Losgistic Regression(66300/100000): loss=-761.3132135708852\n",
      "Losgistic Regression(66400/100000): loss=-761.8347516542223\n",
      "Losgistic Regression(66500/100000): loss=-762.3556263165702\n",
      "Losgistic Regression(66600/100000): loss=-762.8757721589666\n",
      "Losgistic Regression(66700/100000): loss=-763.3950880632078\n",
      "Losgistic Regression(66800/100000): loss=-763.9135578337751\n",
      "Losgistic Regression(66900/100000): loss=-764.4313332992067\n",
      "Losgistic Regression(67000/100000): loss=-764.9483901644261\n",
      "Losgistic Regression(67100/100000): loss=-765.4646258613874\n",
      "Losgistic Regression(67200/100000): loss=-765.9801368174768\n",
      "Losgistic Regression(67300/100000): loss=-766.4949924655472\n",
      "Losgistic Regression(67400/100000): loss=-767.0089856528637\n",
      "Losgistic Regression(67500/100000): loss=-767.522081200821\n",
      "Losgistic Regression(67600/100000): loss=-768.0343468313644\n",
      "Losgistic Regression(67700/100000): loss=-768.5457901922401\n",
      "Losgistic Regression(67800/100000): loss=-769.0563550603187\n",
      "Losgistic Regression(67900/100000): loss=-769.565868882966\n",
      "Losgistic Regression(68000/100000): loss=-770.0744839045911\n",
      "Losgistic Regression(68100/100000): loss=-770.5825376824894\n",
      "Losgistic Regression(68200/100000): loss=-771.0902080288781\n",
      "Losgistic Regression(68300/100000): loss=-771.5972724003334\n",
      "Losgistic Regression(68400/100000): loss=-772.1034898726905\n",
      "Losgistic Regression(68500/100000): loss=-772.6088154409422\n",
      "Losgistic Regression(68600/100000): loss=-773.113356470384\n",
      "Losgistic Regression(68700/100000): loss=-773.6172621473227\n",
      "Losgistic Regression(68800/100000): loss=-774.1204935364487\n",
      "Losgistic Regression(68900/100000): loss=-774.6229330668583\n",
      "Losgistic Regression(69000/100000): loss=-775.12453430427\n",
      "Losgistic Regression(69100/100000): loss=-775.6253458278998\n",
      "Losgistic Regression(69200/100000): loss=-776.125619192483\n",
      "Losgistic Regression(69300/100000): loss=-776.6255774376029\n",
      "Losgistic Regression(69400/100000): loss=-777.1251921462873\n",
      "Losgistic Regression(69500/100000): loss=-777.6242158811523\n",
      "Losgistic Regression(69600/100000): loss=-778.1225034680101\n",
      "Losgistic Regression(69700/100000): loss=-778.6201935857796\n",
      "Losgistic Regression(69800/100000): loss=-779.1174989291565\n",
      "Losgistic Regression(69900/100000): loss=-779.6144738400742\n",
      "Losgistic Regression(70000/100000): loss=-780.1109787282198\n",
      "Losgistic Regression(70100/100000): loss=-780.6068459243676\n",
      "Losgistic Regression(70200/100000): loss=-781.101995323657\n",
      "Losgistic Regression(70300/100000): loss=-781.5964442258438\n",
      "Losgistic Regression(70400/100000): loss=-782.0903504873905\n",
      "Losgistic Regression(70500/100000): loss=-782.5838417146103\n",
      "Losgistic Regression(70600/100000): loss=-783.0769036454428\n",
      "Losgistic Regression(70700/100000): loss=-783.5694801606126\n",
      "Losgistic Regression(70800/100000): loss=-784.0615166873507\n",
      "Losgistic Regression(70900/100000): loss=-784.5529615972357\n",
      "Losgistic Regression(71000/100000): loss=-785.0437808910162\n",
      "Losgistic Regression(71100/100000): loss=-785.5339786064355\n",
      "Losgistic Regression(71200/100000): loss=-786.0235913347988\n",
      "Losgistic Regression(71300/100000): loss=-786.5127037189498\n",
      "Losgistic Regression(71400/100000): loss=-787.0013331292516\n",
      "Losgistic Regression(71500/100000): loss=-787.4893419096536\n",
      "Losgistic Regression(71600/100000): loss=-787.976627236017\n",
      "Losgistic Regression(71700/100000): loss=-788.4632083638955\n",
      "Losgistic Regression(71800/100000): loss=-788.9491445974638\n",
      "Losgistic Regression(71900/100000): loss=-789.4345131679\n",
      "Losgistic Regression(72000/100000): loss=-789.9193723498863\n",
      "Losgistic Regression(72100/100000): loss=-790.4037384813715\n",
      "Losgistic Regression(72200/100000): loss=-790.8876381948296\n",
      "Losgistic Regression(72300/100000): loss=-791.3711004815675\n",
      "Losgistic Regression(72400/100000): loss=-791.8541102988004\n",
      "Losgistic Regression(72500/100000): loss=-792.3366929894768\n",
      "Losgistic Regression(72600/100000): loss=-792.8189503301486\n",
      "Losgistic Regression(72700/100000): loss=-793.3009118358849\n",
      "Losgistic Regression(72800/100000): loss=-793.7825443191758\n",
      "Losgistic Regression(72900/100000): loss=-794.2638514012416\n",
      "Losgistic Regression(73000/100000): loss=-794.7448575751241\n",
      "Losgistic Regression(73100/100000): loss=-795.2255632828688\n",
      "Losgistic Regression(73200/100000): loss=-795.7059478242935\n",
      "Losgistic Regression(73300/100000): loss=-796.1860430002889\n",
      "Losgistic Regression(73400/100000): loss=-796.6659398140981\n",
      "Losgistic Regression(73500/100000): loss=-797.1456633334404\n",
      "Losgistic Regression(73600/100000): loss=-797.62491251369\n",
      "Losgistic Regression(73700/100000): loss=-798.1038795194579\n",
      "Losgistic Regression(73800/100000): loss=-798.5827263783281\n",
      "Losgistic Regression(73900/100000): loss=-799.0611614160466\n",
      "Losgistic Regression(74000/100000): loss=-799.539142593011\n",
      "Losgistic Regression(74100/100000): loss=-800.0168654333964\n",
      "Losgistic Regression(74200/100000): loss=-800.4943928947448\n",
      "Losgistic Regression(74300/100000): loss=-800.9718944029803\n",
      "Losgistic Regression(74400/100000): loss=-801.4493205873068\n",
      "Losgistic Regression(74500/100000): loss=-801.9266463189273\n",
      "Losgistic Regression(74600/100000): loss=-802.4040563999428\n",
      "Losgistic Regression(74700/100000): loss=-802.8814004473442\n",
      "Losgistic Regression(74800/100000): loss=-803.3585182154576\n",
      "Losgistic Regression(74900/100000): loss=-803.8354584809299\n",
      "Losgistic Regression(75000/100000): loss=-804.3121600158906\n",
      "Losgistic Regression(75100/100000): loss=-804.7883349049141\n",
      "Losgistic Regression(75200/100000): loss=-805.2637296008593\n",
      "Losgistic Regression(75300/100000): loss=-805.7385523620458\n",
      "Losgistic Regression(75400/100000): loss=-806.2130121696139\n",
      "Losgistic Regression(75500/100000): loss=-806.6873200765231\n",
      "Losgistic Regression(75600/100000): loss=-807.1615235473163\n",
      "Losgistic Regression(75700/100000): loss=-807.635391387885\n",
      "Losgistic Regression(75800/100000): loss=-808.1086177975425\n",
      "Losgistic Regression(75900/100000): loss=-808.580968353313\n",
      "Losgistic Regression(76000/100000): loss=-809.0525338384246\n",
      "Losgistic Regression(76100/100000): loss=-809.5236465487648\n",
      "Losgistic Regression(76200/100000): loss=-809.9944366804975\n",
      "Losgistic Regression(76300/100000): loss=-810.4650745054932\n",
      "Losgistic Regression(76400/100000): loss=-810.9356658198842\n",
      "Losgistic Regression(76500/100000): loss=-811.4060364281397\n",
      "Losgistic Regression(76600/100000): loss=-811.8761562876027\n",
      "Losgistic Regression(76700/100000): loss=-812.3459860645828\n",
      "Losgistic Regression(76800/100000): loss=-812.8154479271997\n",
      "Losgistic Regression(76900/100000): loss=-813.2845279242988\n",
      "Losgistic Regression(77000/100000): loss=-813.7532021393116\n",
      "Losgistic Regression(77100/100000): loss=-814.2216698928574\n",
      "Losgistic Regression(77200/100000): loss=-814.6899811711675\n",
      "Losgistic Regression(77300/100000): loss=-815.1582287076068\n",
      "Losgistic Regression(77400/100000): loss=-815.6263127256332\n",
      "Losgistic Regression(77500/100000): loss=-816.0941062352993\n",
      "Losgistic Regression(77600/100000): loss=-816.5616643016571\n",
      "Losgistic Regression(77700/100000): loss=-817.0287255324845\n",
      "Losgistic Regression(77800/100000): loss=-817.495055170794\n",
      "Losgistic Regression(77900/100000): loss=-817.9606750816283\n",
      "Losgistic Regression(78000/100000): loss=-818.42578760338\n",
      "Losgistic Regression(78100/100000): loss=-818.8906576465713\n",
      "Losgistic Regression(78200/100000): loss=-819.3553949933063\n",
      "Losgistic Regression(78300/100000): loss=-819.8200896604557\n",
      "Losgistic Regression(78400/100000): loss=-820.2847370303167\n",
      "Losgistic Regression(78500/100000): loss=-820.7491775981104\n",
      "Losgistic Regression(78600/100000): loss=-821.2131563977371\n",
      "Losgistic Regression(78700/100000): loss=-821.6766062313701\n",
      "Losgistic Regression(78800/100000): loss=-822.1396504887139\n",
      "Losgistic Regression(78900/100000): loss=-822.6023161143106\n",
      "Losgistic Regression(79000/100000): loss=-823.064449959353\n",
      "Losgistic Regression(79100/100000): loss=-823.5259581155884\n",
      "Losgistic Regression(79200/100000): loss=-823.9870164621549\n",
      "Losgistic Regression(79300/100000): loss=-824.4478797724479\n",
      "Losgistic Regression(79400/100000): loss=-824.908667769704\n",
      "Losgistic Regression(79500/100000): loss=-825.3692833947309\n",
      "Losgistic Regression(79600/100000): loss=-825.8295179218467\n",
      "Losgistic Regression(79700/100000): loss=-826.2892229356198\n",
      "Losgistic Regression(79800/100000): loss=-826.7483018646509\n",
      "Losgistic Regression(79900/100000): loss=-827.2068243515844\n",
      "Losgistic Regression(80000/100000): loss=-827.6648604444716\n",
      "Losgistic Regression(80100/100000): loss=-828.1224863505336\n",
      "Losgistic Regression(80200/100000): loss=-828.5798394749983\n",
      "Losgistic Regression(80300/100000): loss=-829.0368502082357\n",
      "Losgistic Regression(80400/100000): loss=-829.4933681208807\n",
      "Losgistic Regression(80500/100000): loss=-829.9492590054239\n",
      "Losgistic Regression(80600/100000): loss=-830.4044661359466\n",
      "Losgistic Regression(80700/100000): loss=-830.8590299309814\n",
      "Losgistic Regression(80800/100000): loss=-831.3129515247433\n",
      "Losgistic Regression(80900/100000): loss=-831.7662199328153\n",
      "Losgistic Regression(81000/100000): loss=-832.2188332891159\n",
      "Losgistic Regression(81100/100000): loss=-832.6708379544026\n",
      "Losgistic Regression(81200/100000): loss=-833.1222975438278\n",
      "Losgistic Regression(81300/100000): loss=-833.5732265653068\n",
      "Losgistic Regression(81400/100000): loss=-834.0236996619383\n",
      "Losgistic Regression(81500/100000): loss=-834.4738104761861\n",
      "Losgistic Regression(81600/100000): loss=-834.9235861614264\n",
      "Losgistic Regression(81700/100000): loss=-835.3729707838933\n",
      "Losgistic Regression(81800/100000): loss=-835.821857385228\n",
      "Losgistic Regression(81900/100000): loss=-836.2702103260557\n",
      "Losgistic Regression(82000/100000): loss=-836.7181438255502\n",
      "Losgistic Regression(82100/100000): loss=-837.1657992164253\n",
      "Losgistic Regression(82200/100000): loss=-837.613234143949\n",
      "Losgistic Regression(82300/100000): loss=-838.0604629444042\n",
      "Losgistic Regression(82400/100000): loss=-838.5074450202485\n",
      "Losgistic Regression(82500/100000): loss=-838.9540894736765\n",
      "Losgistic Regression(82600/100000): loss=-839.4003273826229\n",
      "Losgistic Regression(82700/100000): loss=-839.846167727601\n",
      "Losgistic Regression(82800/100000): loss=-840.2916749006343\n",
      "Losgistic Regression(82900/100000): loss=-840.736868417601\n",
      "Losgistic Regression(83000/100000): loss=-841.1817419756443\n",
      "Losgistic Regression(83100/100000): loss=-841.6263165312573\n",
      "Losgistic Regression(83200/100000): loss=-842.0706115869168\n",
      "Losgistic Regression(83300/100000): loss=-842.5146310291018\n",
      "Losgistic Regression(83400/100000): loss=-842.958354956282\n",
      "Losgistic Regression(83500/100000): loss=-843.4017933125306\n",
      "Losgistic Regression(83600/100000): loss=-843.8449718607228\n",
      "Losgistic Regression(83700/100000): loss=-844.287841437737\n",
      "Losgistic Regression(83800/100000): loss=-844.7303330783999\n",
      "Losgistic Regression(83900/100000): loss=-845.1724425396098\n",
      "Losgistic Regression(84000/100000): loss=-845.6142111322727\n",
      "Losgistic Regression(84100/100000): loss=-846.055645400765\n",
      "Losgistic Regression(84200/100000): loss=-846.4965849446937\n",
      "Losgistic Regression(84300/100000): loss=-846.9369788977752\n",
      "Losgistic Regression(84400/100000): loss=-847.3768057983544\n",
      "Losgistic Regression(84500/100000): loss=-847.8159847482776\n",
      "Losgistic Regression(84600/100000): loss=-848.2545322321045\n",
      "Losgistic Regression(84700/100000): loss=-848.6925383297771\n",
      "Losgistic Regression(84800/100000): loss=-849.1300554945535\n",
      "Losgistic Regression(84900/100000): loss=-849.5671111836112\n",
      "Losgistic Regression(85000/100000): loss=-850.00375993519\n",
      "Losgistic Regression(85100/100000): loss=-850.4400458416426\n",
      "Losgistic Regression(85200/100000): loss=-850.8759984185389\n",
      "Losgistic Regression(85300/100000): loss=-851.3116906932096\n",
      "Losgistic Regression(85400/100000): loss=-851.7471613181181\n",
      "Losgistic Regression(85500/100000): loss=-852.1824194459988\n",
      "Losgistic Regression(85600/100000): loss=-852.6174978296634\n",
      "Losgistic Regression(85700/100000): loss=-853.0523849336984\n",
      "Losgistic Regression(85800/100000): loss=-853.4870340912898\n",
      "Losgistic Regression(85900/100000): loss=-853.921464479032\n",
      "Losgistic Regression(86000/100000): loss=-854.3557850522975\n",
      "Losgistic Regression(86100/100000): loss=-854.7899834533022\n",
      "Losgistic Regression(86200/100000): loss=-855.223957238064\n",
      "Losgistic Regression(86300/100000): loss=-855.6576790508896\n",
      "Losgistic Regression(86400/100000): loss=-856.0911858389186\n",
      "Losgistic Regression(86500/100000): loss=-856.5244607012678\n",
      "Losgistic Regression(86600/100000): loss=-856.9574219349248\n",
      "Losgistic Regression(86700/100000): loss=-857.3900178783074\n",
      "Losgistic Regression(86800/100000): loss=-857.8223002043444\n",
      "Losgistic Regression(86900/100000): loss=-858.2543475700132\n",
      "Losgistic Regression(87000/100000): loss=-858.6861947900221\n",
      "Losgistic Regression(87100/100000): loss=-859.1213306866621\n",
      "Losgistic Regression(87200/100000): loss=-859.5592871377427\n",
      "Losgistic Regression(87300/100000): loss=-859.9971248527106\n",
      "Losgistic Regression(87400/100000): loss=-860.4348730379525\n",
      "Losgistic Regression(87500/100000): loss=-860.8725338341299\n",
      "Losgistic Regression(87600/100000): loss=-861.3100895985422\n",
      "Losgistic Regression(87700/100000): loss=-861.7474733278366\n",
      "Losgistic Regression(87800/100000): loss=-862.1846156330265\n",
      "Losgistic Regression(87900/100000): loss=-862.6215020004602\n",
      "Losgistic Regression(88000/100000): loss=-863.0581685248708\n",
      "Losgistic Regression(88100/100000): loss=-863.4946475109489\n",
      "Losgistic Regression(88200/100000): loss=-863.9309438072163\n",
      "Losgistic Regression(88300/100000): loss=-864.3670529216183\n",
      "Losgistic Regression(88400/100000): loss=-864.8029220691496\n",
      "Losgistic Regression(88500/100000): loss=-865.2384406566122\n",
      "Losgistic Regression(88600/100000): loss=-865.6734997069403\n",
      "Losgistic Regression(88700/100000): loss=-866.1080136434529\n",
      "Losgistic Regression(88800/100000): loss=-866.5419637128869\n",
      "Losgistic Regression(88900/100000): loss=-866.9753850543063\n",
      "Losgistic Regression(89000/100000): loss=-867.4082740328857\n",
      "Losgistic Regression(89100/100000): loss=-867.8405964799219\n",
      "Losgistic Regression(89200/100000): loss=-868.272430634694\n",
      "Losgistic Regression(89300/100000): loss=-868.7039781030495\n",
      "Losgistic Regression(89400/100000): loss=-869.1353927613623\n",
      "Losgistic Regression(89500/100000): loss=-869.5666999855733\n",
      "Losgistic Regression(89600/100000): loss=-869.9978619603792\n",
      "Losgistic Regression(89700/100000): loss=-870.4288115150928\n",
      "Losgistic Regression(89800/100000): loss=-870.859457018606\n",
      "Losgistic Regression(89900/100000): loss=-871.2897315158222\n",
      "Losgistic Regression(90000/100000): loss=-871.7196277656932\n",
      "Losgistic Regression(90100/100000): loss=-872.1492021107439\n",
      "Losgistic Regression(90200/100000): loss=-872.5785746302149\n",
      "Losgistic Regression(90300/100000): loss=-873.0078822730128\n",
      "Losgistic Regression(90400/100000): loss=-873.4371704429972\n",
      "Losgistic Regression(90500/100000): loss=-873.866378803873\n",
      "Losgistic Regression(90600/100000): loss=-874.2954321226836\n",
      "Losgistic Regression(90700/100000): loss=-874.724264653975\n",
      "Losgistic Regression(90800/100000): loss=-875.1528056296759\n",
      "Losgistic Regression(90900/100000): loss=-875.5810282618938\n",
      "Losgistic Regression(91000/100000): loss=-876.0089786870179\n",
      "Losgistic Regression(91100/100000): loss=-876.4431781607117\n",
      "Losgistic Regression(91200/100000): loss=-876.8778982302136\n",
      "Losgistic Regression(91300/100000): loss=-877.3123683209329\n",
      "Losgistic Regression(91400/100000): loss=-877.7466029303679\n",
      "Losgistic Regression(91500/100000): loss=-878.1805999296246\n",
      "Losgistic Regression(91600/100000): loss=-878.6143470191024\n",
      "Losgistic Regression(91700/100000): loss=-879.0478618533823\n",
      "Losgistic Regression(91800/100000): loss=-879.4811666127538\n",
      "Losgistic Regression(91900/100000): loss=-879.9142378700299\n",
      "Losgistic Regression(92000/100000): loss=-880.3470004262282\n",
      "Losgistic Regression(92100/100000): loss=-880.7793851973515\n",
      "Losgistic Regression(92200/100000): loss=-881.2113921057149\n",
      "Losgistic Regression(92300/100000): loss=-881.6430933048639\n",
      "Losgistic Regression(92400/100000): loss=-882.074577545771\n",
      "Losgistic Regression(92500/100000): loss=-882.505888507617\n",
      "Losgistic Regression(92600/100000): loss=-882.9370269802719\n",
      "Losgistic Regression(92700/100000): loss=-883.3679940158389\n",
      "Losgistic Regression(92800/100000): loss=-883.79878827473\n",
      "Losgistic Regression(92900/100000): loss=-884.229383342753\n",
      "Losgistic Regression(93000/100000): loss=-884.6597372900953\n",
      "Losgistic Regression(93100/100000): loss=-885.089813112098\n",
      "Losgistic Regression(93200/100000): loss=-885.5195789926403\n",
      "Losgistic Regression(93300/100000): loss=-885.9489940809793\n",
      "Losgistic Regression(93400/100000): loss=-886.3780148890785\n",
      "Losgistic Regression(93500/100000): loss=-886.8066213444455\n",
      "Losgistic Regression(93600/100000): loss=-887.234833882016\n",
      "Losgistic Regression(93700/100000): loss=-887.6626993931987\n",
      "Losgistic Regression(93800/100000): loss=-888.0902588829897\n",
      "Losgistic Regression(93900/100000): loss=-888.5175307637641\n",
      "Losgistic Regression(94000/100000): loss=-888.9445056310012\n",
      "Losgistic Regression(94100/100000): loss=-889.3711372144896\n",
      "Losgistic Regression(94200/100000): loss=-889.7973625684878\n",
      "Losgistic Regression(94300/100000): loss=-890.2231484611341\n",
      "Losgistic Regression(94400/100000): loss=-890.6485030114679\n",
      "Losgistic Regression(94500/100000): loss=-891.0734356203736\n",
      "Losgistic Regression(94600/100000): loss=-891.4979324335733\n",
      "Losgistic Regression(94700/100000): loss=-891.9219882588452\n",
      "Losgistic Regression(94800/100000): loss=-892.3456269466242\n",
      "Losgistic Regression(94900/100000): loss=-892.7688766186359\n",
      "Losgistic Regression(95000/100000): loss=-893.191754795024\n",
      "Losgistic Regression(95100/100000): loss=-893.614274189193\n",
      "Losgistic Regression(95200/100000): loss=-894.0364495373841\n",
      "Losgistic Regression(95300/100000): loss=-894.4583023259238\n",
      "Losgistic Regression(95400/100000): loss=-894.8798484915229\n",
      "Losgistic Regression(95500/100000): loss=-895.3010732589855\n",
      "Losgistic Regression(95600/100000): loss=-895.721931041576\n",
      "Losgistic Regression(95700/100000): loss=-896.1423814892016\n",
      "Losgistic Regression(95800/100000): loss=-896.5624141223777\n",
      "Losgistic Regression(95900/100000): loss=-896.9820318086015\n",
      "Losgistic Regression(96000/100000): loss=-897.4012425370103\n",
      "Losgistic Regression(96100/100000): loss=-897.8200780258045\n",
      "Losgistic Regression(96200/100000): loss=-898.2385846602306\n",
      "Losgistic Regression(96300/100000): loss=-898.6567852900869\n",
      "Losgistic Regression(96400/100000): loss=-899.0746654332812\n",
      "Losgistic Regression(96500/100000): loss=-899.4921950221067\n",
      "Losgistic Regression(96600/100000): loss=-899.90934853096\n",
      "Losgistic Regression(96700/100000): loss=-900.3261140512318\n",
      "Losgistic Regression(96800/100000): loss=-900.7425057760308\n",
      "Losgistic Regression(96900/100000): loss=-901.1585595325331\n",
      "Losgistic Regression(97000/100000): loss=-901.5743002448379\n",
      "Losgistic Regression(97100/100000): loss=-901.9897281153999\n",
      "Losgistic Regression(97200/100000): loss=-902.4048449612617\n",
      "Losgistic Regression(97300/100000): loss=-902.8196757483244\n",
      "Losgistic Regression(97400/100000): loss=-903.2342556962693\n",
      "Losgistic Regression(97500/100000): loss=-903.648607448682\n",
      "Losgistic Regression(97600/100000): loss=-904.0627310044516\n",
      "Losgistic Regression(97700/100000): loss=-904.4766041404455\n",
      "Losgistic Regression(97800/100000): loss=-904.8901939182418\n",
      "Losgistic Regression(97900/100000): loss=-905.3034710211468\n",
      "Losgistic Regression(98000/100000): loss=-905.7164100271526\n",
      "Losgistic Regression(98100/100000): loss=-906.1289847698852\n",
      "Losgistic Regression(98200/100000): loss=-906.5411773468722\n",
      "Losgistic Regression(98300/100000): loss=-906.9529887135254\n",
      "Losgistic Regression(98400/100000): loss=-907.3644295980915\n",
      "Losgistic Regression(98500/100000): loss=-907.7755006932168\n",
      "Losgistic Regression(98600/100000): loss=-908.1861874723331\n",
      "Losgistic Regression(98700/100000): loss=-908.5964687870561\n",
      "Losgistic Regression(98800/100000): loss=-909.0063153764071\n",
      "Losgistic Regression(98900/100000): loss=-909.4156795146939\n",
      "Losgistic Regression(99000/100000): loss=-909.8245020506362\n",
      "Losgistic Regression(99100/100000): loss=-910.2327410009128\n",
      "Losgistic Regression(99200/100000): loss=-910.6403967693702\n",
      "Losgistic Regression(99300/100000): loss=-911.0475111177448\n",
      "Losgistic Regression(99400/100000): loss=-911.4541423784316\n",
      "Losgistic Regression(99500/100000): loss=-911.8603433308514\n",
      "Losgistic Regression(99600/100000): loss=-912.2661539022191\n",
      "Losgistic Regression(99700/100000): loss=-912.6715977481425\n",
      "Losgistic Regression(99800/100000): loss=-913.0766808040088\n",
      "Losgistic Regression(99900/100000): loss=-913.4814004532573\n",
      "0.933333333333 0.68212\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(300)\n",
    "# cols = np.array([ 3, 20, 22, 39])\n",
    "cols = np.arange(train_tX8.shape[1])\n",
    "L = np.linalg.eigvals(train_tX8[idxes][:, cols].T @ train_tX8[idxes][:, cols]).max()\n",
    "print(1/L)\n",
    "\n",
    "w = logistic_AGDR(train_y8[idxes], train_tX8[idxes][:, cols], gamma=1/L, \n",
    "                   max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX8[idxes][:, cols], train_y8[idxes], \n",
    "                                         cv_tX8[:, cols], cv_y8, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 149)"
      ]
     },
     "execution_count": 1126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# engineered_tX = \n",
    "no_missing_featuers = [i for i in range(30) if i not in missing_values]\n",
    "\n",
    "filled_tX           = fill_na(tX, np.median)\n",
    "log_tX              = logs_of_features(filled_tX, Features_with_outlier)\n",
    "decomposed_tX       = decompose_categorical_features(filled_tX)\n",
    "poly_tX             = build_polynomial_without_mixed_term(filled_tX[:, non_categorical], degree=5)\n",
    "\n",
    "tX8, mean_x, std_x = standardize(np.c_[poly_tX, log_tX])\n",
    "\n",
    "training_ratio = 0.9\n",
    "y8 = transform_y(y)\n",
    "train_tX8, cv_tX8, train_y8, cv_y8 = split_data(tX8, y8, training_ratio)\n",
    "cv_tX8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.02316980927e-06\n",
      "Losgistic Regression(0/100000): loss=42093.73794136925\n",
      "Losgistic Regression(100/100000): loss=5276.098018050719\n",
      "Losgistic Regression(200/100000): loss=4544.430018019927\n",
      "Losgistic Regression(300/100000): loss=4398.393022698131\n",
      "Losgistic Regression(400/100000): loss=4335.590978131491\n",
      "Losgistic Regression(500/100000): loss=4295.4604567826955\n",
      "Losgistic Regression(600/100000): loss=4253.764295775035\n",
      "Losgistic Regression(700/100000): loss=4220.572169570715\n",
      "Losgistic Regression(800/100000): loss=4188.1499211171185\n",
      "Losgistic Regression(900/100000): loss=4162.686750729248\n",
      "Losgistic Regression(1000/100000): loss=4150.683255933189\n",
      "Losgistic Regression(1100/100000): loss=4147.632724808571\n",
      "Losgistic Regression(1200/100000): loss=4147.885647637343\n",
      "Losgistic Regression(1300/100000): loss=4148.098606411034\n",
      "Losgistic Regression(1400/100000): loss=4148.27532452301\n",
      "Losgistic Regression(1500/100000): loss=4148.421051091622\n",
      "Losgistic Regression(1600/100000): loss=4148.541389458011\n",
      "Losgistic Regression(1700/100000): loss=4148.641432327882\n",
      "Losgistic Regression(1800/100000): loss=4148.725469979565\n",
      "Losgistic Regression(1900/100000): loss=4148.796984145812\n",
      "Losgistic Regression(2000/100000): loss=4148.858744791348\n",
      "Losgistic Regression(2100/100000): loss=4148.912930510838\n",
      "Losgistic Regression(2200/100000): loss=4148.961242472745\n",
      "Losgistic Regression(2300/100000): loss=4149.0050024722605\n",
      "Losgistic Regression(2400/100000): loss=4149.045233619678\n",
      "Totoal number of iterations =  2400\n",
      "Loss =  4149.04523362\n",
      "0.8191 0.80852\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "# cols = np.array([ 3, 20, 22, 39])\n",
    "cols = np.arange(train_tX8.shape[1])\n",
    "L = np.linalg.eigvals(train_tX8[idxes][:, cols].T @ train_tX8[idxes][:, cols]).max()\n",
    "print(1/L)\n",
    "\n",
    "w = logistic_AGDR(train_y8[idxes], train_tX8[idxes][:, cols], gamma=1/L, \n",
    "                   max_iters = 100000, lambda_=0.1, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX8[idxes][:, cols], train_y8[idxes], \n",
    "                                         cv_tX8[:, cols], cv_y8, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.00702259,  4.57459022,  3.72892952,  7.13106015,  4.95173598,\n",
       "        6.26408165,  3.19347592])"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[abs(w>3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we only use those features not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will lose important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_missing_featuers = [i for i in range(30) if i not in columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 58)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(tX[:, no_missing_featuers], degree = 3)\n",
    "tX6, mean_x6, std_x6 = standardize(tmp)\n",
    "y6 = transform_y(y)\n",
    "tX6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 58)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX6, cv_tX6, train_y6, cv_y6 = split_data(tX6, y6, training_ratio)\n",
    "cv_tX6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5626812771e-05\n",
      "Losgistic Regression(0/10000): loss=56635.91876811694\n",
      "Losgistic Regression(1000/10000): loss=-70010.60927836965\n",
      "Losgistic Regression(2000/10000): loss=-85440.47538549885\n",
      "Losgistic Regression(3000/10000): loss=-94429.26704526579\n",
      "Losgistic Regression(4000/10000): loss=-100765.83908769637\n",
      "Losgistic Regression(5000/10000): loss=-105695.19605704182\n",
      "Losgistic Regression(6000/10000): loss=-109808.0258851113\n",
      "Losgistic Regression(7000/10000): loss=-113411.64391236744\n",
      "Losgistic Regression(8000/10000): loss=-116671.54858691993\n",
      "Losgistic Regression(9000/10000): loss=-119669.28184539902\n",
      "0.798 0.76268\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L = np.linalg.eigvals(train_tX6[idxes].T @ train_tX6[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y6[idxes], train_tX6[idxes], gamma=1/L, \n",
    "                   max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX6[idxes], train_y6[idxes], cv_tX6, cv_y6, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1abce1f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTvpmZgXipG9mViD/DzrGz1vof/aIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d86e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ce67e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nt27h+ENbqg0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1086ab860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a240eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgRmZiXnQGBmVnL/H0IBjmMtESMGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1090f8cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in Suffering_from_outlier:\n",
    "    plt.hist(np.log(non_missing_tX[:, i]+1e-8),bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3cwsQ/8PQ1rW2VVZKOgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1085ba9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 28\n",
    "plt.hist(tX[tX[:, n] != -999, n],bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand features to reduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   0,   1,   0,   1],\n",
       "       [  2,   3,   4,   9,   8,  27],\n",
       "       [  4,   5,  16,  25,  64, 125],\n",
       "       [  6,   7,  36,  49, 216, 343],\n",
       "       [  8,   9,  64,  81, 512, 729]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_polynomial_without_mixed_term(tx, degree=2):\n",
    "    n = tx.shape[0]\n",
    "    tmp = tx\n",
    "    for i in range(2, degree+1):\n",
    "        tmp = np.c_[tmp, tx**i]\n",
    "    # The function standardize will add a column of 1s in the first column\n",
    "    return tmp\n",
    "\n",
    "build_polynomial_without_mixed_term(np.arange(10).reshape(5,2), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier 1\n",
    "\n",
    "specifications\n",
    "    - polynomial of degree 2\n",
    "    - didin't apply log to some features\n",
    "    - validation (not cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill -999 with mean/median/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 16, 19, 21, 22, 23, 26, 29]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_tX = fill_na()\n",
    "\n",
    "columns_non_negative = []\n",
    "for i in range(n_total_features):\n",
    "    if len(filled_tX[filled_tX[:, i] < 0, i]) == 0:\n",
    "        columns_non_negative.append(i)\n",
    "\n",
    "columns_non_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data used for interactive data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105f6fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive(lambda x:plot_hist(filled_tX, x), x=IntSlider(value=1, min=0, max=n_total_features-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some of the data is not allowed to perform log\n",
    "def plot_function(idx):\n",
    "    if idx in columns_non_negative:\n",
    "        plot_hist(filled_tX, idx, lambda y: np.log(y+1e-6))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "interactive(plot_function, idx=IntSlider(value=1, min=0, max=n_total_features-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 61)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(filled_tX, degree = 2)\n",
    "## We can take logs of each column Here *******************************\n",
    "tX1, mean_x1, std_x1 = standardize(tmp)\n",
    "y1 = transform_y(y)\n",
    "tX1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sepearte training sets and cross validation sets and Predict w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 61)"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX1, cv_tX1, train_y1, cv_y1 = split_data(tX1, y1, training_ratio)\n",
    "cv_tX1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_info = np.array(info)\n",
    "sizes = plot_info[:, 2]\n",
    "tr_error = plot_info[:, 0]\n",
    "te_error = plot_info[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEACAYAAACd2SCPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSimlGuX/AXK3XDI07Vs5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1244c95f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sizes, tr_error, 'r*-', sizes, te_error, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For how large the size, we can train it very accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma=0.1\n",
    "lambda=0.01\n",
    "\n",
    "|training size|iteration| acc train| acc test|\n",
    "|:--:|:---:|:---:|\n",
    "|100|1000| 0.98|0.70|\n",
    "|150|1000|0.88|0.7|\n",
    "|150|5000|0.93|0.71|\n",
    "|150|10000|0.96|0.72464|\n",
    "|150|20000|0.947|0.707|\n",
    "|150|20000|0.953333333333| 0.70664|\n",
    "|200|10000|0.885| 0.70876|\n",
    "|10000|1000|0.6934| 0.69128|\n",
    "\n",
    "we can see that with the grow of training size, the same classifier performs bad on traing set very quickly. For the same training sets, the grows of iterations can improve training performance, but can hardly influence test error.\n",
    "\n",
    "Our model suffers from bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(0/2000): loss=-4257.306958500683\n",
      "Losgistic Regression(1000/2000): loss=-16823.984342794996\n",
      "0.845 0.73416\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(200)\n",
    "w, losses = reg_logistic_regression_GD(train_y1[idxes], train_tX1[idxes], gamma=0.001, \n",
    "                       max_iters = 2000, lambda_=0, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX1[idxes], train_y1[idxes], cv_tX1, cv_y1, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the (original) feature 2, 4, 15, 18 has small feature in in it self, but their square can be large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classfier 2: Higher Order Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 16, 19, 21, 22, 23, 26, 29]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_tX = fill_na()\n",
    "\n",
    "columns_non_negative = []\n",
    "for i in range(n_total_features):\n",
    "    if len(filled_tX[filled_tX[:, i] < 0, i]) == 0:\n",
    "        columns_non_negative.append(i)\n",
    "\n",
    "columns_non_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 91)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(filled_tX, degree = 3)\n",
    "## We can take logs of each column Here *******************************\n",
    "tX2, mean_x2, std_x2 = standardize(tmp)\n",
    "y2 = transform_y(y)\n",
    "tX2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sepearte training sets and cross validation sets and Predict w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 91)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX2, cv_tX2, train_y2, cv_y2 = split_data(tX2, y2, training_ratio)\n",
    "cv_tX2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31858812606e-05\n",
      "Losgistic Regression(0/100000): loss=-1576381.4131746194\n",
      "Losgistic Regression(1000/100000): loss=-2609588.730715125\n",
      "Losgistic Regression(2000/100000): loss=-3167813.2138219867\n",
      "Losgistic Regression(3000/100000): loss=-3472116.763273852\n",
      "Losgistic Regression(4000/100000): loss=-3685713.505443715\n",
      "Losgistic Regression(5000/100000): loss=-3849144.4587358832\n",
      "Losgistic Regression(6000/100000): loss=-3981252.780542511\n",
      "Losgistic Regression(7000/100000): loss=-4091986.6941092336\n",
      "Losgistic Regression(8000/100000): loss=-4187103.3739277883\n",
      "Losgistic Regression(9000/100000): loss=-4270247.950210498\n",
      "Losgistic Regression(10000/100000): loss=-4343888.490349865\n",
      "Losgistic Regression(11000/100000): loss=-4409771.966527175\n",
      "Losgistic Regression(12000/100000): loss=-4469173.647771348\n",
      "Losgistic Regression(13000/100000): loss=-4523053.768381426\n",
      "Losgistic Regression(14000/100000): loss=-4572165.075756724\n",
      "Losgistic Regression(15000/100000): loss=-4617120.198719306\n",
      "Losgistic Regression(16000/100000): loss=-4658431.460440949\n",
      "Losgistic Regression(17000/100000): loss=-4696533.776438372\n",
      "Losgistic Regression(18000/100000): loss=-4731798.785862885\n",
      "Losgistic Regression(19000/100000): loss=-4764546.182064939\n",
      "Losgistic Regression(20000/100000): loss=-4795051.625150841\n",
      "Losgistic Regression(21000/100000): loss=-4823553.743449943\n",
      "Losgistic Regression(22000/100000): loss=-4850259.862277383\n",
      "Losgistic Regression(23000/100000): loss=-4875350.531390548\n",
      "Losgistic Regression(24000/100000): loss=-4898983.454344895\n",
      "Losgistic Regression(25000/100000): loss=-4921297.340273586\n",
      "Losgistic Regression(26000/100000): loss=-4942413.462051473\n",
      "Losgistic Regression(27000/100000): loss=-4962438.986907133\n",
      "Losgistic Regression(28000/100000): loss=-4981468.558170063\n",
      "Losgistic Regression(29000/100000): loss=-4999585.841008508\n",
      "Losgistic Regression(30000/100000): loss=-5016864.965058875\n",
      "Losgistic Regression(31000/100000): loss=-5033371.745346901\n",
      "Losgistic Regression(32000/100000): loss=-5049164.741337445\n",
      "Losgistic Regression(33000/100000): loss=-5064296.186086943\n",
      "Losgistic Regression(34000/100000): loss=-5078812.804623643\n",
      "Losgistic Regression(35000/100000): loss=-5092756.393514927\n",
      "Losgistic Regression(36000/100000): loss=-5106165.058016837\n",
      "Losgistic Regression(37000/100000): loss=-5119072.781526758\n",
      "Losgistic Regression(38000/100000): loss=-5131511.020404617\n",
      "Losgistic Regression(39000/100000): loss=-5143507.664347334\n",
      "Losgistic Regression(40000/100000): loss=-5155088.63508416\n",
      "Losgistic Regression(41000/100000): loss=-5166277.743503345\n",
      "Losgistic Regression(42000/100000): loss=-5177096.9263597075\n",
      "Losgistic Regression(43000/100000): loss=-5187566.4487711685\n",
      "Losgistic Regression(44000/100000): loss=-5197705.050099795\n",
      "Losgistic Regression(45000/100000): loss=-5207530.119119958\n",
      "Losgistic Regression(46000/100000): loss=-5217057.79366191\n",
      "Losgistic Regression(47000/100000): loss=-5226303.074583019\n",
      "Losgistic Regression(48000/100000): loss=-5235279.921821131\n",
      "Losgistic Regression(49000/100000): loss=-5244001.340726925\n",
      "Losgistic Regression(50000/100000): loss=-5252479.46022717\n",
      "Losgistic Regression(51000/100000): loss=-5260725.603928999\n",
      "Losgistic Regression(52000/100000): loss=-5268750.354984655\n",
      "Losgistic Regression(53000/100000): loss=-5276563.615347583\n",
      "Losgistic Regression(54000/100000): loss=-5284174.659932345\n",
      "Losgistic Regression(55000/100000): loss=-5291592.18611666\n",
      "Losgistic Regression(56000/100000): loss=-5298824.358974835\n",
      "Losgistic Regression(57000/100000): loss=-5305878.852597846\n",
      "Losgistic Regression(58000/100000): loss=-5312762.887828574\n",
      "Losgistic Regression(59000/100000): loss=-5319483.252422188\n",
      "Losgistic Regression(60000/100000): loss=-5326046.3873747615\n",
      "Losgistic Regression(61000/100000): loss=-5332458.353504039\n",
      "Losgistic Regression(62000/100000): loss=-5338724.852569162\n",
      "Losgistic Regression(63000/100000): loss=-5344851.300808033\n",
      "Losgistic Regression(64000/100000): loss=-5350842.828958583\n",
      "Losgistic Regression(65000/100000): loss=-5356704.300721952\n",
      "Losgistic Regression(66000/100000): loss=-5362440.331261646\n",
      "Losgistic Regression(67000/100000): loss=-5368055.304237758\n",
      "Losgistic Regression(68000/100000): loss=-5373553.387373273\n",
      "Losgistic Regression(69000/100000): loss=-5378938.546697936\n",
      "Losgistic Regression(70000/100000): loss=-5384214.559610502\n",
      "Losgistic Regression(71000/100000): loss=-5389385.026880057\n",
      "Losgistic Regression(72000/100000): loss=-5394453.383690101\n",
      "Losgistic Regression(73000/100000): loss=-5399422.909817011\n",
      "Losgistic Regression(74000/100000): loss=-5404296.739022105\n",
      "Losgistic Regression(75000/100000): loss=-5409077.867729876\n",
      "Losgistic Regression(76000/100000): loss=-5413769.163055664\n",
      "Losgistic Regression(77000/100000): loss=-5418373.370240863\n",
      "Losgistic Regression(78000/100000): loss=-5422893.119547225\n",
      "Losgistic Regression(79000/100000): loss=-5427330.9326570975\n",
      "Losgistic Regression(80000/100000): loss=-5431689.228621795\n",
      "Losgistic Regression(81000/100000): loss=-5435970.329396363\n",
      "Losgistic Regression(82000/100000): loss=-5440176.464994871\n",
      "Losgistic Regression(83000/100000): loss=-5444309.778297924\n",
      "Losgistic Regression(84000/100000): loss=-5448372.329540389\n",
      "Losgistic Regression(85000/100000): loss=-5452366.100504982\n",
      "Losgistic Regression(86000/100000): loss=-5456293.185446158\n",
      "Losgistic Regression(87000/100000): loss=-5460155.1175794825\n",
      "Losgistic Regression(88000/100000): loss=-5463953.779596244\n",
      "Losgistic Regression(89000/100000): loss=-5467690.875100382\n",
      "Losgistic Regression(90000/100000): loss=-5471368.045793549\n",
      "Losgistic Regression(91000/100000): loss=-5474986.876479005\n",
      "Losgistic Regression(92000/100000): loss=-5478548.896940535\n",
      "Losgistic Regression(93000/100000): loss=-5482055.584147429\n",
      "Losgistic Regression(94000/100000): loss=-5485508.364485841\n",
      "Losgistic Regression(95000/100000): loss=-5488908.615921628\n",
      "Losgistic Regression(96000/100000): loss=-5492257.670073381\n",
      "Losgistic Regression(97000/100000): loss=-5495556.814190101\n",
      "Losgistic Regression(98000/100000): loss=-5498807.293032587\n",
      "Losgistic Regression(99000/100000): loss=-5502010.310660532\n",
      "0.8172 0.80656\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L=np.linalg.eigvals(train_tX2[idxes].T @ train_tX2[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y2[idxes], train_tX2[idxes], gamma=1/L, \n",
    "                       max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX2[idxes], train_y2[idxes], cv_tX2, cv_y2, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.6932772662e-06\n",
      "Losgistic Regression(0/100000): loss=434457.21842224855\n",
      "Losgistic Regression(1000/100000): loss=-10234622.776920963\n",
      "Losgistic Regression(2000/100000): loss=-12230873.927219868\n",
      "Losgistic Regression(3000/100000): loss=-13370184.385959959\n",
      "Losgistic Regression(4000/100000): loss=-14177368.686676051\n",
      "Losgistic Regression(5000/100000): loss=-14804224.395295477\n",
      "Losgistic Regression(6000/100000): loss=-15313039.706620015\n",
      "Losgistic Regression(7000/100000): loss=-15737437.263241317\n",
      "Losgistic Regression(8000/100000): loss=-16098504.39866479\n",
      "Losgistic Regression(9000/100000): loss=-16410311.024095053\n",
      "Losgistic Regression(10000/100000): loss=-16682700.350381702\n",
      "Losgistic Regression(11000/100000): loss=-16922849.827691004\n",
      "Losgistic Regression(12000/100000): loss=-17136175.98913687\n",
      "Losgistic Regression(13000/100000): loss=-17326879.368117437\n",
      "Losgistic Regression(14000/100000): loss=-17498293.407778494\n",
      "Losgistic Regression(15000/100000): loss=-17653107.48084559\n",
      "Losgistic Regression(16000/100000): loss=-17793529.65548029\n",
      "Losgistic Regression(17000/100000): loss=-17921393.395987857\n",
      "Losgistic Regression(18000/100000): loss=-18038237.25761377\n",
      "Losgistic Regression(19000/100000): loss=-18145364.019332215\n",
      "Losgistic Regression(20000/100000): loss=-18243884.099737927\n",
      "Losgistic Regression(21000/100000): loss=-18334751.18453073\n",
      "Losgistic Regression(22000/100000): loss=-18418786.030906536\n",
      "Losgistic Regression(23000/100000): loss=-18496700.57322757\n",
      "Losgistic Regression(24000/100000): loss=-18569113.75459297\n",
      "Losgistic Regression(25000/100000): loss=-18636565.985747747\n",
      "Losgistic Regression(26000/100000): loss=-18699530.91251867\n",
      "Losgistic Regression(27000/100000): loss=-18758425.16131391\n",
      "Losgistic Regression(28000/100000): loss=-18813617.131521203\n",
      "Losgistic Regression(29000/100000): loss=-18865433.07428224\n",
      "Losgistic Regression(30000/100000): loss=-18914163.557746664\n",
      "Losgistic Regression(31000/100000): loss=-18960068.0583519\n",
      "Losgistic Regression(32000/100000): loss=-19003378.944174524\n",
      "Losgistic Regression(33000/100000): loss=-19044304.78913666\n",
      "Losgistic Regression(34000/100000): loss=-19083033.163536843\n",
      "Losgistic Regression(35000/100000): loss=-19119733.020641763\n",
      "Losgistic Regression(36000/100000): loss=-19154556.770490967\n",
      "Losgistic Regression(37000/100000): loss=-19187642.10459073\n",
      "Losgistic Regression(38000/100000): loss=-19219113.613513943\n",
      "Losgistic Regression(39000/100000): loss=-19249083.985745035\n",
      "Losgistic Regression(40000/100000): loss=-19277656.17267111\n",
      "Losgistic Regression(41000/100000): loss=-19304923.26221315\n",
      "Losgistic Regression(42000/100000): loss=-19330970.35926909\n",
      "Losgistic Regression(43000/100000): loss=-19355875.180184305\n",
      "Losgistic Regression(44000/100000): loss=-19379708.859251298\n",
      "Losgistic Regression(45000/100000): loss=-19402536.647963177\n",
      "Losgistic Regression(46000/100000): loss=-19424418.52547581\n",
      "Losgistic Regression(47000/100000): loss=-19445409.734691165\n",
      "Losgistic Regression(48000/100000): loss=-19465561.254464686\n",
      "Losgistic Regression(49000/100000): loss=-19484920.21633419\n",
      "Losgistic Regression(50000/100000): loss=-19503530.27297856\n",
      "Losgistic Regression(51000/100000): loss=-19521431.924608763\n",
      "Losgistic Regression(52000/100000): loss=-19538662.808669314\n",
      "Losgistic Regression(53000/100000): loss=-19555257.95751001\n",
      "Losgistic Regression(54000/100000): loss=-19571250.028062366\n",
      "Losgistic Regression(55000/100000): loss=-19586669.621781625\n",
      "Losgistic Regression(56000/100000): loss=-19601544.96572731\n",
      "Losgistic Regression(57000/100000): loss=-19615902.92292718\n",
      "Losgistic Regression(58000/100000): loss=-19629768.48523869\n",
      "Losgistic Regression(59000/100000): loss=-19643165.110032324\n",
      "Losgistic Regression(60000/100000): loss=-19656115.52071395\n",
      "Losgistic Regression(61000/100000): loss=-19668639.440809503\n",
      "Losgistic Regression(62000/100000): loss=-19680756.558475815\n",
      "Losgistic Regression(63000/100000): loss=-19692485.377999134\n",
      "Losgistic Regression(64000/100000): loss=-19703843.2780626\n",
      "Losgistic Regression(65000/100000): loss=-19714846.658731826\n",
      "Losgistic Regression(66000/100000): loss=-19725511.017401554\n",
      "Losgistic Regression(67000/100000): loss=-19735851.0127058\n",
      "Losgistic Regression(68000/100000): loss=-19745880.52210366\n",
      "Losgistic Regression(69000/100000): loss=-19755612.69436495\n",
      "Losgistic Regression(70000/100000): loss=-19765059.997607972\n",
      "Losgistic Regression(71000/100000): loss=-19774234.26339828\n",
      "Losgistic Regression(72000/100000): loss=-19783146.727319214\n",
      "Losgistic Regression(73000/100000): loss=-19791808.066357877\n",
      "Losgistic Regression(74000/100000): loss=-19800228.433412656\n",
      "Losgistic Regression(75000/100000): loss=-19808417.489186414\n",
      "Losgistic Regression(76000/100000): loss=-19816384.43170213\n",
      "Losgistic Regression(77000/100000): loss=-19824138.023650773\n",
      "Losgistic Regression(78000/100000): loss=-19831686.617763985\n",
      "Losgistic Regression(79000/100000): loss=-19839038.180371005\n",
      "Losgistic Regression(80000/100000): loss=-19846200.40784303\n",
      "Losgistic Regression(81000/100000): loss=-19853180.31290167\n",
      "Losgistic Regression(82000/100000): loss=-19859985.01820347\n",
      "Losgistic Regression(83000/100000): loss=-19866621.119001914\n",
      "Losgistic Regression(84000/100000): loss=-19873094.94309165\n",
      "Losgistic Regression(85000/100000): loss=-19879412.547330413\n",
      "Losgistic Regression(86000/100000): loss=-19885579.728835315\n",
      "Losgistic Regression(87000/100000): loss=-19891602.03770426\n",
      "Losgistic Regression(88000/100000): loss=-19897484.78940485\n",
      "Losgistic Regression(89000/100000): loss=-19903233.07653867\n",
      "Losgistic Regression(90000/100000): loss=-19908851.77993583\n",
      "Losgistic Regression(91000/100000): loss=-19914345.57909794\n",
      "Losgistic Regression(92000/100000): loss=-19919718.96202289\n",
      "Losgistic Regression(93000/100000): loss=-19924976.2344513\n",
      "Losgistic Regression(94000/100000): loss=-19930121.528572276\n",
      "Losgistic Regression(95000/100000): loss=-19935158.811225545\n",
      "Losgistic Regression(96000/100000): loss=-19940091.891634226\n",
      "Losgistic Regression(97000/100000): loss=-19944924.428700306\n",
      "Losgistic Regression(98000/100000): loss=-19949659.93789192\n",
      "Losgistic Regression(99000/100000): loss=-19954301.79774978\n",
      "0.8108 0.8062\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L=np.linalg.eigvals(train_tX2[idxes].T @ train_tX2[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y2[idxes], train_tX2[idxes], gamma=1/L, \n",
    "                       max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX2[idxes], train_y2[idxes], cv_tX2, cv_y2, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 10, 13, 16,  0,  2,  8,  9, 13, 19,  0,  1,  3,  9, 13, 19,\n",
       "        0])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(91)[abs(w)>1]%30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   9.99999500e-07,   9.99999000e-13,\n",
       "          9.99998500e-19],\n",
       "       [  2.00000000e+00,   3.00000000e+00,   4.00000000e+00,\n",
       "          9.00000000e+00,   1.09861262e+00,   1.20694969e+00,\n",
       "          1.32597017e+00],\n",
       "       [  4.00000000e+00,   5.00000000e+00,   1.60000000e+01,\n",
       "          2.50000000e+01,   1.60943811e+00,   2.59029104e+00,\n",
       "          4.16891312e+00],\n",
       "       [  6.00000000e+00,   7.00000000e+00,   3.60000000e+01,\n",
       "          4.90000000e+01,   1.94591029e+00,   3.78656686e+00,\n",
       "          7.36831943e+00],\n",
       "       [  8.00000000e+00,   9.00000000e+00,   6.40000000e+01,\n",
       "          8.10000000e+01,   2.19722469e+00,   4.82779633e+00,\n",
       "          1.06077533e+01]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_polynomial_without_mixed_term_with_log(tx, log_cols, degree=2):\n",
    "    n = tx.shape[0]\n",
    "    tmp = tx\n",
    "    for i in range(2, degree+1):\n",
    "        tmp = np.c_[tmp, tx**i]\n",
    "        \n",
    "    tmp = np.c_[tmp, np.log(tx[:, log_cols] + 1e-6)]\n",
    "    tmp = np.c_[tmp, (np.log(tx[:, log_cols] + 1e-6))**2]\n",
    "    tmp = np.c_[tmp, (np.log(tx[:, log_cols] + 1e-6))**3]\n",
    "    return tmp\n",
    "\n",
    "build_polynomial_without_mixed_term_with_log(np.arange(10).reshape(5,2), [1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 133)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term_with_log(filled_tX, log_features, degree = 4)\n",
    "tX3, mean_x3, std_x3 = standardize(tmp)\n",
    "y3 = transform_y(y)\n",
    "tX3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 133)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX3, cv_tX3, train_y3, cv_y3 = split_data(tX3, y3, training_ratio)\n",
    "cv_tX3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18430305201e-05\n",
      "Losgistic Regression(0/10000): loss=-2611163.012880917\n",
      "Losgistic Regression(1000/10000): loss=-3936990.4564067796\n",
      "Losgistic Regression(2000/10000): loss=-4313944.990193981\n",
      "Losgistic Regression(3000/10000): loss=-4530935.028052804\n",
      "Losgistic Regression(4000/10000): loss=-4686759.265161883\n",
      "Losgistic Regression(5000/10000): loss=-4806968.908878522\n",
      "Losgistic Regression(6000/10000): loss=-4904122.84751413\n",
      "Losgistic Regression(7000/10000): loss=-4985372.565744881\n",
      "Losgistic Regression(8000/10000): loss=-5055214.850281095\n",
      "Losgistic Regression(9000/10000): loss=-5116816.102477511\n",
      "0.813 0.80572\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L=np.linalg.eigvals(train_tX3[idxes].T @ train_tX3[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y3[idxes], train_tX3[idxes], gamma=0.0001, \n",
    "                       max_iters = 10000, lambda_=0.00, regularizor=regularizor_ridge)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX3[idxes], train_y3[idxes], cv_tX3, cv_y3, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For higher dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 115)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term_with_log(filled_tX, log_features, degree = 3)\n",
    "tX4, mean_x4, std_x4 = standardize(tmp)\n",
    "y4 = transform_y(y)\n",
    "tX4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 115)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX4, cv_tX4, train_y4, cv_y4 = split_data(tX4, y4, training_ratio)\n",
    "cv_tX4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025368797602\n",
      "Losgistic Regression(0/10000): loss=-893711.0623600627\n",
      "Losgistic Regression(1000/10000): loss=-944014.4622320133\n",
      "Losgistic Regression(2000/10000): loss=-1033972.7961426789\n",
      "Losgistic Regression(3000/10000): loss=-1083628.4283881453\n",
      "Losgistic Regression(4000/10000): loss=-1114844.76407547\n",
      "Losgistic Regression(5000/10000): loss=-1136101.0571259223\n",
      "Losgistic Regression(6000/10000): loss=-1151420.8972547455\n",
      "Losgistic Regression(7000/10000): loss=-1162947.157179648\n",
      "Losgistic Regression(8000/10000): loss=-1171919.5317238276\n",
      "Losgistic Regression(9000/10000): loss=-1179101.937071516\n",
      "0.83 0.80236\n",
      "Losgistic Regression(0/10000): loss=-888225.356995657\n",
      "Losgistic Regression(1000/10000): loss=-914899.2770507383\n",
      "Losgistic Regression(2000/10000): loss=-951666.3112675188\n",
      "Losgistic Regression(3000/10000): loss=-972020.5028958153\n",
      "Losgistic Regression(4000/10000): loss=-986281.9179660012\n",
      "Losgistic Regression(5000/10000): loss=-997211.5103137465\n",
      "Losgistic Regression(6000/10000): loss=-1006039.1417347568\n",
      "Losgistic Regression(7000/10000): loss=-1013453.2140185822\n",
      "Losgistic Regression(8000/10000): loss=-1019863.7194260703\n",
      "Losgistic Regression(9000/10000): loss=-1025526.9867476765\n",
      "0.8255 0.79928\n",
      "Losgistic Regression(0/10000): loss=-187414.56318102175\n",
      "Losgistic Regression(1000/10000): loss=-910476.2669747613\n",
      "Losgistic Regression(2000/10000): loss=-996468.4181271823\n",
      "Losgistic Regression(3000/10000): loss=-1026000.330118845\n",
      "Losgistic Regression(4000/10000): loss=-1043636.8692616977\n",
      "Losgistic Regression(5000/10000): loss=-1056548.6702058925\n",
      "Losgistic Regression(6000/10000): loss=-1067113.3076216786\n",
      "Losgistic Regression(7000/10000): loss=-1076316.6124512162\n",
      "Losgistic Regression(8000/10000): loss=-1084619.7870569746\n",
      "Losgistic Regression(9000/10000): loss=-1092261.81118189\n",
      "0.8165 0.79644\n",
      "Losgistic Regression(0/10000): loss=-842663.2328824318\n",
      "Losgistic Regression(1000/10000): loss=-1047295.7661965976\n",
      "Losgistic Regression(2000/10000): loss=-1079830.0264446877\n",
      "Losgistic Regression(3000/10000): loss=-1097823.3417883508\n",
      "Losgistic Regression(4000/10000): loss=-1109819.1619287694\n",
      "Losgistic Regression(5000/10000): loss=-1118843.2123723465\n",
      "Losgistic Regression(6000/10000): loss=-1126232.0238309514\n",
      "Losgistic Regression(7000/10000): loss=-1132621.3044755156\n",
      "Losgistic Regression(8000/10000): loss=-1138341.6483244142\n",
      "Losgistic Regression(9000/10000): loss=-1143581.6129379468\n",
      "0.8255 0.79868\n",
      "Losgistic Regression(0/10000): loss=-1117242.3208371364\n",
      "Losgistic Regression(1000/10000): loss=-1098595.7163759307\n",
      "Losgistic Regression(2000/10000): loss=-1125809.8024460913\n",
      "Losgistic Regression(3000/10000): loss=-1145343.612964509\n",
      "Losgistic Regression(4000/10000): loss=-1161280.2159424936\n",
      "Losgistic Regression(5000/10000): loss=-1174947.0665290602\n",
      "Losgistic Regression(6000/10000): loss=-1186946.4629059574\n",
      "Losgistic Regression(7000/10000): loss=-1197634.9089258444\n",
      "Losgistic Regression(8000/10000): loss=-1207261.5873182365\n",
      "Losgistic Regression(9000/10000): loss=-1216015.444156253\n",
      "0.831 0.80144\n",
      "Losgistic Regression(0/10000): loss=-643959.0156982732\n",
      "Losgistic Regression(1000/10000): loss=-848835.6779847938\n",
      "Losgistic Regression(2000/10000): loss=-855704.3382310825\n",
      "Losgistic Regression(3000/10000): loss=-851982.8594892009\n",
      "Losgistic Regression(4000/10000): loss=-863694.2728537429\n",
      "Losgistic Regression(5000/10000): loss=-871568.5971944006\n",
      "Losgistic Regression(6000/10000): loss=-876234.9026945467\n",
      "Losgistic Regression(7000/10000): loss=-879035.2786650405\n",
      "Losgistic Regression(8000/10000): loss=-880842.3302220242\n",
      "Losgistic Regression(9000/10000): loss=-882136.956801291\n",
      "0.8355 0.79776\n",
      "Losgistic Regression(0/10000): loss=-611656.6876125119\n",
      "Losgistic Regression(1000/10000): loss=-942485.5925478501\n",
      "Losgistic Regression(2000/10000): loss=-949216.2162596553\n",
      "Losgistic Regression(3000/10000): loss=-959263.9295640655\n",
      "Losgistic Regression(4000/10000): loss=-969699.1326666051\n",
      "Losgistic Regression(5000/10000): loss=-979632.7007027564\n",
      "Losgistic Regression(6000/10000): loss=-988748.808773844\n",
      "Losgistic Regression(7000/10000): loss=-997175.1088148319\n",
      "Losgistic Regression(8000/10000): loss=-1005486.9257729569\n",
      "Losgistic Regression(9000/10000): loss=-1013761.0200689401\n",
      "0.8275 0.79768\n",
      "Losgistic Regression(0/10000): loss=-1140339.7282607036\n",
      "Losgistic Regression(1000/10000): loss=-1369592.4545054086\n",
      "Losgistic Regression(2000/10000): loss=-1369830.5020148335\n",
      "Losgistic Regression(3000/10000): loss=-1373978.547302951\n",
      "Losgistic Regression(4000/10000): loss=-1380831.6398379337\n",
      "Losgistic Regression(5000/10000): loss=-1388917.1492501264\n",
      "Losgistic Regression(6000/10000): loss=-1397399.8462677891\n",
      "Losgistic Regression(7000/10000): loss=-1405842.2714541412\n",
      "Losgistic Regression(8000/10000): loss=-1414019.987908485\n",
      "Losgistic Regression(9000/10000): loss=-1421820.6718661443\n",
      "0.837 0.80384\n",
      "Losgistic Regression(0/10000): loss=-852138.2869366638\n",
      "Losgistic Regression(1000/10000): loss=-1140690.0437953426\n",
      "Losgistic Regression(2000/10000): loss=-1165826.4970581883\n",
      "Losgistic Regression(3000/10000): loss=-1181301.0602005483\n",
      "Losgistic Regression(4000/10000): loss=-1191602.564569472\n",
      "Losgistic Regression(5000/10000): loss=-1201917.7019693856\n",
      "Losgistic Regression(6000/10000): loss=-1209903.2126902395\n",
      "Losgistic Regression(7000/10000): loss=-1215820.7919976818\n",
      "Losgistic Regression(8000/10000): loss=-1220097.5045952047\n",
      "Losgistic Regression(9000/10000): loss=-1222974.4896819138\n",
      "0.843 0.7962\n",
      "Losgistic Regression(0/10000): loss=-901449.4085884681\n",
      "Losgistic Regression(1000/10000): loss=-849347.3003788169\n",
      "Losgistic Regression(2000/10000): loss=-850472.9386638028\n",
      "Losgistic Regression(3000/10000): loss=-853298.5861760261\n",
      "Losgistic Regression(4000/10000): loss=-855070.8852689307\n",
      "Losgistic Regression(5000/10000): loss=-856211.637571377\n",
      "Losgistic Regression(6000/10000): loss=-857233.9460180368\n",
      "Losgistic Regression(7000/10000): loss=-858392.121052195\n",
      "Losgistic Regression(8000/10000): loss=-859730.2400875996\n",
      "Losgistic Regression(9000/10000): loss=-861192.9838514352\n",
      "0.8205 0.80228\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(200)\n",
    "L=np.linalg.eigvals(train_tX4[idxes].T @ train_tX4[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w = np.random.randn(train_tX4.shape[1]) * 0.1\n",
    "maximum_size = train_y4.shape[0]\n",
    "\n",
    "for i in range(10):\n",
    "    idxes = np.random.randint(0, maximum_size, 2000)\n",
    "    w, losses = reg_logistic_regression_SGD(train_y4[idxes], train_tX4[idxes], gamma=1/L, \n",
    "                       max_iters = 10000, lambda_=0.0001, regularizor=regularizor_lasso, w0=w)\n",
    "\n",
    "    tr_acc, te_acc = prediction_and_accuracy(train_tX4[idxes], train_y4[idxes], cv_tX4, cv_y4, w)\n",
    "    print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205 0.80228\n"
     ]
    }
   ],
   "source": [
    "tr_acc, te_acc = prediction_and_accuracy(train_tX4[idxes], train_y4[idxes], cv_tX4, cv_y4, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Higher Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 187)"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term_with_log(filled_tX, log_features, degree = 5)\n",
    "tX5, mean_x5, std_x5 = standardize(tmp)\n",
    "y5 = transform_y(y)\n",
    "tX5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 187)"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX5, cv_tX5, train_y5, cv_y5 = split_data(tX5, y5, training_ratio)\n",
    "cv_tX5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23143465589e-05\n",
      "Losgistic Regression(0/20000): loss=-152948.4985915193\n",
      "Losgistic Regression(1000/20000): loss=-558222.8949306096\n",
      "Losgistic Regression(2000/20000): loss=-641129.1642222997\n",
      "Losgistic Regression(3000/20000): loss=-694583.4965614841\n",
      "Losgistic Regression(4000/20000): loss=-733162.1216511249\n",
      "Losgistic Regression(5000/20000): loss=-763211.2116422988\n",
      "Losgistic Regression(6000/20000): loss=-787804.3128164957\n",
      "Losgistic Regression(7000/20000): loss=-808619.5440130396\n",
      "Losgistic Regression(8000/20000): loss=-826656.972967146\n",
      "Losgistic Regression(9000/20000): loss=-842556.2018722652\n",
      "Losgistic Regression(10000/20000): loss=-856751.7948597504\n",
      "Losgistic Regression(11000/20000): loss=-869553.7814276543\n",
      "Losgistic Regression(12000/20000): loss=-881193.2501136299\n",
      "Losgistic Regression(13000/20000): loss=-891847.882410691\n",
      "Losgistic Regression(14000/20000): loss=-901657.8873610676\n",
      "Losgistic Regression(15000/20000): loss=-910736.4680695792\n",
      "Losgistic Regression(16000/20000): loss=-919176.2815885241\n",
      "Losgistic Regression(17000/20000): loss=-927054.5826593324\n",
      "Losgistic Regression(18000/20000): loss=-934436.5140949821\n",
      "Losgistic Regression(19000/20000): loss=-941377.1463648106\n",
      "0.816 0.78864\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(2000)\n",
    "L = np.linalg.eigvals(train_tX5[idxes].T @ train_tX5[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y5[idxes], train_tX5[idxes], gamma=1/L, \n",
    "                   max_iters = 20000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX5[idxes], train_y5[idxes], cv_tX5, cv_y5, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we only use those features not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will lose important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 58)"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(tX[:, no_missing_featuers], degree = 3)\n",
    "tX6, mean_x6, std_x6 = standardize(tmp)\n",
    "y6 = transform_y(y)\n",
    "tX6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 58)"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX6, cv_tX6, train_y6, cv_y6 = split_data(tX6, y6, training_ratio)\n",
    "cv_tX6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.12022288 -8.43261199 -6.13306021  5.49985935]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3, 20, 22, 39])"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 5\n",
    "print(w[abs(w)>threshold])\n",
    "np.arange(len(w))[abs(w)>threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.38726863049e-06\n",
      "Losgistic Regression(0/10000): loss=172062.31224921555\n",
      "Losgistic Regression(1000/10000): loss=-906856.8877039328\n",
      "Losgistic Regression(2000/10000): loss=-2222880.6517023114\n",
      "Losgistic Regression(3000/10000): loss=-2897843.0219437107\n",
      "Losgistic Regression(4000/10000): loss=-3356243.367217049\n",
      "Losgistic Regression(5000/10000): loss=-3704817.9229270555\n",
      "Losgistic Regression(6000/10000): loss=-3985748.037510811\n",
      "Losgistic Regression(7000/10000): loss=-4219904.215027729\n",
      "Losgistic Regression(8000/10000): loss=-4419567.694428677\n",
      "Losgistic Regression(9000/10000): loss=-4592789.371367343\n",
      "0.7818 0.77852\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "# cols = np.array([ 3, 20, 22, 39])\n",
    "cols = np.arange(train_tX6.shape[1])\n",
    "L = np.linalg.eigvals(train_tX6[idxes][:, cols].T @ train_tX6[idxes][:, cols]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y6[idxes], train_tX6[idxes][:, cols], gamma=1/L, \n",
    "                   max_iters = 10000, lambda_=0.000, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX6[idxes][:, cols], train_y6[idxes], \n",
    "                                         cv_tX6[:, cols], cv_y6, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.38726863049e-06\n",
      "Losgistic Regression(0/100000): loss=-35582399.471180476\n",
      "Losgistic Regression(1000/100000): loss=-7323507.9188665375\n",
      "Totoal number of iterations =  1865\n",
      "Loss =  -8238216.81808\n",
      "0.7911 0.78972\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "# cols = np.array([ 3, 20, 22, 39])\n",
    "cols = np.arange(train_tX6.shape[1])\n",
    "L = np.linalg.eigvals(train_tX6[idxes][:, cols].T @ train_tX6[idxes][:, cols]).max()\n",
    "print(1/L)\n",
    "\n",
    "w = logistic_AGDR(train_y6[idxes], train_tX6[idxes][:, cols], gamma=1/L, \n",
    "                   max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX6[idxes][:, cols], train_y6[idxes], \n",
    "                                         cv_tX6[:, cols], cv_y6, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_missing_featuers = [i for i in range(30) if i not in columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_missing_tX = tX[status_tX == 0, :]\n",
    "missing_tX = tX[status_tX != 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 121)"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(fill_na(tX, np.median), degree=4)\n",
    "tX7, mean_x7, std_x7 = standardize(tmp)\n",
    "y7 = transform_y(y)\n",
    "tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 121)"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX7, cv_tX7, train_y7, cv_y7 = split_data(tX7, y7, training_ratio)\n",
    "cv_tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29134200029e-05\n",
      "Losgistic Regression(0/100000): loss=-1864570.8664358312\n",
      "Losgistic Regression(1000/100000): loss=-3643706.596277618\n",
      "Losgistic Regression(2000/100000): loss=-4075325.443160949\n",
      "Losgistic Regression(3000/100000): loss=-4287686.346307418\n",
      "Losgistic Regression(4000/100000): loss=-4422673.567931138\n",
      "Losgistic Regression(5000/100000): loss=-4520877.584452401\n",
      "Losgistic Regression(6000/100000): loss=-4598137.39963385\n",
      "Losgistic Regression(7000/100000): loss=-4662064.837419295\n",
      "Losgistic Regression(8000/100000): loss=-4716886.13889026\n",
      "Losgistic Regression(9000/100000): loss=-4765307.290995987\n",
      "Losgistic Regression(10000/100000): loss=-4809598.738261102\n",
      "Losgistic Regression(11000/100000): loss=-4851464.061950497\n",
      "Losgistic Regression(12000/100000): loss=-4890956.69237179\n",
      "Losgistic Regression(13000/100000): loss=-4927619.743856777\n",
      "Losgistic Regression(14000/100000): loss=-4961382.561498515\n",
      "Losgistic Regression(15000/100000): loss=-4992435.593707586\n",
      "Losgistic Regression(16000/100000): loss=-5021044.3438392915\n",
      "Losgistic Regression(17000/100000): loss=-5047466.9535095645\n",
      "Losgistic Regression(18000/100000): loss=-5071931.429541111\n",
      "Losgistic Regression(19000/100000): loss=-5094637.569716239\n",
      "Losgistic Regression(20000/100000): loss=-5115765.259024078\n",
      "Losgistic Regression(21000/100000): loss=-5135480.828422836\n",
      "Losgistic Regression(22000/100000): loss=-5153939.896351397\n",
      "Losgistic Regression(23000/100000): loss=-5171287.4732818855\n",
      "Losgistic Regression(24000/100000): loss=-5187657.084715055\n",
      "Losgistic Regression(25000/100000): loss=-5203169.620982077\n",
      "Losgistic Regression(26000/100000): loss=-5217932.40559449\n",
      "Losgistic Regression(27000/100000): loss=-5232038.745916452\n",
      "Losgistic Regression(28000/100000): loss=-5245567.946259948\n",
      "Losgistic Regression(29000/100000): loss=-5258586.0049151555\n",
      "Losgistic Regression(30000/100000): loss=-5271146.967419196\n",
      "Losgistic Regression(31000/100000): loss=-5283294.772769553\n",
      "Losgistic Regression(32000/100000): loss=-5295065.298615652\n",
      "Losgistic Regression(33000/100000): loss=-5306488.2961415425\n",
      "Losgistic Regression(34000/100000): loss=-5317589.000192996\n",
      "Losgistic Regression(35000/100000): loss=-5328389.357797124\n",
      "Losgistic Regression(36000/100000): loss=-5338908.803456933\n",
      "Losgistic Regression(37000/100000): loss=-5349164.85147364\n",
      "Losgistic Regression(38000/100000): loss=-5359173.448142794\n",
      "Losgistic Regression(39000/100000): loss=-5368949.190376322\n",
      "Losgistic Regression(40000/100000): loss=-5378505.483709117\n",
      "Losgistic Regression(41000/100000): loss=-5387854.661161445\n",
      "Losgistic Regression(42000/100000): loss=-5397008.08059066\n",
      "Losgistic Regression(43000/100000): loss=-5405976.20957648\n",
      "Losgistic Regression(44000/100000): loss=-5414768.701932477\n",
      "Losgistic Regression(45000/100000): loss=-5423394.474478419\n",
      "Losgistic Regression(46000/100000): loss=-5431861.766656872\n",
      "Losgistic Regression(47000/100000): loss=-5440178.17512868\n",
      "Losgistic Regression(48000/100000): loss=-5448350.749920648\n",
      "Losgistic Regression(49000/100000): loss=-5456386.028629373\n",
      "Losgistic Regression(50000/100000): loss=-5464290.080584549\n",
      "Losgistic Regression(51000/100000): loss=-5472068.548131858\n",
      "Losgistic Regression(52000/100000): loss=-5479726.684264392\n",
      "Losgistic Regression(53000/100000): loss=-5487269.386710359\n",
      "Losgistic Regression(54000/100000): loss=-5494701.248541947\n",
      "Losgistic Regression(55000/100000): loss=-5502026.980492024\n",
      "Losgistic Regression(56000/100000): loss=-5509250.122016809\n",
      "Losgistic Regression(57000/100000): loss=-5516374.60887975\n",
      "Losgistic Regression(58000/100000): loss=-5523403.684532766\n",
      "Losgistic Regression(59000/100000): loss=-5530340.766697148\n",
      "Losgistic Regression(60000/100000): loss=-5537188.989474765\n",
      "Losgistic Regression(61000/100000): loss=-5543951.306882298\n",
      "Losgistic Regression(62000/100000): loss=-5550630.5075865295\n",
      "Losgistic Regression(63000/100000): loss=-5557229.22848768\n",
      "Losgistic Regression(64000/100000): loss=-5563749.966684369\n",
      "Losgistic Regression(65000/100000): loss=-5570195.090127384\n",
      "Losgistic Regression(66000/100000): loss=-5576566.847204941\n",
      "Losgistic Regression(67000/100000): loss=-5582867.375431598\n",
      "Losgistic Regression(68000/100000): loss=-5589098.709329092\n",
      "Losgistic Regression(69000/100000): loss=-5595262.787616861\n",
      "Losgistic Regression(70000/100000): loss=-5601361.459770841\n",
      "Losgistic Regression(71000/100000): loss=-5607396.492016925\n",
      "Losgistic Regression(72000/100000): loss=-5613369.572812814\n",
      "Losgistic Regression(73000/100000): loss=-5619282.317877667\n",
      "Losgistic Regression(74000/100000): loss=-5625136.274796649\n",
      "Losgistic Regression(75000/100000): loss=-5630932.927238138\n",
      "Losgistic Regression(76000/100000): loss=-5636673.698870133\n",
      "Losgistic Regression(77000/100000): loss=-5642359.956923308\n",
      "Losgistic Regression(78000/100000): loss=-5647993.015484103\n",
      "Losgistic Regression(79000/100000): loss=-5653574.138545258\n",
      "Losgistic Regression(80000/100000): loss=-5659104.542817296\n",
      "Losgistic Regression(81000/100000): loss=-5664585.400329293\n",
      "Losgistic Regression(82000/100000): loss=-5670017.840837759\n",
      "Losgistic Regression(83000/100000): loss=-5675402.95406031\n",
      "Losgistic Regression(84000/100000): loss=-5680741.79175004\n",
      "Losgistic Regression(85000/100000): loss=-5686035.369624019\n",
      "Losgistic Regression(86000/100000): loss=-5691284.669158835\n",
      "Losgistic Regression(87000/100000): loss=-5696490.639264311\n",
      "Losgistic Regression(88000/100000): loss=-5701654.197845706\n",
      "Losgistic Regression(89000/100000): loss=-5706776.233263827\n",
      "Losgistic Regression(90000/100000): loss=-5711857.605701283\n",
      "Losgistic Regression(91000/100000): loss=-5716899.148442664\n",
      "Losgistic Regression(92000/100000): loss=-5721901.669075482\n",
      "Losgistic Regression(93000/100000): loss=-5726865.985871929\n",
      "Losgistic Regression(94000/100000): loss=-5731792.82632645\n",
      "Losgistic Regression(95000/100000): loss=-5736682.914850668\n",
      "Losgistic Regression(96000/100000): loss=-5741536.96905027\n",
      "Losgistic Regression(97000/100000): loss=-5746355.685295286\n",
      "Losgistic Regression(98000/100000): loss=-5751139.739748536\n",
      "Losgistic Regression(99000/100000): loss=-5755889.789231069\n",
      "0.8194 0.81232\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 100000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.54558852   6.64933073  12.05224732   6.48064661]\n",
      "[ 0  7 34 61 62 90 92 97]\n"
     ]
    }
   ],
   "source": [
    "print(w[abs(w>5)])\n",
    "print(np.arange(len(w))[abs(w>3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/150000): loss=-24496155.36677372\n",
      "Losgistic Regression(1000/150000): loss=-16726315.597480152\n",
      "Losgistic Regression(2000/150000): loss=-17252993.276240055\n",
      "Losgistic Regression(3000/150000): loss=-17436995.475730643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-816-f3ae6abf92e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n\u001b[0;32m----> 6\u001b[0;31m                    max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_and_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tX7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_tX7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_y7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mreg_logistic_regression_GD\u001b[0;34m(y, tx, gamma, max_iters, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreg_logistic_regression_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizor_ridge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreg_logistic_regression_GD_with_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregularizor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mreg_logistic_regression_GD_with_init\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Losgistic Regression({bi}/{ti}): loss={l}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x, clip_range)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# to avoid overflow in exponential, clip input x into a reasonable large range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mcliped_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclip_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcliped_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregularizor_lasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/200000): loss=9221233.193410587\n",
      "Losgistic Regression(1000/200000): loss=-8296709.870189696\n",
      "Losgistic Regression(2000/200000): loss=-9862373.335854245\n",
      "Losgistic Regression(3000/200000): loss=-10831655.627667965\n",
      "Losgistic Regression(4000/200000): loss=-11536115.326674325\n",
      "Losgistic Regression(5000/200000): loss=-12075983.19691701\n",
      "Losgistic Regression(6000/200000): loss=-12511679.921146508\n",
      "Losgistic Regression(7000/200000): loss=-12874634.530713344\n",
      "Losgistic Regression(8000/200000): loss=-13183781.813886058\n",
      "Losgistic Regression(9000/200000): loss=-13451475.683294827\n",
      "Losgistic Regression(10000/200000): loss=-13686264.729243582\n",
      "Losgistic Regression(11000/200000): loss=-13894353.282241577\n",
      "Losgistic Regression(12000/200000): loss=-14080409.165111527\n",
      "Losgistic Regression(13000/200000): loss=-14248028.215456463\n",
      "Losgistic Regression(14000/200000): loss=-14400034.902295291\n",
      "Losgistic Regression(15000/200000): loss=-14538685.367201654\n",
      "Losgistic Regression(16000/200000): loss=-14665810.422242574\n",
      "Losgistic Regression(17000/200000): loss=-14782916.78769217\n",
      "Losgistic Regression(18000/200000): loss=-14891254.333433822\n",
      "Losgistic Regression(19000/200000): loss=-14991873.216922691\n",
      "Losgistic Regression(20000/200000): loss=-15085661.818622943\n",
      "Losgistic Regression(21000/200000): loss=-15173374.494184973\n",
      "Losgistic Regression(22000/200000): loss=-15255657.076266274\n",
      "Losgistic Regression(23000/200000): loss=-15333064.691913279\n",
      "Losgistic Regression(24000/200000): loss=-15406077.250464492\n",
      "Losgistic Regression(25000/200000): loss=-15475111.46468787\n",
      "Losgistic Regression(26000/200000): loss=-15540530.164464993\n",
      "Losgistic Regression(27000/200000): loss=-15602652.144422477\n",
      "Losgistic Regression(28000/200000): loss=-15661756.59855738\n",
      "Losgistic Regression(29000/200000): loss=-15718090.681211257\n",
      "Losgistic Regression(30000/200000): loss=-15771873.42853335\n",
      "Losgistic Regression(31000/200000): loss=-15823299.724603375\n",
      "Losgistic Regression(32000/200000): loss=-15872544.122707773\n",
      "Losgistic Regression(33000/200000): loss=-15919762.216177683\n",
      "Losgistic Regression(34000/200000): loss=-15965094.412630633\n",
      "Losgistic Regression(35000/200000): loss=-16008667.626215851\n",
      "Losgistic Regression(36000/200000): loss=-16050596.762161136\n",
      "Losgistic Regression(37000/200000): loss=-16090986.244844718\n",
      "Losgistic Regression(38000/200000): loss=-16129931.068899903\n",
      "Losgistic Regression(39000/200000): loss=-16167518.469641412\n",
      "Losgistic Regression(40000/200000): loss=-16203826.843156993\n",
      "Losgistic Regression(41000/200000): loss=-16238932.811184576\n",
      "Losgistic Regression(42000/200000): loss=-16272902.172215093\n",
      "Losgistic Regression(43000/200000): loss=-16305797.218551645\n",
      "Losgistic Regression(44000/200000): loss=-16337675.812302783\n",
      "Losgistic Regression(45000/200000): loss=-16368591.519090205\n",
      "Losgistic Regression(46000/200000): loss=-16398594.604502432\n",
      "Losgistic Regression(47000/200000): loss=-16427731.442229554\n",
      "Losgistic Regression(48000/200000): loss=-16456045.163175305\n",
      "Losgistic Regression(49000/200000): loss=-16483576.566281792\n",
      "Losgistic Regression(50000/200000): loss=-16510363.634348368\n",
      "Losgistic Regression(51000/200000): loss=-16536442.011038754\n",
      "Losgistic Regression(52000/200000): loss=-16561845.182608223\n",
      "Losgistic Regression(53000/200000): loss=-16586604.646016343\n",
      "Losgistic Regression(54000/200000): loss=-16610750.062645478\n",
      "Losgistic Regression(55000/200000): loss=-16634309.397241011\n",
      "Losgistic Regression(56000/200000): loss=-16657309.043296032\n",
      "Losgistic Regression(57000/200000): loss=-16679774.237536144\n",
      "Losgistic Regression(58000/200000): loss=-16701727.997607468\n",
      "Losgistic Regression(59000/200000): loss=-16723192.803961642\n",
      "Losgistic Regression(60000/200000): loss=-16744190.123113343\n",
      "Losgistic Regression(61000/200000): loss=-16764739.620601697\n",
      "Losgistic Regression(62000/200000): loss=-16784860.445874166\n",
      "Losgistic Regression(63000/200000): loss=-16804570.634906154\n",
      "Losgistic Regression(64000/200000): loss=-16823887.316555038\n",
      "Losgistic Regression(65000/200000): loss=-16842826.765116077\n",
      "Losgistic Regression(66000/200000): loss=-16861404.44877828\n",
      "Losgistic Regression(67000/200000): loss=-16879635.062913246\n",
      "Losgistic Regression(68000/200000): loss=-16897532.499491137\n",
      "Losgistic Regression(69000/200000): loss=-16915109.627233636\n",
      "Losgistic Regression(70000/200000): loss=-16932375.62675917\n",
      "Losgistic Regression(71000/200000): loss=-16949321.877258256\n",
      "Losgistic Regression(72000/200000): loss=-16965873.97368649\n",
      "Losgistic Regression(73000/200000): loss=-16982136.042267747\n",
      "Losgistic Regression(74000/200000): loss=-17000237.152673308\n",
      "Losgistic Regression(75000/200000): loss=-17021583.65598896\n",
      "Losgistic Regression(76000/200000): loss=-17043970.591341488\n",
      "Losgistic Regression(77000/200000): loss=-17065941.128483687\n",
      "Losgistic Regression(78000/200000): loss=-17087095.663208783\n",
      "Losgistic Regression(79000/200000): loss=-17107384.282479513\n",
      "Losgistic Regression(80000/200000): loss=-17126848.869228657\n",
      "Losgistic Regression(81000/200000): loss=-17145552.507693302\n",
      "Losgistic Regression(82000/200000): loss=-17163559.630797774\n",
      "Losgistic Regression(83000/200000): loss=-17180930.142480195\n",
      "Losgistic Regression(84000/200000): loss=-17197717.83383353\n",
      "Losgistic Regression(85000/200000): loss=-17213970.2926287\n",
      "Losgistic Regression(86000/200000): loss=-17229729.377108026\n",
      "Losgistic Regression(87000/200000): loss=-17245031.749546144\n",
      "Losgistic Regression(88000/200000): loss=-17259909.73608141\n",
      "Losgistic Regression(89000/200000): loss=-17274391.761022363\n",
      "Losgistic Regression(90000/200000): loss=-17288502.964557145\n",
      "Losgistic Regression(91000/200000): loss=-17302265.49544166\n",
      "Losgistic Regression(92000/200000): loss=-17315699.62697328\n",
      "Losgistic Regression(93000/200000): loss=-17328822.959768597\n",
      "Losgistic Regression(94000/200000): loss=-17341651.570914235\n",
      "Losgistic Regression(95000/200000): loss=-17354199.99790428\n",
      "Losgistic Regression(96000/200000): loss=-17366481.44938831\n",
      "Losgistic Regression(97000/200000): loss=-17378507.98603965\n",
      "Losgistic Regression(98000/200000): loss=-17390290.671500623\n",
      "Losgistic Regression(99000/200000): loss=-17401839.699547835\n",
      "Losgistic Regression(100000/200000): loss=-17413164.50190193\n",
      "Losgistic Regression(101000/200000): loss=-17424273.83997349\n",
      "Losgistic Regression(102000/200000): loss=-17435175.883154728\n",
      "Losgistic Regression(103000/200000): loss=-17445878.27579568\n",
      "Losgistic Regression(104000/200000): loss=-17456388.19463346\n",
      "Losgistic Regression(105000/200000): loss=-17466712.39815364\n",
      "Losgistic Regression(106000/200000): loss=-17476857.269118346\n",
      "Losgistic Regression(107000/200000): loss=-17486828.851301502\n",
      "Losgistic Regression(108000/200000): loss=-17496632.881289374\n",
      "Losgistic Regression(109000/200000): loss=-17506274.816098414\n",
      "Losgistic Regression(110000/200000): loss=-17515759.857220694\n",
      "Losgistic Regression(111000/200000): loss=-17525092.97162176\n",
      "Losgistic Regression(112000/200000): loss=-17534278.910133857\n",
      "Losgistic Regression(113000/200000): loss=-17543322.22362305\n",
      "Losgistic Regression(114000/200000): loss=-17552227.27724787\n",
      "Losgistic Regression(115000/200000): loss=-17560998.263080467\n",
      "Losgistic Regression(116000/200000): loss=-17569639.21132063\n",
      "Losgistic Regression(117000/200000): loss=-17578154.000299282\n",
      "Losgistic Regression(118000/200000): loss=-17586546.36543788\n",
      "Losgistic Regression(119000/200000): loss=-17594819.907306068\n",
      "Losgistic Regression(120000/200000): loss=-17602978.098898266\n",
      "Losgistic Regression(121000/200000): loss=-17611024.292234577\n",
      "Losgistic Regression(122000/200000): loss=-17618961.724371083\n",
      "Losgistic Regression(123000/200000): loss=-17626793.5228987\n",
      "Losgistic Regression(124000/200000): loss=-17634522.71099164\n",
      "Losgistic Regression(125000/200000): loss=-17642152.212063603\n",
      "Losgistic Regression(126000/200000): loss=-17649684.85407687\n",
      "Losgistic Regression(127000/200000): loss=-17657123.37354593\n",
      "Losgistic Regression(128000/200000): loss=-17664470.41927034\n",
      "Losgistic Regression(129000/200000): loss=-17671728.555826336\n",
      "Losgistic Regression(130000/200000): loss=-17678900.26684325\n",
      "Losgistic Regression(131000/200000): loss=-17685987.958087157\n",
      "Losgistic Regression(132000/200000): loss=-17692993.96037038\n",
      "Losgistic Regression(133000/200000): loss=-17699920.532303743\n",
      "Losgistic Regression(134000/200000): loss=-17706769.86290609\n",
      "Losgistic Regression(135000/200000): loss=-17713544.074083492\n",
      "Losgistic Regression(136000/200000): loss=-17720245.222988892\n",
      "Losgistic Regression(137000/200000): loss=-17726875.304272033\n",
      "Losgistic Regression(138000/200000): loss=-17733436.25222772\n",
      "Losgistic Regression(139000/200000): loss=-17739929.94285017\n",
      "Losgistic Regression(140000/200000): loss=-17746358.1958\n",
      "Losgistic Regression(141000/200000): loss=-17752722.776288506\n",
      "Losgistic Regression(142000/200000): loss=-17759025.396886405\n",
      "Losgistic Regression(143000/200000): loss=-17765267.71926001\n",
      "Losgistic Regression(144000/200000): loss=-17771451.35583935\n",
      "Losgistic Regression(145000/200000): loss=-17777577.87142259\n",
      "Losgistic Regression(146000/200000): loss=-17783648.78471932\n",
      "Losgistic Regression(147000/200000): loss=-17789665.569836583\n",
      "Losgistic Regression(148000/200000): loss=-17795629.6577095\n",
      "Losgistic Regression(149000/200000): loss=-17801542.437480263\n",
      "Losgistic Regression(150000/200000): loss=-17807405.257826913\n",
      "Losgistic Regression(151000/200000): loss=-17813219.42824462\n",
      "Losgistic Regression(152000/200000): loss=-17818986.22028161\n",
      "Losgistic Regression(153000/200000): loss=-17824706.868730955\n",
      "Losgistic Regression(154000/200000): loss=-17830382.57278132\n",
      "Losgistic Regression(155000/200000): loss=-17836014.49712678\n",
      "Losgistic Regression(156000/200000): loss=-17841603.773038916\n",
      "Losgistic Regression(157000/200000): loss=-17847151.499401823\n",
      "Losgistic Regression(158000/200000): loss=-17852658.743710753\n",
      "Losgistic Regression(159000/200000): loss=-17858126.54303678\n",
      "Losgistic Regression(160000/200000): loss=-17863555.90496046\n",
      "Losgistic Regression(161000/200000): loss=-17868947.808470927\n",
      "Losgistic Regression(162000/200000): loss=-17874303.20483662\n",
      "Losgistic Regression(163000/200000): loss=-17879623.018446036\n",
      "Losgistic Regression(164000/200000): loss=-17884908.147619877\n",
      "Losgistic Regression(165000/200000): loss=-17890159.465396725\n",
      "Losgistic Regression(166000/200000): loss=-17895377.82029177\n",
      "Losgistic Regression(167000/200000): loss=-17900564.037030887\n",
      "Losgistic Regression(168000/200000): loss=-17905718.91725986\n",
      "Losgistic Regression(169000/200000): loss=-17910843.24023039\n",
      "Losgistic Regression(170000/200000): loss=-17915938.003268752\n",
      "Losgistic Regression(171000/200000): loss=-17921003.317106325\n",
      "Losgistic Regression(172000/200000): loss=-17926040.29152501\n",
      "Losgistic Regression(173000/200000): loss=-17931049.655804064\n",
      "Losgistic Regression(174000/200000): loss=-17936032.079917077\n",
      "Losgistic Regression(175000/200000): loss=-17940988.21335437\n",
      "Losgistic Regression(176000/200000): loss=-17945918.689526442\n",
      "Losgistic Regression(177000/200000): loss=-17950824.126050126\n",
      "Losgistic Regression(178000/200000): loss=-17955705.12467692\n",
      "Losgistic Regression(179000/200000): loss=-17960562.27135848\n",
      "Losgistic Regression(180000/200000): loss=-17965396.13645041\n",
      "Losgistic Regression(181000/200000): loss=-17970207.275002968\n",
      "Losgistic Regression(182000/200000): loss=-17974996.227101076\n",
      "Losgistic Regression(183000/200000): loss=-17979763.51822546\n",
      "Losgistic Regression(184000/200000): loss=-17984509.659622066\n",
      "Losgistic Regression(185000/200000): loss=-17989235.148673024\n",
      "Losgistic Regression(186000/200000): loss=-17993940.46926314\n",
      "Losgistic Regression(187000/200000): loss=-17998626.09214151\n",
      "Losgistic Regression(188000/200000): loss=-18003292.475274738\n",
      "Losgistic Regression(189000/200000): loss=-18007940.06419316\n",
      "Losgistic Regression(190000/200000): loss=-18012569.292327724\n",
      "Losgistic Regression(191000/200000): loss=-18017180.581338838\n",
      "Losgistic Regression(192000/200000): loss=-18021774.341436442\n",
      "Losgistic Regression(193000/200000): loss=-18026350.971691515\n",
      "Losgistic Regression(194000/200000): loss=-18030910.86033916\n",
      "Losgistic Regression(195000/200000): loss=-18035454.38507335\n",
      "Losgistic Regression(196000/200000): loss=-18039981.91333365\n",
      "Losgistic Regression(197000/200000): loss=-18044493.802584063\n",
      "Losgistic Regression(198000/200000): loss=-18048990.40058408\n",
      "Losgistic Regression(199000/200000): loss=-18053472.0456523\n",
      "0.8153 0.81116\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "                   max_iters = 200000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.69755411847e-06+0j)\n",
      "Losgistic Regression(0/500000): loss=(2050782.064924333+0j)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-763-6c0ef4884038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n\u001b[0;32m----> 5\u001b[0;31m                max_iters = 500000, lambda_= 1e-6, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_and_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tX7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_tX7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_y7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mreg_logistic_regression_GD\u001b[0;34m(y, tx, gamma, max_iters, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreg_logistic_regression_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizor_ridge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreg_logistic_regression_GD_with_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregularizor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mreg_logistic_regression_GD_with_init\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/helie/projects/ML_Projects/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idxes = np.arange(5000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "               max_iters = 500000, lambda_= 1e-6, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(i, tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "for i in range(len(w)-1):\n",
    "    correlation_list.append(np.corrcoef(train_y7, train_tX7[:, i])[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.65129855e-02,  -3.51612919e-01,  -1.38712350e-02,\n",
       "         1.91876052e-01,   2.03718015e-01,   2.13023438e-01,\n",
       "        -1.83174366e-01,   1.31601540e-02,  -1.51333685e-02,\n",
       "         1.52662286e-01,  -1.96198138e-01,   2.72353631e-01,\n",
       "         1.75387841e-01,   2.34864606e-01,  -2.94158012e-04,\n",
       "        -4.79748618e-03,  -3.29888084e-02,   2.48140191e-03,\n",
       "         4.86870767e-03,   2.12883597e-02,   5.62790946e-03,\n",
       "         1.35090525e-01,   1.33094440e-01,   1.15036848e-01,\n",
       "        -2.56697681e-04,   1.39365659e-03,   2.32211127e-02,\n",
       "         1.37242072e-03,  -4.50596689e-03,   1.33843107e-01,\n",
       "        -4.22503370e-02,  -2.09894679e-01,  -6.31467775e-02,\n",
       "         1.06434966e-01,   2.33094856e-01,   1.75096532e-01,\n",
       "         1.81317012e-01,  -1.37815655e-02,   2.06173286e-03,\n",
       "         8.84978519e-02,  -1.00459724e-01,   4.62967153e-02,\n",
       "         2.13143137e-01,   1.26363568e-01,  -1.12019572e-01,\n",
       "        -1.44671666e-03,  -1.92796971e-02,  -1.46377790e-01,\n",
       "        -2.47121748e-03,   2.22883334e-02,   3.23624906e-04,\n",
       "         8.30113265e-02,   8.62348200e-02,   8.07217753e-02,\n",
       "         1.95750276e-01,   8.85521759e-02,   8.77029235e-03,\n",
       "         1.86384871e-01,   9.52482953e-02,   5.11589268e-02,\n",
       "        -4.90664985e-02,  -6.30038984e-02,  -3.63508109e-02,\n",
       "         1.63276303e-02,   2.27360557e-01,   1.18168301e-01,\n",
       "        -1.33579922e-01,  -3.81670787e-02,   2.93492027e-03,\n",
       "         3.91113254e-02,  -3.36458940e-02,   2.50089338e-01,\n",
       "         2.17023486e-01,   3.74269855e-02,   1.29428493e-03,\n",
       "        -2.43513983e-03,  -7.84938556e-03,   2.89080197e-03,\n",
       "         2.86141122e-03,   4.57132011e-03,   2.39176022e-03,\n",
       "         3.90604196e-02,   4.98079693e-02,   4.44785511e-02,\n",
       "         1.97998449e-03,   1.51299647e-03,  -4.17043612e-03,\n",
       "         3.91378005e-04,  -3.77494031e-03,   8.12723929e-03,\n",
       "        -3.44925065e-02,  -1.38334072e-02,  -1.32643427e-02,\n",
       "         4.37465398e-03,   2.10426051e-01,   7.36977091e-02,\n",
       "         1.12715032e-01,  -5.77959765e-02,   2.92614511e-03,\n",
       "         1.38871506e-02,  -1.16356262e-02,   2.98185903e-02,\n",
       "         2.14309616e-01,   7.91169191e-03,  -1.07215211e-01,\n",
       "        -9.44478783e-04,  -3.61987122e-03,  -1.36937512e-01,\n",
       "        -2.15002143e-03,   3.04912762e-03,  -3.29993414e-04,\n",
       "         1.36936533e-02,   2.46375152e-02,   1.97294368e-02,\n",
       "         1.56738089e-01,   6.60647683e-02,  -6.78762277e-03,\n",
       "         1.59555743e-01,   7.45483074e-02,  -4.56185279e-03])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array(correlation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14,  15,  17,  18,  20,  24,  25,  27,  28,  38,  45,  48,  50,\n",
       "        56,  68,  74,  75,  76,  77,  78,  79,  80,  84,  85,  86,  87,\n",
       "        88,  89,  93,  98, 103, 105, 106, 108, 109, 110, 116, 119])"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(c))[abs(c) < 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   6,   9,  11,  14,  15,  20,  24,  25,  27,  36,  41,  47,\n",
       "        48,  50,  55,  56,  68,  72,  74,  75,  77,  80,  82,  84,  85,\n",
       "        87, 101, 104, 108, 110, 115, 117])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(w))[abs(w) < 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/150000): loss=6128.661958753694\n",
      "Losgistic Regression(1000/150000): loss=4382.793973805763\n",
      "Losgistic Regression(2000/150000): loss=4321.439300698346\n",
      "Losgistic Regression(3000/150000): loss=4291.208251980238\n",
      "Losgistic Regression(4000/150000): loss=4270.574245773539\n",
      "Losgistic Regression(5000/150000): loss=4255.255208797125\n",
      "Losgistic Regression(6000/150000): loss=4244.187346417512\n",
      "Losgistic Regression(7000/150000): loss=4236.342382593083\n",
      "Losgistic Regression(8000/150000): loss=4229.2149455352765\n",
      "Losgistic Regression(9000/150000): loss=4222.549848793302\n",
      "Losgistic Regression(10000/150000): loss=4216.256821112506\n",
      "Losgistic Regression(11000/150000): loss=4210.280363793595\n",
      "Losgistic Regression(12000/150000): loss=4204.581620909371\n",
      "Losgistic Regression(13000/150000): loss=4199.130612876477\n",
      "Losgistic Regression(14000/150000): loss=4193.902700925177\n",
      "Losgistic Regression(15000/150000): loss=4188.876869375156\n",
      "Losgistic Regression(16000/150000): loss=4184.034829564825\n",
      "Losgistic Regression(17000/150000): loss=4179.3605048153\n",
      "Losgistic Regression(18000/150000): loss=4175.203975960235\n",
      "Losgistic Regression(19000/150000): loss=4172.128890451348\n",
      "Losgistic Regression(20000/150000): loss=4169.404308525439\n",
      "Losgistic Regression(21000/150000): loss=4166.945582792079\n",
      "Losgistic Regression(22000/150000): loss=4164.697910753656\n",
      "Losgistic Regression(23000/150000): loss=4162.623370955874\n",
      "Losgistic Regression(24000/150000): loss=4160.694444672941\n",
      "Losgistic Regression(25000/150000): loss=4158.890417925157\n",
      "Losgistic Regression(26000/150000): loss=4157.195232293507\n",
      "Losgistic Regression(27000/150000): loss=4155.596121161573\n",
      "Losgistic Regression(28000/150000): loss=4154.082702688859\n",
      "Losgistic Regression(29000/150000): loss=4152.646368926147\n",
      "Losgistic Regression(30000/150000): loss=4151.279853515938\n",
      "Losgistic Regression(31000/150000): loss=4149.976926771299\n",
      "Losgistic Regression(32000/150000): loss=4148.732176552424\n",
      "Losgistic Regression(33000/150000): loss=4147.540849240619\n",
      "Losgistic Regression(34000/150000): loss=4146.398730789539\n",
      "Losgistic Regression(35000/150000): loss=4145.3020601081635\n",
      "Losgistic Regression(36000/150000): loss=4144.247460850778\n",
      "Losgistic Regression(37000/150000): loss=4143.231888176198\n",
      "Losgistic Regression(38000/150000): loss=4142.252586093812\n",
      "Losgistic Regression(39000/150000): loss=4141.307052495995\n",
      "Losgistic Regression(40000/150000): loss=4140.393009940427\n",
      "Losgistic Regression(41000/150000): loss=4139.50838085982\n",
      "Losgistic Regression(42000/150000): loss=4138.65126787957\n",
      "Losgistic Regression(43000/150000): loss=4137.819930481284\n",
      "Losgistic Regression(44000/150000): loss=4137.0127732559595\n",
      "Losgistic Regression(45000/150000): loss=4136.228330432692\n",
      "Losgistic Regression(46000/150000): loss=4135.465253085196\n",
      "Losgistic Regression(47000/150000): loss=4134.1443323593485\n",
      "Losgistic Regression(48000/150000): loss=4132.836213664394\n",
      "Losgistic Regression(49000/150000): loss=4131.548598167255\n",
      "Losgistic Regression(50000/150000): loss=4130.280447452421\n",
      "Losgistic Regression(51000/150000): loss=4129.030794438789\n",
      "Losgistic Regression(52000/150000): loss=4127.7987378407415\n",
      "Losgistic Regression(53000/150000): loss=4126.583436998648\n",
      "Losgistic Regression(54000/150000): loss=4125.3841070212775\n",
      "Losgistic Regression(55000/150000): loss=4124.200014284044\n",
      "Losgistic Regression(56000/150000): loss=4123.030472284158\n",
      "Losgistic Regression(57000/150000): loss=4121.87483784062\n",
      "Losgistic Regression(58000/150000): loss=4120.732507621012\n",
      "Losgistic Regression(59000/150000): loss=4119.602914974805\n",
      "Losgistic Regression(60000/150000): loss=4118.485527050165\n",
      "Losgistic Regression(61000/150000): loss=4117.379842171625\n",
      "Losgistic Regression(62000/150000): loss=4116.28538745605\n",
      "Losgistic Regression(63000/150000): loss=4115.201716646274\n",
      "Losgistic Regression(64000/150000): loss=4114.128408140856\n",
      "Losgistic Regression(65000/150000): loss=4113.065063202537\n",
      "Losgistic Regression(66000/150000): loss=4112.011304327397\n",
      "Losgistic Regression(67000/150000): loss=4110.966773759254\n",
      "Losgistic Regression(68000/150000): loss=4109.931132134869\n",
      "Losgistic Regression(69000/150000): loss=4108.9040572471185\n",
      "Losgistic Regression(70000/150000): loss=4107.88524291448\n",
      "Losgistic Regression(71000/150000): loss=4106.874397946057\n",
      "Losgistic Regression(72000/150000): loss=4105.8712451931\n",
      "Losgistic Regression(73000/150000): loss=4104.875520678365\n",
      "Losgistic Regression(74000/150000): loss=4103.886972795643\n",
      "Losgistic Regression(75000/150000): loss=4102.905361572792\n",
      "Losgistic Regression(76000/150000): loss=4101.930457992193\n",
      "Losgistic Regression(77000/150000): loss=4100.962043363106\n",
      "Losgistic Regression(78000/150000): loss=4099.999908741166\n",
      "Losgistic Regression(79000/150000): loss=4099.043854390546\n",
      "Losgistic Regression(80000/150000): loss=4098.093689285304\n",
      "Losgistic Regression(81000/150000): loss=4097.149230644885\n",
      "Losgistic Regression(82000/150000): loss=4096.2103035032105\n",
      "Losgistic Regression(83000/150000): loss=4095.2767403064836\n",
      "Losgistic Regression(84000/150000): loss=4094.348380641769\n",
      "Losgistic Regression(85000/150000): loss=4093.4250696102904\n",
      "Losgistic Regression(86000/150000): loss=4092.5066611711864\n",
      "Losgistic Regression(87000/150000): loss=4091.593013716083\n",
      "Losgistic Regression(88000/150000): loss=4090.6839912387345\n",
      "Losgistic Regression(89000/150000): loss=4089.779463475005\n",
      "Losgistic Regression(90000/150000): loss=4088.8793054909006\n",
      "Losgistic Regression(91000/150000): loss=4087.9833973753302\n",
      "Losgistic Regression(92000/150000): loss=4087.091623991697\n",
      "Losgistic Regression(93000/150000): loss=4086.2038746801322\n",
      "Losgistic Regression(94000/150000): loss=4085.3200435459976\n",
      "Losgistic Regression(95000/150000): loss=4084.4400271378972\n",
      "Losgistic Regression(96000/150000): loss=4083.563726604002\n",
      "Losgistic Regression(97000/150000): loss=4082.6910490395876\n",
      "Losgistic Regression(98000/150000): loss=4081.821904073697\n",
      "Losgistic Regression(99000/150000): loss=4080.9562047323307\n",
      "Losgistic Regression(100000/150000): loss=4080.0938673345763\n",
      "Losgistic Regression(101000/150000): loss=4079.234811363942\n",
      "Losgistic Regression(102000/150000): loss=4078.3789593505944\n",
      "Losgistic Regression(103000/150000): loss=4077.5262367569426\n",
      "Losgistic Regression(104000/150000): loss=4076.6765718659494\n",
      "Losgistic Regression(105000/150000): loss=4075.8298956724216\n",
      "Losgistic Regression(106000/150000): loss=4074.986141777644\n",
      "Losgistic Regression(107000/150000): loss=4074.145246287765\n",
      "Losgistic Regression(108000/150000): loss=4073.3071477161557\n",
      "Losgistic Regression(109000/150000): loss=4072.471786889442\n",
      "Losgistic Regression(110000/150000): loss=4071.639106857446\n",
      "Losgistic Regression(111000/150000): loss=4070.8090528066728\n",
      "Losgistic Regression(112000/150000): loss=4069.981571977565\n",
      "Losgistic Regression(113000/150000): loss=4069.1566135849607\n",
      "Losgistic Regression(114000/150000): loss=4068.334128741864\n",
      "Losgistic Regression(115000/150000): loss=4067.514070386298\n",
      "Losgistic Regression(116000/150000): loss=4066.6963932110834\n",
      "Losgistic Regression(117000/150000): loss=4065.8810555570626\n",
      "Losgistic Regression(118000/150000): loss=4065.068013712836\n",
      "Losgistic Regression(119000/150000): loss=4064.25722701785\n",
      "Losgistic Regression(120000/150000): loss=4063.44865651399\n",
      "Losgistic Regression(121000/150000): loss=4062.642264707486\n",
      "Losgistic Regression(122000/150000): loss=4061.83801549441\n",
      "Losgistic Regression(123000/150000): loss=4061.0358741047557\n",
      "Losgistic Regression(124000/150000): loss=4060.235807051358\n",
      "Losgistic Regression(125000/150000): loss=4059.437782534584\n",
      "Losgistic Regression(126000/150000): loss=4058.641769063937\n",
      "Losgistic Regression(127000/150000): loss=4057.8477366387892\n",
      "Losgistic Regression(128000/150000): loss=4057.055656489738\n",
      "Losgistic Regression(129000/150000): loss=4056.2655008337697\n",
      "Losgistic Regression(130000/150000): loss=4055.477242900723\n",
      "Losgistic Regression(131000/150000): loss=4054.6908568974927\n",
      "Losgistic Regression(132000/150000): loss=4053.906317968484\n",
      "Losgistic Regression(133000/150000): loss=4053.1236021583495\n",
      "Losgistic Regression(134000/150000): loss=4052.3426863762284\n",
      "Losgistic Regression(135000/150000): loss=4051.5635483610204\n",
      "Losgistic Regression(136000/150000): loss=4050.7861666489475\n",
      "Losgistic Regression(137000/150000): loss=4050.010520541866\n",
      "Losgistic Regression(138000/150000): loss=4049.236590076733\n",
      "Losgistic Regression(139000/150000): loss=4048.46435599626\n",
      "Losgistic Regression(140000/150000): loss=4047.6937997206537\n",
      "Losgistic Regression(141000/150000): loss=4046.9249033204223\n",
      "Losgistic Regression(142000/150000): loss=4046.1576494901965\n",
      "Losgistic Regression(143000/150000): loss=4045.3920215234975\n",
      "Losgistic Regression(144000/150000): loss=4044.62800328853\n",
      "Losgistic Regression(145000/150000): loss=4043.8655792049185\n",
      "Losgistic Regression(146000/150000): loss=4043.1047339495753\n",
      "Losgistic Regression(147000/150000): loss=4042.3454523613855\n",
      "Losgistic Regression(148000/150000): loss=4041.587721752981\n",
      "Losgistic Regression(149000/150000): loss=4040.8315283470806\n",
      "0.8207 0.81404\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.79002650416e-06\n",
      "Losgistic Regression(0/150000): loss=7981.108636380177\n",
      "Losgistic Regression(100/150000): loss=7981.110107054736\n",
      "0.00147067455873\n",
      "Totoal number of iterations =  100\n",
      "Loss =  7981.11010705\n",
      "0.8212 0.8158\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(20000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w_agdr = logistic_AGDR(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso, w0=w_agdr)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], \n",
    "                                         cv_tX7, cv_y7, w_agdr)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.03230119e+01,   8.72829146e-01,   1.80618295e+00,\n",
       "         1.76614532e-01,  -1.91718656e+00,  -1.73407054e+00,\n",
       "        -1.32845968e-01,   9.99409928e+00,  -7.84458112e-02,\n",
       "        -3.32218947e-01,  -2.93981054e+00,   6.25846578e-02,\n",
       "         2.12576775e-01,  -3.39203801e-01,   4.39780896e-02,\n",
       "         1.62641950e-02,   2.70709896e+00,   3.43499201e-02,\n",
       "         4.74550806e-04,  -4.25686853e-01,   3.12686061e-02,\n",
       "         1.08517303e+00,   1.53254173e-01,   1.58320298e+00,\n",
       "        -3.96805792e-02,  -1.08817579e-02,   9.38729270e-01,\n",
       "         4.75880670e-02,  -3.50876529e-02,  -2.82148526e-01,\n",
       "        -2.61762925e+01,  -6.11373725e+00,  -1.23944036e+01,\n",
       "         1.76741777e+00,   7.22834343e+00,   5.69466116e+00,\n",
       "        -8.47058102e-02,  -1.86781609e+01,   8.90956905e-01,\n",
       "        -8.66581649e-01,   4.15964929e+00,   2.46080678e-01,\n",
       "        -3.35989271e-01,   1.86089696e+00,  -1.25807891e-01,\n",
       "        -1.22027872e-02,  -2.70569586e+00,  -5.39981561e-02,\n",
       "        -8.49681047e-03,   1.25436194e+00,   1.27764654e-01,\n",
       "        -3.08091596e+00,  -3.66003968e-01,  -2.30929260e+00,\n",
       "         4.62760443e-01,   4.48733387e-04,  -1.55529567e+00,\n",
       "         2.14080777e-01,  -1.34783452e-01,   4.31905630e-01,\n",
       "         2.63126895e+01,   1.40998997e+01,   2.85073516e+01,\n",
       "        -6.44607326e+00,  -9.56901543e+00,  -8.50945214e+00,\n",
       "         1.17511098e-01,   1.34765696e+01,  -1.35748213e+00,\n",
       "        -1.09208800e+00,  -5.56262088e+00,   4.69336829e-02,\n",
       "         5.51453701e-01,  -3.21813813e+00,  -3.75410025e-02,\n",
       "         1.18378061e-02,   3.13523163e+00,  -3.99101954e-02,\n",
       "        -5.86370670e-03,  -4.10003952e+00,  -4.38851120e-02,\n",
       "         3.42151408e+00,   7.29297450e-01,   1.54673037e+00,\n",
       "         3.68621403e-02,   1.13320625e-02,   1.18483982e+00,\n",
       "        -5.18247532e-02,   4.77361125e-02,   1.89507617e+00,\n",
       "        -1.01275719e+01,  -1.28326833e+01,  -3.10041595e+01,\n",
       "         7.02536302e+00,   4.51901849e+00,   5.29566808e+00,\n",
       "        -1.00078938e-01,  -3.27021273e+00,  -3.11344134e-01,\n",
       "         1.15438954e+00,   3.93413646e+00,  -2.01111235e-01,\n",
       "        -2.05113489e-01,   3.33579368e+00,   1.85179401e-03,\n",
       "        -1.53057287e-02,  -2.01172006e+00,  -1.43486395e-01,\n",
       "         1.24575918e-02,   4.47377555e+00,  -1.64239763e-01,\n",
       "        -1.74098854e+00,  -5.94978937e-01,  -3.97707694e-01,\n",
       "        -9.38507595e-02,  -3.10589372e-02,  -1.01056097e-01,\n",
       "        -7.57379114e-03,   1.24677337e-01,  -2.10009147e+00,\n",
       "        -1.56260684e+00])"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_agdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.03230119e+01,   8.72829146e-01,   1.80618295e+00,\n",
       "         1.76614532e-01,  -1.91718656e+00,  -1.73407054e+00,\n",
       "        -1.32845968e-01,   9.99409928e+00,  -7.84458112e-02,\n",
       "        -3.32218947e-01,  -2.93981054e+00,   6.25846578e-02,\n",
       "         2.12576775e-01,  -3.39203801e-01,   4.39780896e-02,\n",
       "         1.62641950e-02,   2.70709896e+00,   3.43499201e-02,\n",
       "         4.74550806e-04,  -4.25686853e-01,   3.12686061e-02,\n",
       "         1.08517303e+00,   1.53254173e-01,   1.58320298e+00,\n",
       "        -3.96805792e-02,  -1.08817579e-02,   9.38729270e-01,\n",
       "         4.75880670e-02,  -3.50876529e-02,  -2.82148526e-01,\n",
       "        -2.61762925e+01,  -6.11373725e+00,  -1.23944036e+01,\n",
       "         1.76741777e+00,   7.22834343e+00,   5.69466116e+00,\n",
       "        -8.47058102e-02,  -1.86781609e+01,   8.90956905e-01,\n",
       "        -8.66581649e-01,   4.15964929e+00,   2.46080678e-01,\n",
       "        -3.35989271e-01,   1.86089696e+00,  -1.25807891e-01,\n",
       "        -1.22027872e-02,  -2.70569586e+00,  -5.39981561e-02,\n",
       "        -8.49681047e-03,   1.25436194e+00,   1.27764654e-01,\n",
       "        -3.08091596e+00,  -3.66003968e-01,  -2.30929260e+00,\n",
       "         4.62760443e-01,   4.48733387e-04,  -1.55529567e+00,\n",
       "         2.14080777e-01,  -1.34783452e-01,   4.31905630e-01,\n",
       "         2.63126895e+01,   1.40998997e+01,   2.85073516e+01,\n",
       "        -6.44607326e+00,  -9.56901543e+00,  -8.50945214e+00,\n",
       "         1.17511098e-01,   1.34765696e+01,  -1.35748213e+00,\n",
       "        -1.09208800e+00,  -5.56262088e+00,   4.69336829e-02,\n",
       "         5.51453701e-01,  -3.21813813e+00,  -3.75410025e-02,\n",
       "         1.18378061e-02,   3.13523163e+00,  -3.99101954e-02,\n",
       "        -5.86370670e-03,  -4.10003952e+00,  -4.38851120e-02,\n",
       "         3.42151408e+00,   7.29297450e-01,   1.54673037e+00,\n",
       "         3.68621403e-02,   1.13320625e-02,   1.18483982e+00,\n",
       "        -5.18247532e-02,   4.77361125e-02,   1.89507617e+00,\n",
       "        -1.01275719e+01,  -1.28326833e+01,  -3.10041595e+01,\n",
       "         7.02536302e+00,   4.51901849e+00,   5.29566808e+00,\n",
       "        -1.00078938e-01,  -3.27021273e+00,  -3.11344134e-01,\n",
       "         1.15438954e+00,   3.93413646e+00,  -2.01111235e-01,\n",
       "        -2.05113489e-01,   3.33579368e+00,   1.85179401e-03,\n",
       "        -1.53057287e-02,  -2.01172006e+00,  -1.43486395e-01,\n",
       "         1.24575918e-02,   4.47377555e+00,  -1.64239763e-01,\n",
       "        -1.74098854e+00,  -5.94978937e-01,  -3.97707694e-01,\n",
       "        -9.38507595e-02,  -3.10589372e-02,  -1.01056097e-01,\n",
       "        -7.57379114e-03,   1.24677337e-01,  -2.10009147e+00,\n",
       "        -1.56260684e+00])"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = w_agdr\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_missing_featuers = [i for i in range(30) if i not in columns_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_tX =np.sum(tX == -999, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_missing_tX = tX[status_tX == 0, :]\n",
    "missing_tX = tX[status_tX != 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 301)"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(fill_na(tX, np.median), degree=10)\n",
    "tX7, mean_x7, std_x7 = standardize(tmp)\n",
    "y7 = transform_y(y)\n",
    "tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 301)"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ratio = 0.9\n",
    "train_tX7, cv_tX7, train_y7, cv_y7 = split_data(tX7, y7, training_ratio)\n",
    "cv_tX7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.49473614186e-05+0j)\n",
      "Losgistic Regression(0/100000): loss=-44619.0435651786\n",
      "Losgistic Regression(1000/100000): loss=-154856.83025499398\n",
      "Losgistic Regression(2000/100000): loss=-142309.8661505646\n",
      "Losgistic Regression(3000/100000): loss=-145717.54056625118\n",
      "Losgistic Regression(4000/100000): loss=-152960.1357161893\n",
      "Losgistic Regression(5000/100000): loss=-159643.1937239881\n",
      "Losgistic Regression(6000/100000): loss=-165739.41687550774\n",
      "Losgistic Regression(7000/100000): loss=-171345.22204597376\n",
      "Losgistic Regression(8000/100000): loss=-176512.10742339757\n",
      "Losgistic Regression(9000/100000): loss=-181282.82337985822\n",
      "Losgistic Regression(10000/100000): loss=-185699.22739621307\n",
      "Losgistic Regression(11000/100000): loss=-189801.72317867383\n",
      "Losgistic Regression(12000/100000): loss=-193628.15191199194\n",
      "Losgistic Regression(13000/100000): loss=-197213.15342974968\n",
      "Losgistic Regression(14000/100000): loss=-200587.66586620157\n",
      "Losgistic Regression(15000/100000): loss=-203778.91895909168\n",
      "Losgistic Regression(16000/100000): loss=-206810.53935261403\n",
      "Losgistic Regression(17000/100000): loss=-209702.7837786889\n",
      "Losgistic Regression(18000/100000): loss=-212472.59171053203\n",
      "Losgistic Regression(19000/100000): loss=-215134.17057606805\n",
      "Losgistic Regression(20000/100000): loss=-217699.28307508634\n",
      "Losgistic Regression(21000/100000): loss=-220177.58871101835\n",
      "Losgistic Regression(22000/100000): loss=-222576.9659365387\n",
      "Losgistic Regression(23000/100000): loss=-224903.9611453428\n",
      "Losgistic Regression(24000/100000): loss=-227164.09602443394\n",
      "Losgistic Regression(25000/100000): loss=-229362.0411662442\n",
      "Losgistic Regression(26000/100000): loss=-231501.88559864002\n",
      "Losgistic Regression(27000/100000): loss=-233587.2209028704\n",
      "Losgistic Regression(28000/100000): loss=-235621.2218698786\n",
      "Losgistic Regression(29000/100000): loss=-237606.78000594256\n",
      "Losgistic Regression(30000/100000): loss=-239546.52944255868\n",
      "Losgistic Regression(31000/100000): loss=-241442.8217958809\n",
      "Losgistic Regression(32000/100000): loss=-243297.82236092674\n",
      "Losgistic Regression(33000/100000): loss=-245113.5152853879\n",
      "Losgistic Regression(34000/100000): loss=-246891.67364572638\n",
      "Losgistic Regression(35000/100000): loss=-248633.95649644628\n",
      "Losgistic Regression(36000/100000): loss=-250341.88141672823\n",
      "Losgistic Regression(37000/100000): loss=-252016.84054350836\n",
      "Losgistic Regression(38000/100000): loss=-253660.11825247837\n",
      "Losgistic Regression(39000/100000): loss=-255272.90634234785\n",
      "Losgistic Regression(40000/100000): loss=-256856.29370029861\n",
      "Losgistic Regression(41000/100000): loss=-258411.31548304757\n",
      "Losgistic Regression(42000/100000): loss=-259938.95295832277\n",
      "Losgistic Regression(43000/100000): loss=-261440.0928176191\n",
      "Losgistic Regression(44000/100000): loss=-262915.6105595232\n",
      "Losgistic Regression(45000/100000): loss=-264366.3212414645\n",
      "Losgistic Regression(46000/100000): loss=-265793.00051039056\n",
      "Losgistic Regression(47000/100000): loss=-267196.4036268392\n",
      "Losgistic Regression(48000/100000): loss=-268577.2756116806\n",
      "Losgistic Regression(49000/100000): loss=-269936.2498388919\n",
      "Losgistic Regression(50000/100000): loss=-271274.0023760741\n",
      "Losgistic Regression(51000/100000): loss=-272591.1756603653\n",
      "Losgistic Regression(52000/100000): loss=-273888.3871307957\n",
      "Losgistic Regression(53000/100000): loss=-275166.2319600392\n",
      "Losgistic Regression(54000/100000): loss=-276425.2845359411\n",
      "Losgistic Regression(55000/100000): loss=-277666.10064163036\n",
      "Losgistic Regression(56000/100000): loss=-278889.21423312277\n",
      "Losgistic Regression(57000/100000): loss=-280095.13163128117\n",
      "Losgistic Regression(58000/100000): loss=-281284.3803962245\n",
      "Losgistic Regression(59000/100000): loss=-282457.4308686122\n",
      "Losgistic Regression(60000/100000): loss=-283614.73871331936\n",
      "Losgistic Regression(61000/100000): loss=-284756.7512317457\n",
      "Losgistic Regression(62000/100000): loss=-285883.90928247094\n",
      "Losgistic Regression(63000/100000): loss=-286996.6375226465\n",
      "Losgistic Regression(64000/100000): loss=-288095.35342889436\n",
      "Losgistic Regression(65000/100000): loss=-289180.4231357479\n",
      "Losgistic Regression(66000/100000): loss=-290252.2157900949\n",
      "Losgistic Regression(67000/100000): loss=-291311.09705059323\n",
      "Losgistic Regression(68000/100000): loss=-292357.421185898\n",
      "Losgistic Regression(69000/100000): loss=-293391.5282476685\n",
      "Losgistic Regression(70000/100000): loss=-294413.74384509516\n",
      "Losgistic Regression(71000/100000): loss=-295424.3814324547\n",
      "Losgistic Regression(72000/100000): loss=-296423.7389413081\n",
      "Losgistic Regression(73000/100000): loss=-297412.15044429246\n",
      "Losgistic Regression(74000/100000): loss=-298389.82647354365\n",
      "Losgistic Regression(75000/100000): loss=-299357.0674870768\n",
      "Losgistic Regression(76000/100000): loss=-300314.1338254353\n",
      "Losgistic Regression(77000/100000): loss=-301261.27851260814\n",
      "Losgistic Regression(78000/100000): loss=-302198.7446867059\n",
      "Losgistic Regression(79000/100000): loss=-303126.7666095449\n",
      "Losgistic Regression(80000/100000): loss=-304045.5655279103\n",
      "Losgistic Regression(81000/100000): loss=-304955.3565329858\n",
      "Losgistic Regression(82000/100000): loss=-305856.3484860978\n",
      "Losgistic Regression(83000/100000): loss=-306748.75882233016\n",
      "Losgistic Regression(84000/100000): loss=-307632.77709619916\n",
      "Losgistic Regression(85000/100000): loss=-308508.6003767585\n",
      "Losgistic Regression(86000/100000): loss=-309376.48164036963\n",
      "Losgistic Regression(87000/100000): loss=-310236.55093015207\n",
      "Losgistic Regression(88000/100000): loss=-311088.9346418826\n",
      "Losgistic Regression(89000/100000): loss=-311933.8037545129\n",
      "Losgistic Regression(90000/100000): loss=-312771.31804656103\n",
      "Losgistic Regression(91000/100000): loss=-313601.6334482724\n",
      "Losgistic Regression(92000/100000): loss=-314424.8991449588\n",
      "Losgistic Regression(93000/100000): loss=-315241.25893033994\n",
      "Losgistic Regression(94000/100000): loss=-316050.8514492733\n",
      "Losgistic Regression(95000/100000): loss=-316853.808615489\n",
      "Losgistic Regression(96000/100000): loss=-317650.2647809219\n",
      "Losgistic Regression(97000/100000): loss=-318440.3446491304\n",
      "Losgistic Regression(98000/100000): loss=-319224.1700612231\n",
      "Losgistic Regression(99000/100000): loss=-320001.8591833757\n",
      "0.843 0.77476\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(1000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=1e-5, \n",
    "                   max_iters = 100000, lambda_=0.0001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only items whose values are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 16, 19, 21, 22, 23, 26, 29]"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fill_na_test(method=np.mean):\n",
    "    filled = tX_test.copy()\n",
    "    for col in columns_with_missing_values:\n",
    "        tmp = filled[:, col]\n",
    "        tmp[tmp == -999] = method(tmp[tmp != -999])\n",
    "        filled[:, col] = tmp\n",
    "    return filled\n",
    "\n",
    "\n",
    "filled_test_tX = fill_na_test(np.median)\n",
    "\n",
    "columns_non_negative = []\n",
    "for i in range(n_total_features):\n",
    "    if len(filled_test_tX[filled_test_tX[:, i] < 0, i]) == 0:\n",
    "        columns_non_negative.append(i)\n",
    "\n",
    "columns_non_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 121)"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = build_polynomial_without_mixed_term(filled_test_tX, degree=4)\n",
    "## We can take logs of each column Here *******************************\n",
    "test_tX, _, _ = standardize(tmp, mean_x7, std_x7)\n",
    "test_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y0=transform_y_back(prediction(test_tX, w_agdr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178677,)"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0[y0==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176985,)"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0[y0==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238,)"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/result6.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = transform_y_back(y0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "023cb81475a945fe97d29be3cb3600e9": {
     "views": []
    },
    "0656d01812174c589117e89a7f476eeb": {
     "views": []
    },
    "06e08db57e4c4440a3ad1b3951854893": {
     "views": []
    },
    "09d8152e0e4842f281b6992d966f84ff": {
     "views": []
    },
    "0b17fa30a11f499bbbb6b8402e16c6c0": {
     "views": []
    },
    "0e1159612a454425bbac65a45bf46ae4": {
     "views": []
    },
    "0e2681dfe3854507b8e382172c8d24a9": {
     "views": []
    },
    "0e3a8a0386ac4eeabbe0d75c720fe8ca": {
     "views": []
    },
    "12028e93822d44fb98bff43ebb5ae85f": {
     "views": []
    },
    "12c1d615bd694bcea1fd9ab1124e2733": {
     "views": []
    },
    "17693f1c9ae243fba5f88f6bd0563d5c": {
     "views": []
    },
    "19e769fc042640babb5295d1974c62b8": {
     "views": []
    },
    "19ee98066002482e9fc35c5c1daf61ba": {
     "views": []
    },
    "1a632485f2d8478193df72588650d618": {
     "views": []
    },
    "1a86c3fa0e2d4cd0b5251249d12134bc": {
     "views": []
    },
    "1ee72dc401884de7b2faf7aa90592e2e": {
     "views": []
    },
    "1f1d9fd36c85413abd59a0c5d2f6afac": {
     "views": []
    },
    "218b8bbbb96c44759803d1d7bb93518b": {
     "views": []
    },
    "22ac1bf8dc1a42ec9f6bf037c2b351e0": {
     "views": []
    },
    "2691c30802414c95b1e293e0281cb59c": {
     "views": []
    },
    "2802b13066f649798d7bf253e4bb2b91": {
     "views": []
    },
    "2f8de774e24d4bfbbe7fc7a6c589165b": {
     "views": []
    },
    "33a6f920af1a414eb26969a29ca0e9f5": {
     "views": []
    },
    "33d5f66f8db64bb29aee64e9e6f4b00a": {
     "views": []
    },
    "358ab636ff8949958e22afe4df4c92f5": {
     "views": []
    },
    "360a79441c40473eac4ed7aa38bc332e": {
     "views": []
    },
    "3732e557ec5f4420ae4be9ba1b0c6950": {
     "views": []
    },
    "3891a8618a4e46e0a33e3c1fc96e927f": {
     "views": []
    },
    "3b1570fddde04f0caf52f4ff45d29edb": {
     "views": []
    },
    "3d44ad00ad504e909c2c5165179b38f3": {
     "views": []
    },
    "3ff3abd2ef184017be0348ce847f1a3b": {
     "views": []
    },
    "4104a7b34dea4cac9f83200068a87e4e": {
     "views": []
    },
    "411967c7a0484176b05d585607f58205": {
     "views": []
    },
    "42283aa269b24b079eba746badb6b732": {
     "views": []
    },
    "42a2771cc4de461b8e1f7930eb1a5b05": {
     "views": []
    },
    "43533ec6a6424e228d82dedae9d2e033": {
     "views": []
    },
    "44341b3861874225ac2c4a39d9e9b20a": {
     "views": []
    },
    "4514ac6a980d4c1cb0b2865b036f78b4": {
     "views": []
    },
    "462a940358654d2b84aa86fb2ddb7fa5": {
     "views": []
    },
    "467d39db9951416ca21632ab9321781c": {
     "views": []
    },
    "48db65e31eb143489065ea6f27cdafee": {
     "views": []
    },
    "4a47e66ec8f145b2a4ce40452beb05eb": {
     "views": []
    },
    "4e8c9deaad524bda8b1da06a0ec346ca": {
     "views": []
    },
    "514aac3eeee24c6d950fdbc506e2c983": {
     "views": []
    },
    "51600ba1f25f46f28712de210eecac48": {
     "views": []
    },
    "53b9a6a0056f4d3eb3d049a6b79902e6": {
     "views": []
    },
    "54f2df5f3b374a7b92906e2c6f9ec3b6": {
     "views": []
    },
    "55a0f08bc43a45199474b57bfab46e44": {
     "views": []
    },
    "5608d42ea9e94999ab42b8ad0c206f85": {
     "views": []
    },
    "58b9d8f8c94d4706bfa0fb67ecbbd1c8": {
     "views": []
    },
    "59566267562941e28c581d183adfb31b": {
     "views": []
    },
    "59b6af240339412ca33740de32807e72": {
     "views": []
    },
    "5a53fd2cbfcc483e84a2d66c3874dcd4": {
     "views": []
    },
    "5c9a253daa9641bdb41bb1e907995314": {
     "views": []
    },
    "5da1672732bf4560a2f195fbfa18fd2f": {
     "views": []
    },
    "5f61c6f48bc6407f808b26dfd3ff650e": {
     "views": []
    },
    "60025b85d7d7483cbd0389d15fa40b24": {
     "views": []
    },
    "6290e48913ff4b9785efd7c0fc29e29a": {
     "views": []
    },
    "64aa7fe3a5964b7289a6e95b667b7697": {
     "views": []
    },
    "68319ce5b3a74f10b5004a4a7e6e9bde": {
     "views": []
    },
    "69b3066124164f6e9fee59423542e6ce": {
     "views": []
    },
    "6f2439baf7df4a6ba620e3c431601ec5": {
     "views": []
    },
    "712bf0da5abb466daed7cb112ac4be9f": {
     "views": []
    },
    "737b5fb46e5647d58c0167b33056f32d": {
     "views": []
    },
    "74e7c1ac51d44ba99eb3190482e44c9f": {
     "views": []
    },
    "74f46c0299024dbd9fa900a91a41e71c": {
     "views": []
    },
    "7646e6b27e164eb4a922f3369c0d1359": {
     "views": []
    },
    "76716f91d3974e5188e3ce2031b8edc6": {
     "views": []
    },
    "7856af6c59bf4b02ad553e5de1667e6e": {
     "views": []
    },
    "84cc99bad75e48c79fd29af3b3b6cf23": {
     "views": []
    },
    "8554ffcd85804c4480cb9568679288e6": {
     "views": []
    },
    "86d8b3ca74394075a5040754a7d0f112": {
     "views": []
    },
    "8712c2d770b044e580b70be8ff47a70c": {
     "views": []
    },
    "89e5d5a410f148b3b9f038d07b985826": {
     "views": []
    },
    "8bc31e9d2e244f859b9944277b10685e": {
     "views": []
    },
    "8c8280c2e5e843ea912ce840c1f9a815": {
     "views": []
    },
    "8de53ca2548b46ffa4709418cdc5e47a": {
     "views": []
    },
    "8f571f7019b648d6982443a00678ca09": {
     "views": []
    },
    "8fd8e55f342e4cd2854b16ae86e8431b": {
     "views": []
    },
    "905c684e215f46b9851f87a98a19cbe8": {
     "views": []
    },
    "910792a394e442a2b3537417187c38f0": {
     "views": [
      {
       "cell_index": 137
      }
     ]
    },
    "92c8926847d9495698e180d643661298": {
     "views": []
    },
    "957d7a710a95407ba7de2bc66a90cb87": {
     "views": []
    },
    "95b6733c17ac4871b980ba45f329b81e": {
     "views": []
    },
    "971324d43edb41bc906914d34cac4e84": {
     "views": []
    },
    "97aa3c9800f04dff94ded3f655bfda84": {
     "views": []
    },
    "9873d25dd712418e87df9d8e7d1140d3": {
     "views": [
      {
       "cell_index": 137
      }
     ]
    },
    "9cdc11d2dbb545308ed63a064fc63ece": {
     "views": []
    },
    "9f070cfb9678487d907255f42842b0fb": {
     "views": []
    },
    "9fedd7eee0504abbad19cbda3aebb022": {
     "views": []
    },
    "9ffc35d8432c40ee9895c565f3c4abda": {
     "views": []
    },
    "a0a41254ad204744aa98206b576555d0": {
     "views": []
    },
    "a72d857b82d54d389cc8a371a4b03d39": {
     "views": []
    },
    "a74ee4818b264fbb95d165877824da55": {
     "views": []
    },
    "a9f0e192ef384cabb84c130bb60cfaea": {
     "views": []
    },
    "aa47af235b6e4335a9843f236ab8fef9": {
     "views": []
    },
    "aafafbbeae1c40cebeb269ac6833d061": {
     "views": []
    },
    "af20bec9e7a54c8ea66dc4787a3a09ea": {
     "views": []
    },
    "b1b641006f214f70a713be0f41831858": {
     "views": []
    },
    "b4c1f164f05b4f3bb3a437b454a9e499": {
     "views": []
    },
    "ba3ea59619e4415b9aa586a7af6b945a": {
     "views": []
    },
    "bbd68922ec024005a35d8f6c24577f3c": {
     "views": []
    },
    "bc3a26e14c1e4962a46cf7287e8991b6": {
     "views": []
    },
    "bf54fb7095864dcba467fe64fb76a32d": {
     "views": []
    },
    "bfa043dfddd741abb68307bd0bff96b2": {
     "views": []
    },
    "c0f3489352ed4aa3b0b6ab1abb9777d8": {
     "views": []
    },
    "c107053cbde24768a5c39237e591246f": {
     "views": []
    },
    "c2853ffd303b4614aeeac08b8f09141a": {
     "views": []
    },
    "c7dc57c0cd694805bac3f843578ae253": {
     "views": []
    },
    "ca45a1359d0a47bf93208f7941df740a": {
     "views": []
    },
    "cc59f3ba01b04abfa8cd8b84857ed625": {
     "views": []
    },
    "cc6eb0825e3b4e6991f36100c530879f": {
     "views": []
    },
    "cde680dc31024d48b4812e5fefc39266": {
     "views": []
    },
    "ce5f660489ae4eb68bec161d9d3afce6": {
     "views": []
    },
    "cf32fbdee557427e8a9d06c5e36c7d24": {
     "views": []
    },
    "d3c5139772f14218afa6d3cf87edefb9": {
     "views": []
    },
    "d5d8edac095d4c858774ee96236435c0": {
     "views": []
    },
    "d76b311a962d49bd9af777ff5f3f6132": {
     "views": []
    },
    "da3ca8bd2ea84ef39e5e1280c6bebc25": {
     "views": []
    },
    "de3928afe68841acb45bd27c88dcb8f5": {
     "views": []
    },
    "df04152de8aa4696b0ffd12180b89e9c": {
     "views": []
    },
    "df0bf6fbe6514083a3c8dc0d19a23b2d": {
     "views": []
    },
    "e0ea1699749a41e687b5683624033244": {
     "views": []
    },
    "e2645b8789c242e18c5db1e0997cf20d": {
     "views": []
    },
    "e3e681e437c3484cb7f49bd2f0655669": {
     "views": []
    },
    "e841538f7d8742289247f2aa9626b7da": {
     "views": []
    },
    "e972522844ca40e1b9378bcffb5ce313": {
     "views": []
    },
    "e9a93b10f7e84f6d9247f21115304861": {
     "views": []
    },
    "eb932c58dcb649c784bd9ed1aef50a74": {
     "views": []
    },
    "ebf85c945d094c6c94f279c3522cf276": {
     "views": []
    },
    "edd161163b4f4188964b434d9a316a75": {
     "views": []
    },
    "ee2859b43dbe405db03026763ff27e6b": {
     "views": []
    },
    "ee4ecf624eb24702a501c36235e105d7": {
     "views": []
    },
    "f140365f0cef4c11b5c5390d1a64edc6": {
     "views": []
    },
    "f16dce26e29447fb92414f4a56c3d000": {
     "views": []
    },
    "f4888ce60df2468389ab39f32b9585b4": {
     "views": []
    },
    "f6930490a4964d05abfd130fcad37587": {
     "views": []
    },
    "f6aa17847f7a45d4bd2d512633e13fe4": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "f6d9425b6be14c40b50c15979e35be99": {
     "views": []
    },
    "fca522c4fd7a47be86a1aab9fa26494c": {
     "views": []
    },
    "fd7dab3919ab4f0f838fe25f216d2d81": {
     "views": []
    },
    "fec936e794b94e74be9eed7a0c172ba7": {
     "views": []
    }
   },
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
