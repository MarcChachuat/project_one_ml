{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from data_preprocessing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading of the data : done\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "# TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"loading of the data : done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio_of_training):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    p = np.random.permutation(np.arange(y.shape[0]))\n",
    "    n = int(y.shape[0] * ratio_of_training)\n",
    "    return  x[p][:n], x[p][n:], y[p][:n], y[p][n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFyCAYAAAAu+3oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ5IkqVUMJ5IkqVX+PyRetkpZgguTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d455a7358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "\n",
    "from ipywidgets import IntSlider, interact\n",
    "from IPython.display import display\n",
    "\n",
    "def plot_hist(tx, i, transformation=None):\n",
    "    plt.figure()\n",
    "    if transformation is None:\n",
    "        plt.hist(tx[:, i], bins=100)\n",
    "    else:\n",
    "        plt.hist(transformation(tx[:, i]), bins=100)\n",
    "    plt.title(\"feature %i\" % i)\n",
    "    plt.show()\n",
    "\n",
    "interact(lambda x:plot_hist(tX, x), x=IntSlider(min=0, max=29))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Good Features                   : [7, 10, 14, 15, 17, 18, 20\n",
    "    Suffering from outlier          : [3, 8, 19, 23, 26, 29]\n",
    "    Suffering from outlier (skewed) : [1, 2, 5, 9, 13, 16, 21]\n",
    "    missing values                  : [0, 4, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "    categorical                     : [11, 12, 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devide Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Features_Good            = [7, 10, 14, 15, 17, 18, 20]\n",
    "Features_with_outlier    = [3, 8, 19, 23, 26, 29]\n",
    "Features_skewed          = [1, 2, 5, 9, 13, 16, 21]\n",
    "Features_missing_entry   = [0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "Features_categorical     = [11, 12, 22]\n",
    "Features_non_categorical = [x for x in range(30) if x not in Features_categorical]\n",
    "Features_using_log       = np.union1d(Features_with_outlier, Features_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devide Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completeness_status_tX = np.sum(tX == -999, axis=1)\n",
    "non_missing_tX         = tX[completeness_status_tX==0, :]\n",
    "missing_tX             = tX[completeness_status_tX!=0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filled_tX_median     = fill_na(tX, np.median)\n",
    "filled_tX_mean       = fill_na(tX, np.mean)\n",
    "missing_indicator_tX = missing_indicator(tX)\n",
    "log_tX               = logs_of_features(filled_tX_median, Features_using_log)\n",
    "decomposed_tX        = decompose_categorical_features(filled_tX_median)\n",
    "inverse_tX           = inver_terms(filled_tX_median, Features_using_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 351)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_tX = mixed_features(filled_tX_median, Features_non_categorical)\n",
    "mixed_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Features_denominate = Features_using_log\n",
    "Features_numerator  = [x for x in Features_non_categorical if x not in Features_denominate]\n",
    "\n",
    "def mixed_inverse_features(tx, features_denominate, features_numerator):\n",
    "    foo = np.zeros(tx.shape[0])\n",
    "    for i in features_denominate:\n",
    "        for j in features_numerator:\n",
    "            foo = np.c_[foo, tx[:, i]/(tx[:, j] + 1e-8)]\n",
    "            \n",
    "    return foo[:, 1:]\n",
    "\n",
    "mixed_invese_tX = mixed_inverse_features(filled_tX_median, Features_denominate, Features_numerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 182)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_invese_tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# low degree but more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 478)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For non categorical features, build polynomials\n",
    "degree = 3\n",
    "poly_tX = build_polynomial_without_mixed_term(filled_tX_median[:, Features_non_categorical], degree=degree)\n",
    "log_poly_tX = build_polynomial_without_mixed_term(log_tX, degree=degree)\n",
    "# inv_poly_tX = build_polynomial_without_mixed_term(inverse_tX, degree=degree)\n",
    "\n",
    "# Build a design matrix\n",
    "design_matrix = np.c_[poly_tX, decomposed_tX, missing_indicator_tX, log_poly_tX, mixed_tX]\n",
    "tX9, mean_x, std_x = standardize(design_matrix)\n",
    "training_ratio = 0.9\n",
    "y9 = transform_y(y)\n",
    "train_tX9, cv_tX9, train_y9, cv_y9 = split_data(tX9, y9, training_ratio)\n",
    "cv_tX9.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_corr(tx):\n",
    "    print (tx.shape[1])\n",
    "    if (tx.shape[0] < 20000):\n",
    "        print (np.linalg.matrix_rank(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 39)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_poly_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n",
      "437\n"
     ]
    }
   ],
   "source": [
    "detect_corr(tX9[:10000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_var(y, tx, K, func, seed, fpred, faccuracy, max_fold):\n",
    "    \"\"\"Cross validation (varied version)\n",
    "    \n",
    "    When the datasets are large and we want to train on a small subset of data,\n",
    "    and validate them on the rest of datasets. We will first divide the the data\n",
    "    into K parts and train on only one of them, then test them on the rest K-1\n",
    "    parts.\n",
    "    \n",
    "    input:\n",
    "        y   : 1-D array (processed).\n",
    "        tx  : 2-D design matrix (processed).\n",
    "        K   : K-folded cross validation\n",
    "        func: function which takes (y, tx) as argument\n",
    "        seed: random seed\n",
    "        \n",
    "        fpred: prediction function\n",
    "        faccuracy: function for calculating accuracy\n",
    "        \n",
    "    return:\n",
    "        ave_acc : averaged accuracy\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    n = len(y)\n",
    "    tr_size = n // K\n",
    "    perm  = np.random.permutation(np.arange(n)).reshape(K, tr_size)\n",
    "    idx = np.ma.array(np.arange(K), mask=False)\n",
    "                                    \n",
    "    def get_index():\n",
    "        i = 0\n",
    "        while i < K:\n",
    "            idx.mask[i] = True\n",
    "            yield perm[i], perm[idx].flatten()\n",
    "            idx.mask[i] = False\n",
    "            i = i + 1\n",
    "    \n",
    "    tr_accs = []\n",
    "    cv_accs = []\n",
    "    for i, (tr_idx, cv_idx) in zip(np.arange(max_fold), get_index()):\n",
    "        start   = time.time()\n",
    "\n",
    "        w, cost = func(y[tr_idx], tx[tr_idx, :])\n",
    "        end     = time.time()\n",
    "        print (\"Time for {i:2}th cross validation = {t:3.6}s\".format(i=i, t=end-start))\n",
    "        \n",
    "        pred_y  = fpred(tx[cv_idx], w)\n",
    "        cv_acc  = faccuracy(pred_y, y[cv_idx])\n",
    "        pred_y  = fpred(tx[tr_idx], w)\n",
    "        tr_acc  = faccuracy(pred_y, y[tr_idx])\n",
    "        \n",
    "        print (\"Training Accuracy         = {a:6.10}\".format(a=tr_acc))\n",
    "        print (\"Cross Validation Accuracy = {a:6.10}\".format(a=cv_acc))\n",
    "        tr_accs.append(tr_acc)\n",
    "        cv_accs.append(cv_acc)\n",
    "        \n",
    "    return tr_accs, cv_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "               max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n",
    "\n",
    "cross_validation_var(y9, tX9, \n",
    "                     K=25, \n",
    "                     func=set_up_f, \n",
    "                     seed=3, \n",
    "                     fpred=predict_cv,\n",
    "                     faccuracy=accuracy,\n",
    "                     max_fold=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 6821.17151835445\n",
      "Losgistic Regression(     100/10000): loss= 4544.61276932469\n",
      "Losgistic Regression(     200/10000): loss= 4270.87902890875\n",
      "Losgistic Regression(     300/10000): loss= 4136.31849876117\n",
      "Losgistic Regression(     400/10000): loss= 4050.29705059181\n",
      "Losgistic Regression(     500/10000): loss= 3988.49090212743\n",
      "Losgistic Regression(     600/10000): loss= 3943.35903686592\n",
      "Losgistic Regression(     700/10000): loss= 3909.81930111496\n",
      "Losgistic Regression(     800/10000): loss= 3883.24686723897\n",
      "Losgistic Regression(     900/10000): loss= 3861.92395576994\n",
      "Losgistic Regression(    1000/10000): loss= 3844.11086419099\n",
      "Losgistic Regression(    1100/10000): loss= 3828.55174370073\n",
      "Losgistic Regression(    1200/10000): loss= 3814.97516728427\n",
      "Losgistic Regression(    1300/10000): loss= 3803.32463602277\n",
      "Losgistic Regression(    1400/10000): loss= 3793.47852120653\n",
      "Losgistic Regression(    1500/10000): loss= 3785.77659681995\n",
      "Losgistic Regression(    1600/10000): loss= 3779.62877646789\n",
      "Losgistic Regression(    1700/10000): loss= 3773.75118672646\n",
      "Losgistic Regression(    1800/10000): loss= 3768.68044831659\n",
      "Losgistic Regression(    1900/10000): loss= 3763.94138072544\n",
      "Losgistic Regression(    2000/10000): loss= 3760.64417754173\n",
      "Losgistic Regression(    2100/10000): loss= 3757.02012747612\n",
      "Losgistic Regression(    2200/10000): loss= 3753.20645176296\n",
      "Losgistic Regression(    2300/10000): loss= 3749.7145332604\n",
      "Losgistic Regression(    2400/10000): loss= 3746.20468791786\n",
      "Losgistic Regression(    2500/10000): loss= 3743.8349542535\n",
      "Losgistic Regression(    2600/10000): loss= 3741.39386586718\n",
      "Losgistic Regression(    2700/10000): loss= 3739.4868013759\n",
      "Losgistic Regression(    2800/10000): loss= 3737.83976350938\n",
      "Losgistic Regression(    2900/10000): loss= 3736.3970647501\n",
      "Losgistic Regression(    3000/10000): loss= 3735.10126867578\n",
      "Losgistic Regression(    3100/10000): loss= 3734.11172957363\n",
      "Losgistic Regression(    3200/10000): loss= 3733.09911052162\n",
      "Losgistic Regression(    3300/10000): loss= 3732.28750277501\n",
      "Losgistic Regression(    3400/10000): loss= 3731.61532850037\n",
      "Losgistic Regression(    3500/10000): loss= 3730.95700031378\n",
      "Losgistic Regression(    3600/10000): loss= 3730.24830265371\n",
      "Losgistic Regression(    3700/10000): loss= 3729.36354473536\n",
      "Losgistic Regression(    3800/10000): loss= 3728.46027016086\n",
      "Losgistic Regression(    3900/10000): loss= 3727.7599342903\n",
      "Losgistic Regression(    4000/10000): loss= 3727.16397669262\n",
      "Losgistic Regression(    4100/10000): loss= 3726.72127101203\n",
      "Losgistic Regression(    4200/10000): loss= 3726.21397041575\n",
      "Losgistic Regression(    4300/10000): loss= 3725.77445543366\n",
      "Losgistic Regression(    4400/10000): loss= 3725.4681742968\n",
      "Losgistic Regression(    4500/10000): loss= 3725.45189388825\n",
      "Totoal number of iterations =  4500\n",
      "Loss                        =  3725.45189389\n",
      "Time for  0th cross validation = 149.784s\n",
      "Training Accuracy         = 0.8382\n",
      "Cross Validation Accuracy = 0.814204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81420400000000004"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "               max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n",
    "\n",
    "def predict_cv(tx, w):\n",
    "    return 1 * ((tx @ w) > 0)\n",
    "\n",
    "cross_validation_var(y9, tX9, K=25, \n",
    "                     func=set_up_f, \n",
    "                     seed=3,\n",
    "                     fpred=predict_cv, \n",
    "                     faccuracy=accuracy, \n",
    "                     max_fold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(n, lambda_):\n",
    "    set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "                   max_iters = 10000, lambda_=lambda_, regularizor=regularizor_lasso)\n",
    "\n",
    "    return cross_validation_var(y9, tX9, K=(250000//n), \n",
    "                         func=set_up_f, \n",
    "                         seed=3,\n",
    "                         fpred=predict_cv, \n",
    "                         faccuracy=accuracy, \n",
    "                         max_fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 32495.5055050499\n",
      "Losgistic Regression(     100/10000): loss= 21564.8229086324\n",
      "Losgistic Regression(     200/10000): loss= 20662.8263215795\n",
      "Losgistic Regression(     300/10000): loss= 20224.9277174388\n",
      "Losgistic Regression(     400/10000): loss= 19944.7098363013\n",
      "Losgistic Regression(     500/10000): loss= 19817.4213630915\n",
      "Losgistic Regression(     600/10000): loss= 19733.4073985364\n",
      "Losgistic Regression(     700/10000): loss= 19680.1023139437\n",
      "Losgistic Regression(     800/10000): loss= 19630.6836645785\n",
      "Losgistic Regression(     900/10000): loss= 19601.2017266897\n",
      "Losgistic Regression(    1000/10000): loss= 19574.9996255812\n",
      "Losgistic Regression(    1100/10000): loss= 19543.7313412869\n",
      "Losgistic Regression(    1200/10000): loss= 19519.4291790914\n",
      "Losgistic Regression(    1300/10000): loss= 19499.7704212502\n",
      "Losgistic Regression(    1400/10000): loss= 19481.2780787485\n",
      "Losgistic Regression(    1500/10000): loss= 19466.3310751705\n",
      "Losgistic Regression(    1600/10000): loss= 19456.2925233962\n",
      "Losgistic Regression(    1700/10000): loss= 19446.7968686921\n",
      "Losgistic Regression(    1800/10000): loss= 19430.1296293733\n",
      "Losgistic Regression(    1900/10000): loss= 19414.8794986715\n",
      "Losgistic Regression(    2000/10000): loss= 19401.2403779494\n",
      "Losgistic Regression(    2100/10000): loss= 19388.4422266236\n",
      "Losgistic Regression(    2200/10000): loss= 19376.0394609109\n",
      "Losgistic Regression(    2300/10000): loss= 19364.4424184994\n",
      "Losgistic Regression(    2400/10000): loss= 19353.1680009544\n",
      "Losgistic Regression(    2500/10000): loss= 19343.5466977175\n",
      "Losgistic Regression(    2600/10000): loss= 19334.76313084\n",
      "Losgistic Regression(    2700/10000): loss= 19326.5405118335\n",
      "Losgistic Regression(    2800/10000): loss= 19319.6975070728\n",
      "Losgistic Regression(    2900/10000): loss= 19313.4791654836\n",
      "Losgistic Regression(    3000/10000): loss= 19307.7669963319\n",
      "Losgistic Regression(    3100/10000): loss= 19302.9922517587\n",
      "Losgistic Regression(    3200/10000): loss= 19298.5263101605\n",
      "Losgistic Regression(    3300/10000): loss= 19294.2872487719\n",
      "Losgistic Regression(    3400/10000): loss= 19290.510474663\n",
      "Losgistic Regression(    3500/10000): loss= 19287.0232206097\n",
      "Losgistic Regression(    3600/10000): loss= 19283.9838039791\n",
      "Losgistic Regression(    3700/10000): loss= 19281.2497295935\n",
      "Losgistic Regression(    3800/10000): loss= 19278.4036747805\n",
      "Losgistic Regression(    3900/10000): loss= 19275.6320763331\n",
      "Losgistic Regression(    4000/10000): loss= 19273.2294646009\n",
      "Losgistic Regression(    4100/10000): loss= 19271.2126615413\n",
      "Losgistic Regression(    4200/10000): loss= 19269.4212656785\n",
      "Losgistic Regression(    4300/10000): loss= 19267.7510856254\n",
      "Losgistic Regression(    4400/10000): loss= 19266.2355860609\n",
      "Losgistic Regression(    4500/10000): loss= 19264.8376037201\n",
      "Losgistic Regression(    4600/10000): loss= 19263.6093655765\n",
      "Losgistic Regression(    4700/10000): loss= 19262.360608453\n",
      "Losgistic Regression(    4800/10000): loss= 19261.2118432683\n",
      "Losgistic Regression(    4900/10000): loss= 19260.2864330224\n",
      "Losgistic Regression(    5000/10000): loss= 19259.0530503888\n",
      "Losgistic Regression(    5100/10000): loss= 19257.7007585288\n",
      "Losgistic Regression(    5200/10000): loss= 19256.3629636172\n",
      "Losgistic Regression(    5300/10000): loss= 19255.0469901238\n",
      "Losgistic Regression(    5400/10000): loss= 19253.7089056797\n",
      "Losgistic Regression(    5500/10000): loss= 19252.2646709865\n",
      "Losgistic Regression(    5600/10000): loss= 19250.7508678765\n",
      "Losgistic Regression(    5700/10000): loss= 19249.2361377556\n",
      "Losgistic Regression(    5800/10000): loss= 19247.7305356632\n",
      "Losgistic Regression(    5900/10000): loss= 19246.1477649683\n",
      "Losgistic Regression(    6000/10000): loss= 19244.5239920809\n",
      "Losgistic Regression(    6100/10000): loss= 19242.9414615399\n",
      "Losgistic Regression(    6200/10000): loss= 19241.3361522153\n",
      "Losgistic Regression(    6300/10000): loss= 19239.6404644808\n",
      "Losgistic Regression(    6400/10000): loss= 19237.8592857005\n",
      "Losgistic Regression(    6500/10000): loss= 19236.0538789776\n",
      "Losgistic Regression(    6600/10000): loss= 19234.2839598031\n",
      "Losgistic Regression(    6700/10000): loss= 19232.6105693946\n",
      "Losgistic Regression(    6800/10000): loss= 19230.9839074922\n",
      "Losgistic Regression(    6900/10000): loss= 19229.3938640386\n",
      "Losgistic Regression(    7000/10000): loss= 19227.8695328024\n",
      "Losgistic Regression(    7100/10000): loss= 19226.2865725753\n",
      "Losgistic Regression(    7200/10000): loss= 19224.6469802925\n",
      "Losgistic Regression(    7300/10000): loss= 19223.0301257956\n",
      "Losgistic Regression(    7400/10000): loss= 19221.4663956349\n",
      "Losgistic Regression(    7500/10000): loss= 19219.9457605995\n",
      "Losgistic Regression(    7600/10000): loss= 19218.4364582447\n",
      "Losgistic Regression(    7700/10000): loss= 19216.9385404067\n",
      "Losgistic Regression(    7800/10000): loss= 19215.4377534477\n",
      "Losgistic Regression(    7900/10000): loss= 19213.9260851938\n",
      "Losgistic Regression(    8000/10000): loss= 19212.4007157631\n",
      "Losgistic Regression(    8100/10000): loss= 19210.8495470755\n",
      "Losgistic Regression(    8200/10000): loss= 19209.2925796874\n",
      "Losgistic Regression(    8300/10000): loss= 19207.749606543\n",
      "Losgistic Regression(    8400/10000): loss= 19206.227883763\n",
      "Losgistic Regression(    8500/10000): loss= 19204.6941767014\n",
      "Losgistic Regression(    8600/10000): loss= 19203.1360697029\n",
      "Losgistic Regression(    8700/10000): loss= 19201.5847527178\n",
      "Losgistic Regression(    8800/10000): loss= 19200.0574891227\n",
      "Losgistic Regression(    8900/10000): loss= 19198.5758543315\n",
      "Losgistic Regression(    9000/10000): loss= 19197.11025292\n",
      "Losgistic Regression(    9100/10000): loss= 19195.6365615841\n",
      "Losgistic Regression(    9200/10000): loss= 19194.1550204319\n",
      "Losgistic Regression(    9300/10000): loss= 19192.6729795754\n",
      "Losgistic Regression(    9400/10000): loss= 19191.1922809511\n",
      "Losgistic Regression(    9500/10000): loss= 19189.7031778519\n",
      "Losgistic Regression(    9600/10000): loss= 19188.1876812741\n",
      "Losgistic Regression(    9700/10000): loss= 19186.6593824943\n",
      "Losgistic Regression(    9800/10000): loss= 19185.1509804011\n",
      "Losgistic Regression(    9900/10000): loss= 19183.6786951657\n",
      "Time for  0th cross validation = 1572.77s\n",
      "Training Accuracy         = 0.83044\n",
      "Cross Validation Accuracy = 0.825748\n",
      "Losgistic Regression(       0/10000): loss= 32430.8544326402\n",
      "Losgistic Regression(     100/10000): loss= 21599.04124445\n",
      "Losgistic Regression(     200/10000): loss= 20684.7509556007\n",
      "Losgistic Regression(     300/10000): loss= 20318.0835353782\n",
      "Losgistic Regression(     400/10000): loss= 20083.6498651339\n",
      "Losgistic Regression(     500/10000): loss= 19948.6502334014\n",
      "Losgistic Regression(     600/10000): loss= 19858.1024034387\n",
      "Losgistic Regression(     700/10000): loss= 19801.9004766224\n",
      "Losgistic Regression(     800/10000): loss= 19761.1016338108\n",
      "Losgistic Regression(     900/10000): loss= 19730.7388107109\n",
      "Losgistic Regression(    1000/10000): loss= 19707.4077864401\n",
      "Losgistic Regression(    1100/10000): loss= 19686.7018257475\n",
      "Losgistic Regression(    1200/10000): loss= 19661.3332051964\n",
      "Losgistic Regression(    1300/10000): loss= 19638.9430366569\n",
      "Losgistic Regression(    1400/10000): loss= 19619.9674781839\n",
      "Losgistic Regression(    1500/10000): loss= 19604.2506533007\n",
      "Losgistic Regression(    1600/10000): loss= 19591.4402459986\n",
      "Losgistic Regression(    1700/10000): loss= 19579.2677435103\n",
      "Losgistic Regression(    1800/10000): loss= 19567.8663033832\n",
      "Losgistic Regression(    1900/10000): loss= 19559.0501147331\n",
      "Losgistic Regression(    2000/10000): loss= 19551.9113326159\n",
      "Losgistic Regression(    2100/10000): loss= 19545.6312536513\n",
      "Losgistic Regression(    2200/10000): loss= 19540.7460035617\n",
      "Losgistic Regression(    2300/10000): loss= 19537.192415809\n",
      "Losgistic Regression(    2400/10000): loss= 19534.0120381479\n",
      "Losgistic Regression(    2500/10000): loss= 19531.2160441612\n",
      "Losgistic Regression(    2600/10000): loss= 19529.2940500073\n",
      "Losgistic Regression(    2700/10000): loss= 19527.4855490358\n",
      "Losgistic Regression(    2800/10000): loss= 19525.6461239361\n",
      "Losgistic Regression(    2900/10000): loss= 19523.8167067752\n",
      "Losgistic Regression(    3000/10000): loss= 19522.1200255428\n",
      "Losgistic Regression(    3100/10000): loss= 19520.3599119183\n",
      "Losgistic Regression(    3200/10000): loss= 19518.5072965437\n",
      "Losgistic Regression(    3300/10000): loss= 19516.7193299281\n",
      "Losgistic Regression(    3400/10000): loss= 19514.9972773305\n",
      "Losgistic Regression(    3500/10000): loss= 19513.1453568799\n",
      "Losgistic Regression(    3600/10000): loss= 19511.1678434039\n",
      "Losgistic Regression(    3700/10000): loss= 19509.1157468419\n",
      "Losgistic Regression(    3800/10000): loss= 19507.1123795788\n",
      "Losgistic Regression(    3900/10000): loss= 19505.3079178442\n",
      "Losgistic Regression(    4000/10000): loss= 19503.7623416966\n",
      "Losgistic Regression(    4100/10000): loss= 19502.4232547224\n",
      "Losgistic Regression(    4200/10000): loss= 19501.1270749691\n",
      "Losgistic Regression(    4300/10000): loss= 19499.9366733309\n",
      "Losgistic Regression(    4400/10000): loss= 19498.8945842839\n",
      "Losgistic Regression(    4500/10000): loss= 19497.8740349547\n",
      "Losgistic Regression(    4600/10000): loss= 19496.8193865171\n",
      "Losgistic Regression(    4700/10000): loss= 19495.7336653013\n",
      "Losgistic Regression(    4800/10000): loss= 19494.6510244573\n",
      "Losgistic Regression(    4900/10000): loss= 19493.5319267062\n",
      "Losgistic Regression(    5000/10000): loss= 19492.3184969805\n",
      "Losgistic Regression(    5100/10000): loss= 19490.9955074223\n",
      "Losgistic Regression(    5200/10000): loss= 19489.6262335187\n",
      "Losgistic Regression(    5300/10000): loss= 19488.2744658504\n",
      "Losgistic Regression(    5400/10000): loss= 19486.9410118277\n",
      "Losgistic Regression(    5500/10000): loss= 19485.6047547885\n",
      "Losgistic Regression(    5600/10000): loss= 19484.2797492952\n",
      "Losgistic Regression(    5700/10000): loss= 19482.9804151696\n",
      "Losgistic Regression(    5800/10000): loss= 19481.7534268032\n",
      "Losgistic Regression(    5900/10000): loss= 19480.6432090077\n",
      "Losgistic Regression(    6000/10000): loss= 19479.6009673465\n",
      "Losgistic Regression(    6100/10000): loss= 19478.5904876223\n",
      "Losgistic Regression(    6200/10000): loss= 19477.620079243\n",
      "Losgistic Regression(    6300/10000): loss= 19476.6636047198\n",
      "Losgistic Regression(    6400/10000): loss= 19475.6500009469\n",
      "Losgistic Regression(    6500/10000): loss= 19474.5727221311\n",
      "Losgistic Regression(    6600/10000): loss= 19473.4857607022\n",
      "Losgistic Regression(    6700/10000): loss= 19472.4010049531\n",
      "Losgistic Regression(    6800/10000): loss= 19471.2820526971\n",
      "Losgistic Regression(    6900/10000): loss= 19470.1176334237\n",
      "Losgistic Regression(    7000/10000): loss= 19468.9265500786\n",
      "Losgistic Regression(    7100/10000): loss= 19467.6983083102\n",
      "Losgistic Regression(    7200/10000): loss= 19466.4479615605\n",
      "Losgistic Regression(    7300/10000): loss= 19465.1906315022\n",
      "Losgistic Regression(    7400/10000): loss= 19463.9348039651\n",
      "Losgistic Regression(    7500/10000): loss= 19462.6811983829\n",
      "Losgistic Regression(    7600/10000): loss= 19461.4282867996\n",
      "Losgistic Regression(    7700/10000): loss= 19460.1934522517\n",
      "Losgistic Regression(    7800/10000): loss= 19458.9788507323\n",
      "Losgistic Regression(    7900/10000): loss= 19457.7784534106\n",
      "Losgistic Regression(    8000/10000): loss= 19456.594652873\n",
      "Losgistic Regression(    8100/10000): loss= 19455.4209508398\n",
      "Losgistic Regression(    8200/10000): loss= 19454.2350373318\n",
      "Losgistic Regression(    8300/10000): loss= 19453.019423413\n",
      "Losgistic Regression(    8400/10000): loss= 19451.7616202877\n",
      "Losgistic Regression(    8500/10000): loss= 19450.4504596768\n",
      "Losgistic Regression(    8600/10000): loss= 19449.0947115199\n",
      "Losgistic Regression(    8700/10000): loss= 19447.7253669106\n",
      "Losgistic Regression(    8800/10000): loss= 19446.3568607434\n",
      "Losgistic Regression(    8900/10000): loss= 19444.9912535781\n",
      "Losgistic Regression(    9000/10000): loss= 19443.633732636\n",
      "Losgistic Regression(    9100/10000): loss= 19442.2794324418\n",
      "Losgistic Regression(    9200/10000): loss= 19440.9176085427\n",
      "Losgistic Regression(    9300/10000): loss= 19439.5558748685\n",
      "Losgistic Regression(    9400/10000): loss= 19438.2056360286\n",
      "Losgistic Regression(    9500/10000): loss= 19436.8553153137\n",
      "Losgistic Regression(    9600/10000): loss= 19435.4905117289\n",
      "Losgistic Regression(    9700/10000): loss= 19434.1086196866\n",
      "Losgistic Regression(    9800/10000): loss= 19432.719653036\n",
      "Losgistic Regression(    9900/10000): loss= 19431.332054208\n",
      "Time for  1th cross validation = 1565.21s\n",
      "Training Accuracy         = 0.82926\n",
      "Cross Validation Accuracy = 0.825644\n",
      "Losgistic Regression(       0/10000): loss= 32484.361937325\n",
      "Losgistic Regression(     100/10000): loss= 21538.9442937084\n",
      "Losgistic Regression(     200/10000): loss= 20575.8311316921\n",
      "Losgistic Regression(     300/10000): loss= 20209.7026941608\n",
      "Losgistic Regression(     400/10000): loss= 20012.6369425414\n",
      "Losgistic Regression(     500/10000): loss= 19853.121028882\n",
      "Losgistic Regression(     600/10000): loss= 19715.1013911901\n",
      "Losgistic Regression(     700/10000): loss= 19662.0674526783\n",
      "Losgistic Regression(     800/10000): loss= 19633.0034955672\n",
      "Losgistic Regression(     900/10000): loss= 19597.9735716074\n",
      "Losgistic Regression(    1000/10000): loss= 19573.4131690464\n",
      "Losgistic Regression(    1100/10000): loss= 19548.4977381441\n",
      "Losgistic Regression(    1200/10000): loss= 19531.3968325107\n",
      "Losgistic Regression(    1300/10000): loss= 19518.9583564156\n",
      "Losgistic Regression(    1400/10000): loss= 19506.8629717855\n",
      "Losgistic Regression(    1500/10000): loss= 19495.3731409293\n",
      "Losgistic Regression(    1600/10000): loss= 19486.0400701787\n",
      "Losgistic Regression(    1700/10000): loss= 19478.8080753245\n",
      "Losgistic Regression(    1800/10000): loss= 19472.9537104905\n",
      "Losgistic Regression(    1900/10000): loss= 19468.0731428088\n",
      "Losgistic Regression(    2000/10000): loss= 19463.5378629271\n",
      "Losgistic Regression(    2100/10000): loss= 19458.7673362077\n",
      "Losgistic Regression(    2200/10000): loss= 19455.1643927968\n",
      "Losgistic Regression(    2300/10000): loss= 19452.0862111491\n",
      "Losgistic Regression(    2400/10000): loss= 19450.2558239669\n",
      "Losgistic Regression(    2500/10000): loss= 19448.6462799523\n",
      "Losgistic Regression(    2600/10000): loss= 19447.0286706156\n",
      "Losgistic Regression(    2700/10000): loss= 19445.9116019734\n",
      "Losgistic Regression(    2800/10000): loss= 19444.6769142309\n",
      "Losgistic Regression(    2900/10000): loss= 19443.7515794275\n",
      "Losgistic Regression(    3000/10000): loss= 19442.9889807223\n",
      "Losgistic Regression(    3100/10000): loss= 19442.0176534249\n",
      "Losgistic Regression(    3200/10000): loss= 19441.4209142132\n",
      "Losgistic Regression(    3300/10000): loss= 19440.9483728526\n",
      "Losgistic Regression(    3400/10000): loss= 19440.4628895411\n",
      "Losgistic Regression(    3500/10000): loss= 19440.1413454046\n",
      "Losgistic Regression(    3600/10000): loss= 19439.7835446785\n",
      "Losgistic Regression(    3700/10000): loss= 19439.353229629\n",
      "Losgistic Regression(    3800/10000): loss= 19438.9897752017\n",
      "Losgistic Regression(    3900/10000): loss= 19438.7191879495\n",
      "Losgistic Regression(    4000/10000): loss= 19437.9948619698\n",
      "Losgistic Regression(    4100/10000): loss= 19436.9495969903\n",
      "Losgistic Regression(    4200/10000): loss= 19435.6801380499\n",
      "Losgistic Regression(    4300/10000): loss= 19434.1401740793\n",
      "Losgistic Regression(    4400/10000): loss= 19432.6719426484\n",
      "Losgistic Regression(    4500/10000): loss= 19431.1190732386\n",
      "Losgistic Regression(    4600/10000): loss= 19429.4490213616\n",
      "Losgistic Regression(    4700/10000): loss= 19427.7223050537\n",
      "Losgistic Regression(    4800/10000): loss= 19425.7714745321\n",
      "Losgistic Regression(    4900/10000): loss= 19423.7757845546\n",
      "Losgistic Regression(    5000/10000): loss= 19421.7800362859\n",
      "Losgistic Regression(    5100/10000): loss= 19419.3393982303\n",
      "Losgistic Regression(    5200/10000): loss= 19416.8102444281\n",
      "Losgistic Regression(    5300/10000): loss= 19414.3284219976\n",
      "Losgistic Regression(    5400/10000): loss= 19411.8254396992\n",
      "Losgistic Regression(    5500/10000): loss= 19409.410734204\n",
      "Losgistic Regression(    5600/10000): loss= 19407.1637133804\n",
      "Losgistic Regression(    5700/10000): loss= 19405.0174127015\n",
      "Losgistic Regression(    5800/10000): loss= 19402.9017947908\n",
      "Losgistic Regression(    5900/10000): loss= 19400.8641155746\n",
      "Losgistic Regression(    6000/10000): loss= 19398.8299978341\n",
      "Losgistic Regression(    6100/10000): loss= 19396.7641817568\n",
      "Losgistic Regression(    6200/10000): loss= 19394.7340557142\n",
      "Losgistic Regression(    6300/10000): loss= 19392.7255077411\n",
      "Losgistic Regression(    6400/10000): loss= 19390.7099513855\n",
      "Losgistic Regression(    6500/10000): loss= 19388.7357912997\n",
      "Losgistic Regression(    6600/10000): loss= 19386.8156873077\n",
      "Losgistic Regression(    6700/10000): loss= 19384.9118144942\n",
      "Losgistic Regression(    6800/10000): loss= 19383.0387385382\n",
      "Losgistic Regression(    6900/10000): loss= 19381.1874066391\n",
      "Losgistic Regression(    7000/10000): loss= 19379.2576314146\n",
      "Losgistic Regression(    7100/10000): loss= 19377.2426614692\n",
      "Losgistic Regression(    7200/10000): loss= 19375.2477264512\n",
      "Losgistic Regression(    7300/10000): loss= 19373.266153681\n",
      "Losgistic Regression(    7400/10000): loss= 19371.2416695617\n",
      "Losgistic Regression(    7500/10000): loss= 19369.2251417709\n",
      "Losgistic Regression(    7600/10000): loss= 19367.2601323437\n",
      "Losgistic Regression(    7700/10000): loss= 19365.3192188943\n",
      "Losgistic Regression(    7800/10000): loss= 19363.3863785686\n",
      "Losgistic Regression(    7900/10000): loss= 19361.4308000576\n",
      "Losgistic Regression(    8000/10000): loss= 19359.4271418403\n",
      "Losgistic Regression(    8100/10000): loss= 19357.4270371815\n",
      "Losgistic Regression(    8200/10000): loss= 19355.4750915226\n",
      "Losgistic Regression(    8300/10000): loss= 19353.5649251633\n",
      "Losgistic Regression(    8400/10000): loss= 19351.6708951204\n",
      "Losgistic Regression(    8500/10000): loss= 19349.7591068579\n",
      "Losgistic Regression(    8600/10000): loss= 19347.8471723066\n",
      "Losgistic Regression(    8700/10000): loss= 19345.9713048355\n",
      "Losgistic Regression(    8800/10000): loss= 19344.1224012095\n",
      "Losgistic Regression(    8900/10000): loss= 19342.2834165722\n",
      "Losgistic Regression(    9000/10000): loss= 19340.4541345703\n",
      "Losgistic Regression(    9100/10000): loss= 19338.6098361892\n",
      "Losgistic Regression(    9200/10000): loss= 19336.7407319066\n",
      "Losgistic Regression(    9300/10000): loss= 19334.8819758221\n",
      "Losgistic Regression(    9400/10000): loss= 19333.0463120563\n",
      "Losgistic Regression(    9500/10000): loss= 19331.2249260427\n",
      "Losgistic Regression(    9600/10000): loss= 19329.4373537606\n",
      "Losgistic Regression(    9700/10000): loss= 19327.6857901904\n",
      "Losgistic Regression(    9800/10000): loss= 19325.9467255736\n",
      "Losgistic Regression(    9900/10000): loss= 19324.2241508384\n",
      "Time for  2th cross validation = 1564.62s\n",
      "Training Accuracy         = 0.82714\n",
      "Cross Validation Accuracy = 0.825188\n",
      "Losgistic Regression(       0/10000): loss= 32421.0239276688\n",
      "Losgistic Regression(     100/10000): loss= 21290.0972754909\n",
      "Losgistic Regression(     200/10000): loss= 20324.8288503923\n",
      "Losgistic Regression(     300/10000): loss= 19918.0354346579\n",
      "Losgistic Regression(     400/10000): loss= 19711.8976719302\n",
      "Losgistic Regression(     500/10000): loss= 19583.0658325858\n",
      "Losgistic Regression(     600/10000): loss= 19487.3879415566\n",
      "Losgistic Regression(     700/10000): loss= 19412.859150738\n",
      "Losgistic Regression(     800/10000): loss= 19352.375866439\n",
      "Losgistic Regression(     900/10000): loss= 19297.952099343\n",
      "Losgistic Regression(    1000/10000): loss= 19253.1876050299\n",
      "Losgistic Regression(    1100/10000): loss= 19211.7328219639\n",
      "Losgistic Regression(    1200/10000): loss= 19176.9623563348\n",
      "Losgistic Regression(    1300/10000): loss= 19147.9928463556\n",
      "Losgistic Regression(    1400/10000): loss= 19123.9797169901\n",
      "Losgistic Regression(    1500/10000): loss= 19104.3946911485\n",
      "Losgistic Regression(    1600/10000): loss= 19085.6019753314\n",
      "Losgistic Regression(    1700/10000): loss= 19069.7453286016\n",
      "Losgistic Regression(    1800/10000): loss= 19057.2954450652\n",
      "Losgistic Regression(    1900/10000): loss= 19046.7006659654\n",
      "Losgistic Regression(    2000/10000): loss= 19038.1171217579\n",
      "Losgistic Regression(    2100/10000): loss= 19032.4554888733\n",
      "Losgistic Regression(    2200/10000): loss= 19027.8955835587\n",
      "Losgistic Regression(    2300/10000): loss= 19024.4668821426\n",
      "Losgistic Regression(    2400/10000): loss= 19022.0122064944\n",
      "Losgistic Regression(    2500/10000): loss= 19019.4573812493\n",
      "Losgistic Regression(    2600/10000): loss= 19018.1646827277\n",
      "Losgistic Regression(    2700/10000): loss= 19017.7112226037\n",
      "Losgistic Regression(    2800/10000): loss= 19017.1319591682\n",
      "Losgistic Regression(    2900/10000): loss= 19017.1239213613\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  19017.1239214\n",
      "Time for  3th cross validation = 459.049s\n",
      "Training Accuracy         = 0.83356\n",
      "Cross Validation Accuracy = 0.82726\n",
      "Losgistic Regression(       0/10000): loss= 32417.8016499762\n",
      "Losgistic Regression(     100/10000): loss= 21596.8062272154\n",
      "Losgistic Regression(     200/10000): loss= 20706.1146669791\n",
      "Losgistic Regression(     300/10000): loss= 20304.4523962884\n",
      "Losgistic Regression(     400/10000): loss= 20045.9719824263\n",
      "Losgistic Regression(     500/10000): loss= 19890.9194411855\n",
      "Losgistic Regression(     600/10000): loss= 19804.1860952967\n",
      "Losgistic Regression(     700/10000): loss= 19752.9251982065\n",
      "Losgistic Regression(     800/10000): loss= 19711.5037365382\n",
      "Losgistic Regression(     900/10000): loss= 19678.0203392381\n",
      "Losgistic Regression(    1000/10000): loss= 19646.1373816486\n",
      "Losgistic Regression(    1100/10000): loss= 19620.8876680739\n",
      "Losgistic Regression(    1200/10000): loss= 19599.4807333582\n",
      "Losgistic Regression(    1300/10000): loss= 19578.765860511\n",
      "Losgistic Regression(    1400/10000): loss= 19560.5088901117\n",
      "Losgistic Regression(    1500/10000): loss= 19546.0614661924\n",
      "Losgistic Regression(    1600/10000): loss= 19533.8880874613\n",
      "Losgistic Regression(    1700/10000): loss= 19523.534357563\n",
      "Losgistic Regression(    1800/10000): loss= 19515.4597240794\n",
      "Losgistic Regression(    1900/10000): loss= 19509.2060139335\n",
      "Losgistic Regression(    2000/10000): loss= 19504.5643721765\n",
      "Losgistic Regression(    2100/10000): loss= 19501.576850658\n",
      "Losgistic Regression(    2200/10000): loss= 19497.1968574958\n",
      "Losgistic Regression(    2300/10000): loss= 19493.1520798459\n",
      "Losgistic Regression(    2400/10000): loss= 19489.5483639593\n",
      "Losgistic Regression(    2500/10000): loss= 19486.2248316363\n",
      "Losgistic Regression(    2600/10000): loss= 19483.1856873363\n",
      "Losgistic Regression(    2700/10000): loss= 19480.5195172191\n",
      "Losgistic Regression(    2800/10000): loss= 19478.2976238164\n",
      "Losgistic Regression(    2900/10000): loss= 19476.1162660082\n",
      "Losgistic Regression(    3000/10000): loss= 19473.8636982499\n",
      "Losgistic Regression(    3100/10000): loss= 19471.5660460609\n",
      "Losgistic Regression(    3200/10000): loss= 19469.5581251878\n",
      "Losgistic Regression(    3300/10000): loss= 19467.9538517228\n",
      "Losgistic Regression(    3400/10000): loss= 19466.5755614918\n",
      "Losgistic Regression(    3500/10000): loss= 19465.3406722238\n",
      "Losgistic Regression(    3600/10000): loss= 19464.2026304868\n",
      "Losgistic Regression(    3700/10000): loss= 19463.0815980198\n",
      "Losgistic Regression(    3800/10000): loss= 19461.6887045151\n",
      "Losgistic Regression(    3900/10000): loss= 19460.4377434662\n",
      "Losgistic Regression(    4000/10000): loss= 19459.2302902246\n",
      "Losgistic Regression(    4100/10000): loss= 19457.8618792081\n",
      "Losgistic Regression(    4200/10000): loss= 19456.1574462483\n",
      "Losgistic Regression(    4300/10000): loss= 19454.4812579397\n",
      "Losgistic Regression(    4400/10000): loss= 19452.9272381468\n",
      "Losgistic Regression(    4500/10000): loss= 19451.3600527487\n",
      "Losgistic Regression(    4600/10000): loss= 19449.6812916465\n",
      "Losgistic Regression(    4700/10000): loss= 19447.9629420371\n",
      "Losgistic Regression(    4800/10000): loss= 19446.2120439491\n",
      "Losgistic Regression(    4900/10000): loss= 19444.3116967411\n",
      "Losgistic Regression(    5000/10000): loss= 19442.3324439365\n",
      "Losgistic Regression(    5100/10000): loss= 19440.3778609905\n",
      "Losgistic Regression(    5200/10000): loss= 19438.4307659251\n",
      "Losgistic Regression(    5300/10000): loss= 19436.4662582813\n",
      "Losgistic Regression(    5400/10000): loss= 19434.472858109\n",
      "Losgistic Regression(    5500/10000): loss= 19432.43118615\n",
      "Losgistic Regression(    5600/10000): loss= 19430.3647922463\n",
      "Losgistic Regression(    5700/10000): loss= 19428.321342216\n",
      "Losgistic Regression(    5800/10000): loss= 19426.3374289903\n",
      "Losgistic Regression(    5900/10000): loss= 19424.4762239091\n",
      "Losgistic Regression(    6000/10000): loss= 19422.7315956635\n",
      "Losgistic Regression(    6100/10000): loss= 19420.9793350522\n",
      "Losgistic Regression(    6200/10000): loss= 19419.2404585467\n",
      "Losgistic Regression(    6300/10000): loss= 19417.5926559392\n",
      "Losgistic Regression(    6400/10000): loss= 19415.981013043\n",
      "Losgistic Regression(    6500/10000): loss= 19414.3945310661\n",
      "Losgistic Regression(    6600/10000): loss= 19412.8327079024\n",
      "Losgistic Regression(    6700/10000): loss= 19411.2463695409\n",
      "Losgistic Regression(    6800/10000): loss= 19409.6346475727\n",
      "Losgistic Regression(    6900/10000): loss= 19407.8529706486\n",
      "Losgistic Regression(    7000/10000): loss= 19406.0014994338\n",
      "Losgistic Regression(    7100/10000): loss= 19404.108520918\n",
      "Losgistic Regression(    7200/10000): loss= 19402.1588819238\n",
      "Losgistic Regression(    7300/10000): loss= 19400.1864517289\n",
      "Losgistic Regression(    7400/10000): loss= 19398.224044968\n",
      "Losgistic Regression(    7500/10000): loss= 19396.291790151\n",
      "Losgistic Regression(    7600/10000): loss= 19394.3609644204\n",
      "Losgistic Regression(    7700/10000): loss= 19392.4115743509\n",
      "Losgistic Regression(    7800/10000): loss= 19390.4800788593\n",
      "Losgistic Regression(    7900/10000): loss= 19388.591723337\n",
      "Losgistic Regression(    8000/10000): loss= 19386.7179383853\n",
      "Losgistic Regression(    8100/10000): loss= 19384.8200226443\n",
      "Losgistic Regression(    8200/10000): loss= 19382.9041905644\n",
      "Losgistic Regression(    8300/10000): loss= 19380.9909623974\n",
      "Losgistic Regression(    8400/10000): loss= 19379.0689488139\n",
      "Losgistic Regression(    8500/10000): loss= 19377.1188839603\n",
      "Losgistic Regression(    8600/10000): loss= 19375.1517563732\n",
      "Losgistic Regression(    8700/10000): loss= 19373.1777428547\n",
      "Losgistic Regression(    8800/10000): loss= 19371.1994226595\n",
      "Losgistic Regression(    8900/10000): loss= 19369.2359466874\n",
      "Losgistic Regression(    9000/10000): loss= 19367.2997020075\n",
      "Losgistic Regression(    9100/10000): loss= 19365.3976705077\n",
      "Losgistic Regression(    9200/10000): loss= 19363.5212669917\n",
      "Losgistic Regression(    9300/10000): loss= 19361.6554803801\n",
      "Losgistic Regression(    9400/10000): loss= 19359.7878011779\n",
      "Losgistic Regression(    9500/10000): loss= 19357.9075675084\n",
      "Losgistic Regression(    9600/10000): loss= 19356.0186738403\n",
      "Losgistic Regression(    9700/10000): loss= 19354.1179969646\n",
      "Losgistic Regression(    9800/10000): loss= 19352.1965247422\n",
      "Losgistic Regression(    9900/10000): loss= 19350.256329904\n",
      "Time for  4th cross validation = 1684.07s\n",
      "Training Accuracy         = 0.8291\n",
      "Cross Validation Accuracy = 0.825568\n",
      "*************** ([0.83043999999999996, 0.82926, 0.82713999999999999, 0.83355999999999997, 0.82909999999999995], [0.82574800000000004, 0.82564400000000004, 0.82518800000000003, 0.82726, 0.82556799999999997])\n",
      "Losgistic Regression(       0/10000): loss= 32495.5338517655\n",
      "Losgistic Regression(     100/10000): loss= 21565.9185576866\n",
      "Losgistic Regression(     200/10000): loss= 20664.7369350125\n",
      "Losgistic Regression(     300/10000): loss= 20227.441085223\n",
      "Losgistic Regression(     400/10000): loss= 19947.8159830081\n",
      "Losgistic Regression(     500/10000): loss= 19820.9621864405\n",
      "Losgistic Regression(     600/10000): loss= 19737.5322217533\n",
      "Losgistic Regression(     700/10000): loss= 19684.5143223687\n",
      "Losgistic Regression(     800/10000): loss= 19635.3634658876\n",
      "Losgistic Regression(     900/10000): loss= 19606.096615829\n",
      "Losgistic Regression(    1000/10000): loss= 19580.0575225466\n",
      "Losgistic Regression(    1100/10000): loss= 19548.967610773\n",
      "Losgistic Regression(    1200/10000): loss= 19524.737690304\n",
      "Losgistic Regression(    1300/10000): loss= 19505.1881864485\n",
      "Losgistic Regression(    1400/10000): loss= 19486.7929873234\n",
      "Losgistic Regression(    1500/10000): loss= 19471.9885240641\n",
      "Losgistic Regression(    1600/10000): loss= 19462.1887576097\n",
      "Losgistic Regression(    1700/10000): loss= 19453.0121325785\n",
      "Losgistic Regression(    1800/10000): loss= 19436.8140416209\n",
      "Losgistic Regression(    1900/10000): loss= 19421.7640484339\n",
      "Losgistic Regression(    2000/10000): loss= 19408.2692216567\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-478-bb74acb4624f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccu_50000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***************\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccu_50000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-472-59aa1226bca6>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(n, lambda_)\u001b[0m\n\u001b[1;32m      8\u001b[0m                          \u001b[0mfpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_cv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                          \u001b[0mfaccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                          max_fold=5)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-457-77a042919e9f>\u001b[0m in \u001b[0;36mcross_validation_var\u001b[0;34m(y, tx, K, func, seed, fpred, faccuracy, max_fold)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mstart\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mend\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Time for {i:2}th cross validation = {t:3.6}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-472-59aa1226bca6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n\u001b[0;32m----> 3\u001b[0;31m                    max_iters = 10000, lambda_=lambda_, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return cross_validation_var(y9, tX9, K=(250000//n), \n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_AGDR\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Restart if the loss of new weight is larger than the original one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accu_50000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(50000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_50000.append(tmp)\n",
    "\n",
    "accu_50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 16721.0280439384\n",
      "Losgistic Regression(     100/10000): loss= 10932.1251833644\n",
      "Losgistic Regression(     200/10000): loss= 10407.2173779291\n",
      "Losgistic Regression(     300/10000): loss= 10146.086705963\n",
      "Losgistic Regression(     400/10000): loss= 9983.54157283347\n",
      "Losgistic Regression(     500/10000): loss= 9871.30073461271\n",
      "Losgistic Regression(     600/10000): loss= 9784.79324806552\n",
      "Losgistic Regression(     700/10000): loss= 9713.16026138604\n",
      "Losgistic Regression(     800/10000): loss= 9661.16929158904\n",
      "Losgistic Regression(     900/10000): loss= 9626.34455776342\n",
      "Losgistic Regression(    1000/10000): loss= 9598.86646649527\n",
      "Losgistic Regression(    1100/10000): loss= 9579.13566108009\n",
      "Losgistic Regression(    1200/10000): loss= 9563.33988045476\n",
      "Losgistic Regression(    1300/10000): loss= 9551.09499138008\n",
      "Losgistic Regression(    1400/10000): loss= 9540.77399486591\n",
      "Losgistic Regression(    1500/10000): loss= 9527.90641571044\n",
      "Losgistic Regression(    1600/10000): loss= 9518.19280008319\n",
      "Losgistic Regression(    1700/10000): loss= 9507.02890365546\n",
      "Losgistic Regression(    1800/10000): loss= 9494.12967261127\n",
      "Losgistic Regression(    1900/10000): loss= 9480.87049530409\n",
      "Losgistic Regression(    2000/10000): loss= 9468.9080013743\n",
      "Losgistic Regression(    2100/10000): loss= 9457.29856634738\n",
      "Losgistic Regression(    2200/10000): loss= 9446.8428943709\n",
      "Losgistic Regression(    2300/10000): loss= 9436.77907482099\n",
      "Losgistic Regression(    2400/10000): loss= 9427.56197755646\n",
      "Losgistic Regression(    2500/10000): loss= 9419.46080765679\n",
      "Losgistic Regression(    2600/10000): loss= 9412.4065635616\n",
      "Losgistic Regression(    2700/10000): loss= 9406.31978101675\n",
      "Losgistic Regression(    2800/10000): loss= 9401.31538337997\n",
      "Losgistic Regression(    2900/10000): loss= 9397.37332053804\n",
      "Losgistic Regression(    3000/10000): loss= 9393.94328517541\n",
      "Losgistic Regression(    3100/10000): loss= 9390.83831188612\n",
      "Losgistic Regression(    3200/10000): loss= 9388.4933381221\n",
      "Losgistic Regression(    3300/10000): loss= 9387.19360881593\n",
      "Losgistic Regression(    3400/10000): loss= 9386.82497816026\n",
      "Losgistic Regression(    3500/10000): loss= 9386.82975164646\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9386.82975165\n",
      "Time for  0th cross validation = 284.766s\n",
      "Training Accuracy         = 0.83272\n",
      "Cross Validation Accuracy = 0.821352\n",
      "Losgistic Regression(       0/10000): loss= 16695.4133420063\n",
      "Losgistic Regression(     100/10000): loss= 11024.8493125464\n",
      "Losgistic Regression(     200/10000): loss= 10527.9254877966\n",
      "Losgistic Regression(     300/10000): loss= 10280.7722145521\n",
      "Losgistic Regression(     400/10000): loss= 10117.1777811684\n",
      "Losgistic Regression(     500/10000): loss= 9989.43397523596\n",
      "Losgistic Regression(     600/10000): loss= 9903.08525417924\n",
      "Losgistic Regression(     700/10000): loss= 9842.80252060514\n",
      "Losgistic Regression(     800/10000): loss= 9798.60328737496\n",
      "Losgistic Regression(     900/10000): loss= 9758.73156174187\n",
      "Losgistic Regression(    1000/10000): loss= 9726.26550303955\n",
      "Losgistic Regression(    1100/10000): loss= 9698.29450818791\n",
      "Losgistic Regression(    1200/10000): loss= 9676.83477793912\n",
      "Losgistic Regression(    1300/10000): loss= 9659.68839709488\n",
      "Losgistic Regression(    1400/10000): loss= 9644.96212442297\n",
      "Losgistic Regression(    1500/10000): loss= 9635.36018099662\n",
      "Losgistic Regression(    1600/10000): loss= 9630.10245104386\n",
      "Losgistic Regression(    1700/10000): loss= 9622.36598304255\n",
      "Losgistic Regression(    1800/10000): loss= 9613.1053851671\n",
      "Losgistic Regression(    1900/10000): loss= 9605.52831353813\n",
      "Losgistic Regression(    2000/10000): loss= 9598.37863763116\n",
      "Losgistic Regression(    2100/10000): loss= 9592.45807121214\n",
      "Losgistic Regression(    2200/10000): loss= 9586.81676312802\n",
      "Losgistic Regression(    2300/10000): loss= 9581.94081897188\n",
      "Losgistic Regression(    2400/10000): loss= 9577.69378573401\n",
      "Losgistic Regression(    2500/10000): loss= 9574.23823948438\n",
      "Losgistic Regression(    2600/10000): loss= 9571.37162260243\n",
      "Losgistic Regression(    2700/10000): loss= 9568.72458366685\n",
      "Losgistic Regression(    2800/10000): loss= 9566.6012452941\n",
      "Losgistic Regression(    2900/10000): loss= 9565.07701807539\n",
      "Losgistic Regression(    3000/10000): loss= 9564.21563849379\n",
      "Losgistic Regression(    3100/10000): loss= 9563.53966545955\n",
      "Losgistic Regression(    3200/10000): loss= 9562.66519939677\n",
      "Losgistic Regression(    3300/10000): loss= 9561.77341405994\n",
      "Losgistic Regression(    3400/10000): loss= 9560.46584688226\n",
      "Losgistic Regression(    3500/10000): loss= 9559.17962151433\n",
      "Losgistic Regression(    3600/10000): loss= 9557.78114467458\n",
      "Losgistic Regression(    3700/10000): loss= 9556.68886710333\n",
      "Losgistic Regression(    3800/10000): loss= 9555.66432627887\n",
      "Losgistic Regression(    3900/10000): loss= 9554.77829550035\n",
      "Losgistic Regression(    4000/10000): loss= 9554.25193688196\n",
      "Losgistic Regression(    4100/10000): loss= 9553.79904748376\n",
      "Losgistic Regression(    4200/10000): loss= 9553.42069502146\n",
      "Losgistic Regression(    4300/10000): loss= 9552.89696137457\n",
      "Losgistic Regression(    4400/10000): loss= 9552.26965785784\n",
      "Losgistic Regression(    4500/10000): loss= 9551.63393153141\n",
      "Losgistic Regression(    4600/10000): loss= 9551.0836998664\n",
      "Losgistic Regression(    4700/10000): loss= 9550.49482972752\n",
      "Losgistic Regression(    4800/10000): loss= 9549.92868559249\n",
      "Losgistic Regression(    4900/10000): loss= 9549.50212308435\n",
      "Losgistic Regression(    5000/10000): loss= 9549.20003865299\n",
      "Losgistic Regression(    5100/10000): loss= 9549.02811984574\n",
      "Losgistic Regression(    5200/10000): loss= 9548.86049096284\n",
      "Losgistic Regression(    5300/10000): loss= 9548.61393139534\n",
      "Losgistic Regression(    5400/10000): loss= 9548.22989806208\n",
      "Losgistic Regression(    5500/10000): loss= 9547.74841211251\n",
      "Losgistic Regression(    5600/10000): loss= 9547.18284095746\n",
      "Losgistic Regression(    5700/10000): loss= 9546.56393815477\n",
      "Losgistic Regression(    5800/10000): loss= 9545.95371159331\n",
      "Losgistic Regression(    5900/10000): loss= 9545.37779461361\n",
      "Losgistic Regression(    6000/10000): loss= 9544.86890721462\n",
      "Losgistic Regression(    6100/10000): loss= 9544.45881774947\n",
      "Losgistic Regression(    6200/10000): loss= 9544.09995009084\n",
      "Losgistic Regression(    6300/10000): loss= 9543.76305441762\n",
      "Losgistic Regression(    6400/10000): loss= 9543.46521390689\n",
      "Losgistic Regression(    6500/10000): loss= 9543.20097016025\n",
      "Losgistic Regression(    6600/10000): loss= 9542.9928154358\n",
      "Losgistic Regression(    6700/10000): loss= 9542.83757263533\n",
      "Losgistic Regression(    6800/10000): loss= 9542.70898806803\n",
      "Losgistic Regression(    6900/10000): loss= 9542.58440889526\n",
      "Losgistic Regression(    7000/10000): loss= 9542.51651164391\n",
      "Totoal number of iterations =  7000\n",
      "Loss                        =  9542.51651164\n",
      "Time for  1th cross validation = 563.123s\n",
      "Training Accuracy         = 0.83068\n",
      "Cross Validation Accuracy = 0.822732\n",
      "Losgistic Regression(       0/10000): loss= 16686.5652133638\n",
      "Losgistic Regression(     100/10000): loss= 11087.2056284524\n",
      "Losgistic Regression(     200/10000): loss= 10554.7831890192\n",
      "Losgistic Regression(     300/10000): loss= 10275.4472594309\n",
      "Losgistic Regression(     400/10000): loss= 10109.4905212616\n",
      "Losgistic Regression(     500/10000): loss= 10004.0330360775\n",
      "Losgistic Regression(     600/10000): loss= 9935.57473006465\n",
      "Losgistic Regression(     700/10000): loss= 9885.4894909498\n",
      "Losgistic Regression(     800/10000): loss= 9842.4850305777\n",
      "Losgistic Regression(     900/10000): loss= 9809.57508334043\n",
      "Losgistic Regression(    1000/10000): loss= 9784.4086002437\n",
      "Losgistic Regression(    1100/10000): loss= 9763.52936433341\n",
      "Losgistic Regression(    1200/10000): loss= 9745.57510848865\n",
      "Losgistic Regression(    1300/10000): loss= 9731.03778363196\n",
      "Losgistic Regression(    1400/10000): loss= 9720.01557565414\n",
      "Losgistic Regression(    1500/10000): loss= 9712.2085780036\n",
      "Losgistic Regression(    1600/10000): loss= 9706.12952008095\n",
      "Losgistic Regression(    1700/10000): loss= 9701.35953879297\n",
      "Losgistic Regression(    1800/10000): loss= 9698.33031116415\n",
      "Losgistic Regression(    1900/10000): loss= 9695.52185461822\n",
      "Losgistic Regression(    2000/10000): loss= 9692.94766145774\n",
      "Losgistic Regression(    2100/10000): loss= 9689.94131733353\n",
      "Losgistic Regression(    2200/10000): loss= 9686.60787544773\n",
      "Losgistic Regression(    2300/10000): loss= 9683.50248068297\n",
      "Losgistic Regression(    2400/10000): loss= 9680.22042262246\n",
      "Losgistic Regression(    2500/10000): loss= 9676.99700913357\n",
      "Losgistic Regression(    2600/10000): loss= 9674.08819040773\n",
      "Losgistic Regression(    2700/10000): loss= 9671.41371781924\n",
      "Losgistic Regression(    2800/10000): loss= 9668.80153507162\n",
      "Losgistic Regression(    2900/10000): loss= 9665.24557464973\n",
      "Losgistic Regression(    3000/10000): loss= 9661.68645513005\n",
      "Losgistic Regression(    3100/10000): loss= 9658.20248541139\n",
      "Losgistic Regression(    3200/10000): loss= 9654.80145749821\n",
      "Losgistic Regression(    3300/10000): loss= 9651.62232077611\n",
      "Losgistic Regression(    3400/10000): loss= 9648.55407604312\n",
      "Losgistic Regression(    3500/10000): loss= 9645.40099544163\n",
      "Losgistic Regression(    3600/10000): loss= 9642.34608321495\n",
      "Losgistic Regression(    3700/10000): loss= 9639.53976587508\n",
      "Losgistic Regression(    3800/10000): loss= 9636.89709201946\n",
      "Losgistic Regression(    3900/10000): loss= 9634.26173958574\n",
      "Losgistic Regression(    4000/10000): loss= 9631.77957921416\n",
      "Losgistic Regression(    4100/10000): loss= 9629.56422075222\n",
      "Losgistic Regression(    4200/10000): loss= 9627.40287645155\n",
      "Losgistic Regression(    4300/10000): loss= 9625.24081396458\n",
      "Losgistic Regression(    4400/10000): loss= 9623.23978160724\n",
      "Losgistic Regression(    4500/10000): loss= 9621.43169090652\n",
      "Losgistic Regression(    4600/10000): loss= 9619.81295387361\n",
      "Losgistic Regression(    4700/10000): loss= 9618.47810744658\n",
      "Losgistic Regression(    4800/10000): loss= 9617.40125849227\n",
      "Losgistic Regression(    4900/10000): loss= 9616.47518474468\n",
      "Losgistic Regression(    5000/10000): loss= 9615.69656820779\n",
      "Losgistic Regression(    5100/10000): loss= 9615.1093672392\n",
      "Losgistic Regression(    5200/10000): loss= 9614.57657532777\n",
      "Losgistic Regression(    5300/10000): loss= 9614.01873768287\n",
      "Losgistic Regression(    5400/10000): loss= 9613.46214384261\n",
      "Losgistic Regression(    5500/10000): loss= 9612.91385074185\n",
      "Losgistic Regression(    5600/10000): loss= 9612.37917512868\n",
      "Losgistic Regression(    5700/10000): loss= 9611.87533785296\n",
      "Losgistic Regression(    5800/10000): loss= 9611.40597594536\n",
      "Losgistic Regression(    5900/10000): loss= 9610.96614936513\n",
      "Losgistic Regression(    6000/10000): loss= 9610.58587048752\n",
      "Losgistic Regression(    6100/10000): loss= 9610.35317225968\n",
      "Losgistic Regression(    6200/10000): loss= 9610.28100524076\n",
      "Totoal number of iterations =  6200\n",
      "Loss                        =  9610.28100524\n",
      "Time for  2th cross validation = 497.753s\n",
      "Training Accuracy         = 0.82776\n",
      "Cross Validation Accuracy = 0.823048\n",
      "Losgistic Regression(       0/10000): loss= 16690.1693326792\n",
      "Losgistic Regression(     100/10000): loss= 10979.5159762389\n",
      "Losgistic Regression(     200/10000): loss= 10418.9388205117\n",
      "Losgistic Regression(     300/10000): loss= 10171.7054110725\n",
      "Losgistic Regression(     400/10000): loss= 10039.0545799237\n",
      "Losgistic Regression(     500/10000): loss= 9954.64772694487\n",
      "Losgistic Regression(     600/10000): loss= 9892.91761976932\n",
      "Losgistic Regression(     700/10000): loss= 9842.1088375306\n",
      "Losgistic Regression(     800/10000): loss= 9797.81404989021\n",
      "Losgistic Regression(     900/10000): loss= 9758.48794737446\n",
      "Losgistic Regression(    1000/10000): loss= 9728.25367270769\n",
      "Losgistic Regression(    1100/10000): loss= 9707.3345845467\n",
      "Losgistic Regression(    1200/10000): loss= 9686.96608261127\n",
      "Losgistic Regression(    1300/10000): loss= 9672.74443639289\n",
      "Losgistic Regression(    1400/10000): loss= 9659.64506256327\n",
      "Losgistic Regression(    1500/10000): loss= 9648.06103597778\n",
      "Losgistic Regression(    1600/10000): loss= 9637.83079102741\n",
      "Losgistic Regression(    1700/10000): loss= 9628.788310467\n",
      "Losgistic Regression(    1800/10000): loss= 9620.82630474579\n",
      "Losgistic Regression(    1900/10000): loss= 9612.93118523668\n",
      "Losgistic Regression(    2000/10000): loss= 9605.1249307962\n",
      "Losgistic Regression(    2100/10000): loss= 9597.99543555041\n",
      "Losgistic Regression(    2200/10000): loss= 9591.05431930921\n",
      "Losgistic Regression(    2300/10000): loss= 9584.00742195772\n",
      "Losgistic Regression(    2400/10000): loss= 9577.31902479083\n",
      "Losgistic Regression(    2500/10000): loss= 9572.38957038244\n",
      "Losgistic Regression(    2600/10000): loss= 9569.09708242644\n",
      "Losgistic Regression(    2700/10000): loss= 9565.86116606872\n",
      "Losgistic Regression(    2800/10000): loss= 9561.97944128419\n",
      "Losgistic Regression(    2900/10000): loss= 9557.99463821147\n",
      "Losgistic Regression(    3000/10000): loss= 9554.43864242975\n",
      "Losgistic Regression(    3100/10000): loss= 9551.53774922746\n",
      "Losgistic Regression(    3200/10000): loss= 9549.16805757507\n",
      "Losgistic Regression(    3300/10000): loss= 9547.1413365329\n",
      "Losgistic Regression(    3400/10000): loss= 9545.3998331123\n",
      "Losgistic Regression(    3500/10000): loss= 9543.7583113791\n",
      "Losgistic Regression(    3600/10000): loss= 9541.99825680293\n",
      "Losgistic Regression(    3700/10000): loss= 9540.38144436245\n",
      "Losgistic Regression(    3800/10000): loss= 9538.93848008997\n",
      "Losgistic Regression(    3900/10000): loss= 9537.75982455805\n",
      "Losgistic Regression(    4000/10000): loss= 9536.57010697605\n",
      "Losgistic Regression(    4100/10000): loss= 9535.24729157959\n",
      "Losgistic Regression(    4200/10000): loss= 9534.03747361881\n",
      "Losgistic Regression(    4300/10000): loss= 9532.98161344499\n",
      "Losgistic Regression(    4400/10000): loss= 9531.84002202419\n",
      "Losgistic Regression(    4500/10000): loss= 9530.5423148372\n",
      "Losgistic Regression(    4600/10000): loss= 9529.19315115943\n",
      "Losgistic Regression(    4700/10000): loss= 9527.79438656501\n",
      "Losgistic Regression(    4800/10000): loss= 9526.2141124816\n",
      "Losgistic Regression(    4900/10000): loss= 9524.51309717104\n",
      "Losgistic Regression(    5000/10000): loss= 9522.79267180633\n",
      "Losgistic Regression(    5100/10000): loss= 9521.07146548451\n",
      "Losgistic Regression(    5200/10000): loss= 9519.28666452641\n",
      "Losgistic Regression(    5300/10000): loss= 9517.37854936752\n",
      "Losgistic Regression(    5400/10000): loss= 9515.46218771572\n",
      "Losgistic Regression(    5500/10000): loss= 9513.63635110087\n",
      "Losgistic Regression(    5600/10000): loss= 9511.94108202126\n",
      "Losgistic Regression(    5700/10000): loss= 9510.36263287728\n",
      "Losgistic Regression(    5800/10000): loss= 9508.89625092476\n",
      "Losgistic Regression(    5900/10000): loss= 9507.47834944432\n",
      "Losgistic Regression(    6000/10000): loss= 9505.96934252285\n",
      "Losgistic Regression(    6100/10000): loss= 9504.42006731406\n",
      "Losgistic Regression(    6200/10000): loss= 9502.97518882886\n",
      "Losgistic Regression(    6300/10000): loss= 9501.62168753329\n",
      "Losgistic Regression(    6400/10000): loss= 9500.26667724121\n",
      "Losgistic Regression(    6500/10000): loss= 9498.8557189934\n",
      "Losgistic Regression(    6600/10000): loss= 9497.46049658491\n",
      "Losgistic Regression(    6700/10000): loss= 9496.21976729171\n",
      "Losgistic Regression(    6800/10000): loss= 9495.17262144168\n",
      "Losgistic Regression(    6900/10000): loss= 9494.20892376746\n",
      "Losgistic Regression(    7000/10000): loss= 9493.21935392627\n",
      "Losgistic Regression(    7100/10000): loss= 9492.14373712686\n",
      "Losgistic Regression(    7200/10000): loss= 9490.9430519648\n",
      "Losgistic Regression(    7300/10000): loss= 9489.64256881825\n",
      "Losgistic Regression(    7400/10000): loss= 9488.31631456605\n",
      "Losgistic Regression(    7500/10000): loss= 9486.9436834399\n",
      "Losgistic Regression(    7600/10000): loss= 9485.5568884501\n",
      "Losgistic Regression(    7700/10000): loss= 9484.15908637886\n",
      "Losgistic Regression(    7800/10000): loss= 9482.68791122197\n",
      "Losgistic Regression(    7900/10000): loss= 9481.06281881004\n",
      "Losgistic Regression(    8000/10000): loss= 9479.25139327432\n",
      "Losgistic Regression(    8100/10000): loss= 9477.28360638952\n",
      "Losgistic Regression(    8200/10000): loss= 9475.23273799423\n",
      "Losgistic Regression(    8300/10000): loss= 9473.18384816998\n",
      "Losgistic Regression(    8400/10000): loss= 9471.15839990888\n",
      "Losgistic Regression(    8500/10000): loss= 9469.10821597562\n",
      "Losgistic Regression(    8600/10000): loss= 9467.00998836247\n",
      "Losgistic Regression(    8700/10000): loss= 9464.88562612693\n",
      "Losgistic Regression(    8800/10000): loss= 9462.75030636073\n",
      "Losgistic Regression(    8900/10000): loss= 9460.590086386\n",
      "Losgistic Regression(    9000/10000): loss= 9458.40958751683\n",
      "Losgistic Regression(    9100/10000): loss= 9456.24915811995\n",
      "Losgistic Regression(    9200/10000): loss= 9454.16019776404\n",
      "Losgistic Regression(    9300/10000): loss= 9452.01435410186\n",
      "Losgistic Regression(    9400/10000): loss= 9449.94721802146\n",
      "Losgistic Regression(    9500/10000): loss= 9447.93586722961\n",
      "Losgistic Regression(    9600/10000): loss= 9445.96730703219\n",
      "Losgistic Regression(    9700/10000): loss= 9444.02636761827\n",
      "Losgistic Regression(    9800/10000): loss= 9442.1256695905\n",
      "Losgistic Regression(    9900/10000): loss= 9440.29066802287\n",
      "Time for  3th cross validation = 800.495s\n",
      "Training Accuracy         = 0.83224\n",
      "Cross Validation Accuracy = 0.822012\n",
      "Losgistic Regression(       0/10000): loss= 16714.9352168016\n",
      "Losgistic Regression(     100/10000): loss= 10877.4068136371\n",
      "Losgistic Regression(     200/10000): loss= 10336.663718486\n",
      "Losgistic Regression(     300/10000): loss= 10056.4744083637\n",
      "Losgistic Regression(     400/10000): loss= 9894.25179536515\n",
      "Losgistic Regression(     500/10000): loss= 9804.35986047491\n",
      "Losgistic Regression(     600/10000): loss= 9733.96853983712\n",
      "Losgistic Regression(     700/10000): loss= 9663.225499949\n",
      "Losgistic Regression(     800/10000): loss= 9588.31055740017\n",
      "Losgistic Regression(     900/10000): loss= 9517.21791325575\n",
      "Losgistic Regression(    1000/10000): loss= 9458.66564005038\n",
      "Losgistic Regression(    1100/10000): loss= 9417.90534222798\n",
      "Losgistic Regression(    1200/10000): loss= 9395.82932222514\n",
      "Losgistic Regression(    1300/10000): loss= 9389.56665158512\n",
      "Losgistic Regression(    1400/10000): loss= 9389.70092713105\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  9389.70092713\n",
      "Time for  4th cross validation = 115.26s\n",
      "Training Accuracy         = 0.82672\n",
      "Cross Validation Accuracy = 0.82068\n",
      "*************** ([0.83272000000000002, 0.83067999999999997, 0.82776000000000005, 0.83223999999999998, 0.82672000000000001], [0.82135199999999997, 0.82273200000000002, 0.823048, 0.82201199999999996, 0.82067999999999997])\n",
      "Losgistic Regression(       0/10000): loss= 16721.0421576809\n",
      "Losgistic Regression(     100/10000): loss= 10933.0023648913\n",
      "Losgistic Regression(     200/10000): loss= 10408.8168137154\n",
      "Losgistic Regression(     300/10000): loss= 10148.2448093156\n",
      "Losgistic Regression(     400/10000): loss= 9986.16449739506\n",
      "Losgistic Regression(     500/10000): loss= 9874.34573879399\n",
      "Losgistic Regression(     600/10000): loss= 9788.25099483393\n",
      "Losgistic Regression(     700/10000): loss= 9717.04281795243\n",
      "Losgistic Regression(     800/10000): loss= 9665.48673809017\n",
      "Losgistic Regression(     900/10000): loss= 9630.91201303741\n",
      "Losgistic Regression(    1000/10000): loss= 9603.66714220648\n",
      "Losgistic Regression(    1100/10000): loss= 9584.13614968008\n",
      "Losgistic Regression(    1200/10000): loss= 9568.51955831975\n",
      "Losgistic Regression(    1300/10000): loss= 9556.43415892366\n",
      "Losgistic Regression(    1400/10000): loss= 9546.25870158415\n",
      "Losgistic Regression(    1500/10000): loss= 9533.58841715284\n",
      "Losgistic Regression(    1600/10000): loss= 9524.07890166127\n",
      "Losgistic Regression(    1700/10000): loss= 9513.02990232278\n",
      "Losgistic Regression(    1800/10000): loss= 9500.27703523255\n",
      "Losgistic Regression(    1900/10000): loss= 9487.15482382217\n",
      "Losgistic Regression(    2000/10000): loss= 9475.32179850338\n",
      "Losgistic Regression(    2100/10000): loss= 9463.85053804286\n",
      "Losgistic Regression(    2200/10000): loss= 9453.55992039426\n",
      "Losgistic Regression(    2300/10000): loss= 9443.67541294906\n",
      "Losgistic Regression(    2400/10000): loss= 9434.60532973884\n",
      "Losgistic Regression(    2500/10000): loss= 9426.63240357161\n",
      "Losgistic Regression(    2600/10000): loss= 9419.68903426864\n",
      "Losgistic Regression(    2700/10000): loss= 9413.73780964354\n",
      "Losgistic Regression(    2800/10000): loss= 9408.87294451709\n",
      "Losgistic Regression(    2900/10000): loss= 9405.05996385695\n",
      "Losgistic Regression(    3000/10000): loss= 9401.74179673284\n",
      "Losgistic Regression(    3100/10000): loss= 9398.7087800385\n",
      "Losgistic Regression(    3200/10000): loss= 9396.39369736093\n",
      "Losgistic Regression(    3300/10000): loss= 9395.10096778915\n",
      "Losgistic Regression(    3400/10000): loss= 9394.74497754403\n",
      "Losgistic Regression(    3500/10000): loss= 9394.75012869785\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9394.7501287\n",
      "Time for  0th cross validation = 282.152s\n",
      "Training Accuracy         = 0.83264\n",
      "Cross Validation Accuracy = 0.821412\n",
      "Losgistic Regression(       0/10000): loss= 16695.4278216259\n",
      "Losgistic Regression(     100/10000): loss= 11025.7085863454\n",
      "Losgistic Regression(     200/10000): loss= 10529.4213367516\n",
      "Losgistic Regression(     300/10000): loss= 10282.8303102075\n",
      "Losgistic Regression(     400/10000): loss= 10119.7876855842\n",
      "Losgistic Regression(     500/10000): loss= 9992.56547277029\n",
      "Losgistic Regression(     600/10000): loss= 9906.68306899201\n",
      "Losgistic Regression(     700/10000): loss= 9846.74419002081\n",
      "Losgistic Regression(     800/10000): loss= 9802.92079249081\n",
      "Losgistic Regression(     900/10000): loss= 9763.35544155573\n",
      "Losgistic Regression(    1000/10000): loss= 9731.23076848458\n",
      "Losgistic Regression(    1100/10000): loss= 9703.58778112456\n",
      "Losgistic Regression(    1200/10000): loss= 9682.45756572605\n",
      "Losgistic Regression(    1300/10000): loss= 9665.60056025506\n",
      "Losgistic Regression(    1400/10000): loss= 9651.14719527853\n",
      "Losgistic Regression(    1500/10000): loss= 9641.78412154364\n",
      "Losgistic Regression(    1600/10000): loss= 9636.71600148426\n",
      "Losgistic Regression(    1700/10000): loss= 9628.85306225774\n",
      "Losgistic Regression(    1800/10000): loss= 9619.82057178049\n",
      "Losgistic Regression(    1900/10000): loss= 9612.46212673555\n",
      "Losgistic Regression(    2000/10000): loss= 9605.52874032473\n",
      "Losgistic Regression(    2100/10000): loss= 9599.74240765766\n",
      "Losgistic Regression(    2200/10000): loss= 9594.19724589564\n",
      "Losgistic Regression(    2300/10000): loss= 9589.41510386927\n",
      "Losgistic Regression(    2400/10000): loss= 9585.26371006755\n",
      "Losgistic Regression(    2500/10000): loss= 9581.89178575116\n",
      "Losgistic Regression(    2600/10000): loss= 9579.09066067271\n",
      "Losgistic Regression(    2700/10000): loss= 9576.5359048911\n",
      "Losgistic Regression(    2800/10000): loss= 9574.43838601375\n",
      "Losgistic Regression(    2900/10000): loss= 9572.90356086495\n",
      "Losgistic Regression(    3000/10000): loss= 9572.01586654096\n",
      "Losgistic Regression(    3100/10000): loss= 9571.26684875474\n",
      "Losgistic Regression(    3200/10000): loss= 9570.33117973201\n",
      "Losgistic Regression(    3300/10000): loss= 9569.36651549529\n",
      "Losgistic Regression(    3400/10000): loss= 9568.01943652614\n",
      "Losgistic Regression(    3500/10000): loss= 9566.68821715583\n",
      "Losgistic Regression(    3600/10000): loss= 9565.25069570449\n",
      "Losgistic Regression(    3700/10000): loss= 9564.17172744081\n",
      "Losgistic Regression(    3800/10000): loss= 9563.13343217954\n",
      "Losgistic Regression(    3900/10000): loss= 9562.29480860953\n",
      "Losgistic Regression(    4000/10000): loss= 9561.7668972331\n",
      "Losgistic Regression(    4100/10000): loss= 9561.32792469389\n",
      "Losgistic Regression(    4200/10000): loss= 9560.98103173438\n",
      "Losgistic Regression(    4300/10000): loss= 9560.48757688918\n",
      "Losgistic Regression(    4400/10000): loss= 9559.93025937728\n",
      "Losgistic Regression(    4500/10000): loss= 9559.3683467058\n",
      "Losgistic Regression(    4600/10000): loss= 9558.89635107345\n",
      "Losgistic Regression(    4700/10000): loss= 9558.39252021453\n",
      "Losgistic Regression(    4800/10000): loss= 9557.93914269809\n",
      "Losgistic Regression(    4900/10000): loss= 9557.63021986005\n",
      "Losgistic Regression(    5000/10000): loss= 9557.47426490812\n",
      "Losgistic Regression(    5100/10000): loss= 9557.43954326042\n",
      "Totoal number of iterations =  5100\n",
      "Loss                        =  9557.43954326\n",
      "Time for  1th cross validation = 412.263s\n",
      "Training Accuracy         = 0.83056\n",
      "Cross Validation Accuracy = 0.822716\n",
      "Losgistic Regression(       0/10000): loss= 16686.5796193506\n",
      "Losgistic Regression(     100/10000): loss= 11088.0837504582\n",
      "Losgistic Regression(     200/10000): loss= 10556.3733438348\n",
      "Losgistic Regression(     300/10000): loss= 10277.6674871079\n",
      "Losgistic Regression(     400/10000): loss= 10112.2241386922\n",
      "Losgistic Regression(     500/10000): loss= 10007.1209266958\n",
      "Losgistic Regression(     600/10000): loss= 9939.03216319392\n",
      "Losgistic Regression(     700/10000): loss= 9889.24226311191\n",
      "Losgistic Regression(     800/10000): loss= 9846.48471149141\n",
      "Losgistic Regression(     900/10000): loss= 9813.80335268373\n",
      "Losgistic Regression(    1000/10000): loss= 9788.89443910435\n",
      "Losgistic Regression(    1100/10000): loss= 9768.18893189943\n",
      "Losgistic Regression(    1200/10000): loss= 9750.47180171595\n",
      "Losgistic Regression(    1300/10000): loss= 9736.13689653235\n",
      "Losgistic Regression(    1400/10000): loss= 9725.232890057\n",
      "Losgistic Regression(    1500/10000): loss= 9717.53751302159\n",
      "Losgistic Regression(    1600/10000): loss= 9711.514503594\n",
      "Losgistic Regression(    1700/10000): loss= 9706.78690301911\n",
      "Losgistic Regression(    1800/10000): loss= 9703.81232869219\n",
      "Losgistic Regression(    1900/10000): loss= 9701.02309190103\n",
      "Losgistic Regression(    2000/10000): loss= 9698.46237672667\n",
      "Losgistic Regression(    2100/10000): loss= 9695.47954503396\n",
      "Losgistic Regression(    2200/10000): loss= 9692.16964160209\n",
      "Losgistic Regression(    2300/10000): loss= 9689.11922708213\n",
      "Losgistic Regression(    2400/10000): loss= 9685.8722132956\n",
      "Losgistic Regression(    2500/10000): loss= 9682.66252139504\n",
      "Losgistic Regression(    2600/10000): loss= 9679.73535232115\n",
      "Losgistic Regression(    2700/10000): loss= 9677.04390705188\n",
      "Losgistic Regression(    2800/10000): loss= 9674.44786774357\n",
      "Losgistic Regression(    2900/10000): loss= 9670.66107310514\n",
      "Losgistic Regression(    3000/10000): loss= 9667.15397362024\n",
      "Losgistic Regression(    3100/10000): loss= 9663.74408169828\n",
      "Losgistic Regression(    3200/10000): loss= 9660.43095231369\n",
      "Losgistic Regression(    3300/10000): loss= 9657.36669126088\n",
      "Losgistic Regression(    3400/10000): loss= 9654.40684018234\n",
      "Losgistic Regression(    3500/10000): loss= 9651.35426536782\n",
      "Losgistic Regression(    3600/10000): loss= 9648.37907601146\n",
      "Losgistic Regression(    3700/10000): loss= 9645.62861036523\n",
      "Losgistic Regression(    3800/10000): loss= 9643.04416421516\n",
      "Losgistic Regression(    3900/10000): loss= 9640.47769309787\n",
      "Losgistic Regression(    4000/10000): loss= 9638.07700062413\n",
      "Losgistic Regression(    4100/10000): loss= 9635.92361870277\n",
      "Losgistic Regression(    4200/10000): loss= 9633.82345605112\n",
      "Losgistic Regression(    4300/10000): loss= 9631.72843947436\n",
      "Losgistic Regression(    4400/10000): loss= 9629.79960970847\n",
      "Losgistic Regression(    4500/10000): loss= 9628.10038406764\n",
      "Losgistic Regression(    4600/10000): loss= 9626.62573169338\n",
      "Losgistic Regression(    4700/10000): loss= 9625.44561501992\n",
      "Losgistic Regression(    4800/10000): loss= 9624.54116037393\n",
      "Losgistic Regression(    4900/10000): loss= 9623.81368936401\n",
      "Losgistic Regression(    5000/10000): loss= 9623.20965136125\n",
      "Losgistic Regression(    5100/10000): loss= 9622.74238161199\n",
      "Losgistic Regression(    5200/10000): loss= 9622.30339354479\n",
      "Losgistic Regression(    5300/10000): loss= 9621.85064941348\n",
      "Losgistic Regression(    5400/10000): loss= 9621.40954646163\n",
      "Losgistic Regression(    5500/10000): loss= 9620.99964269766\n",
      "Losgistic Regression(    5600/10000): loss= 9620.62098026767\n",
      "Losgistic Regression(    5700/10000): loss= 9620.2585085731\n",
      "Losgistic Regression(    5800/10000): loss= 9619.92072355267\n",
      "Losgistic Regression(    5900/10000): loss= 9619.60269689478\n",
      "Losgistic Regression(    6000/10000): loss= 9619.33320948524\n",
      "Losgistic Regression(    6100/10000): loss= 9619.18646732037\n",
      "Losgistic Regression(    6200/10000): loss= 9619.17151900179\n",
      "Totoal number of iterations =  6200\n",
      "Loss                        =  9619.171519\n",
      "Time for  2th cross validation = 500.717s\n",
      "Training Accuracy         = 0.8278\n",
      "Cross Validation Accuracy = 0.823104\n",
      "Losgistic Regression(       0/10000): loss= 16690.1837477403\n",
      "Losgistic Regression(     100/10000): loss= 10980.3798588872\n",
      "Losgistic Regression(     200/10000): loss= 10420.5116168448\n",
      "Losgistic Regression(     300/10000): loss= 10173.8261621578\n",
      "Losgistic Regression(     400/10000): loss= 10041.5811885348\n",
      "Losgistic Regression(     500/10000): loss= 9957.58080645194\n",
      "Losgistic Regression(     600/10000): loss= 9896.25354992434\n",
      "Losgistic Regression(     700/10000): loss= 9845.79244007287\n",
      "Losgistic Regression(     800/10000): loss= 9801.84866806661\n",
      "Losgistic Regression(     900/10000): loss= 9762.85422127077\n",
      "Losgistic Regression(    1000/10000): loss= 9732.86632051771\n",
      "Losgistic Regression(    1100/10000): loss= 9712.19160145377\n",
      "Losgistic Regression(    1200/10000): loss= 9692.0482876422\n",
      "Losgistic Regression(    1300/10000): loss= 9678.04402683495\n",
      "Losgistic Regression(    1400/10000): loss= 9665.1881998807\n",
      "Losgistic Regression(    1500/10000): loss= 9653.80849733323\n",
      "Losgistic Regression(    1600/10000): loss= 9643.85031723867\n",
      "Losgistic Regression(    1700/10000): loss= 9635.05932541047\n",
      "Losgistic Regression(    1800/10000): loss= 9627.31847852033\n",
      "Losgistic Regression(    1900/10000): loss= 9619.77626222667\n",
      "Losgistic Regression(    2000/10000): loss= 9612.17742407279\n",
      "Losgistic Regression(    2100/10000): loss= 9605.22662542327\n",
      "Losgistic Regression(    2200/10000): loss= 9598.44868782878\n",
      "Losgistic Regression(    2300/10000): loss= 9591.53269759336\n",
      "Losgistic Regression(    2400/10000): loss= 9585.16933309722\n",
      "Losgistic Regression(    2500/10000): loss= 9580.35134973012\n",
      "Losgistic Regression(    2600/10000): loss= 9577.1340562341\n",
      "Losgistic Regression(    2700/10000): loss= 9573.94673215025\n",
      "Losgistic Regression(    2800/10000): loss= 9570.11637815423\n",
      "Losgistic Regression(    2900/10000): loss= 9566.21043001911\n",
      "Losgistic Regression(    3000/10000): loss= 9562.76024956759\n",
      "Losgistic Regression(    3100/10000): loss= 9560.00189091251\n",
      "Losgistic Regression(    3200/10000): loss= 9557.85226463326\n",
      "Losgistic Regression(    3300/10000): loss= 9556.00995232541\n",
      "Losgistic Regression(    3400/10000): loss= 9554.26433867799\n",
      "Losgistic Regression(    3500/10000): loss= 9552.49724589246\n",
      "Losgistic Regression(    3600/10000): loss= 9550.79117712974\n",
      "Losgistic Regression(    3700/10000): loss= 9549.22941993512\n",
      "Losgistic Regression(    3800/10000): loss= 9547.85037441519\n",
      "Losgistic Regression(    3900/10000): loss= 9546.75651839106\n",
      "Losgistic Regression(    4000/10000): loss= 9545.60991729142\n",
      "Losgistic Regression(    4100/10000): loss= 9544.33776044445\n",
      "Losgistic Regression(    4200/10000): loss= 9543.16056905719\n",
      "Losgistic Regression(    4300/10000): loss= 9542.12748508555\n",
      "Losgistic Regression(    4400/10000): loss= 9541.00292085815\n",
      "Losgistic Regression(    4500/10000): loss= 9539.74449981691\n",
      "Losgistic Regression(    4600/10000): loss= 9538.46083894593\n",
      "Losgistic Regression(    4700/10000): loss= 9537.10950429994\n",
      "Losgistic Regression(    4800/10000): loss= 9535.58144871012\n",
      "Losgistic Regression(    4900/10000): loss= 9533.97552709521\n",
      "Losgistic Regression(    5000/10000): loss= 9532.39826658095\n",
      "Losgistic Regression(    5100/10000): loss= 9530.84685330232\n",
      "Losgistic Regression(    5200/10000): loss= 9529.29773681176\n",
      "Losgistic Regression(    5300/10000): loss= 9527.70443514096\n",
      "Losgistic Regression(    5400/10000): loss= 9526.07615179907\n",
      "Losgistic Regression(    5500/10000): loss= 9524.49357352857\n",
      "Losgistic Regression(    5600/10000): loss= 9523.03529654325\n",
      "Losgistic Regression(    5700/10000): loss= 9521.69295289985\n",
      "Losgistic Regression(    5800/10000): loss= 9520.46101118708\n",
      "Losgistic Regression(    5900/10000): loss= 9519.24446130326\n",
      "Losgistic Regression(    6000/10000): loss= 9517.9355491897\n",
      "Losgistic Regression(    6100/10000): loss= 9516.5981279653\n",
      "Losgistic Regression(    6200/10000): loss= 9515.38128813542\n",
      "Losgistic Regression(    6300/10000): loss= 9514.24648991327\n",
      "Losgistic Regression(    6400/10000): loss= 9513.10880943702\n",
      "Losgistic Regression(    6500/10000): loss= 9511.93225065026\n",
      "Losgistic Regression(    6600/10000): loss= 9510.77176039691\n",
      "Losgistic Regression(    6700/10000): loss= 9509.7613289093\n",
      "Losgistic Regression(    6800/10000): loss= 9508.92828591733\n",
      "Losgistic Regression(    6900/10000): loss= 9508.15164511961\n",
      "Losgistic Regression(    7000/10000): loss= 9507.31217912346\n",
      "Losgistic Regression(    7100/10000): loss= 9506.36592481769\n",
      "Losgistic Regression(    7200/10000): loss= 9505.30815284533\n",
      "Losgistic Regression(    7300/10000): loss= 9504.20816491387\n",
      "Losgistic Regression(    7400/10000): loss= 9503.11962053664\n",
      "Losgistic Regression(    7500/10000): loss= 9502.09657933542\n",
      "Losgistic Regression(    7600/10000): loss= 9501.08068471695\n",
      "Losgistic Regression(    7700/10000): loss= 9500.00776171984\n",
      "Losgistic Regression(    7800/10000): loss= 9498.80592652385\n",
      "Losgistic Regression(    7900/10000): loss= 9497.42727283992\n",
      "Losgistic Regression(    8000/10000): loss= 9495.82961061232\n",
      "Losgistic Regression(    8100/10000): loss= 9494.01851444303\n",
      "Losgistic Regression(    8200/10000): loss= 9492.17208298815\n",
      "Losgistic Regression(    8300/10000): loss= 9490.31018165395\n",
      "Losgistic Regression(    8400/10000): loss= 9488.44760895114\n",
      "Losgistic Regression(    8500/10000): loss= 9486.57772316667\n",
      "Losgistic Regression(    8600/10000): loss= 9484.65064326267\n",
      "Losgistic Regression(    8700/10000): loss= 9482.70081878749\n",
      "Losgistic Regression(    8800/10000): loss= 9480.76410571921\n",
      "Losgistic Regression(    8900/10000): loss= 9478.82942165348\n",
      "Losgistic Regression(    9000/10000): loss= 9476.8854254138\n",
      "Losgistic Regression(    9100/10000): loss= 9474.97229872542\n",
      "Losgistic Regression(    9200/10000): loss= 9473.13454240482\n",
      "Losgistic Regression(    9300/10000): loss= 9471.36201661531\n",
      "Losgistic Regression(    9400/10000): loss= 9469.64239467118\n",
      "Losgistic Regression(    9500/10000): loss= 9467.93944021414\n",
      "Losgistic Regression(    9600/10000): loss= 9466.14479077979\n",
      "Losgistic Regression(    9700/10000): loss= 9464.33764166155\n",
      "Losgistic Regression(    9800/10000): loss= 9462.56793118899\n",
      "Losgistic Regression(    9900/10000): loss= 9460.84762816548\n",
      "Time for  3th cross validation = 803.074s\n",
      "Training Accuracy         = 0.83252\n",
      "Cross Validation Accuracy = 0.82216\n",
      "Losgistic Regression(       0/10000): loss= 16714.9493482786\n",
      "Losgistic Regression(     100/10000): loss= 10878.4204629477\n",
      "Losgistic Regression(     200/10000): loss= 10338.6248930628\n",
      "Losgistic Regression(     300/10000): loss= 10059.2102892487\n",
      "Losgistic Regression(     400/10000): loss= 9897.19572573546\n",
      "Losgistic Regression(     500/10000): loss= 9807.33207808054\n",
      "Losgistic Regression(     600/10000): loss= 9737.15110445913\n",
      "Losgistic Regression(     700/10000): loss= 9666.47400771273\n",
      "Losgistic Regression(     800/10000): loss= 9591.35084381282\n",
      "Losgistic Regression(     900/10000): loss= 9520.09600754885\n",
      "Losgistic Regression(    1000/10000): loss= 9461.51416730307\n",
      "Losgistic Regression(    1100/10000): loss= 9421.07191396316\n",
      "Losgistic Regression(    1200/10000): loss= 9399.65001480346\n",
      "Losgistic Regression(    1300/10000): loss= 9394.36784136525\n",
      "Losgistic Regression(    1400/10000): loss= 9394.4936517976\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  9394.4936518\n",
      "Time for  4th cross validation = 112.324s\n",
      "Training Accuracy         = 0.82672\n",
      "Cross Validation Accuracy = 0.82072\n",
      "*************** ([0.83264000000000005, 0.83055999999999996, 0.82779999999999998, 0.83252000000000004, 0.82672000000000001], [0.82141200000000003, 0.822716, 0.82310399999999995, 0.82216, 0.82072000000000001])\n",
      "Losgistic Regression(       0/10000): loss= 16721.091304587\n",
      "Losgistic Regression(     100/10000): loss= 10936.0526058619\n",
      "Losgistic Regression(     200/10000): loss= 10414.3726339898\n",
      "Losgistic Regression(     300/10000): loss= 10155.731123023\n",
      "Losgistic Regression(     400/10000): loss= 9995.25526194143\n",
      "Losgistic Regression(     500/10000): loss= 9884.88871887163\n",
      "Losgistic Regression(     600/10000): loss= 9800.21526053378\n",
      "Losgistic Regression(     700/10000): loss= 9730.48444233151\n",
      "Losgistic Regression(     800/10000): loss= 9680.41176015505\n",
      "Losgistic Regression(     900/10000): loss= 9646.67598515329\n",
      "Losgistic Regression(    1000/10000): loss= 9620.21795128593\n",
      "Losgistic Regression(    1100/10000): loss= 9601.36225469408\n",
      "Losgistic Regression(    1200/10000): loss= 9586.3109368133\n",
      "Losgistic Regression(    1300/10000): loss= 9574.73710927979\n",
      "Losgistic Regression(    1400/10000): loss= 9565.04250766908\n",
      "Losgistic Regression(    1500/10000): loss= 9553.03923890673\n",
      "Losgistic Regression(    1600/10000): loss= 9544.21224917404\n",
      "Losgistic Regression(    1700/10000): loss= 9533.56917571797\n",
      "Losgistic Regression(    1800/10000): loss= 9521.2810988079\n",
      "Losgistic Regression(    1900/10000): loss= 9508.58689512414\n",
      "Losgistic Regression(    2000/10000): loss= 9497.115137197\n",
      "Losgistic Regression(    2100/10000): loss= 9486.05205660678\n",
      "Losgistic Regression(    2200/10000): loss= 9476.22746031768\n",
      "Losgistic Regression(    2300/10000): loss= 9466.88378208882\n",
      "Losgistic Regression(    2400/10000): loss= 9458.32806016362\n",
      "Losgistic Regression(    2500/10000): loss= 9450.81572373807\n",
      "Losgistic Regression(    2600/10000): loss= 9444.30279257935\n",
      "Losgistic Regression(    2700/10000): loss= 9438.85145771383\n",
      "Losgistic Regression(    2800/10000): loss= 9434.44436250407\n",
      "Losgistic Regression(    2900/10000): loss= 9431.09032869824\n",
      "Losgistic Regression(    3000/10000): loss= 9427.96888898309\n",
      "Losgistic Regression(    3100/10000): loss= 9425.08195501731\n",
      "Losgistic Regression(    3200/10000): loss= 9422.85641870474\n",
      "Losgistic Regression(    3300/10000): loss= 9421.58429139092\n",
      "Losgistic Regression(    3400/10000): loss= 9421.19623502915\n",
      "Losgistic Regression(    3500/10000): loss= 9421.20240352749\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9421.20240353\n",
      "Time for  0th cross validation = 277.441s\n",
      "Training Accuracy         = 0.83244\n",
      "Cross Validation Accuracy = 0.821492\n",
      "Losgistic Regression(       0/10000): loss= 16695.4782425898\n",
      "Losgistic Regression(     100/10000): loss= 11028.6959061135\n",
      "Losgistic Regression(     200/10000): loss= 10534.6178848483\n",
      "Losgistic Regression(     300/10000): loss= 10289.9709971647\n",
      "Losgistic Regression(     400/10000): loss= 10128.8361743364\n",
      "Losgistic Regression(     500/10000): loss= 10003.4063266209\n",
      "Losgistic Regression(     600/10000): loss= 9919.12681267107\n",
      "Losgistic Regression(     700/10000): loss= 9860.36951793835\n",
      "Losgistic Regression(     800/10000): loss= 9817.65100796121\n",
      "Losgistic Regression(     900/10000): loss= 9779.30441802105\n",
      "Losgistic Regression(    1000/10000): loss= 9748.35483031871\n",
      "Losgistic Regression(    1100/10000): loss= 9721.85252943847\n",
      "Losgistic Regression(    1200/10000): loss= 9701.85626528388\n",
      "Losgistic Regression(    1300/10000): loss= 9685.98714090103\n",
      "Losgistic Regression(    1400/10000): loss= 9672.43560679753\n",
      "Losgistic Regression(    1500/10000): loss= 9663.88692151256\n",
      "Losgistic Regression(    1600/10000): loss= 9659.45222815212\n",
      "Losgistic Regression(    1700/10000): loss= 9651.11521807579\n",
      "Losgistic Regression(    1800/10000): loss= 9642.86702957717\n",
      "Losgistic Regression(    1900/10000): loss= 9636.17046772785\n",
      "Losgistic Regression(    2000/10000): loss= 9629.84752257046\n",
      "Losgistic Regression(    2100/10000): loss= 9624.46943559927\n",
      "Losgistic Regression(    2200/10000): loss= 9619.23317303885\n",
      "Losgistic Regression(    2300/10000): loss= 9614.69536527383\n",
      "Losgistic Regression(    2400/10000): loss= 9610.80337267044\n",
      "Losgistic Regression(    2500/10000): loss= 9607.62988361923\n",
      "Losgistic Regression(    2600/10000): loss= 9604.96581204463\n",
      "Losgistic Regression(    2700/10000): loss= 9602.44048598588\n",
      "Losgistic Regression(    2800/10000): loss= 9600.42974581809\n",
      "Losgistic Regression(    2900/10000): loss= 9598.74263158923\n",
      "Losgistic Regression(    3000/10000): loss= 9597.68237610166\n",
      "Losgistic Regression(    3100/10000): loss= 9596.74103344191\n",
      "Losgistic Regression(    3200/10000): loss= 9595.68743001474\n",
      "Losgistic Regression(    3300/10000): loss= 9594.56688465504\n",
      "Losgistic Regression(    3400/10000): loss= 9593.07559433585\n",
      "Losgistic Regression(    3500/10000): loss= 9591.52044426908\n",
      "Losgistic Regression(    3600/10000): loss= 9589.85917070778\n",
      "Losgistic Regression(    3700/10000): loss= 9588.61624206253\n",
      "Losgistic Regression(    3800/10000): loss= 9587.3747349568\n",
      "Losgistic Regression(    3900/10000): loss= 9586.562394016\n",
      "Losgistic Regression(    4000/10000): loss= 9586.0452037337\n",
      "Losgistic Regression(    4100/10000): loss= 9585.77425206892\n",
      "Losgistic Regression(    4200/10000): loss= 9585.57331821204\n",
      "Losgistic Regression(    4300/10000): loss= 9585.2642041944\n",
      "Losgistic Regression(    4400/10000): loss= 9584.94698109085\n",
      "Losgistic Regression(    4500/10000): loss= 9584.55901761962\n",
      "Losgistic Regression(    4600/10000): loss= 9584.27031797266\n",
      "Losgistic Regression(    4700/10000): loss= 9583.91697156737\n",
      "Losgistic Regression(    4800/10000): loss= 9583.69034729638\n",
      "Losgistic Regression(    4900/10000): loss= 9583.64878160017\n",
      "Totoal number of iterations =  4900\n",
      "Loss                        =  9583.6487816\n",
      "Time for  1th cross validation = 385.513s\n",
      "Training Accuracy         = 0.83072\n",
      "Cross Validation Accuracy = 0.8228\n",
      "Losgistic Regression(       0/10000): loss= 16686.6297839103\n",
      "Losgistic Regression(     100/10000): loss= 11091.1370922306\n",
      "Losgistic Regression(     200/10000): loss= 10561.8973491022\n",
      "Losgistic Regression(     300/10000): loss= 10285.3693008534\n",
      "Losgistic Regression(     400/10000): loss= 10121.6978193796\n",
      "Losgistic Regression(     500/10000): loss= 10017.8078487521\n",
      "Losgistic Regression(     600/10000): loss= 9950.98977296483\n",
      "Losgistic Regression(     700/10000): loss= 9902.20468131666\n",
      "Losgistic Regression(     800/10000): loss= 9860.2952267051\n",
      "Losgistic Regression(     900/10000): loss= 9828.39410585365\n",
      "Losgistic Regression(    1000/10000): loss= 9804.36224561317\n",
      "Losgistic Regression(    1100/10000): loss= 9784.25516644986\n",
      "Losgistic Regression(    1200/10000): loss= 9767.30003506198\n",
      "Losgistic Regression(    1300/10000): loss= 9753.60640731326\n",
      "Losgistic Regression(    1400/10000): loss= 9743.10890830768\n",
      "Losgistic Regression(    1500/10000): loss= 9735.79825875322\n",
      "Losgistic Regression(    1600/10000): loss= 9729.96627013441\n",
      "Losgistic Regression(    1700/10000): loss= 9725.35717018079\n",
      "Losgistic Regression(    1800/10000): loss= 9722.50721341525\n",
      "Losgistic Regression(    1900/10000): loss= 9719.76497637012\n",
      "Losgistic Regression(    2000/10000): loss= 9717.20007324784\n",
      "Losgistic Regression(    2100/10000): loss= 9714.26386903721\n",
      "Losgistic Regression(    2200/10000): loss= 9711.07187849286\n",
      "Losgistic Regression(    2300/10000): loss= 9708.10567201388\n",
      "Losgistic Regression(    2400/10000): loss= 9704.84093907163\n",
      "Losgistic Regression(    2500/10000): loss= 9701.56660895622\n",
      "Losgistic Regression(    2600/10000): loss= 9698.53970115352\n",
      "Losgistic Regression(    2700/10000): loss= 9695.84813462532\n",
      "Losgistic Regression(    2800/10000): loss= 9692.60293296007\n",
      "Losgistic Regression(    2900/10000): loss= 9688.82835481766\n",
      "Losgistic Regression(    3000/10000): loss= 9685.74341747396\n",
      "Losgistic Regression(    3100/10000): loss= 9682.8135080163\n",
      "Losgistic Regression(    3200/10000): loss= 9679.98043050356\n",
      "Losgistic Regression(    3300/10000): loss= 9677.39248432264\n",
      "Losgistic Regression(    3400/10000): loss= 9674.94037128921\n",
      "Losgistic Regression(    3500/10000): loss= 9672.28259002137\n",
      "Losgistic Regression(    3600/10000): loss= 9669.62880875658\n",
      "Losgistic Regression(    3700/10000): loss= 9667.32347990844\n",
      "Losgistic Regression(    3800/10000): loss= 9665.23147726253\n",
      "Losgistic Regression(    3900/10000): loss= 9663.1021198243\n",
      "Losgistic Regression(    4000/10000): loss= 9661.1406508041\n",
      "Losgistic Regression(    4100/10000): loss= 9659.31003936511\n",
      "Losgistic Regression(    4200/10000): loss= 9657.4692203962\n",
      "Losgistic Regression(    4300/10000): loss= 9655.75799543275\n",
      "Losgistic Regression(    4400/10000): loss= 9654.25970192411\n",
      "Losgistic Regression(    4500/10000): loss= 9652.96829581873\n",
      "Losgistic Regression(    4600/10000): loss= 9651.71888664003\n",
      "Losgistic Regression(    4700/10000): loss= 9650.5628976475\n",
      "Losgistic Regression(    4800/10000): loss= 9649.64311171651\n",
      "Losgistic Regression(    4900/10000): loss= 9648.76644836411\n",
      "Losgistic Regression(    5000/10000): loss= 9647.90089133798\n",
      "Losgistic Regression(    5100/10000): loss= 9647.19659389657\n",
      "Losgistic Regression(    5200/10000): loss= 9646.5562786354\n",
      "Losgistic Regression(    5300/10000): loss= 9645.94069246872\n",
      "Losgistic Regression(    5400/10000): loss= 9645.46772781944\n",
      "Losgistic Regression(    5500/10000): loss= 9645.0172709149\n",
      "Losgistic Regression(    5600/10000): loss= 9644.67519188592\n",
      "Losgistic Regression(    5700/10000): loss= 9644.57457374211\n",
      "Losgistic Regression(    5800/10000): loss= 9644.57835508777\n",
      "Totoal number of iterations =  5800\n",
      "Loss                        =  9644.57835509\n",
      "Time for  2th cross validation = 458.499s\n",
      "Training Accuracy         = 0.82804\n",
      "Cross Validation Accuracy = 0.823088\n",
      "Losgistic Regression(       0/10000): loss= 16690.2339438988\n",
      "Losgistic Regression(     100/10000): loss= 10983.3840439224\n",
      "Losgistic Regression(     200/10000): loss= 10425.9755236729\n",
      "Losgistic Regression(     300/10000): loss= 10181.182842698\n",
      "Losgistic Regression(     400/10000): loss= 10050.3367271457\n",
      "Losgistic Regression(     500/10000): loss= 9967.73633191565\n",
      "Losgistic Regression(     600/10000): loss= 9907.80214528752\n",
      "Losgistic Regression(     700/10000): loss= 9858.52880200592\n",
      "Losgistic Regression(     800/10000): loss= 9815.77338683402\n",
      "Losgistic Regression(     900/10000): loss= 9777.90640403298\n",
      "Losgistic Regression(    1000/10000): loss= 9748.74180785145\n",
      "Losgistic Regression(    1100/10000): loss= 9728.87945769567\n",
      "Losgistic Regression(    1200/10000): loss= 9709.50658864753\n",
      "Losgistic Regression(    1300/10000): loss= 9696.23544563214\n",
      "Losgistic Regression(    1400/10000): loss= 9684.17715961754\n",
      "Losgistic Regression(    1500/10000): loss= 9673.53999844441\n",
      "Losgistic Regression(    1600/10000): loss= 9664.50977039053\n",
      "Losgistic Regression(    1700/10000): loss= 9656.54973148011\n",
      "Losgistic Regression(    1800/10000): loss= 9649.57488337599\n",
      "Losgistic Regression(    1900/10000): loss= 9642.83985378839\n",
      "Losgistic Regression(    2000/10000): loss= 9636.30892236921\n",
      "Losgistic Regression(    2100/10000): loss= 9629.88351191363\n",
      "Losgistic Regression(    2200/10000): loss= 9623.52509683449\n",
      "Losgistic Regression(    2300/10000): loss= 9616.92249464744\n",
      "Losgistic Regression(    2400/10000): loss= 9610.98162252214\n",
      "Losgistic Regression(    2500/10000): loss= 9606.66879484613\n",
      "Losgistic Regression(    2600/10000): loss= 9603.64519501404\n",
      "Losgistic Regression(    2700/10000): loss= 9600.58243372121\n",
      "Losgistic Regression(    2800/10000): loss= 9596.96841944911\n",
      "Losgistic Regression(    2900/10000): loss= 9593.4989669053\n",
      "Losgistic Regression(    3000/10000): loss= 9590.60389109848\n",
      "Losgistic Regression(    3100/10000): loss= 9588.37471499348\n",
      "Losgistic Regression(    3200/10000): loss= 9586.69080937532\n",
      "Losgistic Regression(    3300/10000): loss= 9585.17261888798\n",
      "Losgistic Regression(    3400/10000): loss= 9583.49977525948\n",
      "Losgistic Regression(    3500/10000): loss= 9581.80063135496\n",
      "Losgistic Regression(    3600/10000): loss= 9580.07653240336\n",
      "Losgistic Regression(    3700/10000): loss= 9578.58475653974\n",
      "Losgistic Regression(    3800/10000): loss= 9577.29993386328\n",
      "Losgistic Regression(    3900/10000): loss= 9576.35913214205\n",
      "Losgistic Regression(    4000/10000): loss= 9575.42579540014\n",
      "Losgistic Regression(    4100/10000): loss= 9574.35304386991\n",
      "Losgistic Regression(    4200/10000): loss= 9573.25816862389\n",
      "Losgistic Regression(    4300/10000): loss= 9572.16478093979\n",
      "Losgistic Regression(    4400/10000): loss= 9571.02435014099\n",
      "Losgistic Regression(    4500/10000): loss= 9569.89111168101\n",
      "Losgistic Regression(    4600/10000): loss= 9568.84420309492\n",
      "Losgistic Regression(    4700/10000): loss= 9567.84902383676\n",
      "Losgistic Regression(    4800/10000): loss= 9566.61782887522\n",
      "Losgistic Regression(    4900/10000): loss= 9565.41108778656\n",
      "Losgistic Regression(    5000/10000): loss= 9564.30228054102\n",
      "Losgistic Regression(    5100/10000): loss= 9563.28144853639\n",
      "Losgistic Regression(    5200/10000): loss= 9562.28773029316\n",
      "Losgistic Regression(    5300/10000): loss= 9561.24128981942\n",
      "Losgistic Regression(    5400/10000): loss= 9560.24569712098\n",
      "Losgistic Regression(    5500/10000): loss= 9559.39204930765\n",
      "Losgistic Regression(    5600/10000): loss= 9558.65266230111\n",
      "Losgistic Regression(    5700/10000): loss= 9558.0828314444\n",
      "Losgistic Regression(    5800/10000): loss= 9557.70667980397\n",
      "Losgistic Regression(    5900/10000): loss= 9557.41331342192\n",
      "Losgistic Regression(    6000/10000): loss= 9556.96926128826\n",
      "Losgistic Regression(    6100/10000): loss= 9556.42335186172\n",
      "Losgistic Regression(    6200/10000): loss= 9555.94641490751\n",
      "Losgistic Regression(    6300/10000): loss= 9555.50467552407\n",
      "Losgistic Regression(    6400/10000): loss= 9555.03960316595\n",
      "Losgistic Regression(    6500/10000): loss= 9554.5180790282\n",
      "Losgistic Regression(    6600/10000): loss= 9553.93553835943\n",
      "Losgistic Regression(    6700/10000): loss= 9553.31592109706\n",
      "Losgistic Regression(    6800/10000): loss= 9552.7332412116\n",
      "Losgistic Regression(    6900/10000): loss= 9552.25930700968\n",
      "Losgistic Regression(    7000/10000): loss= 9551.80281191108\n",
      "Losgistic Regression(    7100/10000): loss= 9551.33307683714\n",
      "Losgistic Regression(    7200/10000): loss= 9550.51318127614\n",
      "Losgistic Regression(    7300/10000): loss= 9549.46685442684\n",
      "Losgistic Regression(    7400/10000): loss= 9548.43867386808\n",
      "Losgistic Regression(    7500/10000): loss= 9547.35783612892\n",
      "Losgistic Regression(    7600/10000): loss= 9546.28878756737\n",
      "Losgistic Regression(    7700/10000): loss= 9545.25672416815\n",
      "Losgistic Regression(    7800/10000): loss= 9544.24562738953\n",
      "Losgistic Regression(    7900/10000): loss= 9543.0561909878\n",
      "Losgistic Regression(    8000/10000): loss= 9541.76019525054\n",
      "Losgistic Regression(    8100/10000): loss= 9540.60086324227\n",
      "Losgistic Regression(    8200/10000): loss= 9539.66486082162\n",
      "Losgistic Regression(    8300/10000): loss= 9538.90550482484\n",
      "Losgistic Regression(    8400/10000): loss= 9538.26633414399\n",
      "Losgistic Regression(    8500/10000): loss= 9537.59893596293\n",
      "Losgistic Regression(    8600/10000): loss= 9536.57226351879\n",
      "Losgistic Regression(    8700/10000): loss= 9535.32076374909\n",
      "Losgistic Regression(    8800/10000): loss= 9534.10131707739\n",
      "Losgistic Regression(    8900/10000): loss= 9533.11898746135\n",
      "Losgistic Regression(    9000/10000): loss= 9532.33143501366\n",
      "Losgistic Regression(    9100/10000): loss= 9531.65546746468\n",
      "Losgistic Regression(    9200/10000): loss= 9530.89571133369\n",
      "Losgistic Regression(    9300/10000): loss= 9530.03671908566\n",
      "Losgistic Regression(    9400/10000): loss= 9529.21592199698\n",
      "Losgistic Regression(    9500/10000): loss= 9528.60748752368\n",
      "Losgistic Regression(    9600/10000): loss= 9528.19642599625\n",
      "Losgistic Regression(    9700/10000): loss= 9527.91008093668\n",
      "Losgistic Regression(    9800/10000): loss= 9527.57513982141\n",
      "Losgistic Regression(    9900/10000): loss= 9527.06792242297\n",
      "Time for  3th cross validation = 787.663s\n",
      "Training Accuracy         = 0.83252\n",
      "Cross Validation Accuracy = 0.822112\n",
      "Losgistic Regression(       0/10000): loss= 16714.9985569395\n",
      "Losgistic Regression(     100/10000): loss= 10881.943169028\n",
      "Losgistic Regression(     200/10000): loss= 10345.4206920151\n",
      "Losgistic Regression(     300/10000): loss= 10068.5885117552\n",
      "Losgistic Regression(     400/10000): loss= 9907.19782143131\n",
      "Losgistic Regression(     500/10000): loss= 9817.43912537262\n",
      "Losgistic Regression(     600/10000): loss= 9747.65690067111\n",
      "Losgistic Regression(     700/10000): loss= 9676.80727181613\n",
      "Losgistic Regression(     800/10000): loss= 9601.276430175\n",
      "Losgistic Regression(     900/10000): loss= 9529.74985783295\n",
      "Losgistic Regression(    1000/10000): loss= 9471.7342897017\n",
      "Losgistic Regression(    1100/10000): loss= 9432.98012582129\n",
      "Losgistic Regression(    1200/10000): loss= 9414.56611718086\n",
      "Losgistic Regression(    1300/10000): loss= 9412.36424032938\n",
      "Losgistic Regression(    1400/10000): loss= 9412.45436237425\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  9412.45436237\n",
      "Time for  4th cross validation = 113.345s\n",
      "Training Accuracy         = 0.82668\n",
      "Cross Validation Accuracy = 0.82078\n",
      "*************** ([0.83243999999999996, 0.83072000000000001, 0.82804, 0.83252000000000004, 0.82667999999999997], [0.821492, 0.82279999999999998, 0.82308800000000004, 0.82211199999999995, 0.82077999999999995])\n",
      "Losgistic Regression(       0/10000): loss= 16721.2624440539\n",
      "Losgistic Regression(     100/10000): loss= 10946.6251665517\n",
      "Losgistic Regression(     200/10000): loss= 10433.5457267884\n",
      "Losgistic Regression(     300/10000): loss= 10181.4447805036\n",
      "Losgistic Regression(     400/10000): loss= 10026.3788350419\n",
      "Losgistic Regression(     500/10000): loss= 9920.88186922522\n",
      "Losgistic Regression(     600/10000): loss= 9840.98818002754\n",
      "Losgistic Regression(     700/10000): loss= 9776.31574511151\n",
      "Losgistic Regression(     800/10000): loss= 9730.52778806981\n",
      "Losgistic Regression(     900/10000): loss= 9699.76261636863\n",
      "Losgistic Regression(    1000/10000): loss= 9675.7208903328\n",
      "Losgistic Regression(    1100/10000): loss= 9658.78122942283\n",
      "Losgistic Regression(    1200/10000): loss= 9645.30029126262\n",
      "Losgistic Regression(    1300/10000): loss= 9635.30700773646\n",
      "Losgistic Regression(    1400/10000): loss= 9627.25474097081\n",
      "Losgistic Regression(    1500/10000): loss= 9617.33507502821\n",
      "Losgistic Regression(    1600/10000): loss= 9610.57424935072\n",
      "Losgistic Regression(    1700/10000): loss= 9600.61284926307\n",
      "Losgistic Regression(    1800/10000): loss= 9589.37306119173\n",
      "Losgistic Regression(    1900/10000): loss= 9578.0445296365\n",
      "Losgistic Regression(    2000/10000): loss= 9567.62524890282\n",
      "Losgistic Regression(    2100/10000): loss= 9557.24534755544\n",
      "Losgistic Regression(    2200/10000): loss= 9548.34224705974\n",
      "Losgistic Regression(    2300/10000): loss= 9540.17346380781\n",
      "Losgistic Regression(    2400/10000): loss= 9532.54524373905\n",
      "Losgistic Regression(    2500/10000): loss= 9525.65934633094\n",
      "Losgistic Regression(    2600/10000): loss= 9519.58224257528\n",
      "Losgistic Regression(    2700/10000): loss= 9515.40580248406\n",
      "Losgistic Regression(    2800/10000): loss= 9512.39395079248\n",
      "Losgistic Regression(    2900/10000): loss= 9510.19202630449\n",
      "Losgistic Regression(    3000/10000): loss= 9508.1827091303\n",
      "Losgistic Regression(    3100/10000): loss= 9505.96711384184\n",
      "Losgistic Regression(    3200/10000): loss= 9504.1876318013\n",
      "Losgistic Regression(    3300/10000): loss= 9503.15579728043\n",
      "Losgistic Regression(    3400/10000): loss= 9502.8216728384\n",
      "Losgistic Regression(    3500/10000): loss= 9502.76894286725\n",
      "Totoal number of iterations =  3500\n",
      "Loss                        =  9502.76894287\n",
      "Time for  0th cross validation = 277.884s\n",
      "Training Accuracy         = 0.83248\n",
      "Cross Validation Accuracy = 0.821712\n",
      "Losgistic Regression(       0/10000): loss= 16695.653818584\n",
      "Losgistic Regression(     100/10000): loss= 11039.0430777893\n",
      "Losgistic Regression(     200/10000): loss= 10552.5696178489\n",
      "Losgistic Regression(     300/10000): loss= 10314.5341629487\n",
      "Losgistic Regression(     400/10000): loss= 10159.8667369206\n",
      "Losgistic Regression(     500/10000): loss= 10040.3718578313\n",
      "Losgistic Regression(     600/10000): loss= 9960.88147836901\n",
      "Losgistic Regression(     700/10000): loss= 9906.61815954285\n",
      "Losgistic Regression(     800/10000): loss= 9867.36338090964\n",
      "Losgistic Regression(     900/10000): loss= 9833.16692382987\n",
      "Losgistic Regression(    1000/10000): loss= 9806.18649388618\n",
      "Losgistic Regression(    1100/10000): loss= 9783.53922922653\n",
      "Losgistic Regression(    1200/10000): loss= 9767.03125666039\n",
      "Losgistic Regression(    1300/10000): loss= 9754.25389361999\n",
      "Losgistic Regression(    1400/10000): loss= 9743.60845844768\n",
      "Losgistic Regression(    1500/10000): loss= 9737.55239215542\n",
      "Losgistic Regression(    1600/10000): loss= 9731.12789017477\n",
      "Losgistic Regression(    1700/10000): loss= 9724.22653678499\n",
      "Losgistic Regression(    1800/10000): loss= 9718.05969231096\n",
      "Losgistic Regression(    1900/10000): loss= 9712.75164684079\n",
      "Losgistic Regression(    2000/10000): loss= 9707.40944379963\n",
      "Losgistic Regression(    2100/10000): loss= 9702.20185578917\n",
      "Losgistic Regression(    2200/10000): loss= 9696.96538694731\n",
      "Losgistic Regression(    2300/10000): loss= 9692.77512904693\n",
      "Losgistic Regression(    2400/10000): loss= 9689.24431211395\n",
      "Losgistic Regression(    2500/10000): loss= 9686.03071307643\n",
      "Losgistic Regression(    2600/10000): loss= 9682.95296289568\n",
      "Losgistic Regression(    2700/10000): loss= 9679.87855167727\n",
      "Losgistic Regression(    2800/10000): loss= 9677.69971910872\n",
      "Losgistic Regression(    2900/10000): loss= 9675.86016989673\n",
      "Losgistic Regression(    3000/10000): loss= 9674.4740064768\n",
      "Losgistic Regression(    3100/10000): loss= 9672.96650317727\n",
      "Losgistic Regression(    3200/10000): loss= 9671.5870819644\n",
      "Losgistic Regression(    3300/10000): loss= 9670.46734218973\n",
      "Losgistic Regression(    3400/10000): loss= 9669.40237459971\n",
      "Losgistic Regression(    3500/10000): loss= 9668.74428200433\n",
      "Losgistic Regression(    3600/10000): loss= 9668.2205159927\n",
      "Losgistic Regression(    3700/10000): loss= 9667.5996216833\n",
      "Losgistic Regression(    3800/10000): loss= 9667.14503035138\n",
      "Losgistic Regression(    3900/10000): loss= 9666.46140412358\n",
      "Losgistic Regression(    4000/10000): loss= 9666.05632010942\n",
      "Losgistic Regression(    4100/10000): loss= 9665.76507741594\n",
      "Losgistic Regression(    4200/10000): loss= 9665.75732850197\n",
      "Totoal number of iterations =  4200\n",
      "Loss                        =  9665.7573285\n",
      "Time for  1th cross validation = 333.306s\n",
      "Training Accuracy         = 0.82984\n",
      "Cross Validation Accuracy = 0.822668\n",
      "Losgistic Regression(       0/10000): loss= 16686.8044670533\n",
      "Losgistic Regression(     100/10000): loss= 11101.7212188433\n",
      "Losgistic Regression(     200/10000): loss= 10580.9739921932\n",
      "Losgistic Regression(     300/10000): loss= 10311.8329899163\n",
      "Losgistic Regression(     400/10000): loss= 10153.9262938186\n",
      "Losgistic Regression(     500/10000): loss= 10054.2423057729\n",
      "Losgistic Regression(     600/10000): loss= 9991.57924478226\n",
      "Losgistic Regression(     700/10000): loss= 9946.0944708421\n",
      "Losgistic Regression(     800/10000): loss= 9906.98631553382\n",
      "Losgistic Regression(     900/10000): loss= 9877.66378725193\n",
      "Losgistic Regression(    1000/10000): loss= 9856.28150960937\n",
      "Losgistic Regression(    1100/10000): loss= 9837.73625677029\n",
      "Losgistic Regression(    1200/10000): loss= 9823.11146970019\n",
      "Losgistic Regression(    1300/10000): loss= 9811.42178752724\n",
      "Losgistic Regression(    1400/10000): loss= 9802.38166326734\n",
      "Losgistic Regression(    1500/10000): loss= 9796.26372734218\n",
      "Losgistic Regression(    1600/10000): loss= 9791.12840539899\n",
      "Losgistic Regression(    1700/10000): loss= 9786.26395625285\n",
      "Losgistic Regression(    1800/10000): loss= 9783.43483106129\n",
      "Losgistic Regression(    1900/10000): loss= 9780.3643771672\n",
      "Losgistic Regression(    2000/10000): loss= 9777.26105260389\n",
      "Losgistic Regression(    2100/10000): loss= 9773.93867538115\n",
      "Losgistic Regression(    2200/10000): loss= 9770.59199678838\n",
      "Losgistic Regression(    2300/10000): loss= 9767.76275706632\n",
      "Losgistic Regression(    2400/10000): loss= 9765.34595851585\n",
      "Losgistic Regression(    2500/10000): loss= 9762.73033173488\n",
      "Losgistic Regression(    2600/10000): loss= 9759.15042198031\n",
      "Losgistic Regression(    2700/10000): loss= 9754.94501172304\n",
      "Losgistic Regression(    2800/10000): loss= 9751.9384615807\n",
      "Losgistic Regression(    2900/10000): loss= 9749.7709576034\n",
      "Losgistic Regression(    3000/10000): loss= 9747.57432721668\n",
      "Losgistic Regression(    3100/10000): loss= 9745.5322289777\n",
      "Losgistic Regression(    3200/10000): loss= 9743.84379352834\n",
      "Losgistic Regression(    3300/10000): loss= 9742.82882427208\n",
      "Losgistic Regression(    3400/10000): loss= 9741.7400916429\n",
      "Losgistic Regression(    3500/10000): loss= 9740.32174095624\n",
      "Losgistic Regression(    3600/10000): loss= 9739.11690565591\n",
      "Losgistic Regression(    3700/10000): loss= 9737.38649143465\n",
      "Losgistic Regression(    3800/10000): loss= 9735.7595277348\n",
      "Losgistic Regression(    3900/10000): loss= 9734.47096085053\n",
      "Losgistic Regression(    4000/10000): loss= 9733.02781717829\n",
      "Losgistic Regression(    4100/10000): loss= 9731.39908121793\n",
      "Losgistic Regression(    4200/10000): loss= 9730.66609779005\n",
      "Losgistic Regression(    4300/10000): loss= 9729.76205668208\n",
      "Losgistic Regression(    4400/10000): loss= 9728.17902969481\n",
      "Losgistic Regression(    4500/10000): loss= 9726.82927160023\n",
      "Losgistic Regression(    4600/10000): loss= 9725.43705966827\n",
      "Losgistic Regression(    4700/10000): loss= 9724.02061815094\n",
      "Losgistic Regression(    4800/10000): loss= 9723.01990414636\n",
      "Losgistic Regression(    4900/10000): loss= 9722.40775488239\n",
      "Losgistic Regression(    5000/10000): loss= 9721.60971891114\n",
      "Losgistic Regression(    5100/10000): loss= 9721.10410541109\n",
      "Losgistic Regression(    5200/10000): loss= 9720.83440529371\n",
      "Losgistic Regression(    5300/10000): loss= 9720.84078148852\n",
      "Totoal number of iterations =  5300\n",
      "Loss                        =  9720.84078149\n",
      "Time for  2th cross validation = 420.676s\n",
      "Training Accuracy         = 0.82784\n",
      "Cross Validation Accuracy = 0.823372\n",
      "Losgistic Regression(       0/10000): loss= 16690.4087370749\n",
      "Losgistic Regression(     100/10000): loss= 10993.7966473165\n",
      "Losgistic Regression(     200/10000): loss= 10444.8320383217\n",
      "Losgistic Regression(     300/10000): loss= 10206.4503261246\n",
      "Losgistic Regression(     400/10000): loss= 10080.2855216729\n",
      "Losgistic Regression(     500/10000): loss= 10002.3880517233\n",
      "Losgistic Regression(     600/10000): loss= 9946.78761105693\n",
      "Losgistic Regression(     700/10000): loss= 9901.66328284575\n",
      "Losgistic Regression(     800/10000): loss= 9862.60284198492\n",
      "Losgistic Regression(     900/10000): loss= 9828.27290311745\n",
      "Losgistic Regression(    1000/10000): loss= 9801.53467292411\n",
      "Losgistic Regression(    1100/10000): loss= 9783.98575962645\n",
      "Losgistic Regression(    1200/10000): loss= 9767.28484929203\n",
      "Losgistic Regression(    1300/10000): loss= 9756.11631900788\n",
      "Losgistic Regression(    1400/10000): loss= 9746.76648288565\n",
      "Losgistic Regression(    1500/10000): loss= 9738.86495500157\n",
      "Losgistic Regression(    1600/10000): loss= 9732.5025347369\n",
      "Losgistic Regression(    1700/10000): loss= 9726.66640842661\n",
      "Losgistic Regression(    1800/10000): loss= 9721.76083732983\n",
      "Losgistic Regression(    1900/10000): loss= 9716.91100682346\n",
      "Losgistic Regression(    2000/10000): loss= 9711.78777051042\n",
      "Losgistic Regression(    2100/10000): loss= 9706.75639929927\n",
      "Losgistic Regression(    2200/10000): loss= 9701.91685439848\n",
      "Losgistic Regression(    2300/10000): loss= 9697.30665358594\n",
      "Losgistic Regression(    2400/10000): loss= 9693.5462925947\n",
      "Losgistic Regression(    2500/10000): loss= 9690.84153728487\n",
      "Losgistic Regression(    2600/10000): loss= 9688.65510416599\n",
      "Losgistic Regression(    2700/10000): loss= 9686.6358084802\n",
      "Losgistic Regression(    2800/10000): loss= 9684.5941301104\n",
      "Losgistic Regression(    2900/10000): loss= 9682.70071473235\n",
      "Losgistic Regression(    3000/10000): loss= 9681.07896924447\n",
      "Losgistic Regression(    3100/10000): loss= 9679.23307873352\n",
      "Losgistic Regression(    3200/10000): loss= 9676.78423927811\n",
      "Losgistic Regression(    3300/10000): loss= 9674.16294943758\n",
      "Losgistic Regression(    3400/10000): loss= 9672.09772875954\n",
      "Losgistic Regression(    3500/10000): loss= 9669.97960079299\n",
      "Losgistic Regression(    3600/10000): loss= 9668.19159693515\n",
      "Losgistic Regression(    3700/10000): loss= 9666.7142968788\n",
      "Losgistic Regression(    3800/10000): loss= 9665.27657149272\n",
      "Losgistic Regression(    3900/10000): loss= 9664.45480365733\n",
      "Losgistic Regression(    4000/10000): loss= 9664.26861371216\n",
      "Losgistic Regression(    4100/10000): loss= 9664.27477187301\n",
      "Totoal number of iterations =  4100\n",
      "Loss                        =  9664.27477187\n",
      "Time for  3th cross validation = 325.603s\n",
      "Training Accuracy         =  0.831\n",
      "Cross Validation Accuracy = 0.821792\n",
      "Losgistic Regression(       0/10000): loss= 16715.1699114494\n",
      "Losgistic Regression(     100/10000): loss= 10894.1525817255\n",
      "Losgistic Regression(     200/10000): loss= 10368.7133476968\n",
      "Losgistic Regression(     300/10000): loss= 10099.743000909\n",
      "Losgistic Regression(     400/10000): loss= 9939.54997824465\n",
      "Losgistic Regression(     500/10000): loss= 9848.26063301553\n",
      "Losgistic Regression(     600/10000): loss= 9776.66057259851\n",
      "Losgistic Regression(     700/10000): loss= 9706.93623228905\n",
      "Losgistic Regression(     800/10000): loss= 9632.43176509744\n",
      "Losgistic Regression(     900/10000): loss= 9563.58321165782\n",
      "Losgistic Regression(    1000/10000): loss= 9514.5858343743\n",
      "Losgistic Regression(    1100/10000): loss= 9492.83569919\n",
      "Losgistic Regression(    1200/10000): loss= 9489.23250764509\n",
      "Losgistic Regression(    1300/10000): loss= 9489.34635129625\n",
      "Totoal number of iterations =  1300\n",
      "Loss                        =  9489.3463513\n",
      "Time for  4th cross validation = 105.325s\n",
      "Training Accuracy         = 0.82632\n",
      "Cross Validation Accuracy = 0.820936\n",
      "*************** ([0.83248, 0.82984000000000002, 0.82784000000000002, 0.83099999999999996, 0.82632000000000005], [0.821712, 0.82266799999999995, 0.82337199999999999, 0.82179199999999997, 0.820936])\n",
      "Losgistic Regression(       0/10000): loss= 16721.8583862913\n",
      "Losgistic Regression(     100/10000): loss= 10982.8521313403\n",
      "Losgistic Regression(     200/10000): loss= 10498.1457217551\n",
      "Losgistic Regression(     300/10000): loss= 10266.598613569\n",
      "Losgistic Regression(     400/10000): loss= 10128.3875469559\n",
      "Losgistic Regression(     500/10000): loss= 10037.6485379787\n",
      "Losgistic Regression(     600/10000): loss= 9972.51416980651\n",
      "Losgistic Regression(     700/10000): loss= 9921.7797199019\n",
      "Losgistic Regression(     800/10000): loss= 9885.36426891207\n",
      "Losgistic Regression(     900/10000): loss= 9860.41068750564\n",
      "Losgistic Regression(    1000/10000): loss= 9842.67850954585\n",
      "Losgistic Regression(    1100/10000): loss= 9831.00283804039\n",
      "Losgistic Regression(    1200/10000): loss= 9820.83749641587\n",
      "Losgistic Regression(    1300/10000): loss= 9814.89773969567\n",
      "Losgistic Regression(    1400/10000): loss= 9810.33412097896\n",
      "Losgistic Regression(    1500/10000): loss= 9805.1387502044\n",
      "Losgistic Regression(    1600/10000): loss= 9801.91006163809\n",
      "Losgistic Regression(    1700/10000): loss= 9793.91804066177\n",
      "Losgistic Regression(    1800/10000): loss= 9785.09384149965\n",
      "Losgistic Regression(    1900/10000): loss= 9776.25067142524\n",
      "Losgistic Regression(    2000/10000): loss= 9767.08279903104\n",
      "Losgistic Regression(    2100/10000): loss= 9757.20891395547\n",
      "Losgistic Regression(    2200/10000): loss= 9749.17852497934\n",
      "Losgistic Regression(    2300/10000): loss= 9744.56082471362\n",
      "Losgistic Regression(    2400/10000): loss= 9740.32319806407\n",
      "Losgistic Regression(    2500/10000): loss= 9735.36586976199\n",
      "Losgistic Regression(    2600/10000): loss= 9730.88486812033\n",
      "Losgistic Regression(    2700/10000): loss= 9726.66067084185\n",
      "Losgistic Regression(    2800/10000): loss= 9723.26363679378\n",
      "Losgistic Regression(    2900/10000): loss= 9721.04231781269\n",
      "Losgistic Regression(    3000/10000): loss= 9719.96047554111\n",
      "Losgistic Regression(    3100/10000): loss= 9718.11593903565\n",
      "Losgistic Regression(    3200/10000): loss= 9716.535568413\n",
      "Losgistic Regression(    3300/10000): loss= 9715.29602100419\n",
      "Losgistic Regression(    3400/10000): loss= 9714.92471244568\n",
      "Losgistic Regression(    3500/10000): loss= 9714.31305621\n",
      "Losgistic Regression(    3600/10000): loss= 9714.31697198307\n",
      "Totoal number of iterations =  3600\n",
      "Loss                        =  9714.31697198\n",
      "Time for  0th cross validation = 286.066s\n",
      "Training Accuracy         = 0.82992\n",
      "Cross Validation Accuracy = 0.821944\n",
      "Losgistic Regression(       0/10000): loss= 16696.2652097066\n",
      "Losgistic Regression(     100/10000): loss= 11074.3995197931\n",
      "Losgistic Regression(     200/10000): loss= 10613.2089343278\n",
      "Losgistic Regression(     300/10000): loss= 10396.3787980569\n",
      "Losgistic Regression(     400/10000): loss= 10261.5568289762\n",
      "Losgistic Regression(     500/10000): loss= 10159.6302706345\n",
      "Losgistic Regression(     600/10000): loss= 10092.1520244811\n",
      "Losgistic Regression(     700/10000): loss= 10049.4618350641\n",
      "Losgistic Regression(     800/10000): loss= 10017.4053551599\n",
      "Losgistic Regression(     900/10000): loss= 9994.16788149977\n",
      "Losgistic Regression(    1000/10000): loss= 9976.25344793681\n",
      "Losgistic Regression(    1100/10000): loss= 9963.87391142412\n",
      "Losgistic Regression(    1200/10000): loss= 9954.09470279626\n",
      "Losgistic Regression(    1300/10000): loss= 9945.65045354374\n",
      "Losgistic Regression(    1400/10000): loss= 9937.83886898157\n",
      "Losgistic Regression(    1500/10000): loss= 9931.77136892229\n",
      "Losgistic Regression(    1600/10000): loss= 9927.53754518025\n",
      "Losgistic Regression(    1700/10000): loss= 9923.58920263851\n",
      "Losgistic Regression(    1800/10000): loss= 9919.11052787436\n",
      "Losgistic Regression(    1900/10000): loss= 9915.4751536306\n",
      "Losgistic Regression(    2000/10000): loss= 9912.57690174935\n",
      "Losgistic Regression(    2100/10000): loss= 9909.55474587976\n",
      "Losgistic Regression(    2200/10000): loss= 9906.97001962981\n",
      "Losgistic Regression(    2300/10000): loss= 9905.13940322704\n",
      "Losgistic Regression(    2400/10000): loss= 9903.4613292668\n",
      "Losgistic Regression(    2500/10000): loss= 9902.11610735553\n",
      "Losgistic Regression(    2600/10000): loss= 9900.84321169425\n",
      "Losgistic Regression(    2700/10000): loss= 9899.64549619643\n",
      "Losgistic Regression(    2800/10000): loss= 9898.40314957304\n",
      "Losgistic Regression(    2900/10000): loss= 9897.64453719621\n",
      "Losgistic Regression(    3000/10000): loss= 9896.342845697\n",
      "Losgistic Regression(    3100/10000): loss= 9895.96952097528\n",
      "Losgistic Regression(    3200/10000): loss= 9895.7604573934\n",
      "Losgistic Regression(    3300/10000): loss= 9895.76820495099\n",
      "Totoal number of iterations =  3300\n",
      "Loss                        =  9895.76820495\n",
      "Time for  1th cross validation = 262.325s\n",
      "Training Accuracy         = 0.82904\n",
      "Cross Validation Accuracy = 0.8226\n",
      "Losgistic Regression(       0/10000): loss= 16687.4127490871\n",
      "Losgistic Regression(     100/10000): loss= 11137.9803014237\n",
      "Losgistic Regression(     200/10000): loss= 10645.3405010559\n",
      "Losgistic Regression(     300/10000): loss= 10399.6383187369\n",
      "Losgistic Regression(     400/10000): loss= 10259.4654057609\n",
      "Losgistic Regression(     500/10000): loss= 10172.0681850022\n",
      "Losgistic Regression(     600/10000): loss= 10119.1758345097\n",
      "Losgistic Regression(     700/10000): loss= 10083.7860578297\n",
      "Losgistic Regression(     800/10000): loss= 10052.5892444755\n",
      "Losgistic Regression(     900/10000): loss= 10029.4338682771\n",
      "Losgistic Regression(    1000/10000): loss= 10013.9356023682\n",
      "Losgistic Regression(    1100/10000): loss= 10000.8523207217\n",
      "Losgistic Regression(    1200/10000): loss= 9991.23678950141\n",
      "Losgistic Regression(    1300/10000): loss= 9982.14376490506\n",
      "Losgistic Regression(    1400/10000): loss= 9974.73347206288\n",
      "Losgistic Regression(    1500/10000): loss= 9969.49998806045\n",
      "Losgistic Regression(    1600/10000): loss= 9966.36705041716\n",
      "Losgistic Regression(    1700/10000): loss= 9963.68362768783\n",
      "Losgistic Regression(    1800/10000): loss= 9962.63029981149\n",
      "Losgistic Regression(    1900/10000): loss= 9961.50983674224\n",
      "Losgistic Regression(    2000/10000): loss= 9958.15940045101\n",
      "Losgistic Regression(    2100/10000): loss= 9954.95688455108\n",
      "Losgistic Regression(    2200/10000): loss= 9951.61720347619\n",
      "Losgistic Regression(    2300/10000): loss= 9948.67610742012\n",
      "Losgistic Regression(    2400/10000): loss= 9944.58084007753\n",
      "Losgistic Regression(    2500/10000): loss= 9940.85073871997\n",
      "Losgistic Regression(    2600/10000): loss= 9936.90140996474\n",
      "Losgistic Regression(    2700/10000): loss= 9932.69845835253\n",
      "Losgistic Regression(    2800/10000): loss= 9930.32514749126\n",
      "Losgistic Regression(    2900/10000): loss= 9929.93378497696\n",
      "Losgistic Regression(    3000/10000): loss= 9929.92932527883\n",
      "Totoal number of iterations =  3000\n",
      "Loss                        =  9929.92932528\n",
      "Time for  2th cross validation = 237.261s\n",
      "Training Accuracy         = 0.82604\n",
      "Cross Validation Accuracy = 0.822168\n",
      "Losgistic Regression(       0/10000): loss= 16691.0174022664\n",
      "Losgistic Regression(     100/10000): loss= 11029.4345427207\n",
      "Losgistic Regression(     200/10000): loss= 10508.4197418987\n",
      "Losgistic Regression(     300/10000): loss= 10290.1427736607\n",
      "Losgistic Regression(     400/10000): loss= 10178.5436430266\n",
      "Losgistic Regression(     500/10000): loss= 10114.8907440946\n",
      "Losgistic Regression(     600/10000): loss= 10070.1881155992\n",
      "Losgistic Regression(     700/10000): loss= 10035.3747189551\n",
      "Losgistic Regression(     800/10000): loss= 10006.5112331344\n",
      "Losgistic Regression(     900/10000): loss= 9981.64866280908\n",
      "Losgistic Regression(    1000/10000): loss= 9960.63524987647\n",
      "Losgistic Regression(    1100/10000): loss= 9948.42619182902\n",
      "Losgistic Regression(    1200/10000): loss= 9937.33512826621\n",
      "Losgistic Regression(    1300/10000): loss= 9930.59862203345\n",
      "Losgistic Regression(    1400/10000): loss= 9926.22613004531\n",
      "Losgistic Regression(    1500/10000): loss= 9922.99835284452\n",
      "Losgistic Regression(    1600/10000): loss= 9920.06298058787\n",
      "Losgistic Regression(    1700/10000): loss= 9917.74619456122\n",
      "Losgistic Regression(    1800/10000): loss= 9915.77953218314\n",
      "Losgistic Regression(    1900/10000): loss= 9913.7752373408\n",
      "Losgistic Regression(    2000/10000): loss= 9911.48738140065\n",
      "Losgistic Regression(    2100/10000): loss= 9909.11297684184\n",
      "Losgistic Regression(    2200/10000): loss= 9906.58803827215\n",
      "Losgistic Regression(    2300/10000): loss= 9904.4006983915\n",
      "Losgistic Regression(    2400/10000): loss= 9902.08639542949\n",
      "Losgistic Regression(    2500/10000): loss= 9900.16510618406\n",
      "Losgistic Regression(    2600/10000): loss= 9899.42263703858\n",
      "Losgistic Regression(    2700/10000): loss= 9898.30276586366\n",
      "Losgistic Regression(    2800/10000): loss= 9896.21762122402\n",
      "Losgistic Regression(    2900/10000): loss= 9894.65451408487\n",
      "Losgistic Regression(    3000/10000): loss= 9893.41297821304\n",
      "Losgistic Regression(    3100/10000): loss= 9892.16713621958\n",
      "Losgistic Regression(    3200/10000): loss= 9891.10606796329\n",
      "Losgistic Regression(    3300/10000): loss= 9890.09669740604\n",
      "Losgistic Regression(    3400/10000): loss= 9889.26989368818\n",
      "Losgistic Regression(    3500/10000): loss= 9888.37868193138\n",
      "Losgistic Regression(    3600/10000): loss= 9887.96207291368\n",
      "Losgistic Regression(    3700/10000): loss= 9887.96361859201\n",
      "Totoal number of iterations =  3700\n",
      "Loss                        =  9887.96361859\n",
      "Time for  3th cross validation = 293.184s\n",
      "Training Accuracy         = 0.82876\n",
      "Cross Validation Accuracy = 0.821312\n",
      "Losgistic Regression(       0/10000): loss= 16715.7666025098\n",
      "Losgistic Regression(     100/10000): loss= 10935.8687864398\n",
      "Losgistic Regression(     200/10000): loss= 10444.6894392419\n",
      "Losgistic Regression(     300/10000): loss= 10187.4856572566\n",
      "Losgistic Regression(     400/10000): loss= 10025.0904348039\n",
      "Losgistic Regression(     500/10000): loss= 9937.92191388982\n",
      "Losgistic Regression(     600/10000): loss= 9880.94743986352\n",
      "Losgistic Regression(     700/10000): loss= 9857.12597211467\n",
      "Losgistic Regression(     800/10000): loss= 9834.35034776617\n",
      "Losgistic Regression(     900/10000): loss= 9815.01992652649\n",
      "Losgistic Regression(    1000/10000): loss= 9808.88757563303\n",
      "Losgistic Regression(    1100/10000): loss= 9809.06431710969\n",
      "Totoal number of iterations =  1100\n",
      "Loss                        =  9809.06431711\n",
      "Time for  4th cross validation = 88.62s\n",
      "Training Accuracy         = 0.82412\n",
      "Cross Validation Accuracy = 0.820236\n",
      "*************** ([0.82991999999999999, 0.82904, 0.82604, 0.82876000000000005, 0.82411999999999996], [0.82194400000000001, 0.8226, 0.82216800000000001, 0.82131200000000004, 0.82023599999999997])\n",
      "Losgistic Regression(       0/10000): loss= 16723.9335776932\n",
      "Losgistic Regression(     100/10000): loss= 11101.75650462\n",
      "Losgistic Regression(     200/10000): loss= 10697.2328224722\n",
      "Losgistic Regression(     300/10000): loss= 10516.5684081286\n",
      "Losgistic Regression(     400/10000): loss= 10417.0179563702\n",
      "Losgistic Regression(     500/10000): loss= 10358.3983655369\n",
      "Losgistic Regression(     600/10000): loss= 10319.5718426809\n",
      "Losgistic Regression(     700/10000): loss= 10290.5642963996\n",
      "Losgistic Regression(     800/10000): loss= 10266.6160933921\n",
      "Losgistic Regression(     900/10000): loss= 10253.8211365203\n",
      "Losgistic Regression(    1000/10000): loss= 10244.2828769461\n",
      "Losgistic Regression(    1100/10000): loss= 10238.5360631278\n",
      "Losgistic Regression(    1200/10000): loss= 10232.0326073113\n",
      "Losgistic Regression(    1300/10000): loss= 10226.5820252357\n",
      "Losgistic Regression(    1400/10000): loss= 10221.7884079877\n",
      "Losgistic Regression(    1500/10000): loss= 10219.248141659\n",
      "Losgistic Regression(    1600/10000): loss= 10217.914876272\n",
      "Losgistic Regression(    1700/10000): loss= 10217.2990819287\n",
      "Losgistic Regression(    1800/10000): loss= 10216.877927515\n",
      "Losgistic Regression(    1900/10000): loss= 10216.5637045813\n",
      "Losgistic Regression(    2000/10000): loss= 10216.1711575193\n",
      "Losgistic Regression(    2100/10000): loss= 10215.6964131659\n",
      "Losgistic Regression(    2200/10000): loss= 10215.1612709599\n",
      "Losgistic Regression(    2300/10000): loss= 10214.6055580156\n",
      "Losgistic Regression(    2400/10000): loss= 10214.0129434474\n",
      "Losgistic Regression(    2500/10000): loss= 10213.3608775448\n",
      "Losgistic Regression(    2600/10000): loss= 10212.6377882087\n",
      "Losgistic Regression(    2700/10000): loss= 10211.96768926\n",
      "Losgistic Regression(    2800/10000): loss= 10211.4255234487\n",
      "Losgistic Regression(    2900/10000): loss= 10211.4278803163\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  10211.4278803\n",
      "Time for  0th cross validation = 230.608s\n",
      "Training Accuracy         = 0.82524\n",
      "Cross Validation Accuracy = 0.819088\n",
      "Losgistic Regression(       0/10000): loss= 16698.3941972512\n",
      "Losgistic Regression(     100/10000): loss= 11189.6547118671\n",
      "Losgistic Regression(     200/10000): loss= 10800.8730242779\n",
      "Losgistic Regression(     300/10000): loss= 10636.0700217868\n",
      "Losgistic Regression(     400/10000): loss= 10544.7433436366\n",
      "Losgistic Regression(     500/10000): loss= 10476.6088027393\n",
      "Losgistic Regression(     600/10000): loss= 10423.7428553438\n",
      "Losgistic Regression(     700/10000): loss= 10391.1800173886\n",
      "Losgistic Regression(     800/10000): loss= 10372.3436553313\n",
      "Losgistic Regression(     900/10000): loss= 10360.5130703321\n",
      "Losgistic Regression(    1000/10000): loss= 10352.6698851795\n",
      "Losgistic Regression(    1100/10000): loss= 10349.0653350035\n",
      "Losgistic Regression(    1200/10000): loss= 10345.4208437305\n",
      "Losgistic Regression(    1300/10000): loss= 10343.3990026023\n",
      "Losgistic Regression(    1400/10000): loss= 10341.2909437123\n",
      "Losgistic Regression(    1500/10000): loss= 10339.1295065299\n",
      "Losgistic Regression(    1600/10000): loss= 10337.2251021201\n",
      "Losgistic Regression(    1700/10000): loss= 10335.7797786728\n",
      "Losgistic Regression(    1800/10000): loss= 10334.3237634777\n",
      "Losgistic Regression(    1900/10000): loss= 10332.8718931793\n",
      "Losgistic Regression(    2000/10000): loss= 10331.6540401918\n",
      "Losgistic Regression(    2100/10000): loss= 10330.4480563478\n",
      "Losgistic Regression(    2200/10000): loss= 10329.9406544527\n",
      "Losgistic Regression(    2300/10000): loss= 10329.8970353859\n",
      "Totoal number of iterations =  2300\n",
      "Loss                        =  10329.8970354\n",
      "Time for  1th cross validation = 182.321s\n",
      "Training Accuracy         = 0.82508\n",
      "Cross Validation Accuracy = 0.81982\n",
      "Losgistic Regression(       0/10000): loss= 16689.5309101559\n",
      "Losgistic Regression(     100/10000): loss= 11256.8009245689\n",
      "Losgistic Regression(     200/10000): loss= 10843.1970988731\n",
      "Losgistic Regression(     300/10000): loss= 10657.1425586428\n",
      "Losgistic Regression(     400/10000): loss= 10559.3172484275\n",
      "Losgistic Regression(     500/10000): loss= 10495.0619466027\n",
      "Losgistic Regression(     600/10000): loss= 10457.6816038635\n",
      "Losgistic Regression(     700/10000): loss= 10436.3808740446\n",
      "Losgistic Regression(     800/10000): loss= 10420.9619495324\n",
      "Losgistic Regression(     900/10000): loss= 10411.2143612425\n",
      "Losgistic Regression(    1000/10000): loss= 10407.7890089172\n",
      "Losgistic Regression(    1100/10000): loss= 10401.6644678745\n",
      "Losgistic Regression(    1200/10000): loss= 10399.1578836176\n",
      "Losgistic Regression(    1300/10000): loss= 10395.0763492884\n",
      "Losgistic Regression(    1400/10000): loss= 10394.6557484098\n",
      "Losgistic Regression(    1500/10000): loss= 10394.0672045146\n",
      "Losgistic Regression(    1600/10000): loss= 10393.4581322249\n",
      "Losgistic Regression(    1700/10000): loss= 10392.940632816\n",
      "Losgistic Regression(    1800/10000): loss= 10392.642972619\n",
      "Losgistic Regression(    1900/10000): loss= 10392.6181668703\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  10392.6181669\n",
      "Time for  2th cross validation = 151.563s\n",
      "Training Accuracy         = 0.82176\n",
      "Cross Validation Accuracy = 0.818972\n",
      "Losgistic Regression(       0/10000): loss= 16693.1368975679\n",
      "Losgistic Regression(     100/10000): loss= 11146.2893883318\n",
      "Losgistic Regression(     200/10000): loss= 10704.706745316\n",
      "Losgistic Regression(     300/10000): loss= 10532.9030088531\n",
      "Losgistic Regression(     400/10000): loss= 10452.5210062322\n",
      "Losgistic Regression(     500/10000): loss= 10410.4498428393\n",
      "Losgistic Regression(     600/10000): loss= 10380.9612189497\n",
      "Losgistic Regression(     700/10000): loss= 10361.8785881401\n",
      "Losgistic Regression(     800/10000): loss= 10345.5708127807\n",
      "Losgistic Regression(     900/10000): loss= 10332.1169449937\n",
      "Losgistic Regression(    1000/10000): loss= 10321.7396276716\n",
      "Losgistic Regression(    1100/10000): loss= 10314.2812955952\n",
      "Losgistic Regression(    1200/10000): loss= 10307.7875291368\n",
      "Losgistic Regression(    1300/10000): loss= 10301.8362259316\n",
      "Losgistic Regression(    1400/10000): loss= 10298.5981724748\n",
      "Losgistic Regression(    1500/10000): loss= 10295.4852951154\n",
      "Losgistic Regression(    1600/10000): loss= 10292.5068458326\n",
      "Losgistic Regression(    1700/10000): loss= 10290.0754206187\n",
      "Losgistic Regression(    1800/10000): loss= 10288.1181652019\n",
      "Losgistic Regression(    1900/10000): loss= 10286.7952492177\n",
      "Losgistic Regression(    2000/10000): loss= 10286.3245437701\n",
      "Losgistic Regression(    2100/10000): loss= 10286.0407337739\n",
      "Losgistic Regression(    2200/10000): loss= 10285.825689834\n",
      "Losgistic Regression(    2300/10000): loss= 10285.8211931824\n",
      "Totoal number of iterations =  2300\n",
      "Loss                        =  10285.8211932\n",
      "Time for  3th cross validation = 181.319s\n",
      "Training Accuracy         = 0.82424\n",
      "Cross Validation Accuracy = 0.818072\n",
      "Losgistic Regression(       0/10000): loss= 16717.8444014649\n",
      "Losgistic Regression(     100/10000): loss= 11071.6421280721\n",
      "Losgistic Regression(     200/10000): loss= 10656.5628533349\n",
      "Losgistic Regression(     300/10000): loss= 10461.522117538\n",
      "Losgistic Regression(     400/10000): loss= 10356.8793627645\n",
      "Losgistic Regression(     500/10000): loss= 10325.4689155993\n",
      "Losgistic Regression(     600/10000): loss= 10325.3706081141\n",
      "Totoal number of iterations =  600\n",
      "Loss                        =  10325.3706081\n",
      "Time for  4th cross validation = 48.6226s\n",
      "Training Accuracy         = 0.81832\n",
      "Cross Validation Accuracy = 0.816936\n",
      "*************** ([0.82523999999999997, 0.82508000000000004, 0.82176000000000005, 0.82423999999999997, 0.81832000000000005], [0.81908800000000004, 0.81981999999999999, 0.81897200000000003, 0.81807200000000002, 0.816936])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.83272000000000002,\n",
       "   0.83067999999999997,\n",
       "   0.82776000000000005,\n",
       "   0.83223999999999998,\n",
       "   0.82672000000000001],\n",
       "  [0.82135199999999997,\n",
       "   0.82273200000000002,\n",
       "   0.823048,\n",
       "   0.82201199999999996,\n",
       "   0.82067999999999997]),\n",
       " ([0.83264000000000005,\n",
       "   0.83055999999999996,\n",
       "   0.82779999999999998,\n",
       "   0.83252000000000004,\n",
       "   0.82672000000000001],\n",
       "  [0.82141200000000003,\n",
       "   0.822716,\n",
       "   0.82310399999999995,\n",
       "   0.82216,\n",
       "   0.82072000000000001]),\n",
       " ([0.83243999999999996,\n",
       "   0.83072000000000001,\n",
       "   0.82804,\n",
       "   0.83252000000000004,\n",
       "   0.82667999999999997],\n",
       "  [0.821492,\n",
       "   0.82279999999999998,\n",
       "   0.82308800000000004,\n",
       "   0.82211199999999995,\n",
       "   0.82077999999999995]),\n",
       " ([0.83248,\n",
       "   0.82984000000000002,\n",
       "   0.82784000000000002,\n",
       "   0.83099999999999996,\n",
       "   0.82632000000000005],\n",
       "  [0.821712,\n",
       "   0.82266799999999995,\n",
       "   0.82337199999999999,\n",
       "   0.82179199999999997,\n",
       "   0.820936]),\n",
       " ([0.82991999999999999,\n",
       "   0.82904,\n",
       "   0.82604,\n",
       "   0.82876000000000005,\n",
       "   0.82411999999999996],\n",
       "  [0.82194400000000001,\n",
       "   0.8226,\n",
       "   0.82216800000000001,\n",
       "   0.82131200000000004,\n",
       "   0.82023599999999997]),\n",
       " ([0.82523999999999997,\n",
       "   0.82508000000000004,\n",
       "   0.82176000000000005,\n",
       "   0.82423999999999997,\n",
       "   0.81832000000000005],\n",
       "  [0.81908800000000004,\n",
       "   0.81981999999999999,\n",
       "   0.81897200000000003,\n",
       "   0.81807200000000002,\n",
       "   0.816936])]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_25000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(25000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_25000.append(tmp)\n",
    "\n",
    "accu_25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 6821.09725390924\n",
      "Losgistic Regression(     100/10000): loss= 4536.47032779892\n",
      "Losgistic Regression(     200/10000): loss= 4256.02807380162\n",
      "Losgistic Regression(     300/10000): loss= 4115.29060027381\n",
      "Losgistic Regression(     400/10000): loss= 4023.8808452216\n",
      "Losgistic Regression(     500/10000): loss= 3957.02846304881\n",
      "Losgistic Regression(     600/10000): loss= 3907.54728747435\n",
      "Losgistic Regression(     700/10000): loss= 3870.04094953132\n",
      "Losgistic Regression(     800/10000): loss= 3839.50710073522\n",
      "Losgistic Regression(     900/10000): loss= 3814.22003875712\n",
      "Losgistic Regression(    1000/10000): loss= 3792.47865657709\n",
      "Losgistic Regression(    1100/10000): loss= 3773.58087112654\n",
      "Losgistic Regression(    1200/10000): loss= 3756.89606693645\n",
      "Losgistic Regression(    1300/10000): loss= 3742.30381021611\n",
      "Losgistic Regression(    1400/10000): loss= 3729.82165615005\n",
      "Losgistic Regression(    1500/10000): loss= 3719.83789594026\n",
      "Losgistic Regression(    1600/10000): loss= 3710.99462178656\n",
      "Losgistic Regression(    1700/10000): loss= 3702.29339867727\n",
      "Losgistic Regression(    1800/10000): loss= 3695.755161003\n",
      "Losgistic Regression(    1900/10000): loss= 3689.33076819024\n",
      "Losgistic Regression(    2000/10000): loss= 3684.86096185856\n",
      "Losgistic Regression(    2100/10000): loss= 3679.79067934966\n",
      "Losgistic Regression(    2200/10000): loss= 3674.93497346774\n",
      "Losgistic Regression(    2300/10000): loss= 3669.90642676627\n",
      "Losgistic Regression(    2400/10000): loss= 3665.12627576681\n",
      "Losgistic Regression(    2500/10000): loss= 3661.16530540809\n",
      "Losgistic Regression(    2600/10000): loss= 3657.27739629252\n",
      "Losgistic Regression(    2700/10000): loss= 3653.96210973395\n",
      "Losgistic Regression(    2800/10000): loss= 3650.8281992305\n",
      "Losgistic Regression(    2900/10000): loss= 3647.94956665106\n",
      "Losgistic Regression(    3000/10000): loss= 3645.31631841196\n",
      "Losgistic Regression(    3100/10000): loss= 3643.04937775782\n",
      "Losgistic Regression(    3200/10000): loss= 3640.78775678797\n",
      "Losgistic Regression(    3300/10000): loss= 3638.54637834018\n",
      "Losgistic Regression(    3400/10000): loss= 3636.40201747567\n",
      "Losgistic Regression(    3500/10000): loss= 3634.48092454769\n",
      "Losgistic Regression(    3600/10000): loss= 3632.631419508\n",
      "Losgistic Regression(    3700/10000): loss= 3630.84329722812\n",
      "Losgistic Regression(    3800/10000): loss= 3628.98309351774\n",
      "Losgistic Regression(    3900/10000): loss= 3627.29894866134\n",
      "Losgistic Regression(    4000/10000): loss= 3625.60554449185\n",
      "Losgistic Regression(    4100/10000): loss= 3624.10031183926\n",
      "Losgistic Regression(    4200/10000): loss= 3622.74834818769\n",
      "Losgistic Regression(    4300/10000): loss= 3621.39496344953\n",
      "Losgistic Regression(    4400/10000): loss= 3620.09329014787\n",
      "Losgistic Regression(    4500/10000): loss= 3618.92331191956\n",
      "Losgistic Regression(    4600/10000): loss= 3617.84728202779\n",
      "Losgistic Regression(    4700/10000): loss= 3616.95270242473\n",
      "Losgistic Regression(    4800/10000): loss= 3616.15819005866\n",
      "Losgistic Regression(    4900/10000): loss= 3615.51223707674\n",
      "Losgistic Regression(    5000/10000): loss= 3614.8823724289\n",
      "Losgistic Regression(    5100/10000): loss= 3614.24509644526\n",
      "Losgistic Regression(    5200/10000): loss= 3613.56177492293\n",
      "Losgistic Regression(    5300/10000): loss= 3612.79506976239\n",
      "Losgistic Regression(    5400/10000): loss= 3612.08630029261\n",
      "Losgistic Regression(    5500/10000): loss= 3611.48697727197\n",
      "Losgistic Regression(    5600/10000): loss= 3611.00664126175\n",
      "Losgistic Regression(    5700/10000): loss= 3610.5840587415\n",
      "Losgistic Regression(    5800/10000): loss= 3610.26477349265\n",
      "Losgistic Regression(    5900/10000): loss= 3609.9827613366\n",
      "Losgistic Regression(    6000/10000): loss= 3609.75422714179\n",
      "Losgistic Regression(    6100/10000): loss= 3609.59084586038\n",
      "Losgistic Regression(    6200/10000): loss= 3609.41598140494\n",
      "Losgistic Regression(    6300/10000): loss= 3609.26029849406\n",
      "Losgistic Regression(    6400/10000): loss= 3609.12889285022\n",
      "Losgistic Regression(    6500/10000): loss= 3608.98397581842\n",
      "Losgistic Regression(    6600/10000): loss= 3608.87600186244\n",
      "Losgistic Regression(    6700/10000): loss= 3608.80921433873\n",
      "Losgistic Regression(    6800/10000): loss= 3608.7761722824\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  3608.77617228\n",
      "Time for  0th cross validation = 218.209s\n",
      "Training Accuracy         = 0.8411\n",
      "Cross Validation Accuracy = 0.813632\n",
      "Losgistic Regression(       0/10000): loss= 6827.55281501584\n",
      "Losgistic Regression(     100/10000): loss= 4486.15730921931\n",
      "Losgistic Regression(     200/10000): loss= 4210.37151528254\n",
      "Losgistic Regression(     300/10000): loss= 4065.61704434822\n",
      "Losgistic Regression(     400/10000): loss= 3967.90899469504\n",
      "Losgistic Regression(     500/10000): loss= 3897.39882322951\n",
      "Losgistic Regression(     600/10000): loss= 3845.96648130426\n",
      "Losgistic Regression(     700/10000): loss= 3806.91929532483\n",
      "Losgistic Regression(     800/10000): loss= 3775.94729305364\n",
      "Losgistic Regression(     900/10000): loss= 3751.03823401674\n",
      "Losgistic Regression(    1000/10000): loss= 3731.25705310498\n",
      "Losgistic Regression(    1100/10000): loss= 3715.54691324431\n",
      "Losgistic Regression(    1200/10000): loss= 3702.76954914785\n",
      "Losgistic Regression(    1300/10000): loss= 3691.98547310976\n",
      "Losgistic Regression(    1400/10000): loss= 3682.56822303035\n",
      "Losgistic Regression(    1500/10000): loss= 3674.56037162838\n",
      "Losgistic Regression(    1600/10000): loss= 3667.34142769706\n",
      "Losgistic Regression(    1700/10000): loss= 3660.65497715595\n",
      "Losgistic Regression(    1800/10000): loss= 3654.45179650949\n",
      "Losgistic Regression(    1900/10000): loss= 3648.68421249811\n",
      "Losgistic Regression(    2000/10000): loss= 3643.37050406782\n",
      "Losgistic Regression(    2100/10000): loss= 3638.43197587157\n",
      "Losgistic Regression(    2200/10000): loss= 3633.88387769689\n",
      "Losgistic Regression(    2300/10000): loss= 3629.69290576271\n",
      "Losgistic Regression(    2400/10000): loss= 3625.80766753003\n",
      "Losgistic Regression(    2500/10000): loss= 3622.00556007863\n",
      "Losgistic Regression(    2600/10000): loss= 3618.22065993334\n",
      "Losgistic Regression(    2700/10000): loss= 3614.75623876174\n",
      "Losgistic Regression(    2800/10000): loss= 3611.49796894757\n",
      "Losgistic Regression(    2900/10000): loss= 3608.36382995066\n",
      "Losgistic Regression(    3000/10000): loss= 3605.3133201999\n",
      "Losgistic Regression(    3100/10000): loss= 3602.39292826917\n",
      "Losgistic Regression(    3200/10000): loss= 3598.70577144588\n",
      "Losgistic Regression(    3300/10000): loss= 3593.8280994607\n",
      "Losgistic Regression(    3400/10000): loss= 3588.87820012485\n",
      "Losgistic Regression(    3500/10000): loss= 3583.93631592543\n",
      "Losgistic Regression(    3600/10000): loss= 3579.11863341078\n",
      "Losgistic Regression(    3700/10000): loss= 3574.41817056885\n",
      "Losgistic Regression(    3800/10000): loss= 3569.86883768702\n",
      "Losgistic Regression(    3900/10000): loss= 3565.55532024963\n",
      "Losgistic Regression(    4000/10000): loss= 3561.50247450286\n",
      "Losgistic Regression(    4100/10000): loss= 3557.6274618016\n",
      "Losgistic Regression(    4200/10000): loss= 3553.88456657269\n",
      "Losgistic Regression(    4300/10000): loss= 3550.22473277582\n",
      "Losgistic Regression(    4400/10000): loss= 3546.70768318694\n",
      "Losgistic Regression(    4500/10000): loss= 3543.29013972685\n",
      "Losgistic Regression(    4600/10000): loss= 3539.98498969817\n",
      "Losgistic Regression(    4700/10000): loss= 3536.76037758526\n",
      "Losgistic Regression(    4800/10000): loss= 3533.70158200311\n",
      "Losgistic Regression(    4900/10000): loss= 3530.85602940552\n",
      "Losgistic Regression(    5000/10000): loss= 3528.13876831843\n",
      "Losgistic Regression(    5100/10000): loss= 3525.53319846317\n",
      "Losgistic Regression(    5200/10000): loss= 3522.95568608759\n",
      "Losgistic Regression(    5300/10000): loss= 3520.52642809706\n",
      "Losgistic Regression(    5400/10000): loss= 3518.29208366415\n",
      "Losgistic Regression(    5500/10000): loss= 3516.19272383074\n",
      "Losgistic Regression(    5600/10000): loss= 3514.27084905422\n",
      "Losgistic Regression(    5700/10000): loss= 3512.60325449367\n",
      "Losgistic Regression(    5800/10000): loss= 3511.1641461549\n",
      "Losgistic Regression(    5900/10000): loss= 3509.9506000589\n",
      "Losgistic Regression(    6000/10000): loss= 3508.90109067525\n",
      "Losgistic Regression(    6100/10000): loss= 3508.08916943377\n",
      "Losgistic Regression(    6200/10000): loss= 3507.43293601175\n",
      "Losgistic Regression(    6300/10000): loss= 3506.55747595855\n",
      "Losgistic Regression(    6400/10000): loss= 3506.0211938821\n",
      "Losgistic Regression(    6500/10000): loss= 3506.02568399804\n",
      "Totoal number of iterations =  6500\n",
      "Loss                        =  3506.025684\n",
      "Time for  1th cross validation = 206.051s\n",
      "Training Accuracy         = 0.8424\n",
      "Cross Validation Accuracy = 0.81584\n",
      "Losgistic Regression(       0/10000): loss= 6825.28848387494\n",
      "Losgistic Regression(     100/10000): loss= 4452.38912978921\n",
      "Losgistic Regression(     200/10000): loss= 4192.93522375195\n",
      "Losgistic Regression(     300/10000): loss= 4053.63139725418\n",
      "Losgistic Regression(     400/10000): loss= 3957.84343818041\n",
      "Losgistic Regression(     500/10000): loss= 3887.43008272319\n",
      "Losgistic Regression(     600/10000): loss= 3831.09951329145\n",
      "Losgistic Regression(     700/10000): loss= 3785.67876784513\n",
      "Losgistic Regression(     800/10000): loss= 3749.62133913377\n",
      "Losgistic Regression(     900/10000): loss= 3719.55318080366\n",
      "Losgistic Regression(    1000/10000): loss= 3698.64407103227\n",
      "Losgistic Regression(    1100/10000): loss= 3682.88934419654\n",
      "Losgistic Regression(    1200/10000): loss= 3671.05493523719\n",
      "Losgistic Regression(    1300/10000): loss= 3662.60991519184\n",
      "Losgistic Regression(    1400/10000): loss= 3655.59128981526\n",
      "Losgistic Regression(    1500/10000): loss= 3650.72440878683\n",
      "Losgistic Regression(    1600/10000): loss= 3647.11301674684\n",
      "Losgistic Regression(    1700/10000): loss= 3644.81737212364\n",
      "Losgistic Regression(    1800/10000): loss= 3642.11874682446\n",
      "Losgistic Regression(    1900/10000): loss= 3639.68663508657\n",
      "Losgistic Regression(    2000/10000): loss= 3636.91828724213\n",
      "Losgistic Regression(    2100/10000): loss= 3634.62202900534\n",
      "Losgistic Regression(    2200/10000): loss= 3628.37146400405\n",
      "Losgistic Regression(    2300/10000): loss= 3622.59354242267\n",
      "Losgistic Regression(    2400/10000): loss= 3616.87134141163\n",
      "Losgistic Regression(    2500/10000): loss= 3611.45329303555\n",
      "Losgistic Regression(    2600/10000): loss= 3606.29151852791\n",
      "Losgistic Regression(    2700/10000): loss= 3601.24202391751\n",
      "Losgistic Regression(    2800/10000): loss= 3596.61265939686\n",
      "Losgistic Regression(    2900/10000): loss= 3592.27899917709\n",
      "Losgistic Regression(    3000/10000): loss= 3588.34345986879\n",
      "Losgistic Regression(    3100/10000): loss= 3584.59064072915\n",
      "Losgistic Regression(    3200/10000): loss= 3581.07800204054\n",
      "Losgistic Regression(    3300/10000): loss= 3577.66007886299\n",
      "Losgistic Regression(    3400/10000): loss= 3574.45813416066\n",
      "Losgistic Regression(    3500/10000): loss= 3571.68231932675\n",
      "Losgistic Regression(    3600/10000): loss= 3569.11641094898\n",
      "Losgistic Regression(    3700/10000): loss= 3566.40647634032\n",
      "Losgistic Regression(    3800/10000): loss= 3563.76137895912\n",
      "Losgistic Regression(    3900/10000): loss= 3561.3432672318\n",
      "Losgistic Regression(    4000/10000): loss= 3559.26088895059\n",
      "Losgistic Regression(    4100/10000): loss= 3557.35824456816\n",
      "Losgistic Regression(    4200/10000): loss= 3554.55935312491\n",
      "Losgistic Regression(    4300/10000): loss= 3551.2812640693\n",
      "Losgistic Regression(    4400/10000): loss= 3548.31300294579\n",
      "Losgistic Regression(    4500/10000): loss= 3545.52612079283\n",
      "Losgistic Regression(    4600/10000): loss= 3543.01661761467\n",
      "Losgistic Regression(    4700/10000): loss= 3540.70716659172\n",
      "Losgistic Regression(    4800/10000): loss= 3538.57879690862\n",
      "Losgistic Regression(    4900/10000): loss= 3536.61207217966\n",
      "Losgistic Regression(    5000/10000): loss= 3534.70696391758\n",
      "Losgistic Regression(    5100/10000): loss= 3532.87563838046\n",
      "Losgistic Regression(    5200/10000): loss= 3531.18100624496\n",
      "Losgistic Regression(    5300/10000): loss= 3529.54695814995\n",
      "Losgistic Regression(    5400/10000): loss= 3527.97111244586\n",
      "Losgistic Regression(    5500/10000): loss= 3526.46458690127\n",
      "Losgistic Regression(    5600/10000): loss= 3525.02381876062\n",
      "Losgistic Regression(    5700/10000): loss= 3523.68524274419\n",
      "Losgistic Regression(    5800/10000): loss= 3522.4552978419\n",
      "Losgistic Regression(    5900/10000): loss= 3521.23707895158\n",
      "Losgistic Regression(    6000/10000): loss= 3519.91712971573\n",
      "Losgistic Regression(    6100/10000): loss= 3518.54305761564\n",
      "Losgistic Regression(    6200/10000): loss= 3517.24078699671\n",
      "Losgistic Regression(    6300/10000): loss= 3516.02207400009\n",
      "Losgistic Regression(    6400/10000): loss= 3514.81217516427\n",
      "Losgistic Regression(    6500/10000): loss= 3513.61947393014\n",
      "Losgistic Regression(    6600/10000): loss= 3512.52191735784\n",
      "Losgistic Regression(    6700/10000): loss= 3511.55589978075\n",
      "Losgistic Regression(    6800/10000): loss= 3510.70158199986\n",
      "Losgistic Regression(    6900/10000): loss= 3509.92982186972\n",
      "Losgistic Regression(    7000/10000): loss= 3509.21151050509\n",
      "Losgistic Regression(    7100/10000): loss= 3508.5054005912\n",
      "Losgistic Regression(    7200/10000): loss= 3507.78892735464\n",
      "Losgistic Regression(    7300/10000): loss= 3507.05872097227\n",
      "Losgistic Regression(    7400/10000): loss= 3506.32259408663\n",
      "Losgistic Regression(    7500/10000): loss= 3505.60363729734\n",
      "Losgistic Regression(    7600/10000): loss= 3504.90038851805\n",
      "Losgistic Regression(    7700/10000): loss= 3504.23828550184\n",
      "Losgistic Regression(    7800/10000): loss= 3503.60794752634\n",
      "Losgistic Regression(    7900/10000): loss= 3502.96934269664\n",
      "Losgistic Regression(    8000/10000): loss= 3502.34079768002\n",
      "Losgistic Regression(    8100/10000): loss= 3501.78040171295\n",
      "Losgistic Regression(    8200/10000): loss= 3501.31146211217\n",
      "Losgistic Regression(    8300/10000): loss= 3500.92591456353\n",
      "Losgistic Regression(    8400/10000): loss= 3500.6092895109\n",
      "Losgistic Regression(    8500/10000): loss= 3500.3358423528\n",
      "Losgistic Regression(    8600/10000): loss= 3500.05026780842\n",
      "Losgistic Regression(    8700/10000): loss= 3499.72387016441\n",
      "Losgistic Regression(    8800/10000): loss= 3499.39579825961\n",
      "Losgistic Regression(    8900/10000): loss= 3499.10830162825\n",
      "Losgistic Regression(    9000/10000): loss= 3498.87291275335\n",
      "Losgistic Regression(    9100/10000): loss= 3498.6944176382\n",
      "Losgistic Regression(    9200/10000): loss= 3498.57453425655\n",
      "Losgistic Regression(    9300/10000): loss= 3498.51736415954\n",
      "Losgistic Regression(    9400/10000): loss= 3498.49679580909\n",
      "Totoal number of iterations =  9400\n",
      "Loss                        =  3498.49679581\n",
      "Time for  2th cross validation = 301.883s\n",
      "Training Accuracy         = 0.8474\n",
      "Cross Validation Accuracy = 0.81414\n",
      "Losgistic Regression(       0/10000): loss= 6822.87301767037\n",
      "Losgistic Regression(     100/10000): loss= 4535.5752645038\n",
      "Losgistic Regression(     200/10000): loss= 4275.6250702742\n",
      "Losgistic Regression(     300/10000): loss= 4133.78984297124\n",
      "Losgistic Regression(     400/10000): loss= 4044.52656870019\n",
      "Losgistic Regression(     500/10000): loss= 3978.58344075658\n",
      "Losgistic Regression(     600/10000): loss= 3923.93600735\n",
      "Losgistic Regression(     700/10000): loss= 3878.669209997\n",
      "Losgistic Regression(     800/10000): loss= 3848.14671580426\n",
      "Losgistic Regression(     900/10000): loss= 3822.95943095151\n",
      "Losgistic Regression(    1000/10000): loss= 3804.89485684971\n",
      "Losgistic Regression(    1100/10000): loss= 3789.63604642773\n",
      "Losgistic Regression(    1200/10000): loss= 3777.63780234782\n",
      "Losgistic Regression(    1300/10000): loss= 3765.37886524585\n",
      "Losgistic Regression(    1400/10000): loss= 3752.94265416502\n",
      "Losgistic Regression(    1500/10000): loss= 3741.34352515083\n",
      "Losgistic Regression(    1600/10000): loss= 3731.27079585311\n",
      "Losgistic Regression(    1700/10000): loss= 3721.74478273051\n",
      "Losgistic Regression(    1800/10000): loss= 3712.58885846474\n",
      "Losgistic Regression(    1900/10000): loss= 3704.26627072269\n",
      "Losgistic Regression(    2000/10000): loss= 3696.59973364103\n",
      "Losgistic Regression(    2100/10000): loss= 3689.12875975084\n",
      "Losgistic Regression(    2200/10000): loss= 3681.55605407147\n",
      "Losgistic Regression(    2300/10000): loss= 3673.96704365038\n",
      "Losgistic Regression(    2400/10000): loss= 3666.65413815554\n",
      "Losgistic Regression(    2500/10000): loss= 3659.70127059471\n",
      "Losgistic Regression(    2600/10000): loss= 3652.92289989249\n",
      "Losgistic Regression(    2700/10000): loss= 3646.15145029826\n",
      "Losgistic Regression(    2800/10000): loss= 3639.47796203109\n",
      "Losgistic Regression(    2900/10000): loss= 3633.07589900235\n",
      "Losgistic Regression(    3000/10000): loss= 3627.00917501861\n",
      "Losgistic Regression(    3100/10000): loss= 3621.32683317696\n",
      "Losgistic Regression(    3200/10000): loss= 3616.00487860234\n",
      "Losgistic Regression(    3300/10000): loss= 3611.05309833332\n",
      "Losgistic Regression(    3400/10000): loss= 3606.46226247929\n",
      "Losgistic Regression(    3500/10000): loss= 3602.10050688913\n",
      "Losgistic Regression(    3600/10000): loss= 3597.96725891524\n",
      "Losgistic Regression(    3700/10000): loss= 3594.15440610075\n",
      "Losgistic Regression(    3800/10000): loss= 3590.66041122675\n",
      "Losgistic Regression(    3900/10000): loss= 3587.45134672074\n",
      "Losgistic Regression(    4000/10000): loss= 3584.40780880247\n",
      "Losgistic Regression(    4100/10000): loss= 3581.44441846038\n",
      "Losgistic Regression(    4200/10000): loss= 3578.49593950712\n",
      "Losgistic Regression(    4300/10000): loss= 3575.59433478113\n",
      "Losgistic Regression(    4400/10000): loss= 3572.79109108423\n",
      "Losgistic Regression(    4500/10000): loss= 3570.1567765734\n",
      "Losgistic Regression(    4600/10000): loss= 3567.68994333009\n",
      "Losgistic Regression(    4700/10000): loss= 3565.28247319566\n",
      "Losgistic Regression(    4800/10000): loss= 3562.86697746271\n",
      "Losgistic Regression(    4900/10000): loss= 3560.48811226756\n",
      "Losgistic Regression(    5000/10000): loss= 3558.19408389371\n",
      "Losgistic Regression(    5100/10000): loss= 3556.04905990489\n",
      "Losgistic Regression(    5200/10000): loss= 3554.02318113895\n",
      "Losgistic Regression(    5300/10000): loss= 3552.07453529846\n",
      "Losgistic Regression(    5400/10000): loss= 3550.21432087675\n",
      "Losgistic Regression(    5500/10000): loss= 3548.41387731671\n",
      "Losgistic Regression(    5600/10000): loss= 3546.71906688348\n",
      "Losgistic Regression(    5700/10000): loss= 3545.12543300751\n",
      "Losgistic Regression(    5800/10000): loss= 3543.64316718264\n",
      "Losgistic Regression(    5900/10000): loss= 3542.29995588667\n",
      "Losgistic Regression(    6000/10000): loss= 3541.05565823854\n",
      "Losgistic Regression(    6100/10000): loss= 3539.85540329267\n",
      "Losgistic Regression(    6200/10000): loss= 3538.67137680455\n",
      "Losgistic Regression(    6300/10000): loss= 3537.49438495402\n",
      "Losgistic Regression(    6400/10000): loss= 3536.3393125164\n",
      "Losgistic Regression(    6500/10000): loss= 3535.21057526574\n",
      "Losgistic Regression(    6600/10000): loss= 3534.10355295302\n",
      "Losgistic Regression(    6700/10000): loss= 3532.93273587926\n",
      "Losgistic Regression(    6800/10000): loss= 3531.75784639114\n",
      "Losgistic Regression(    6900/10000): loss= 3530.59988328037\n",
      "Losgistic Regression(    7000/10000): loss= 3529.46183846758\n",
      "Losgistic Regression(    7100/10000): loss= 3528.34744830693\n",
      "Losgistic Regression(    7200/10000): loss= 3527.23339565186\n",
      "Losgistic Regression(    7300/10000): loss= 3526.10480807012\n",
      "Losgistic Regression(    7400/10000): loss= 3524.92770945445\n",
      "Losgistic Regression(    7500/10000): loss= 3523.70682237709\n",
      "Losgistic Regression(    7600/10000): loss= 3522.51875957879\n",
      "Losgistic Regression(    7700/10000): loss= 3521.35672485011\n",
      "Losgistic Regression(    7800/10000): loss= 3520.20705210703\n",
      "Losgistic Regression(    7900/10000): loss= 3519.07688760096\n",
      "Losgistic Regression(    8000/10000): loss= 3517.96825349937\n",
      "Losgistic Regression(    8100/10000): loss= 3516.88038664028\n",
      "Losgistic Regression(    8200/10000): loss= 3515.8045286569\n",
      "Losgistic Regression(    8300/10000): loss= 3514.7381709091\n",
      "Losgistic Regression(    8400/10000): loss= 3513.70719868308\n",
      "Losgistic Regression(    8500/10000): loss= 3512.7368634915\n",
      "Losgistic Regression(    8600/10000): loss= 3511.83165280684\n",
      "Losgistic Regression(    8700/10000): loss= 3510.98627235701\n",
      "Losgistic Regression(    8800/10000): loss= 3510.18588410985\n",
      "Losgistic Regression(    8900/10000): loss= 3509.43039113572\n",
      "Losgistic Regression(    9000/10000): loss= 3508.73318107809\n",
      "Losgistic Regression(    9100/10000): loss= 3508.09569184624\n",
      "Losgistic Regression(    9200/10000): loss= 3507.51547562535\n",
      "Losgistic Regression(    9300/10000): loss= 3506.996016696\n",
      "Losgistic Regression(    9400/10000): loss= 3506.52661741855\n",
      "Losgistic Regression(    9500/10000): loss= 3506.09040923293\n",
      "Losgistic Regression(    9600/10000): loss= 3505.67637962338\n",
      "Losgistic Regression(    9700/10000): loss= 3505.28836376236\n",
      "Losgistic Regression(    9800/10000): loss= 3504.92940617249\n",
      "Losgistic Regression(    9900/10000): loss= 3504.60502845513\n",
      "Time for  3th cross validation = 316.615s\n",
      "Training Accuracy         =  0.845\n",
      "Cross Validation Accuracy = 0.81414\n",
      "Losgistic Regression(       0/10000): loss= 6820.81257140681\n",
      "Losgistic Regression(     100/10000): loss= 4549.08737631688\n",
      "Losgistic Regression(     200/10000): loss= 4304.07914827624\n",
      "Losgistic Regression(     300/10000): loss= 4187.0563956635\n",
      "Losgistic Regression(     400/10000): loss= 4113.86147606653\n",
      "Losgistic Regression(     500/10000): loss= 4059.88786775868\n",
      "Losgistic Regression(     600/10000): loss= 4013.19119597406\n",
      "Losgistic Regression(     700/10000): loss= 3973.81369196939\n",
      "Losgistic Regression(     800/10000): loss= 3939.81536905797\n",
      "Losgistic Regression(     900/10000): loss= 3911.09815501102\n",
      "Losgistic Regression(    1000/10000): loss= 3887.80597497603\n",
      "Losgistic Regression(    1100/10000): loss= 3867.34712626692\n",
      "Losgistic Regression(    1200/10000): loss= 3849.59421962292\n",
      "Losgistic Regression(    1300/10000): loss= 3834.62515970259\n",
      "Losgistic Regression(    1400/10000): loss= 3823.32699882631\n",
      "Losgistic Regression(    1500/10000): loss= 3812.90189812583\n",
      "Losgistic Regression(    1600/10000): loss= 3802.987247851\n",
      "Losgistic Regression(    1700/10000): loss= 3792.71551629668\n",
      "Losgistic Regression(    1800/10000): loss= 3783.56281283207\n",
      "Losgistic Regression(    1900/10000): loss= 3775.40178739603\n",
      "Losgistic Regression(    2000/10000): loss= 3768.46311256192\n",
      "Losgistic Regression(    2100/10000): loss= 3761.97108960724\n",
      "Losgistic Regression(    2200/10000): loss= 3755.43429155067\n",
      "Losgistic Regression(    2300/10000): loss= 3749.36034727366\n",
      "Losgistic Regression(    2400/10000): loss= 3744.47302208722\n",
      "Losgistic Regression(    2500/10000): loss= 3737.8383013543\n",
      "Losgistic Regression(    2600/10000): loss= 3730.29523815014\n",
      "Losgistic Regression(    2700/10000): loss= 3722.94047241162\n",
      "Losgistic Regression(    2800/10000): loss= 3716.11126468113\n",
      "Losgistic Regression(    2900/10000): loss= 3709.16304992761\n",
      "Losgistic Regression(    3000/10000): loss= 3702.09539189451\n",
      "Losgistic Regression(    3100/10000): loss= 3694.7670295661\n",
      "Losgistic Regression(    3200/10000): loss= 3686.6057669833\n",
      "Losgistic Regression(    3300/10000): loss= 3678.13873361311\n",
      "Losgistic Regression(    3400/10000): loss= 3669.73863352366\n",
      "Losgistic Regression(    3500/10000): loss= 3661.99941436619\n",
      "Losgistic Regression(    3600/10000): loss= 3654.4159493689\n",
      "Losgistic Regression(    3700/10000): loss= 3647.49764041543\n",
      "Losgistic Regression(    3800/10000): loss= 3641.23378139557\n",
      "Losgistic Regression(    3900/10000): loss= 3635.673522877\n",
      "Losgistic Regression(    4000/10000): loss= 3630.75804161485\n",
      "Losgistic Regression(    4100/10000): loss= 3626.12354626119\n",
      "Losgistic Regression(    4200/10000): loss= 3621.80678128647\n",
      "Losgistic Regression(    4300/10000): loss= 3617.45440453766\n",
      "Losgistic Regression(    4400/10000): loss= 3613.36209574162\n",
      "Losgistic Regression(    4500/10000): loss= 3609.23929787846\n",
      "Losgistic Regression(    4600/10000): loss= 3605.46240818641\n",
      "Losgistic Regression(    4700/10000): loss= 3601.72455644528\n",
      "Losgistic Regression(    4800/10000): loss= 3598.14340512103\n",
      "Losgistic Regression(    4900/10000): loss= 3594.70765790642\n",
      "Losgistic Regression(    5000/10000): loss= 3591.35720713416\n",
      "Losgistic Regression(    5100/10000): loss= 3587.78556431035\n",
      "Losgistic Regression(    5200/10000): loss= 3584.31196049074\n",
      "Losgistic Regression(    5300/10000): loss= 3580.87640757516\n",
      "Losgistic Regression(    5400/10000): loss= 3577.4974843726\n",
      "Losgistic Regression(    5500/10000): loss= 3574.42891550958\n",
      "Losgistic Regression(    5600/10000): loss= 3571.55450744441\n",
      "Losgistic Regression(    5700/10000): loss= 3568.90770312022\n",
      "Losgistic Regression(    5800/10000): loss= 3566.28157065168\n",
      "Losgistic Regression(    5900/10000): loss= 3563.66741949419\n",
      "Losgistic Regression(    6000/10000): loss= 3561.36826248488\n",
      "Losgistic Regression(    6100/10000): loss= 3559.33910243898\n",
      "Losgistic Regression(    6200/10000): loss= 3557.56729990133\n",
      "Losgistic Regression(    6300/10000): loss= 3556.04514961189\n",
      "Losgistic Regression(    6400/10000): loss= 3554.89412038511\n",
      "Losgistic Regression(    6500/10000): loss= 3553.95929615754\n",
      "Losgistic Regression(    6600/10000): loss= 3553.15121412892\n",
      "Losgistic Regression(    6700/10000): loss= 3552.58578004309\n",
      "Losgistic Regression(    6800/10000): loss= 3551.87482990902\n",
      "Losgistic Regression(    6900/10000): loss= 3551.18674513383\n",
      "Losgistic Regression(    7000/10000): loss= 3550.57865476888\n",
      "Losgistic Regression(    7100/10000): loss= 3550.10731129597\n",
      "Losgistic Regression(    7200/10000): loss= 3549.67142265136\n",
      "Losgistic Regression(    7300/10000): loss= 3549.243687084\n",
      "Losgistic Regression(    7400/10000): loss= 3548.92136028517\n",
      "Losgistic Regression(    7500/10000): loss= 3548.63403249947\n",
      "Losgistic Regression(    7600/10000): loss= 3548.23704217327\n",
      "Losgistic Regression(    7700/10000): loss= 3547.80363010904\n",
      "Losgistic Regression(    7800/10000): loss= 3547.36375536062\n",
      "Losgistic Regression(    7900/10000): loss= 3546.88968800852\n",
      "Losgistic Regression(    8000/10000): loss= 3546.35562592364\n",
      "Losgistic Regression(    8100/10000): loss= 3545.81103672763\n",
      "Losgistic Regression(    8200/10000): loss= 3545.19569799344\n",
      "Losgistic Regression(    8300/10000): loss= 3544.40423518826\n",
      "Losgistic Regression(    8400/10000): loss= 3543.52113374772\n",
      "Losgistic Regression(    8500/10000): loss= 3542.5203384672\n",
      "Losgistic Regression(    8600/10000): loss= 3541.36756304777\n",
      "Losgistic Regression(    8700/10000): loss= 3540.16044721759\n",
      "Losgistic Regression(    8800/10000): loss= 3538.94753290106\n",
      "Losgistic Regression(    8900/10000): loss= 3537.71449950819\n",
      "Losgistic Regression(    9000/10000): loss= 3536.46976818941\n",
      "Losgistic Regression(    9100/10000): loss= 3535.19426958348\n",
      "Losgistic Regression(    9200/10000): loss= 3533.8915690139\n",
      "Losgistic Regression(    9300/10000): loss= 3532.49137249303\n",
      "Losgistic Regression(    9400/10000): loss= 3531.01275612247\n",
      "Losgistic Regression(    9500/10000): loss= 3529.51853116795\n",
      "Losgistic Regression(    9600/10000): loss= 3527.96901763913\n",
      "Losgistic Regression(    9700/10000): loss= 3526.35288085581\n",
      "Losgistic Regression(    9800/10000): loss= 3524.73018731022\n",
      "Losgistic Regression(    9900/10000): loss= 3523.0964202242\n",
      "Time for  4th cross validation = 315.235s\n",
      "Training Accuracy         = 0.8353\n",
      "Cross Validation Accuracy = 0.815752\n",
      "*************** ([0.84109999999999996, 0.84240000000000004, 0.84740000000000004, 0.84499999999999997, 0.83530000000000004], [0.81363200000000002, 0.81584000000000001, 0.81413999999999997, 0.81413999999999997, 0.81575200000000003])\n",
      "Losgistic Regression(       0/10000): loss= 6821.10320034063\n",
      "Losgistic Regression(     100/10000): loss= 4537.12604080686\n",
      "Losgistic Regression(     200/10000): loss= 4257.22888395114\n",
      "Losgistic Regression(     300/10000): loss= 4116.99743899103\n",
      "Losgistic Regression(     400/10000): loss= 4026.0334837952\n",
      "Losgistic Regression(     500/10000): loss= 3959.60742953169\n",
      "Losgistic Regression(     600/10000): loss= 3910.48813453314\n",
      "Losgistic Regression(     700/10000): loss= 3873.32274673794\n",
      "Losgistic Regression(     800/10000): loss= 3843.1270691045\n",
      "Losgistic Regression(     900/10000): loss= 3818.18013609114\n",
      "Losgistic Regression(    1000/10000): loss= 3796.78591672888\n",
      "Losgistic Regression(    1100/10000): loss= 3778.20543966407\n",
      "Losgistic Regression(    1200/10000): loss= 3761.80020038578\n",
      "Losgistic Regression(    1300/10000): loss= 3747.47628341895\n",
      "Losgistic Regression(    1400/10000): loss= 3735.25312186498\n",
      "Losgistic Regression(    1500/10000): loss= 3725.60243230438\n",
      "Losgistic Regression(    1600/10000): loss= 3716.95884052755\n",
      "Losgistic Regression(    1700/10000): loss= 3708.46699230361\n",
      "Losgistic Regression(    1800/10000): loss= 3702.11059780219\n",
      "Losgistic Regression(    1900/10000): loss= 3695.84656716069\n",
      "Losgistic Regression(    2000/10000): loss= 3691.49624330236\n",
      "Losgistic Regression(    2100/10000): loss= 3686.57258912659\n",
      "Losgistic Regression(    2200/10000): loss= 3681.84462218514\n",
      "Losgistic Regression(    2300/10000): loss= 3676.96366849071\n",
      "Losgistic Regression(    2400/10000): loss= 3672.32474608363\n",
      "Losgistic Regression(    2500/10000): loss= 3668.53925421463\n",
      "Losgistic Regression(    2600/10000): loss= 3664.80143213296\n",
      "Losgistic Regression(    2700/10000): loss= 3661.63987475747\n",
      "Losgistic Regression(    2800/10000): loss= 3658.66698685367\n",
      "Losgistic Regression(    2900/10000): loss= 3655.95207527631\n",
      "Losgistic Regression(    3000/10000): loss= 3653.46178996921\n",
      "Losgistic Regression(    3100/10000): loss= 3651.34892121807\n",
      "Losgistic Regression(    3200/10000): loss= 3649.24596392702\n",
      "Losgistic Regression(    3300/10000): loss= 3647.16348754646\n",
      "Losgistic Regression(    3400/10000): loss= 3645.17442845754\n",
      "Losgistic Regression(    3500/10000): loss= 3643.41248228478\n",
      "Losgistic Regression(    3600/10000): loss= 3641.71439628295\n",
      "Losgistic Regression(    3700/10000): loss= 3640.09686353183\n",
      "Losgistic Regression(    3800/10000): loss= 3638.42533242078\n",
      "Losgistic Regression(    3900/10000): loss= 3636.91743333129\n",
      "Losgistic Regression(    4000/10000): loss= 3635.40151881901\n",
      "Losgistic Regression(    4100/10000): loss= 3634.0495684487\n",
      "Losgistic Regression(    4200/10000): loss= 3632.81304579602\n",
      "Losgistic Regression(    4300/10000): loss= 3631.56889876559\n",
      "Losgistic Regression(    4400/10000): loss= 3630.36754114809\n",
      "Losgistic Regression(    4500/10000): loss= 3629.29809275446\n",
      "Losgistic Regression(    4600/10000): loss= 3628.34016733721\n",
      "Losgistic Regression(    4700/10000): loss= 3627.56582936312\n",
      "Losgistic Regression(    4800/10000): loss= 3626.89140975789\n",
      "Losgistic Regression(    4900/10000): loss= 3626.39147944546\n",
      "Losgistic Regression(    5000/10000): loss= 3625.8719506205\n",
      "Losgistic Regression(    5100/10000): loss= 3625.33207923379\n",
      "Losgistic Regression(    5200/10000): loss= 3624.77860876813\n",
      "Losgistic Regression(    5300/10000): loss= 3624.23328574074\n",
      "Losgistic Regression(    5400/10000): loss= 3623.74638209885\n",
      "Losgistic Regression(    5500/10000): loss= 3623.34523892728\n",
      "Losgistic Regression(    5600/10000): loss= 3622.96564881513\n",
      "Losgistic Regression(    5700/10000): loss= 3622.635778493\n",
      "Losgistic Regression(    5800/10000): loss= 3622.38910082147\n",
      "Losgistic Regression(    5900/10000): loss= 3622.18734298869\n",
      "Losgistic Regression(    6000/10000): loss= 3622.03092226599\n",
      "Losgistic Regression(    6100/10000): loss= 3621.94302499184\n",
      "Losgistic Regression(    6200/10000): loss= 3621.83702002586\n",
      "Losgistic Regression(    6300/10000): loss= 3621.7406397971\n",
      "Losgistic Regression(    6400/10000): loss= 3621.65624896022\n",
      "Losgistic Regression(    6500/10000): loss= 3621.57368095895\n",
      "Losgistic Regression(    6600/10000): loss= 3621.53952198526\n",
      "Totoal number of iterations =  6600\n",
      "Loss                        =  3621.53952199\n",
      "Time for  0th cross validation = 207.727s\n",
      "Training Accuracy         = 0.8405\n",
      "Cross Validation Accuracy = 0.813684\n",
      "Losgistic Regression(       0/10000): loss= 6827.55841056485\n",
      "Losgistic Regression(     100/10000): loss= 4486.79934276993\n",
      "Losgistic Regression(     200/10000): loss= 4211.53404631562\n",
      "Losgistic Regression(     300/10000): loss= 4067.26244127669\n",
      "Losgistic Regression(     400/10000): loss= 3970.00447028755\n",
      "Losgistic Regression(     500/10000): loss= 3899.93643162957\n",
      "Losgistic Regression(     600/10000): loss= 3848.89194903242\n",
      "Losgistic Regression(     700/10000): loss= 3810.18365775755\n",
      "Losgistic Regression(     800/10000): loss= 3779.53474821211\n",
      "Losgistic Regression(     900/10000): loss= 3754.95448578011\n",
      "Losgistic Regression(    1000/10000): loss= 3735.5033093954\n",
      "Losgistic Regression(    1100/10000): loss= 3720.09666972197\n",
      "Losgistic Regression(    1200/10000): loss= 3707.59875401394\n",
      "Losgistic Regression(    1300/10000): loss= 3697.0770546328\n",
      "Losgistic Regression(    1400/10000): loss= 3687.90635999333\n",
      "Losgistic Regression(    1500/10000): loss= 3680.11725184779\n",
      "Losgistic Regression(    1600/10000): loss= 3673.09816442947\n",
      "Losgistic Regression(    1700/10000): loss= 3666.60894680289\n",
      "Losgistic Regression(    1800/10000): loss= 3660.59063388618\n",
      "Losgistic Regression(    1900/10000): loss= 3655.00897649625\n",
      "Losgistic Regression(    2000/10000): loss= 3649.88367898672\n",
      "Losgistic Regression(    2100/10000): loss= 3645.12594152167\n",
      "Losgistic Regression(    2200/10000): loss= 3640.75058685289\n",
      "Losgistic Regression(    2300/10000): loss= 3636.73127386408\n",
      "Losgistic Regression(    2400/10000): loss= 3633.01751106433\n",
      "Losgistic Regression(    2500/10000): loss= 3629.58123760419\n",
      "Losgistic Regression(    2600/10000): loss= 3626.00735133652\n",
      "Losgistic Regression(    2700/10000): loss= 3622.7061853556\n",
      "Losgistic Regression(    2800/10000): loss= 3619.61003462154\n",
      "Losgistic Regression(    2900/10000): loss= 3616.63430358069\n",
      "Losgistic Regression(    3000/10000): loss= 3613.74576116612\n",
      "Losgistic Regression(    3100/10000): loss= 3610.97933607324\n",
      "Losgistic Regression(    3200/10000): loss= 3607.68247958944\n",
      "Losgistic Regression(    3300/10000): loss= 3602.95375111659\n",
      "Losgistic Regression(    3400/10000): loss= 3598.15566474635\n",
      "Losgistic Regression(    3500/10000): loss= 3593.38450858773\n",
      "Losgistic Regression(    3600/10000): loss= 3588.74867595204\n",
      "Losgistic Regression(    3700/10000): loss= 3584.2453926548\n",
      "Losgistic Regression(    3800/10000): loss= 3579.8963750055\n",
      "Losgistic Regression(    3900/10000): loss= 3575.7929056014\n",
      "Losgistic Regression(    4000/10000): loss= 3571.93926486994\n",
      "Losgistic Regression(    4100/10000): loss= 3568.24373175751\n",
      "Losgistic Regression(    4200/10000): loss= 3564.64260638992\n",
      "Losgistic Regression(    4300/10000): loss= 3561.08478122176\n",
      "Losgistic Regression(    4400/10000): loss= 3557.6470348205\n",
      "Losgistic Regression(    4500/10000): loss= 3554.34378156298\n",
      "Losgistic Regression(    4600/10000): loss= 3551.22391901735\n",
      "Losgistic Regression(    4700/10000): loss= 3548.27955791508\n",
      "Losgistic Regression(    4800/10000): loss= 3545.41791556375\n",
      "Losgistic Regression(    4900/10000): loss= 3542.65502338705\n",
      "Losgistic Regression(    5000/10000): loss= 3539.98372524614\n",
      "Losgistic Regression(    5100/10000): loss= 3537.41568845685\n",
      "Losgistic Regression(    5200/10000): loss= 3534.98558236292\n",
      "Losgistic Regression(    5300/10000): loss= 3532.71353218993\n",
      "Losgistic Regression(    5400/10000): loss= 3530.61777880853\n",
      "Losgistic Regression(    5500/10000): loss= 3528.61995434254\n",
      "Losgistic Regression(    5600/10000): loss= 3526.70704641848\n",
      "Losgistic Regression(    5700/10000): loss= 3525.00931270624\n",
      "Losgistic Regression(    5800/10000): loss= 3523.51140921885\n",
      "Losgistic Regression(    5900/10000): loss= 3522.22113580624\n",
      "Losgistic Regression(    6000/10000): loss= 3521.10853650939\n",
      "Losgistic Regression(    6100/10000): loss= 3520.2001390732\n",
      "Losgistic Regression(    6200/10000): loss= 3519.4680822387\n",
      "Losgistic Regression(    6300/10000): loss= 3518.44332174321\n",
      "Losgistic Regression(    6400/10000): loss= 3517.93386118074\n",
      "Losgistic Regression(    6500/10000): loss= 3517.93431806882\n",
      "Totoal number of iterations =  6500\n",
      "Loss                        =  3517.93431807\n",
      "Time for  1th cross validation = 205.318s\n",
      "Training Accuracy         = 0.8417\n",
      "Cross Validation Accuracy = 0.8159\n",
      "Losgistic Regression(       0/10000): loss= 6825.29433723592\n",
      "Losgistic Regression(     100/10000): loss= 4453.05105807843\n",
      "Losgistic Regression(     200/10000): loss= 4194.11104561502\n",
      "Losgistic Regression(     300/10000): loss= 4055.27529336639\n",
      "Losgistic Regression(     400/10000): loss= 3959.93422566799\n",
      "Losgistic Regression(     500/10000): loss= 3889.96439156754\n",
      "Losgistic Regression(     600/10000): loss= 3834.05858246469\n",
      "Losgistic Regression(     700/10000): loss= 3789.05074464555\n",
      "Losgistic Regression(     800/10000): loss= 3753.37531677857\n",
      "Losgistic Regression(     900/10000): loss= 3723.68531919097\n",
      "Losgistic Regression(    1000/10000): loss= 3703.10371301037\n",
      "Losgistic Regression(    1100/10000): loss= 3687.64743466905\n",
      "Losgistic Regression(    1200/10000): loss= 3676.07027402628\n",
      "Losgistic Regression(    1300/10000): loss= 3667.83623220377\n",
      "Losgistic Regression(    1400/10000): loss= 3661.22378527716\n",
      "Losgistic Regression(    1500/10000): loss= 3656.60140943531\n",
      "Losgistic Regression(    1600/10000): loss= 3653.24259794237\n",
      "Losgistic Regression(    1700/10000): loss= 3651.11055640743\n",
      "Losgistic Regression(    1800/10000): loss= 3648.5353721041\n",
      "Losgistic Regression(    1900/10000): loss= 3646.28892273601\n",
      "Losgistic Regression(    2000/10000): loss= 3643.72285085812\n",
      "Losgistic Regression(    2100/10000): loss= 3641.65783446504\n",
      "Losgistic Regression(    2200/10000): loss= 3636.25689682407\n",
      "Losgistic Regression(    2300/10000): loss= 3630.72448876221\n",
      "Losgistic Regression(    2400/10000): loss= 3625.44726571805\n",
      "Losgistic Regression(    2500/10000): loss= 3620.27167376393\n",
      "Losgistic Regression(    2600/10000): loss= 3615.34860388951\n",
      "Losgistic Regression(    2700/10000): loss= 3610.57474544961\n",
      "Losgistic Regression(    2800/10000): loss= 3606.1777436806\n",
      "Losgistic Regression(    2900/10000): loss= 3602.07186668596\n",
      "Losgistic Regression(    3000/10000): loss= 3598.3121816075\n",
      "Losgistic Regression(    3100/10000): loss= 3594.7098122159\n",
      "Losgistic Regression(    3200/10000): loss= 3591.417159207\n",
      "Losgistic Regression(    3300/10000): loss= 3588.18672899738\n",
      "Losgistic Regression(    3400/10000): loss= 3585.18199419997\n",
      "Losgistic Regression(    3500/10000): loss= 3582.52982571415\n",
      "Losgistic Regression(    3600/10000): loss= 3580.16695919752\n",
      "Losgistic Regression(    3700/10000): loss= 3577.68010968439\n",
      "Losgistic Regression(    3800/10000): loss= 3575.22530581421\n",
      "Losgistic Regression(    3900/10000): loss= 3572.92493144482\n",
      "Losgistic Regression(    4000/10000): loss= 3570.93792876208\n",
      "Losgistic Regression(    4100/10000): loss= 3569.16381297399\n",
      "Losgistic Regression(    4200/10000): loss= 3567.04604290247\n",
      "Losgistic Regression(    4300/10000): loss= 3563.91558418164\n",
      "Losgistic Regression(    4400/10000): loss= 3561.07794487578\n",
      "Losgistic Regression(    4500/10000): loss= 3558.40845263389\n",
      "Losgistic Regression(    4600/10000): loss= 3556.01277926068\n",
      "Losgistic Regression(    4700/10000): loss= 3553.82050658679\n",
      "Losgistic Regression(    4800/10000): loss= 3551.80979064927\n",
      "Losgistic Regression(    4900/10000): loss= 3549.96760305575\n",
      "Losgistic Regression(    5000/10000): loss= 3548.19705992147\n",
      "Losgistic Regression(    5100/10000): loss= 3546.46636389727\n",
      "Losgistic Regression(    5200/10000): loss= 3544.8421851928\n",
      "Losgistic Regression(    5300/10000): loss= 3543.30785951412\n",
      "Losgistic Regression(    5400/10000): loss= 3541.84409486957\n",
      "Losgistic Regression(    5500/10000): loss= 3540.45786913891\n",
      "Losgistic Regression(    5600/10000): loss= 3539.13658503739\n",
      "Losgistic Regression(    5700/10000): loss= 3537.92026047549\n",
      "Losgistic Regression(    5800/10000): loss= 3536.82204769838\n",
      "Losgistic Regression(    5900/10000): loss= 3535.75767774555\n",
      "Losgistic Regression(    6000/10000): loss= 3534.62756380828\n",
      "Losgistic Regression(    6100/10000): loss= 3533.43845248306\n",
      "Losgistic Regression(    6200/10000): loss= 3532.27943639934\n",
      "Losgistic Regression(    6300/10000): loss= 3531.17248468666\n",
      "Losgistic Regression(    6400/10000): loss= 3530.08291227213\n",
      "Losgistic Regression(    6500/10000): loss= 3529.01752516448\n",
      "Losgistic Regression(    6600/10000): loss= 3528.03098495074\n",
      "Losgistic Regression(    6700/10000): loss= 3527.1648570088\n",
      "Losgistic Regression(    6800/10000): loss= 3526.4071125748\n",
      "Losgistic Regression(    6900/10000): loss= 3525.72887454435\n",
      "Losgistic Regression(    7000/10000): loss= 3525.10520016655\n",
      "Losgistic Regression(    7100/10000): loss= 3524.4676552736\n",
      "Losgistic Regression(    7200/10000): loss= 3523.8137860684\n",
      "Losgistic Regression(    7300/10000): loss= 3523.16496251243\n",
      "Losgistic Regression(    7400/10000): loss= 3522.52021336607\n",
      "Losgistic Regression(    7500/10000): loss= 3521.89334251771\n",
      "Losgistic Regression(    7600/10000): loss= 3521.28692238574\n",
      "Losgistic Regression(    7700/10000): loss= 3520.70588734179\n",
      "Losgistic Regression(    7800/10000): loss= 3520.14935462887\n",
      "Losgistic Regression(    7900/10000): loss= 3519.57614850131\n",
      "Losgistic Regression(    8000/10000): loss= 3519.01356975164\n",
      "Losgistic Regression(    8100/10000): loss= 3518.53543280592\n",
      "Losgistic Regression(    8200/10000): loss= 3518.15951577026\n",
      "Losgistic Regression(    8300/10000): loss= 3517.8784583826\n",
      "Losgistic Regression(    8400/10000): loss= 3517.6698144666\n",
      "Losgistic Regression(    8500/10000): loss= 3517.48611795173\n",
      "Losgistic Regression(    8600/10000): loss= 3517.27812124225\n",
      "Losgistic Regression(    8700/10000): loss= 3517.02169046692\n",
      "Losgistic Regression(    8800/10000): loss= 3516.75749296208\n",
      "Losgistic Regression(    8900/10000): loss= 3516.52416623547\n",
      "Losgistic Regression(    9000/10000): loss= 3516.33260301923\n",
      "Losgistic Regression(    9100/10000): loss= 3516.19109644156\n",
      "Losgistic Regression(    9200/10000): loss= 3516.10853339583\n",
      "Losgistic Regression(    9300/10000): loss= 3516.0592710376\n",
      "Losgistic Regression(    9400/10000): loss= 3516.04505954321\n",
      "Totoal number of iterations =  9400\n",
      "Loss                        =  3516.04505954\n",
      "Time for  2th cross validation = 296.374s\n",
      "Training Accuracy         = 0.8469\n",
      "Cross Validation Accuracy = 0.814356\n",
      "Losgistic Regression(       0/10000): loss= 6822.87879470309\n",
      "Losgistic Regression(     100/10000): loss= 4536.21573776033\n",
      "Losgistic Regression(     200/10000): loss= 4276.75800159108\n",
      "Losgistic Regression(     300/10000): loss= 4135.41202988982\n",
      "Losgistic Regression(     400/10000): loss= 4046.56034254423\n",
      "Losgistic Regression(     500/10000): loss= 3981.01823964973\n",
      "Losgistic Regression(     600/10000): loss= 3926.77493141299\n",
      "Losgistic Regression(     700/10000): loss= 3881.85997274755\n",
      "Losgistic Regression(     800/10000): loss= 3851.64006375499\n",
      "Losgistic Regression(     900/10000): loss= 3826.83263988254\n",
      "Losgistic Regression(    1000/10000): loss= 3809.05870704307\n",
      "Losgistic Regression(    1100/10000): loss= 3794.07919404159\n",
      "Losgistic Regression(    1200/10000): loss= 3782.29078068065\n",
      "Losgistic Regression(    1300/10000): loss= 3770.42093102867\n",
      "Losgistic Regression(    1400/10000): loss= 3758.23859677475\n",
      "Losgistic Regression(    1500/10000): loss= 3746.88354065016\n",
      "Losgistic Regression(    1600/10000): loss= 3737.04122057362\n",
      "Losgistic Regression(    1700/10000): loss= 3727.74605482993\n",
      "Losgistic Regression(    1800/10000): loss= 3718.81079083259\n",
      "Losgistic Regression(    1900/10000): loss= 3710.70648367946\n",
      "Losgistic Regression(    2000/10000): loss= 3703.24693250154\n",
      "Losgistic Regression(    2100/10000): loss= 3695.98660681175\n",
      "Losgistic Regression(    2200/10000): loss= 3688.6347245841\n",
      "Losgistic Regression(    2300/10000): loss= 3681.27210234362\n",
      "Losgistic Regression(    2400/10000): loss= 3674.18288332862\n",
      "Losgistic Regression(    2500/10000): loss= 3667.45794202597\n",
      "Losgistic Regression(    2600/10000): loss= 3660.92081038034\n",
      "Losgistic Regression(    2700/10000): loss= 3654.39629396646\n",
      "Losgistic Regression(    2800/10000): loss= 3647.95353590946\n",
      "Losgistic Regression(    2900/10000): loss= 3641.76524909056\n",
      "Losgistic Regression(    3000/10000): loss= 3635.9103945746\n",
      "Losgistic Regression(    3100/10000): loss= 3630.44933087365\n",
      "Losgistic Regression(    3200/10000): loss= 3625.34044565259\n",
      "Losgistic Regression(    3300/10000): loss= 3620.59470723546\n",
      "Losgistic Regression(    3400/10000): loss= 3616.19490781888\n",
      "Losgistic Regression(    3500/10000): loss= 3612.00743637796\n",
      "Losgistic Regression(    3600/10000): loss= 3608.03401814932\n",
      "Losgistic Regression(    3700/10000): loss= 3604.37967520112\n",
      "Losgistic Regression(    3800/10000): loss= 3601.0446698444\n",
      "Losgistic Regression(    3900/10000): loss= 3597.98588540031\n",
      "Losgistic Regression(    4000/10000): loss= 3595.08327277298\n",
      "Losgistic Regression(    4100/10000): loss= 3592.25574305927\n",
      "Losgistic Regression(    4200/10000): loss= 3589.44404117059\n",
      "Losgistic Regression(    4300/10000): loss= 3586.68130104467\n",
      "Losgistic Regression(    4400/10000): loss= 3584.01301086592\n",
      "Losgistic Regression(    4500/10000): loss= 3581.52038342633\n",
      "Losgistic Regression(    4600/10000): loss= 3579.19484563616\n",
      "Losgistic Regression(    4700/10000): loss= 3576.92410158351\n",
      "Losgistic Regression(    4800/10000): loss= 3574.649515238\n",
      "Losgistic Regression(    4900/10000): loss= 3572.43139550568\n",
      "Losgistic Regression(    5000/10000): loss= 3570.31654477343\n",
      "Losgistic Regression(    5100/10000): loss= 3568.3213607293\n",
      "Losgistic Regression(    5200/10000): loss= 3566.42284653435\n",
      "Losgistic Regression(    5300/10000): loss= 3564.58917352956\n",
      "Losgistic Regression(    5400/10000): loss= 3562.83742902949\n",
      "Losgistic Regression(    5500/10000): loss= 3561.17693798975\n",
      "Losgistic Regression(    5600/10000): loss= 3559.61522074092\n",
      "Losgistic Regression(    5700/10000): loss= 3558.1607042199\n",
      "Losgistic Regression(    5800/10000): loss= 3556.81522809381\n",
      "Losgistic Regression(    5900/10000): loss= 3555.57609138705\n",
      "Losgistic Regression(    6000/10000): loss= 3554.42470833689\n",
      "Losgistic Regression(    6100/10000): loss= 3553.33394677866\n",
      "Losgistic Regression(    6200/10000): loss= 3552.27520180151\n",
      "Losgistic Regression(    6300/10000): loss= 3551.24239968148\n",
      "Losgistic Regression(    6400/10000): loss= 3550.24105226524\n",
      "Losgistic Regression(    6500/10000): loss= 3549.26532042406\n",
      "Losgistic Regression(    6600/10000): loss= 3548.32015019003\n",
      "Losgistic Regression(    6700/10000): loss= 3547.39033907193\n",
      "Losgistic Regression(    6800/10000): loss= 3546.48002088744\n",
      "Losgistic Regression(    6900/10000): loss= 3545.59914040371\n",
      "Losgistic Regression(    7000/10000): loss= 3544.68620960462\n",
      "Losgistic Regression(    7100/10000): loss= 3543.71209067003\n",
      "Losgistic Regression(    7200/10000): loss= 3542.72291475405\n",
      "Losgistic Regression(    7300/10000): loss= 3541.7106747631\n",
      "Losgistic Regression(    7400/10000): loss= 3540.69756847751\n",
      "Losgistic Regression(    7500/10000): loss= 3539.71051059809\n",
      "Losgistic Regression(    7600/10000): loss= 3538.75705280957\n",
      "Losgistic Regression(    7700/10000): loss= 3537.81736620786\n",
      "Losgistic Regression(    7800/10000): loss= 3536.87678371564\n",
      "Losgistic Regression(    7900/10000): loss= 3535.95573579321\n",
      "Losgistic Regression(    8000/10000): loss= 3535.05765202224\n",
      "Losgistic Regression(    8100/10000): loss= 3534.13032809717\n",
      "Losgistic Regression(    8200/10000): loss= 3533.18849180878\n",
      "Losgistic Regression(    8300/10000): loss= 3532.2661139289\n",
      "Losgistic Regression(    8400/10000): loss= 3531.39161004654\n",
      "Losgistic Regression(    8500/10000): loss= 3530.569255798\n",
      "Losgistic Regression(    8600/10000): loss= 3529.80708373888\n",
      "Losgistic Regression(    8700/10000): loss= 3529.09921420191\n",
      "Losgistic Regression(    8800/10000): loss= 3528.43779625312\n",
      "Losgistic Regression(    8900/10000): loss= 3527.82895604759\n",
      "Losgistic Regression(    9000/10000): loss= 3527.28278497702\n",
      "Losgistic Regression(    9100/10000): loss= 3526.7831092694\n",
      "Losgistic Regression(    9200/10000): loss= 3526.31212622863\n",
      "Losgistic Regression(    9300/10000): loss= 3525.87293760819\n",
      "Losgistic Regression(    9400/10000): loss= 3525.46582486917\n",
      "Losgistic Regression(    9500/10000): loss= 3525.08950492385\n",
      "Losgistic Regression(    9600/10000): loss= 3524.7411012861\n",
      "Losgistic Regression(    9700/10000): loss= 3524.42716980013\n",
      "Losgistic Regression(    9800/10000): loss= 3524.13222041681\n",
      "Losgistic Regression(    9900/10000): loss= 3523.85744949444\n",
      "Time for  3th cross validation = 314.902s\n",
      "Training Accuracy         = 0.8451\n",
      "Cross Validation Accuracy = 0.814524\n",
      "Losgistic Regression(       0/10000): loss= 6820.81851328814\n",
      "Losgistic Regression(     100/10000): loss= 4549.72419558616\n",
      "Losgistic Regression(     200/10000): loss= 4305.22159019334\n",
      "Losgistic Regression(     300/10000): loss= 4188.62662424448\n",
      "Losgistic Regression(     400/10000): loss= 4115.8102899981\n",
      "Losgistic Regression(     500/10000): loss= 4062.21856468941\n",
      "Losgistic Regression(     600/10000): loss= 4016.05912125216\n",
      "Losgistic Regression(     700/10000): loss= 3977.07419966934\n",
      "Losgistic Regression(     800/10000): loss= 3943.44074460786\n",
      "Losgistic Regression(     900/10000): loss= 3915.06015278496\n",
      "Losgistic Regression(    1000/10000): loss= 3892.11899832645\n",
      "Losgistic Regression(    1100/10000): loss= 3871.92493247575\n",
      "Losgistic Regression(    1200/10000): loss= 3854.38327507597\n",
      "Losgistic Regression(    1300/10000): loss= 3839.65634917048\n",
      "Losgistic Regression(    1400/10000): loss= 3828.6465951455\n",
      "Losgistic Regression(    1500/10000): loss= 3818.40607529825\n",
      "Losgistic Regression(    1600/10000): loss= 3809.82459336349\n",
      "Losgistic Regression(    1700/10000): loss= 3800.08100900925\n",
      "Losgistic Regression(    1800/10000): loss= 3791.14380437002\n",
      "Losgistic Regression(    1900/10000): loss= 3783.22601343862\n",
      "Losgistic Regression(    2000/10000): loss= 3776.53335321701\n",
      "Losgistic Regression(    2100/10000): loss= 3770.37372158257\n",
      "Losgistic Regression(    2200/10000): loss= 3764.08025700599\n",
      "Losgistic Regression(    2300/10000): loss= 3758.37714184235\n",
      "Losgistic Regression(    2400/10000): loss= 3752.49666197449\n",
      "Losgistic Regression(    2500/10000): loss= 3745.44752266746\n",
      "Losgistic Regression(    2600/10000): loss= 3738.19475949608\n",
      "Losgistic Regression(    2700/10000): loss= 3731.09171682208\n",
      "Losgistic Regression(    2800/10000): loss= 3724.55890036097\n",
      "Losgistic Regression(    2900/10000): loss= 3717.83859909727\n",
      "Losgistic Regression(    3000/10000): loss= 3711.07977256468\n",
      "Losgistic Regression(    3100/10000): loss= 3704.17381532208\n",
      "Losgistic Regression(    3200/10000): loss= 3696.53884945684\n",
      "Losgistic Regression(    3300/10000): loss= 3688.74458395711\n",
      "Losgistic Regression(    3400/10000): loss= 3680.8520680725\n",
      "Losgistic Regression(    3500/10000): loss= 3673.65865530997\n",
      "Losgistic Regression(    3600/10000): loss= 3666.56874157313\n",
      "Losgistic Regression(    3700/10000): loss= 3660.17837818988\n",
      "Losgistic Regression(    3800/10000): loss= 3654.34285280248\n",
      "Losgistic Regression(    3900/10000): loss= 3649.31661452835\n",
      "Losgistic Regression(    4000/10000): loss= 3644.78904381323\n",
      "Losgistic Regression(    4100/10000): loss= 3640.61148543595\n",
      "Losgistic Regression(    4200/10000): loss= 3636.68844352054\n",
      "Losgistic Regression(    4300/10000): loss= 3632.9260001811\n",
      "Losgistic Regression(    4400/10000): loss= 3629.26000487741\n",
      "Losgistic Regression(    4500/10000): loss= 3625.67350406699\n",
      "Losgistic Regression(    4600/10000): loss= 3622.27166356721\n",
      "Losgistic Regression(    4700/10000): loss= 3618.87769108789\n",
      "Losgistic Regression(    4800/10000): loss= 3615.602728907\n",
      "Losgistic Regression(    4900/10000): loss= 3612.48419054154\n",
      "Losgistic Regression(    5000/10000): loss= 3609.41999356521\n",
      "Losgistic Regression(    5100/10000): loss= 3606.1788120342\n",
      "Losgistic Regression(    5200/10000): loss= 3603.07041150912\n",
      "Losgistic Regression(    5300/10000): loss= 3599.91994166588\n",
      "Losgistic Regression(    5400/10000): loss= 3596.90909642467\n",
      "Losgistic Regression(    5500/10000): loss= 3594.05604364013\n",
      "Losgistic Regression(    5600/10000): loss= 3591.38427521852\n",
      "Losgistic Regression(    5700/10000): loss= 3588.8495151123\n",
      "Losgistic Regression(    5800/10000): loss= 3586.3619770439\n",
      "Losgistic Regression(    5900/10000): loss= 3583.93989342366\n",
      "Losgistic Regression(    6000/10000): loss= 3581.86945831954\n",
      "Losgistic Regression(    6100/10000): loss= 3580.06381599105\n",
      "Losgistic Regression(    6200/10000): loss= 3578.48391965109\n",
      "Losgistic Regression(    6300/10000): loss= 3577.12163457026\n",
      "Losgistic Regression(    6400/10000): loss= 3575.98073273547\n",
      "Losgistic Regression(    6500/10000): loss= 3574.91386583542\n",
      "Losgistic Regression(    6600/10000): loss= 3574.03918886232\n",
      "Losgistic Regression(    6700/10000): loss= 3573.36479618753\n",
      "Losgistic Regression(    6800/10000): loss= 3572.72635358321\n",
      "Losgistic Regression(    6900/10000): loss= 3572.17269912993\n",
      "Losgistic Regression(    7000/10000): loss= 3571.77044663277\n",
      "Losgistic Regression(    7100/10000): loss= 3571.43925763086\n",
      "Losgistic Regression(    7200/10000): loss= 3571.15687129347\n",
      "Losgistic Regression(    7300/10000): loss= 3570.90197255033\n",
      "Losgistic Regression(    7400/10000): loss= 3570.71239194332\n",
      "Losgistic Regression(    7500/10000): loss= 3570.53087605796\n",
      "Losgistic Regression(    7600/10000): loss= 3570.19599835232\n",
      "Losgistic Regression(    7700/10000): loss= 3569.83137169983\n",
      "Losgistic Regression(    7800/10000): loss= 3569.41948588794\n",
      "Losgistic Regression(    7900/10000): loss= 3568.9961863199\n",
      "Losgistic Regression(    8000/10000): loss= 3568.59390190856\n",
      "Losgistic Regression(    8100/10000): loss= 3568.13872759246\n",
      "Losgistic Regression(    8200/10000): loss= 3567.62932269274\n",
      "Losgistic Regression(    8300/10000): loss= 3567.00631707579\n",
      "Losgistic Regression(    8400/10000): loss= 3566.36689889153\n",
      "Losgistic Regression(    8500/10000): loss= 3565.61030829619\n",
      "Losgistic Regression(    8600/10000): loss= 3564.69333503553\n",
      "Losgistic Regression(    8700/10000): loss= 3563.76429902412\n",
      "Losgistic Regression(    8800/10000): loss= 3562.82099361418\n",
      "Losgistic Regression(    8900/10000): loss= 3561.86463236118\n",
      "Losgistic Regression(    9000/10000): loss= 3560.91330522943\n",
      "Losgistic Regression(    9100/10000): loss= 3559.94464867408\n",
      "Losgistic Regression(    9200/10000): loss= 3558.9414918975\n",
      "Losgistic Regression(    9300/10000): loss= 3557.82933423656\n",
      "Losgistic Regression(    9400/10000): loss= 3556.62997431957\n",
      "Losgistic Regression(    9500/10000): loss= 3555.41835399786\n",
      "Losgistic Regression(    9600/10000): loss= 3554.13592987804\n",
      "Losgistic Regression(    9700/10000): loss= 3552.87231304727\n",
      "Losgistic Regression(    9800/10000): loss= 3551.62920278574\n",
      "Losgistic Regression(    9900/10000): loss= 3550.42858448201\n",
      "Time for  4th cross validation = 323.383s\n",
      "Training Accuracy         = 0.8355\n",
      "Cross Validation Accuracy = 0.816112\n",
      "*************** ([0.84050000000000002, 0.8417, 0.84689999999999999, 0.84509999999999996, 0.83550000000000002], [0.81368399999999996, 0.81589999999999996, 0.81435599999999997, 0.81452400000000003, 0.81611199999999995])\n",
      "Losgistic Regression(       0/10000): loss= 6821.12390701744\n",
      "Losgistic Regression(     100/10000): loss= 4539.40434818244\n",
      "Losgistic Regression(     200/10000): loss= 4261.39437880599\n",
      "Losgistic Regression(     300/10000): loss= 4122.9101373504\n",
      "Losgistic Regression(     400/10000): loss= 4033.47735794233\n",
      "Losgistic Regression(     500/10000): loss= 3968.50483336188\n",
      "Losgistic Regression(     600/10000): loss= 3920.62473052878\n",
      "Losgistic Regression(     700/10000): loss= 3884.61839020908\n",
      "Losgistic Regression(     800/10000): loss= 3855.56201623562\n",
      "Losgistic Regression(     900/10000): loss= 3831.78222566842\n",
      "Losgistic Regression(    1000/10000): loss= 3811.54811075492\n",
      "Losgistic Regression(    1100/10000): loss= 3794.01403644776\n",
      "Losgistic Regression(    1200/10000): loss= 3778.5196843037\n",
      "Losgistic Regression(    1300/10000): loss= 3765.07820916068\n",
      "Losgistic Regression(    1400/10000): loss= 3753.66957267993\n",
      "Losgistic Regression(    1500/10000): loss= 3744.85614696765\n",
      "Losgistic Regression(    1600/10000): loss= 3737.13254725842\n",
      "Losgistic Regression(    1700/10000): loss= 3729.32163738255\n",
      "Losgistic Regression(    1800/10000): loss= 3723.51476287897\n",
      "Losgistic Regression(    1900/10000): loss= 3717.73425750115\n",
      "Losgistic Regression(    2000/10000): loss= 3713.75815775948\n",
      "Losgistic Regression(    2100/10000): loss= 3709.32376280191\n",
      "Losgistic Regression(    2200/10000): loss= 3705.05061218333\n",
      "Losgistic Regression(    2300/10000): loss= 3700.69641003877\n",
      "Losgistic Regression(    2400/10000): loss= 3696.48691059737\n",
      "Losgistic Regression(    2500/10000): loss= 3693.22086762785\n",
      "Losgistic Regression(    2600/10000): loss= 3689.97106393283\n",
      "Losgistic Regression(    2700/10000): loss= 3687.24844103026\n",
      "Losgistic Regression(    2800/10000): loss= 3684.75900944712\n",
      "Losgistic Regression(    2900/10000): loss= 3682.47515976195\n",
      "Losgistic Regression(    3000/10000): loss= 3680.51128740333\n",
      "Losgistic Regression(    3100/10000): loss= 3678.94846053441\n",
      "Losgistic Regression(    3200/10000): loss= 3677.3652156598\n",
      "Losgistic Regression(    3300/10000): loss= 3675.74426670422\n",
      "Losgistic Regression(    3400/10000): loss= 3674.10572173903\n",
      "Losgistic Regression(    3500/10000): loss= 3672.75657906036\n",
      "Losgistic Regression(    3600/10000): loss= 3671.49844123958\n",
      "Losgistic Regression(    3700/10000): loss= 3670.42746999667\n",
      "Losgistic Regression(    3800/10000): loss= 3669.28878097342\n",
      "Losgistic Regression(    3900/10000): loss= 3668.2954384576\n",
      "Losgistic Regression(    4000/10000): loss= 3667.16128104936\n",
      "Losgistic Regression(    4100/10000): loss= 3666.1750815125\n",
      "Losgistic Regression(    4200/10000): loss= 3665.25772557602\n",
      "Losgistic Regression(    4300/10000): loss= 3664.41345625772\n",
      "Losgistic Regression(    4400/10000): loss= 3663.63818477553\n",
      "Losgistic Regression(    4500/10000): loss= 3663.03929112314\n",
      "Losgistic Regression(    4600/10000): loss= 3662.60069479442\n",
      "Losgistic Regression(    4700/10000): loss= 3662.20644633691\n",
      "Losgistic Regression(    4800/10000): loss= 3661.79174069188\n",
      "Losgistic Regression(    4900/10000): loss= 3661.44546540057\n",
      "Losgistic Regression(    5000/10000): loss= 3661.11634743278\n",
      "Losgistic Regression(    5100/10000): loss= 3660.84872008028\n",
      "Losgistic Regression(    5200/10000): loss= 3660.624157939\n",
      "Losgistic Regression(    5300/10000): loss= 3660.29867431565\n",
      "Losgistic Regression(    5400/10000): loss= 3659.94680856064\n",
      "Losgistic Regression(    5500/10000): loss= 3659.60455197742\n",
      "Losgistic Regression(    5600/10000): loss= 3659.29418449892\n",
      "Losgistic Regression(    5700/10000): loss= 3659.00908682148\n",
      "Losgistic Regression(    5800/10000): loss= 3658.83164550446\n",
      "Losgistic Regression(    5900/10000): loss= 3658.74527427713\n",
      "Losgistic Regression(    6000/10000): loss= 3658.74512738899\n",
      "Totoal number of iterations =  6000\n",
      "Loss                        =  3658.74512739\n",
      "Time for  0th cross validation = 192.729s\n",
      "Training Accuracy         = 0.8395\n",
      "Cross Validation Accuracy = 0.813976\n",
      "Losgistic Regression(       0/10000): loss= 6827.57789539823\n",
      "Losgistic Regression(     100/10000): loss= 4489.03067516677\n",
      "Losgistic Regression(     200/10000): loss= 4215.56743716573\n",
      "Losgistic Regression(     300/10000): loss= 4072.96147311587\n",
      "Losgistic Regression(     400/10000): loss= 3977.25105360246\n",
      "Losgistic Regression(     500/10000): loss= 3908.7029109439\n",
      "Losgistic Regression(     600/10000): loss= 3858.97872835935\n",
      "Losgistic Regression(     700/10000): loss= 3821.4154885278\n",
      "Losgistic Regression(     800/10000): loss= 3791.86333021649\n",
      "Losgistic Regression(     900/10000): loss= 3768.40016039389\n",
      "Losgistic Regression(    1000/10000): loss= 3750.0554786232\n",
      "Losgistic Regression(    1100/10000): loss= 3735.65783335196\n",
      "Losgistic Regression(    1200/10000): loss= 3724.08945807058\n",
      "Losgistic Regression(    1300/10000): loss= 3714.42772000454\n",
      "Losgistic Regression(    1400/10000): loss= 3706.06242947876\n",
      "Losgistic Regression(    1500/10000): loss= 3698.97456467067\n",
      "Losgistic Regression(    1600/10000): loss= 3692.59872105043\n",
      "Losgistic Regression(    1700/10000): loss= 3686.72691794822\n",
      "Losgistic Regression(    1800/10000): loss= 3681.30394071065\n",
      "Losgistic Regression(    1900/10000): loss= 3676.3118604022\n",
      "Losgistic Regression(    2000/10000): loss= 3671.7842252384\n",
      "Losgistic Regression(    2100/10000): loss= 3667.59575285673\n",
      "Losgistic Regression(    2200/10000): loss= 3663.75487475357\n",
      "Losgistic Regression(    2300/10000): loss= 3660.25579805878\n",
      "Losgistic Regression(    2400/10000): loss= 3657.09236581591\n",
      "Losgistic Regression(    2500/10000): loss= 3654.17485389577\n",
      "Losgistic Regression(    2600/10000): loss= 3651.46668001697\n",
      "Losgistic Regression(    2700/10000): loss= 3648.94231258779\n",
      "Losgistic Regression(    2800/10000): loss= 3646.48420087201\n",
      "Losgistic Regression(    2900/10000): loss= 3644.0044591071\n",
      "Losgistic Regression(    3000/10000): loss= 3641.6766900497\n",
      "Losgistic Regression(    3100/10000): loss= 3639.44018650985\n",
      "Losgistic Regression(    3200/10000): loss= 3637.22561351303\n",
      "Losgistic Regression(    3300/10000): loss= 3633.3165601776\n",
      "Losgistic Regression(    3400/10000): loss= 3629.09462481208\n",
      "Losgistic Regression(    3500/10000): loss= 3624.91954490989\n",
      "Losgistic Regression(    3600/10000): loss= 3620.94364517648\n",
      "Losgistic Regression(    3700/10000): loss= 3617.10516780363\n",
      "Losgistic Regression(    3800/10000): loss= 3613.48218207101\n",
      "Losgistic Regression(    3900/10000): loss= 3610.02626975693\n",
      "Losgistic Regression(    4000/10000): loss= 3606.7698214593\n",
      "Losgistic Regression(    4100/10000): loss= 3603.59897460089\n",
      "Losgistic Regression(    4200/10000): loss= 3600.47389884927\n",
      "Losgistic Regression(    4300/10000): loss= 3597.32134848417\n",
      "Losgistic Regression(    4400/10000): loss= 3594.19147349222\n",
      "Losgistic Regression(    4500/10000): loss= 3591.13021982451\n",
      "Losgistic Regression(    4600/10000): loss= 3588.17633103829\n",
      "Losgistic Regression(    4700/10000): loss= 3585.33657922843\n",
      "Losgistic Regression(    4800/10000): loss= 3582.60636121233\n",
      "Losgistic Regression(    4900/10000): loss= 3580.03798885976\n",
      "Losgistic Regression(    5000/10000): loss= 3577.54395971521\n",
      "Losgistic Regression(    5100/10000): loss= 3575.12734908131\n",
      "Losgistic Regression(    5200/10000): loss= 3572.74192067228\n",
      "Losgistic Regression(    5300/10000): loss= 3570.47321722582\n",
      "Losgistic Regression(    5400/10000): loss= 3568.33235930755\n",
      "Losgistic Regression(    5500/10000): loss= 3566.36755345559\n",
      "Losgistic Regression(    5600/10000): loss= 3564.5842586748\n",
      "Losgistic Regression(    5700/10000): loss= 3563.08587731192\n",
      "Losgistic Regression(    5800/10000): loss= 3561.79766065684\n",
      "Losgistic Regression(    5900/10000): loss= 3560.6946498662\n",
      "Losgistic Regression(    6000/10000): loss= 3559.70910796327\n",
      "Losgistic Regression(    6100/10000): loss= 3558.78443796495\n",
      "Losgistic Regression(    6200/10000): loss= 3557.82991081564\n",
      "Losgistic Regression(    6300/10000): loss= 3556.78847377081\n",
      "Losgistic Regression(    6400/10000): loss= 3556.18765196814\n",
      "Losgistic Regression(    6500/10000): loss= 3555.74951458511\n",
      "Losgistic Regression(    6600/10000): loss= 3555.46339566589\n",
      "Losgistic Regression(    6700/10000): loss= 3555.33917118289\n",
      "Losgistic Regression(    6800/10000): loss= 3555.34426427375\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  3555.34426427\n",
      "Time for  1th cross validation = 216.197s\n",
      "Training Accuracy         = 0.8416\n",
      "Cross Validation Accuracy = 0.816628\n",
      "Losgistic Regression(       0/10000): loss= 6825.31471982272\n",
      "Losgistic Regression(     100/10000): loss= 4455.350954496\n",
      "Losgistic Regression(     200/10000): loss= 4198.18983185897\n",
      "Losgistic Regression(     300/10000): loss= 4060.969783813\n",
      "Losgistic Regression(     400/10000): loss= 3967.17453464469\n",
      "Losgistic Regression(     500/10000): loss= 3898.71624247546\n",
      "Losgistic Regression(     600/10000): loss= 3844.25395885569\n",
      "Losgistic Regression(     700/10000): loss= 3800.65152842151\n",
      "Losgistic Regression(     800/10000): loss= 3766.27190419612\n",
      "Losgistic Regression(     900/10000): loss= 3737.87337849323\n",
      "Losgistic Regression(    1000/10000): loss= 3718.36243369096\n",
      "Losgistic Regression(    1100/10000): loss= 3703.88348389155\n",
      "Losgistic Regression(    1200/10000): loss= 3693.17448923374\n",
      "Losgistic Regression(    1300/10000): loss= 3685.63195018787\n",
      "Losgistic Regression(    1400/10000): loss= 3680.28861986441\n",
      "Losgistic Regression(    1500/10000): loss= 3676.38899482963\n",
      "Losgistic Regression(    1600/10000): loss= 3673.79107968759\n",
      "Losgistic Regression(    1700/10000): loss= 3672.27564434049\n",
      "Losgistic Regression(    1800/10000): loss= 3670.09879593479\n",
      "Losgistic Regression(    1900/10000): loss= 3668.18516438016\n",
      "Losgistic Regression(    2000/10000): loss= 3666.35152243903\n",
      "Losgistic Regression(    2100/10000): loss= 3665.01245530389\n",
      "Losgistic Regression(    2200/10000): loss= 3662.71790564344\n",
      "Losgistic Regression(    2300/10000): loss= 3657.97939042699\n",
      "Losgistic Regression(    2400/10000): loss= 3653.70267536304\n",
      "Losgistic Regression(    2500/10000): loss= 3649.67788016759\n",
      "Losgistic Regression(    2600/10000): loss= 3645.72853307407\n",
      "Losgistic Regression(    2700/10000): loss= 3641.79753880098\n",
      "Losgistic Regression(    2800/10000): loss= 3638.17240316526\n",
      "Losgistic Regression(    2900/10000): loss= 3634.77311084913\n",
      "Losgistic Regression(    3000/10000): loss= 3631.52480229812\n",
      "Losgistic Regression(    3100/10000): loss= 3628.38077678281\n",
      "Losgistic Regression(    3200/10000): loss= 3625.53217375787\n",
      "Losgistic Regression(    3300/10000): loss= 3622.95700583485\n",
      "Losgistic Regression(    3400/10000): loss= 3620.57636427839\n",
      "Losgistic Regression(    3500/10000): loss= 3618.45582606295\n",
      "Losgistic Regression(    3600/10000): loss= 3616.54913255838\n",
      "Losgistic Regression(    3700/10000): loss= 3614.81056607598\n",
      "Losgistic Regression(    3800/10000): loss= 3613.00151027836\n",
      "Losgistic Regression(    3900/10000): loss= 3611.22997359583\n",
      "Losgistic Regression(    4000/10000): loss= 3609.49299613324\n",
      "Losgistic Regression(    4100/10000): loss= 3607.96643859131\n",
      "Losgistic Regression(    4200/10000): loss= 3606.56535952207\n",
      "Losgistic Regression(    4300/10000): loss= 3605.28818252537\n",
      "Losgistic Regression(    4400/10000): loss= 3602.69893026809\n",
      "Losgistic Regression(    4500/10000): loss= 3600.29012327237\n",
      "Losgistic Regression(    4600/10000): loss= 3598.08639706195\n",
      "Losgistic Regression(    4700/10000): loss= 3596.12237264552\n",
      "Losgistic Regression(    4800/10000): loss= 3594.47997726066\n",
      "Losgistic Regression(    4900/10000): loss= 3593.03265837266\n",
      "Losgistic Regression(    5000/10000): loss= 3591.78198236653\n",
      "Losgistic Regression(    5100/10000): loss= 3590.55478028253\n",
      "Losgistic Regression(    5200/10000): loss= 3589.37665242794\n",
      "Losgistic Regression(    5300/10000): loss= 3588.28688680559\n",
      "Losgistic Regression(    5400/10000): loss= 3587.23755904622\n",
      "Losgistic Regression(    5500/10000): loss= 3586.18207848653\n",
      "Losgistic Regression(    5600/10000): loss= 3585.21619772312\n",
      "Losgistic Regression(    5700/10000): loss= 3584.29577361111\n",
      "Losgistic Regression(    5800/10000): loss= 3583.46004095508\n",
      "Losgistic Regression(    5900/10000): loss= 3582.63394420276\n",
      "Losgistic Regression(    6000/10000): loss= 3581.80669182313\n",
      "Losgistic Regression(    6100/10000): loss= 3580.925468226\n",
      "Losgistic Regression(    6200/10000): loss= 3580.06732678284\n",
      "Losgistic Regression(    6300/10000): loss= 3579.25218299725\n",
      "Losgistic Regression(    6400/10000): loss= 3578.50951276638\n",
      "Losgistic Regression(    6500/10000): loss= 3577.86941824712\n",
      "Losgistic Regression(    6600/10000): loss= 3577.26217741513\n",
      "Losgistic Regression(    6700/10000): loss= 3576.6138967605\n",
      "Losgistic Regression(    6800/10000): loss= 3575.99214795494\n",
      "Losgistic Regression(    6900/10000): loss= 3575.50473014118\n",
      "Losgistic Regression(    7000/10000): loss= 3575.12701785561\n",
      "Losgistic Regression(    7100/10000): loss= 3574.76880620824\n",
      "Losgistic Regression(    7200/10000): loss= 3574.44874644761\n",
      "Losgistic Regression(    7300/10000): loss= 3574.18433647415\n",
      "Losgistic Regression(    7400/10000): loss= 3573.93233500349\n",
      "Losgistic Regression(    7500/10000): loss= 3573.69064711135\n",
      "Losgistic Regression(    7600/10000): loss= 3573.4365941539\n",
      "Losgistic Regression(    7700/10000): loss= 3573.17009091994\n",
      "Losgistic Regression(    7800/10000): loss= 3572.96344236034\n",
      "Losgistic Regression(    7900/10000): loss= 3572.73934709054\n",
      "Losgistic Regression(    8000/10000): loss= 3572.50232903227\n",
      "Losgistic Regression(    8100/10000): loss= 3572.34319983233\n",
      "Losgistic Regression(    8200/10000): loss= 3572.22215181955\n",
      "Losgistic Regression(    8300/10000): loss= 3572.19525201211\n",
      "Totoal number of iterations =  8300\n",
      "Loss                        =  3572.19525201\n",
      "Time for  2th cross validation = 262.224s\n",
      "Training Accuracy         = 0.8469\n",
      "Cross Validation Accuracy = 0.81508\n",
      "Losgistic Regression(       0/10000): loss= 6822.89891149946\n",
      "Losgistic Regression(     100/10000): loss= 4538.44070029444\n",
      "Losgistic Regression(     200/10000): loss= 4280.68661486095\n",
      "Losgistic Regression(     300/10000): loss= 4141.02931631007\n",
      "Losgistic Regression(     400/10000): loss= 4053.59009875736\n",
      "Losgistic Regression(     500/10000): loss= 3989.42668671112\n",
      "Losgistic Regression(     600/10000): loss= 3936.56628571643\n",
      "Losgistic Regression(     700/10000): loss= 3892.84422640086\n",
      "Losgistic Regression(     800/10000): loss= 3863.64368011025\n",
      "Losgistic Regression(     900/10000): loss= 3840.12839896731\n",
      "Losgistic Regression(    1000/10000): loss= 3823.14754706151\n",
      "Losgistic Regression(    1100/10000): loss= 3809.18172531068\n",
      "Losgistic Regression(    1200/10000): loss= 3798.1620169894\n",
      "Losgistic Regression(    1300/10000): loss= 3787.614458764\n",
      "Losgistic Regression(    1400/10000): loss= 3776.26130304444\n",
      "Losgistic Regression(    1500/10000): loss= 3765.71191031071\n",
      "Losgistic Regression(    1600/10000): loss= 3756.61472863941\n",
      "Losgistic Regression(    1700/10000): loss= 3748.0929838081\n",
      "Losgistic Regression(    1800/10000): loss= 3739.87983198303\n",
      "Losgistic Regression(    1900/10000): loss= 3732.49579939693\n",
      "Losgistic Regression(    2000/10000): loss= 3725.71551509489\n",
      "Losgistic Regression(    2100/10000): loss= 3719.1232890698\n",
      "Losgistic Regression(    2200/10000): loss= 3712.44261675411\n",
      "Losgistic Regression(    2300/10000): loss= 3705.76736880674\n",
      "Losgistic Regression(    2400/10000): loss= 3699.43176160714\n",
      "Losgistic Regression(    2500/10000): loss= 3693.50660865585\n",
      "Losgistic Regression(    2600/10000): loss= 3687.78728772755\n",
      "Losgistic Regression(    2700/10000): loss= 3681.99474898398\n",
      "Losgistic Regression(    2800/10000): loss= 3676.30197437472\n",
      "Losgistic Regression(    2900/10000): loss= 3670.8435166397\n",
      "Losgistic Regression(    3000/10000): loss= 3665.72923406277\n",
      "Losgistic Regression(    3100/10000): loss= 3660.94633098396\n",
      "Losgistic Regression(    3200/10000): loss= 3656.45675266272\n",
      "Losgistic Regression(    3300/10000): loss= 3652.29368409763\n",
      "Losgistic Regression(    3400/10000): loss= 3648.47628125071\n",
      "Losgistic Regression(    3500/10000): loss= 3644.88175460777\n",
      "Losgistic Regression(    3600/10000): loss= 3641.45973020425\n",
      "Losgistic Regression(    3700/10000): loss= 3638.24734312599\n",
      "Losgistic Regression(    3800/10000): loss= 3635.34479816964\n",
      "Losgistic Regression(    3900/10000): loss= 3632.68173918766\n",
      "Losgistic Regression(    4000/10000): loss= 3630.20559767706\n",
      "Losgistic Regression(    4100/10000): loss= 3627.79590646506\n",
      "Losgistic Regression(    4200/10000): loss= 3625.40291911727\n",
      "Losgistic Regression(    4300/10000): loss= 3623.06470500056\n",
      "Losgistic Regression(    4400/10000): loss= 3620.82285603699\n",
      "Losgistic Regression(    4500/10000): loss= 3618.71807754792\n",
      "Losgistic Regression(    4600/10000): loss= 3616.76544826681\n",
      "Losgistic Regression(    4700/10000): loss= 3614.87232009002\n",
      "Losgistic Regression(    4800/10000): loss= 3613.00649355076\n",
      "Losgistic Regression(    4900/10000): loss= 3611.22007281689\n",
      "Losgistic Regression(    5000/10000): loss= 3609.50518722705\n",
      "Losgistic Regression(    5100/10000): loss= 3607.88490207694\n",
      "Losgistic Regression(    5200/10000): loss= 3606.35176557861\n",
      "Losgistic Regression(    5300/10000): loss= 3604.88105235854\n",
      "Losgistic Regression(    5400/10000): loss= 3603.44200537198\n",
      "Losgistic Regression(    5500/10000): loss= 3602.11270275191\n",
      "Losgistic Regression(    5600/10000): loss= 3600.88208601519\n",
      "Losgistic Regression(    5700/10000): loss= 3599.77453756353\n",
      "Losgistic Regression(    5800/10000): loss= 3598.8079857554\n",
      "Losgistic Regression(    5900/10000): loss= 3597.90107365136\n",
      "Losgistic Regression(    6000/10000): loss= 3597.09966970016\n",
      "Losgistic Regression(    6100/10000): loss= 3596.3735565994\n",
      "Losgistic Regression(    6200/10000): loss= 3595.63147417315\n",
      "Losgistic Regression(    6300/10000): loss= 3594.91835231296\n",
      "Losgistic Regression(    6400/10000): loss= 3594.28585067934\n",
      "Losgistic Regression(    6500/10000): loss= 3593.67942409763\n",
      "Losgistic Regression(    6600/10000): loss= 3593.09116905395\n",
      "Losgistic Regression(    6700/10000): loss= 3592.46392683777\n",
      "Losgistic Regression(    6800/10000): loss= 3591.80633838311\n",
      "Losgistic Regression(    6900/10000): loss= 3591.18448667649\n",
      "Losgistic Regression(    7000/10000): loss= 3590.58327275931\n",
      "Losgistic Regression(    7100/10000): loss= 3590.01936142162\n",
      "Losgistic Regression(    7200/10000): loss= 3589.49146990494\n",
      "Losgistic Regression(    7300/10000): loss= 3588.9657824807\n",
      "Losgistic Regression(    7400/10000): loss= 3588.42619573839\n",
      "Losgistic Regression(    7500/10000): loss= 3587.91277403369\n",
      "Losgistic Regression(    7600/10000): loss= 3587.38452960643\n",
      "Losgistic Regression(    7700/10000): loss= 3586.83487631288\n",
      "Losgistic Regression(    7800/10000): loss= 3586.24896677335\n",
      "Losgistic Regression(    7900/10000): loss= 3585.71517821332\n",
      "Losgistic Regression(    8000/10000): loss= 3585.22834868615\n",
      "Losgistic Regression(    8100/10000): loss= 3584.81017424025\n",
      "Losgistic Regression(    8200/10000): loss= 3584.37991936657\n",
      "Losgistic Regression(    8300/10000): loss= 3583.92187006998\n",
      "Losgistic Regression(    8400/10000): loss= 3583.46316060042\n",
      "Losgistic Regression(    8500/10000): loss= 3583.03103400501\n",
      "Losgistic Regression(    8600/10000): loss= 3582.6415803567\n",
      "Losgistic Regression(    8700/10000): loss= 3582.31494461965\n",
      "Losgistic Regression(    8800/10000): loss= 3582.02819853592\n",
      "Losgistic Regression(    8900/10000): loss= 3581.69684064073\n",
      "Losgistic Regression(    9000/10000): loss= 3581.38315648675\n",
      "Losgistic Regression(    9100/10000): loss= 3581.05377758846\n",
      "Losgistic Regression(    9200/10000): loss= 3580.76477020924\n",
      "Losgistic Regression(    9300/10000): loss= 3580.45253624063\n",
      "Losgistic Regression(    9400/10000): loss= 3580.14165053674\n",
      "Losgistic Regression(    9500/10000): loss= 3579.86272850281\n",
      "Losgistic Regression(    9600/10000): loss= 3579.58818190091\n",
      "Losgistic Regression(    9700/10000): loss= 3579.32382158728\n",
      "Losgistic Regression(    9800/10000): loss= 3579.10554529884\n",
      "Losgistic Regression(    9900/10000): loss= 3578.86596775926\n",
      "Time for  3th cross validation = 316.125s\n",
      "Training Accuracy         = 0.8447\n",
      "Cross Validation Accuracy = 0.815308\n",
      "Losgistic Regression(       0/10000): loss= 6820.83920412068\n",
      "Losgistic Regression(     100/10000): loss= 4551.93702133672\n",
      "Losgistic Regression(     200/10000): loss= 4309.18320776818\n",
      "Losgistic Regression(     300/10000): loss= 4194.06089306936\n",
      "Losgistic Regression(     400/10000): loss= 4122.54482053096\n",
      "Losgistic Regression(     500/10000): loss= 4070.26015868395\n",
      "Losgistic Regression(     600/10000): loss= 4025.93610175644\n",
      "Losgistic Regression(     700/10000): loss= 3988.27991890188\n",
      "Losgistic Regression(     800/10000): loss= 3955.86396565954\n",
      "Losgistic Regression(     900/10000): loss= 3928.61476641128\n",
      "Losgistic Regression(    1000/10000): loss= 3906.62011818016\n",
      "Losgistic Regression(    1100/10000): loss= 3887.52458179105\n",
      "Losgistic Regression(    1200/10000): loss= 3870.73295482379\n",
      "Losgistic Regression(    1300/10000): loss= 3856.82669555067\n",
      "Losgistic Regression(    1400/10000): loss= 3846.70578564764\n",
      "Losgistic Regression(    1500/10000): loss= 3837.16568858799\n",
      "Losgistic Regression(    1600/10000): loss= 3829.29187112903\n",
      "Losgistic Regression(    1700/10000): loss= 3821.37909026902\n",
      "Losgistic Regression(    1800/10000): loss= 3814.50627182758\n",
      "Losgistic Regression(    1900/10000): loss= 3807.68228203902\n",
      "Losgistic Regression(    2000/10000): loss= 3801.75748893634\n",
      "Losgistic Regression(    2100/10000): loss= 3795.88382505914\n",
      "Losgistic Regression(    2200/10000): loss= 3789.23656567495\n",
      "Losgistic Regression(    2300/10000): loss= 3783.22569661688\n",
      "Losgistic Regression(    2400/10000): loss= 3777.47771191121\n",
      "Losgistic Regression(    2500/10000): loss= 3771.35530962177\n",
      "Losgistic Regression(    2600/10000): loss= 3765.09505493932\n",
      "Losgistic Regression(    2700/10000): loss= 3759.40690614458\n",
      "Losgistic Regression(    2800/10000): loss= 3753.82292543695\n",
      "Losgistic Regression(    2900/10000): loss= 3747.97776473957\n",
      "Losgistic Regression(    3000/10000): loss= 3742.20264377501\n",
      "Losgistic Regression(    3100/10000): loss= 3736.38830989611\n",
      "Losgistic Regression(    3200/10000): loss= 3730.38654095942\n",
      "Losgistic Regression(    3300/10000): loss= 3724.44629290486\n",
      "Losgistic Regression(    3400/10000): loss= 3718.46528501342\n",
      "Losgistic Regression(    3500/10000): loss= 3712.84447764721\n",
      "Losgistic Regression(    3600/10000): loss= 3707.50812990584\n",
      "Losgistic Regression(    3700/10000): loss= 3702.5446837738\n",
      "Losgistic Regression(    3800/10000): loss= 3698.15138681832\n",
      "Losgistic Regression(    3900/10000): loss= 3694.44449534334\n",
      "Losgistic Regression(    4000/10000): loss= 3691.02816214406\n",
      "Losgistic Regression(    4100/10000): loss= 3688.05993934114\n",
      "Losgistic Regression(    4200/10000): loss= 3685.14840998056\n",
      "Losgistic Regression(    4300/10000): loss= 3682.38586712128\n",
      "Losgistic Regression(    4400/10000): loss= 3679.59000073732\n",
      "Losgistic Regression(    4500/10000): loss= 3677.10011477217\n",
      "Losgistic Regression(    4600/10000): loss= 3674.60381039956\n",
      "Losgistic Regression(    4700/10000): loss= 3672.00223862866\n",
      "Losgistic Regression(    4800/10000): loss= 3669.42254582967\n",
      "Losgistic Regression(    4900/10000): loss= 3666.90797838906\n",
      "Losgistic Regression(    5000/10000): loss= 3664.31135028421\n",
      "Losgistic Regression(    5100/10000): loss= 3661.79058778048\n",
      "Losgistic Regression(    5200/10000): loss= 3659.05278283092\n",
      "Losgistic Regression(    5300/10000): loss= 3655.97957796117\n",
      "Losgistic Regression(    5400/10000): loss= 3652.67424918128\n",
      "Losgistic Regression(    5500/10000): loss= 3649.63140549899\n",
      "Losgistic Regression(    5600/10000): loss= 3646.85404729559\n",
      "Losgistic Regression(    5700/10000): loss= 3644.56533646791\n",
      "Losgistic Regression(    5800/10000): loss= 3642.72268540058\n",
      "Losgistic Regression(    5900/10000): loss= 3641.50559926859\n",
      "Losgistic Regression(    6000/10000): loss= 3641.02788181574\n",
      "Losgistic Regression(    6100/10000): loss= 3640.97727032541\n",
      "Losgistic Regression(    6200/10000): loss= 3640.98292727901\n",
      "Totoal number of iterations =  6200\n",
      "Loss                        =  3640.98292728\n",
      "Time for  4th cross validation = 196.907s\n",
      "Training Accuracy         = 0.8348\n",
      "Cross Validation Accuracy = 0.816352\n",
      "*************** ([0.83950000000000002, 0.84160000000000001, 0.84689999999999999, 0.84470000000000001, 0.83479999999999999], [0.81397600000000003, 0.81662800000000002, 0.81508000000000003, 0.81530800000000003, 0.81635199999999997])\n",
      "Losgistic Regression(       0/10000): loss= 6821.19601185404\n",
      "Losgistic Regression(     100/10000): loss= 4547.27767250635\n",
      "Losgistic Regression(     200/10000): loss= 4275.70784547716\n",
      "Losgistic Regression(     300/10000): loss= 4143.11554561968\n",
      "Losgistic Regression(     400/10000): loss= 4058.77990659877\n",
      "Losgistic Regression(     500/10000): loss= 3998.51833307041\n",
      "Losgistic Regression(     600/10000): loss= 3954.73693057267\n",
      "Losgistic Regression(     700/10000): loss= 3922.38684815212\n",
      "Losgistic Regression(     800/10000): loss= 3896.99378842093\n",
      "Losgistic Regression(     900/10000): loss= 3876.83274253617\n",
      "Losgistic Regression(    1000/10000): loss= 3860.058530871\n",
      "Losgistic Regression(    1100/10000): loss= 3845.36863657506\n",
      "Losgistic Regression(    1200/10000): loss= 3832.72148509227\n",
      "Losgistic Regression(    1300/10000): loss= 3821.80165116201\n",
      "Losgistic Regression(    1400/10000): loss= 3812.53437089857\n",
      "Losgistic Regression(    1500/10000): loss= 3805.28399785965\n",
      "Losgistic Regression(    1600/10000): loss= 3799.64879801657\n",
      "Losgistic Regression(    1700/10000): loss= 3794.14337022261\n",
      "Losgistic Regression(    1800/10000): loss= 3789.90322155263\n",
      "Losgistic Regression(    1900/10000): loss= 3785.51216050405\n",
      "Losgistic Regression(    2000/10000): loss= 3782.15859216737\n",
      "Losgistic Regression(    2100/10000): loss= 3778.80022673189\n",
      "Losgistic Regression(    2200/10000): loss= 3775.13897871308\n",
      "Losgistic Regression(    2300/10000): loss= 3771.8990013737\n",
      "Losgistic Regression(    2400/10000): loss= 3768.9178207216\n",
      "Losgistic Regression(    2500/10000): loss= 3767.2286482323\n",
      "Losgistic Regression(    2600/10000): loss= 3765.33089283631\n",
      "Losgistic Regression(    2700/10000): loss= 3763.70958174039\n",
      "Losgistic Regression(    2800/10000): loss= 3761.91040351399\n",
      "Losgistic Regression(    2900/10000): loss= 3760.1790257565\n",
      "Losgistic Regression(    3000/10000): loss= 3759.07948545236\n",
      "Losgistic Regression(    3100/10000): loss= 3758.19859865199\n",
      "Losgistic Regression(    3200/10000): loss= 3757.27268016391\n",
      "Losgistic Regression(    3300/10000): loss= 3756.9729801343\n",
      "Losgistic Regression(    3400/10000): loss= 3756.68963803937\n",
      "Losgistic Regression(    3500/10000): loss= 3756.12006100886\n",
      "Losgistic Regression(    3600/10000): loss= 3755.60577844019\n",
      "Losgistic Regression(    3700/10000): loss= 3754.84118540386\n",
      "Losgistic Regression(    3800/10000): loss= 3754.04055593161\n",
      "Losgistic Regression(    3900/10000): loss= 3753.56306427069\n",
      "Losgistic Regression(    4000/10000): loss= 3753.16467217924\n",
      "Losgistic Regression(    4100/10000): loss= 3752.87172149847\n",
      "Losgistic Regression(    4200/10000): loss= 3752.70255112611\n",
      "Losgistic Regression(    4300/10000): loss= 3752.52521833311\n",
      "Losgistic Regression(    4400/10000): loss= 3752.10823704896\n",
      "Losgistic Regression(    4500/10000): loss= 3751.84672484568\n",
      "Losgistic Regression(    4600/10000): loss= 3751.6513878687\n",
      "Losgistic Regression(    4700/10000): loss= 3751.46945465587\n",
      "Losgistic Regression(    4800/10000): loss= 3751.47115384526\n",
      "Totoal number of iterations =  4800\n",
      "Loss                        =  3751.47115385\n",
      "Time for  0th cross validation = 152.269s\n",
      "Training Accuracy         =  0.838\n",
      "Cross Validation Accuracy = 0.814188\n",
      "Losgistic Regression(       0/10000): loss= 6827.64574552893\n",
      "Losgistic Regression(     100/10000): loss= 4496.74717744117\n",
      "Losgistic Regression(     200/10000): loss= 4229.43024447799\n",
      "Losgistic Regression(     300/10000): loss= 4092.43613725544\n",
      "Losgistic Regression(     400/10000): loss= 4001.90257944396\n",
      "Losgistic Regression(     500/10000): loss= 3938.35052846538\n",
      "Losgistic Regression(     600/10000): loss= 3892.82524177014\n",
      "Losgistic Regression(     700/10000): loss= 3858.8640523506\n",
      "Losgistic Regression(     800/10000): loss= 3832.8653872002\n",
      "Losgistic Regression(     900/10000): loss= 3812.89839300294\n",
      "Losgistic Regression(    1000/10000): loss= 3797.8861446984\n",
      "Losgistic Regression(    1100/10000): loss= 3786.48668711512\n",
      "Losgistic Regression(    1200/10000): loss= 3777.56954216463\n",
      "Losgistic Regression(    1300/10000): loss= 3770.26514143495\n",
      "Losgistic Regression(    1400/10000): loss= 3763.95753651214\n",
      "Losgistic Regression(    1500/10000): loss= 3758.51140469299\n",
      "Losgistic Regression(    1600/10000): loss= 3753.80684918732\n",
      "Losgistic Regression(    1700/10000): loss= 3749.58966956652\n",
      "Losgistic Regression(    1800/10000): loss= 3745.81902389617\n",
      "Losgistic Regression(    1900/10000): loss= 3742.37441185931\n",
      "Losgistic Regression(    2000/10000): loss= 3739.25824293513\n",
      "Losgistic Regression(    2100/10000): loss= 3736.42482576289\n",
      "Losgistic Regression(    2200/10000): loss= 3733.84425433274\n",
      "Losgistic Regression(    2300/10000): loss= 3731.54592157394\n",
      "Losgistic Regression(    2400/10000): loss= 3729.5413623109\n",
      "Losgistic Regression(    2500/10000): loss= 3727.65937711026\n",
      "Losgistic Regression(    2600/10000): loss= 3725.9834928281\n",
      "Losgistic Regression(    2700/10000): loss= 3724.4718498099\n",
      "Losgistic Regression(    2800/10000): loss= 3723.02146053503\n",
      "Losgistic Regression(    2900/10000): loss= 3721.635897142\n",
      "Losgistic Regression(    3000/10000): loss= 3720.31707459575\n",
      "Losgistic Regression(    3100/10000): loss= 3719.0548163392\n",
      "Losgistic Regression(    3200/10000): loss= 3717.57440769686\n",
      "Losgistic Regression(    3300/10000): loss= 3714.88500842554\n",
      "Losgistic Regression(    3400/10000): loss= 3712.11455728058\n",
      "Losgistic Regression(    3500/10000): loss= 3709.525124127\n",
      "Losgistic Regression(    3600/10000): loss= 3707.04346087901\n",
      "Losgistic Regression(    3700/10000): loss= 3704.66766338564\n",
      "Losgistic Regression(    3800/10000): loss= 3702.26235117427\n",
      "Losgistic Regression(    3900/10000): loss= 3700.01077310567\n",
      "Losgistic Regression(    4000/10000): loss= 3698.39388265564\n",
      "Losgistic Regression(    4100/10000): loss= 3697.4141974836\n",
      "Losgistic Regression(    4200/10000): loss= 3697.07289237904\n",
      "Losgistic Regression(    4300/10000): loss= 3697.07714586026\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  3697.07714586\n",
      "Time for  1th cross validation = 138.56s\n",
      "Training Accuracy         = 0.8386\n",
      "Cross Validation Accuracy = 0.817816\n",
      "Losgistic Regression(       0/10000): loss= 6825.38569611239\n",
      "Losgistic Regression(     100/10000): loss= 4463.30096730272\n",
      "Losgistic Regression(     200/10000): loss= 4212.20522478998\n",
      "Losgistic Regression(     300/10000): loss= 4080.45341419572\n",
      "Losgistic Regression(     400/10000): loss= 3991.83833588258\n",
      "Losgistic Regression(     500/10000): loss= 3928.30188685378\n",
      "Losgistic Regression(     600/10000): loss= 3878.45776056371\n",
      "Losgistic Regression(     700/10000): loss= 3839.31378507815\n",
      "Losgistic Regression(     800/10000): loss= 3809.1766834991\n",
      "Losgistic Regression(     900/10000): loss= 3784.70209422253\n",
      "Losgistic Regression(    1000/10000): loss= 3768.16720999086\n",
      "Losgistic Regression(    1100/10000): loss= 3756.50748233301\n",
      "Losgistic Regression(    1200/10000): loss= 3748.39721846564\n",
      "Losgistic Regression(    1300/10000): loss= 3742.72215628147\n",
      "Losgistic Regression(    1400/10000): loss= 3738.93415633896\n",
      "Losgistic Regression(    1500/10000): loss= 3736.90925103914\n",
      "Losgistic Regression(    1600/10000): loss= 3735.9822937897\n",
      "Losgistic Regression(    1700/10000): loss= 3735.91156958176\n",
      "Losgistic Regression(    1800/10000): loss= 3735.7210114798\n",
      "Losgistic Regression(    1900/10000): loss= 3735.38363658339\n",
      "Losgistic Regression(    2000/10000): loss= 3735.02647823128\n",
      "Losgistic Regression(    2100/10000): loss= 3734.67181031909\n",
      "Losgistic Regression(    2200/10000): loss= 3734.37855172299\n",
      "Losgistic Regression(    2300/10000): loss= 3733.94127933032\n",
      "Losgistic Regression(    2400/10000): loss= 3733.6063846825\n",
      "Losgistic Regression(    2500/10000): loss= 3733.34508823905\n",
      "Losgistic Regression(    2600/10000): loss= 3733.24956335122\n",
      "Losgistic Regression(    2700/10000): loss= 3733.22371318422\n",
      "Totoal number of iterations =  2700\n",
      "Loss                        =  3733.22371318\n",
      "Time for  2th cross validation = 88.8427s\n",
      "Training Accuracy         = 0.8415\n",
      "Cross Validation Accuracy = 0.814108\n",
      "Losgistic Regression(       0/10000): loss= 6822.96896225311\n",
      "Losgistic Regression(     100/10000): loss= 4546.12283109772\n",
      "Losgistic Regression(     200/10000): loss= 4294.18589566902\n",
      "Losgistic Regression(     300/10000): loss= 4160.22286663216\n",
      "Losgistic Regression(     400/10000): loss= 4077.45774065672\n",
      "Losgistic Regression(     500/10000): loss= 4017.86280550195\n",
      "Losgistic Regression(     600/10000): loss= 3969.48956401046\n",
      "Losgistic Regression(     700/10000): loss= 3929.54105695448\n",
      "Losgistic Regression(     800/10000): loss= 3903.48541432756\n",
      "Losgistic Regression(     900/10000): loss= 3883.8769146636\n",
      "Losgistic Regression(    1000/10000): loss= 3869.33665660877\n",
      "Losgistic Regression(    1100/10000): loss= 3858.04993692861\n",
      "Losgistic Regression(    1200/10000): loss= 3849.60840548319\n",
      "Losgistic Regression(    1300/10000): loss= 3842.608214811\n",
      "Losgistic Regression(    1400/10000): loss= 3834.34077846141\n",
      "Losgistic Regression(    1500/10000): loss= 3826.05746802808\n",
      "Losgistic Regression(    1600/10000): loss= 3818.98365906454\n",
      "Losgistic Regression(    1700/10000): loss= 3812.56327486369\n",
      "Losgistic Regression(    1800/10000): loss= 3806.39266700697\n",
      "Losgistic Regression(    1900/10000): loss= 3800.95531607713\n",
      "Losgistic Regression(    2000/10000): loss= 3795.89932181656\n",
      "Losgistic Regression(    2100/10000): loss= 3791.22927942886\n",
      "Losgistic Regression(    2200/10000): loss= 3786.71518077218\n",
      "Losgistic Regression(    2300/10000): loss= 3782.047438967\n",
      "Losgistic Regression(    2400/10000): loss= 3777.77845310858\n",
      "Losgistic Regression(    2500/10000): loss= 3773.66049650978\n",
      "Losgistic Regression(    2600/10000): loss= 3769.60422019757\n",
      "Losgistic Regression(    2700/10000): loss= 3765.58241087033\n",
      "Losgistic Regression(    2800/10000): loss= 3761.73451255747\n",
      "Losgistic Regression(    2900/10000): loss= 3758.21337598668\n",
      "Losgistic Regression(    3000/10000): loss= 3755.02123474238\n",
      "Losgistic Regression(    3100/10000): loss= 3752.03671227754\n",
      "Losgistic Regression(    3200/10000): loss= 3749.05279687312\n",
      "Losgistic Regression(    3300/10000): loss= 3746.22123021311\n",
      "Losgistic Regression(    3400/10000): loss= 3743.58240611896\n",
      "Losgistic Regression(    3500/10000): loss= 3741.04198573664\n",
      "Losgistic Regression(    3600/10000): loss= 3738.76020404834\n",
      "Losgistic Regression(    3700/10000): loss= 3736.69561029991\n",
      "Losgistic Regression(    3800/10000): loss= 3734.95670027256\n",
      "Losgistic Regression(    3900/10000): loss= 3733.41792036762\n",
      "Losgistic Regression(    4000/10000): loss= 3731.87917449673\n",
      "Losgistic Regression(    4100/10000): loss= 3730.45722122203\n",
      "Losgistic Regression(    4200/10000): loss= 3728.86133715315\n",
      "Losgistic Regression(    4300/10000): loss= 3727.31080467655\n",
      "Losgistic Regression(    4400/10000): loss= 3726.09092189378\n",
      "Losgistic Regression(    4500/10000): loss= 3724.94881543595\n",
      "Losgistic Regression(    4600/10000): loss= 3723.6808224514\n",
      "Losgistic Regression(    4700/10000): loss= 3722.29442497797\n",
      "Losgistic Regression(    4800/10000): loss= 3720.83361096541\n",
      "Losgistic Regression(    4900/10000): loss= 3719.37553723942\n",
      "Losgistic Regression(    5000/10000): loss= 3718.15874982647\n",
      "Losgistic Regression(    5100/10000): loss= 3717.07409482989\n",
      "Losgistic Regression(    5200/10000): loss= 3716.1545073818\n",
      "Losgistic Regression(    5300/10000): loss= 3715.28473330159\n",
      "Losgistic Regression(    5400/10000): loss= 3714.52443142141\n",
      "Losgistic Regression(    5500/10000): loss= 3713.64337347154\n",
      "Losgistic Regression(    5600/10000): loss= 3712.73288922214\n",
      "Losgistic Regression(    5700/10000): loss= 3711.77444410727\n",
      "Losgistic Regression(    5800/10000): loss= 3711.16628150655\n",
      "Losgistic Regression(    5900/10000): loss= 3710.81040985372\n",
      "Losgistic Regression(    6000/10000): loss= 3710.43043959604\n",
      "Losgistic Regression(    6100/10000): loss= 3709.96745404925\n",
      "Losgistic Regression(    6200/10000): loss= 3709.81351445771\n",
      "Losgistic Regression(    6300/10000): loss= 3709.55663827198\n",
      "Losgistic Regression(    6400/10000): loss= 3709.30006402134\n",
      "Losgistic Regression(    6500/10000): loss= 3709.13467011116\n",
      "Losgistic Regression(    6600/10000): loss= 3708.95891124053\n",
      "Losgistic Regression(    6700/10000): loss= 3708.94827518763\n",
      "Totoal number of iterations =  6700\n",
      "Loss                        =  3708.94827519\n",
      "Time for  3th cross validation = 219.63s\n",
      "Training Accuracy         = 0.8413\n",
      "Cross Validation Accuracy = 0.816776\n",
      "Losgistic Regression(       0/10000): loss= 6820.91125378439\n",
      "Losgistic Regression(     100/10000): loss= 4559.58614407347\n",
      "Losgistic Regression(     200/10000): loss= 4322.78036329876\n",
      "Losgistic Regression(     300/10000): loss= 4212.58827903687\n",
      "Losgistic Regression(     400/10000): loss= 4145.39541767585\n",
      "Losgistic Regression(     500/10000): loss= 4097.39360331468\n",
      "Losgistic Regression(     600/10000): loss= 4058.93413064483\n",
      "Losgistic Regression(     700/10000): loss= 4025.42509164483\n",
      "Losgistic Regression(     800/10000): loss= 3996.91322650454\n",
      "Losgistic Regression(     900/10000): loss= 3972.83543999672\n",
      "Losgistic Regression(    1000/10000): loss= 3953.45205528581\n",
      "Losgistic Regression(    1100/10000): loss= 3937.09038059216\n",
      "Losgistic Regression(    1200/10000): loss= 3923.59557950607\n",
      "Losgistic Regression(    1300/10000): loss= 3912.52969736445\n",
      "Losgistic Regression(    1400/10000): loss= 3904.00510259237\n",
      "Losgistic Regression(    1500/10000): loss= 3896.46860659959\n",
      "Losgistic Regression(    1600/10000): loss= 3890.26336489818\n",
      "Losgistic Regression(    1700/10000): loss= 3884.7246238823\n",
      "Losgistic Regression(    1800/10000): loss= 3879.88649867786\n",
      "Losgistic Regression(    1900/10000): loss= 3874.84676826114\n",
      "Losgistic Regression(    2000/10000): loss= 3870.96925911921\n",
      "Losgistic Regression(    2100/10000): loss= 3866.8076943115\n",
      "Losgistic Regression(    2200/10000): loss= 3862.12384419375\n",
      "Losgistic Regression(    2300/10000): loss= 3858.51169736109\n",
      "Losgistic Regression(    2400/10000): loss= 3855.24594486773\n",
      "Losgistic Regression(    2500/10000): loss= 3851.95436389553\n",
      "Losgistic Regression(    2600/10000): loss= 3848.43091156079\n",
      "Losgistic Regression(    2700/10000): loss= 3845.38642492046\n",
      "Losgistic Regression(    2800/10000): loss= 3842.84800514538\n",
      "Losgistic Regression(    2900/10000): loss= 3840.2690419078\n",
      "Losgistic Regression(    3000/10000): loss= 3837.68222387158\n",
      "Losgistic Regression(    3100/10000): loss= 3835.64898872552\n",
      "Losgistic Regression(    3200/10000): loss= 3833.35807971309\n",
      "Losgistic Regression(    3300/10000): loss= 3830.43645616306\n",
      "Losgistic Regression(    3400/10000): loss= 3827.37019078906\n",
      "Losgistic Regression(    3500/10000): loss= 3824.06568574358\n",
      "Losgistic Regression(    3600/10000): loss= 3820.30481843232\n",
      "Losgistic Regression(    3700/10000): loss= 3816.77960144624\n",
      "Losgistic Regression(    3800/10000): loss= 3814.15927301094\n",
      "Losgistic Regression(    3900/10000): loss= 3812.78181213882\n",
      "Losgistic Regression(    4000/10000): loss= 3811.52132244719\n",
      "Losgistic Regression(    4100/10000): loss= 3810.65366029621\n",
      "Losgistic Regression(    4200/10000): loss= 3809.1497708419\n",
      "Losgistic Regression(    4300/10000): loss= 3808.19984361065\n",
      "Losgistic Regression(    4400/10000): loss= 3808.15906643229\n",
      "Losgistic Regression(    4500/10000): loss= 3808.16755869348\n",
      "Totoal number of iterations =  4500\n",
      "Loss                        =  3808.16755869\n",
      "Time for  4th cross validation = 144.278s\n",
      "Training Accuracy         = 0.8337\n",
      "Cross Validation Accuracy = 0.816508\n",
      "*************** ([0.83799999999999997, 0.83860000000000001, 0.84150000000000003, 0.84130000000000005, 0.8337], [0.81418800000000002, 0.81781599999999999, 0.81410800000000005, 0.81677599999999995, 0.81650800000000001])\n",
      "Losgistic Regression(       0/10000): loss= 6821.44709547855\n",
      "Losgistic Regression(     100/10000): loss= 4573.96580663368\n",
      "Losgistic Regression(     200/10000): loss= 4323.25409774488\n",
      "Losgistic Regression(     300/10000): loss= 4208.88135446421\n",
      "Losgistic Regression(     400/10000): loss= 4139.15739985591\n",
      "Losgistic Regression(     500/10000): loss= 4092.17190973676\n",
      "Losgistic Regression(     600/10000): loss= 4058.4455616405\n",
      "Losgistic Regression(     700/10000): loss= 4035.12533804903\n",
      "Losgistic Regression(     800/10000): loss= 4017.80865567601\n",
      "Losgistic Regression(     900/10000): loss= 4005.59028968458\n",
      "Losgistic Regression(    1000/10000): loss= 3995.6269910383\n",
      "Losgistic Regression(    1100/10000): loss= 3986.81066155132\n",
      "Losgistic Regression(    1200/10000): loss= 3979.01721352283\n",
      "Losgistic Regression(    1300/10000): loss= 3972.27010645776\n",
      "Losgistic Regression(    1400/10000): loss= 3966.56671629153\n",
      "Losgistic Regression(    1500/10000): loss= 3961.49941751261\n",
      "Losgistic Regression(    1600/10000): loss= 3957.9179572071\n",
      "Losgistic Regression(    1700/10000): loss= 3955.17192151825\n",
      "Losgistic Regression(    1800/10000): loss= 3952.81527420916\n",
      "Losgistic Regression(    1900/10000): loss= 3949.97727898706\n",
      "Losgistic Regression(    2000/10000): loss= 3947.43397839232\n",
      "Losgistic Regression(    2100/10000): loss= 3944.17707112557\n",
      "Losgistic Regression(    2200/10000): loss= 3942.36004585886\n",
      "Losgistic Regression(    2300/10000): loss= 3941.65646431599\n",
      "Losgistic Regression(    2400/10000): loss= 3940.88659711468\n",
      "Losgistic Regression(    2500/10000): loss= 3939.68355239464\n",
      "Losgistic Regression(    2600/10000): loss= 3939.62133892823\n",
      "Losgistic Regression(    2700/10000): loss= 3939.62296535131\n",
      "Totoal number of iterations =  2700\n",
      "Loss                        =  3939.62296535\n",
      "Time for  0th cross validation = 86.6655s\n",
      "Training Accuracy         = 0.8312\n",
      "Cross Validation Accuracy = 0.813672\n",
      "Losgistic Regression(       0/10000): loss= 6827.88201340692\n",
      "Losgistic Regression(     100/10000): loss= 4522.93641228144\n",
      "Losgistic Regression(     200/10000): loss= 4275.48503597931\n",
      "Losgistic Regression(     300/10000): loss= 4155.99867746796\n",
      "Losgistic Regression(     400/10000): loss= 4080.44997036909\n",
      "Losgistic Regression(     500/10000): loss= 4030.49631328745\n",
      "Losgistic Regression(     600/10000): loss= 3995.97865353272\n",
      "Losgistic Regression(     700/10000): loss= 3971.48700845288\n",
      "Losgistic Regression(     800/10000): loss= 3954.09874912893\n",
      "Losgistic Regression(     900/10000): loss= 3941.3230182347\n",
      "Losgistic Regression(    1000/10000): loss= 3932.65352623668\n",
      "Losgistic Regression(    1100/10000): loss= 3926.1613748029\n",
      "Losgistic Regression(    1200/10000): loss= 3920.93929157532\n",
      "Losgistic Regression(    1300/10000): loss= 3916.83471293914\n",
      "Losgistic Regression(    1400/10000): loss= 3913.40555773104\n",
      "Losgistic Regression(    1500/10000): loss= 3910.24187607108\n",
      "Losgistic Regression(    1600/10000): loss= 3907.32655001728\n",
      "Losgistic Regression(    1700/10000): loss= 3904.28159951787\n",
      "Losgistic Regression(    1800/10000): loss= 3901.88132205939\n",
      "Losgistic Regression(    1900/10000): loss= 3899.9423381412\n",
      "Losgistic Regression(    2000/10000): loss= 3898.605557845\n",
      "Losgistic Regression(    2100/10000): loss= 3897.2263429327\n",
      "Losgistic Regression(    2200/10000): loss= 3896.20940830023\n",
      "Losgistic Regression(    2300/10000): loss= 3894.83040524775\n",
      "Losgistic Regression(    2400/10000): loss= 3893.65609177922\n",
      "Losgistic Regression(    2500/10000): loss= 3892.70691051706\n",
      "Losgistic Regression(    2600/10000): loss= 3891.57676424149\n",
      "Losgistic Regression(    2700/10000): loss= 3890.52632933275\n",
      "Losgistic Regression(    2800/10000): loss= 3890.19627343042\n",
      "Losgistic Regression(    2900/10000): loss= 3889.67320389976\n",
      "Losgistic Regression(    3000/10000): loss= 3889.23407076343\n",
      "Losgistic Regression(    3100/10000): loss= 3888.89630300627\n",
      "Losgistic Regression(    3200/10000): loss= 3888.39589092077\n",
      "Losgistic Regression(    3300/10000): loss= 3887.89914760036\n",
      "Losgistic Regression(    3400/10000): loss= 3887.46477581751\n",
      "Losgistic Regression(    3500/10000): loss= 3887.01558450503\n",
      "Losgistic Regression(    3600/10000): loss= 3886.62040269234\n",
      "Losgistic Regression(    3700/10000): loss= 3886.3069282487\n",
      "Losgistic Regression(    3800/10000): loss= 3885.88412522691\n",
      "Losgistic Regression(    3900/10000): loss= 3885.60824805139\n",
      "Losgistic Regression(    4000/10000): loss= 3885.48237880664\n",
      "Losgistic Regression(    4100/10000): loss= 3885.38225607665\n",
      "Losgistic Regression(    4200/10000): loss= 3885.28432418764\n",
      "Losgistic Regression(    4300/10000): loss= 3885.17973810262\n",
      "Losgistic Regression(    4400/10000): loss= 3885.06069127808\n",
      "Losgistic Regression(    4500/10000): loss= 3885.04480013143\n",
      "Totoal number of iterations =  4500\n",
      "Loss                        =  3885.04480013\n",
      "Time for  1th cross validation = 142.804s\n",
      "Training Accuracy         = 0.8347\n",
      "Cross Validation Accuracy = 0.818452\n",
      "Losgistic Regression(       0/10000): loss= 6825.63284990822\n",
      "Losgistic Regression(     100/10000): loss= 4490.27242022065\n",
      "Losgistic Regression(     200/10000): loss= 4258.73131104639\n",
      "Losgistic Regression(     300/10000): loss= 4143.88803205751\n",
      "Losgistic Regression(     400/10000): loss= 4070.6383671931\n",
      "Losgistic Regression(     500/10000): loss= 4020.366226236\n",
      "Losgistic Regression(     600/10000): loss= 3982.31067488434\n",
      "Losgistic Regression(     700/10000): loss= 3954.57845765958\n",
      "Losgistic Regression(     800/10000): loss= 3934.81084934352\n",
      "Losgistic Regression(     900/10000): loss= 3918.52899144519\n",
      "Losgistic Regression(    1000/10000): loss= 3907.59020775087\n",
      "Losgistic Regression(    1100/10000): loss= 3900.73127179208\n",
      "Losgistic Regression(    1200/10000): loss= 3896.49783453559\n",
      "Losgistic Regression(    1300/10000): loss= 3893.48594295076\n",
      "Losgistic Regression(    1400/10000): loss= 3890.54743819048\n",
      "Losgistic Regression(    1500/10000): loss= 3888.42114956024\n",
      "Losgistic Regression(    1600/10000): loss= 3886.75241179823\n",
      "Losgistic Regression(    1700/10000): loss= 3884.84598751816\n",
      "Losgistic Regression(    1800/10000): loss= 3882.69715647884\n",
      "Losgistic Regression(    1900/10000): loss= 3880.95719898872\n",
      "Losgistic Regression(    2000/10000): loss= 3879.58561438923\n",
      "Losgistic Regression(    2100/10000): loss= 3878.68006812545\n",
      "Losgistic Regression(    2200/10000): loss= 3877.55521772635\n",
      "Losgistic Regression(    2300/10000): loss= 3876.44136004708\n",
      "Losgistic Regression(    2400/10000): loss= 3875.5884360031\n",
      "Losgistic Regression(    2500/10000): loss= 3875.37933637713\n",
      "Losgistic Regression(    2600/10000): loss= 3875.16350487644\n",
      "Losgistic Regression(    2700/10000): loss= 3875.02078631558\n",
      "Losgistic Regression(    2800/10000): loss= 3874.94533273142\n",
      "Losgistic Regression(    2900/10000): loss= 3874.85258177798\n",
      "Losgistic Regression(    3000/10000): loss= 3874.75864104803\n",
      "Losgistic Regression(    3100/10000): loss= 3874.65570759387\n",
      "Losgistic Regression(    3200/10000): loss= 3874.52609672767\n",
      "Losgistic Regression(    3300/10000): loss= 3874.37967701788\n",
      "Losgistic Regression(    3400/10000): loss= 3874.20771248106\n",
      "Losgistic Regression(    3500/10000): loss= 3874.02246022367\n",
      "Losgistic Regression(    3600/10000): loss= 3873.86851166224\n",
      "Losgistic Regression(    3700/10000): loss= 3873.63481583187\n",
      "Losgistic Regression(    3800/10000): loss= 3873.453944747\n",
      "Losgistic Regression(    3900/10000): loss= 3873.27040088579\n",
      "Losgistic Regression(    4000/10000): loss= 3873.06480775581\n",
      "Losgistic Regression(    4100/10000): loss= 3872.83918002478\n",
      "Losgistic Regression(    4200/10000): loss= 3872.57322291142\n",
      "Losgistic Regression(    4300/10000): loss= 3872.36558018553\n",
      "Losgistic Regression(    4400/10000): loss= 3872.18862071807\n",
      "Losgistic Regression(    4500/10000): loss= 3871.96574150152\n",
      "Losgistic Regression(    4600/10000): loss= 3871.68629864322\n",
      "Losgistic Regression(    4700/10000): loss= 3871.44611973453\n",
      "Losgistic Regression(    4800/10000): loss= 3871.23662090677\n",
      "Losgistic Regression(    4900/10000): loss= 3871.05387961523\n",
      "Losgistic Regression(    5000/10000): loss= 3870.78780984398\n",
      "Losgistic Regression(    5100/10000): loss= 3870.53378310865\n",
      "Losgistic Regression(    5200/10000): loss= 3870.27898959203\n",
      "Losgistic Regression(    5300/10000): loss= 3870.14068141313\n",
      "Losgistic Regression(    5400/10000): loss= 3869.91772260805\n",
      "Losgistic Regression(    5500/10000): loss= 3869.66520388749\n",
      "Losgistic Regression(    5600/10000): loss= 3869.41579577136\n",
      "Losgistic Regression(    5700/10000): loss= 3869.39239715689\n",
      "Totoal number of iterations =  5700\n",
      "Loss                        =  3869.39239716\n",
      "Time for  2th cross validation = 179.756s\n",
      "Training Accuracy         = 0.8398\n",
      "Cross Validation Accuracy = 0.815976\n",
      "Losgistic Regression(       0/10000): loss= 6823.21289314529\n",
      "Losgistic Regression(     100/10000): loss= 4572.11257979059\n",
      "Losgistic Regression(     200/10000): loss= 4338.98000703059\n",
      "Losgistic Regression(     300/10000): loss= 4222.52864956585\n",
      "Losgistic Regression(     400/10000): loss= 4153.56199138003\n",
      "Losgistic Regression(     500/10000): loss= 4106.79219624772\n",
      "Losgistic Regression(     600/10000): loss= 4069.94718632773\n",
      "Losgistic Regression(     700/10000): loss= 4038.50319261785\n",
      "Losgistic Regression(     800/10000): loss= 4019.27228091873\n",
      "Losgistic Regression(     900/10000): loss= 4006.48519679986\n",
      "Losgistic Regression(    1000/10000): loss= 3997.65785938332\n",
      "Losgistic Regression(    1100/10000): loss= 3990.49857967702\n",
      "Losgistic Regression(    1200/10000): loss= 3985.70860930194\n",
      "Losgistic Regression(    1300/10000): loss= 3982.72295118608\n",
      "Losgistic Regression(    1400/10000): loss= 3980.75918507006\n",
      "Losgistic Regression(    1500/10000): loss= 3978.59502877097\n",
      "Losgistic Regression(    1600/10000): loss= 3976.58230257175\n",
      "Losgistic Regression(    1700/10000): loss= 3974.22999630962\n",
      "Losgistic Regression(    1800/10000): loss= 3971.73171166184\n",
      "Losgistic Regression(    1900/10000): loss= 3969.591247001\n",
      "Losgistic Regression(    2000/10000): loss= 3967.58422246234\n",
      "Losgistic Regression(    2100/10000): loss= 3966.07720701759\n",
      "Losgistic Regression(    2200/10000): loss= 3965.28440975821\n",
      "Losgistic Regression(    2300/10000): loss= 3964.02040523555\n",
      "Losgistic Regression(    2400/10000): loss= 3963.29015333276\n",
      "Losgistic Regression(    2500/10000): loss= 3961.83928178559\n",
      "Losgistic Regression(    2600/10000): loss= 3961.25061570216\n",
      "Losgistic Regression(    2700/10000): loss= 3960.66283123614\n",
      "Losgistic Regression(    2800/10000): loss= 3960.62243904108\n",
      "Losgistic Regression(    2900/10000): loss= 3960.61562824058\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  3960.61562824\n",
      "Time for  3th cross validation = 92.1413s\n",
      "Training Accuracy         = 0.8351\n",
      "Cross Validation Accuracy = 0.816056\n",
      "Losgistic Regression(       0/10000): loss= 6821.16214528569\n",
      "Losgistic Regression(     100/10000): loss= 4585.54504358286\n",
      "Losgistic Regression(     200/10000): loss= 4367.66014471588\n",
      "Losgistic Regression(     300/10000): loss= 4272.28524937707\n",
      "Losgistic Regression(     400/10000): loss= 4217.8018539104\n",
      "Losgistic Regression(     500/10000): loss= 4181.24944485811\n",
      "Losgistic Regression(     600/10000): loss= 4154.13265052421\n",
      "Losgistic Regression(     700/10000): loss= 4131.96282243003\n",
      "Losgistic Regression(     800/10000): loss= 4112.90241847431\n",
      "Losgistic Regression(     900/10000): loss= 4097.04898424702\n",
      "Losgistic Regression(    1000/10000): loss= 4083.4959641867\n",
      "Losgistic Regression(    1100/10000): loss= 4072.27639165412\n",
      "Losgistic Regression(    1200/10000): loss= 4063.00382665799\n",
      "Losgistic Regression(    1300/10000): loss= 4055.8212745669\n",
      "Losgistic Regression(    1400/10000): loss= 4051.01357492056\n",
      "Losgistic Regression(    1500/10000): loss= 4047.7296126623\n",
      "Losgistic Regression(    1600/10000): loss= 4044.6876686168\n",
      "Losgistic Regression(    1700/10000): loss= 4042.4380115336\n",
      "Losgistic Regression(    1800/10000): loss= 4040.60366788832\n",
      "Losgistic Regression(    1900/10000): loss= 4038.92944383189\n",
      "Losgistic Regression(    2000/10000): loss= 4037.42625080367\n",
      "Losgistic Regression(    2100/10000): loss= 4036.05286746361\n",
      "Losgistic Regression(    2200/10000): loss= 4034.58543468739\n",
      "Losgistic Regression(    2300/10000): loss= 4033.32072388513\n",
      "Losgistic Regression(    2400/10000): loss= 4032.32873987172\n",
      "Losgistic Regression(    2500/10000): loss= 4031.23799492487\n",
      "Losgistic Regression(    2600/10000): loss= 4030.26804408339\n",
      "Losgistic Regression(    2700/10000): loss= 4029.42879819842\n",
      "Losgistic Regression(    2800/10000): loss= 4028.59818720035\n",
      "Losgistic Regression(    2900/10000): loss= 4027.71742359962\n",
      "Losgistic Regression(    3000/10000): loss= 4026.88311413484\n",
      "Losgistic Regression(    3100/10000): loss= 4026.26484305289\n",
      "Losgistic Regression(    3200/10000): loss= 4025.17739694776\n",
      "Losgistic Regression(    3300/10000): loss= 4024.36717756995\n",
      "Losgistic Regression(    3400/10000): loss= 4023.99641558941\n",
      "Losgistic Regression(    3500/10000): loss= 4023.37800713988\n",
      "Losgistic Regression(    3600/10000): loss= 4022.96987194275\n",
      "Losgistic Regression(    3700/10000): loss= 4022.56342885829\n",
      "Losgistic Regression(    3800/10000): loss= 4022.16389630712\n",
      "Losgistic Regression(    3900/10000): loss= 4021.83322822475\n",
      "Losgistic Regression(    4000/10000): loss= 4021.43150664276\n",
      "Losgistic Regression(    4100/10000): loss= 4021.07012227506\n",
      "Losgistic Regression(    4200/10000): loss= 4020.7333322312\n",
      "Losgistic Regression(    4300/10000): loss= 4020.69638574055\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  4020.69638574\n",
      "Time for  4th cross validation = 136.345s\n",
      "Training Accuracy         = 0.8286\n",
      "Cross Validation Accuracy = 0.816032\n",
      "*************** ([0.83120000000000005, 0.8347, 0.83979999999999999, 0.83509999999999995, 0.8286], [0.81367199999999995, 0.81845199999999996, 0.81597600000000003, 0.816056, 0.81603199999999998])\n",
      "Losgistic Regression(       0/10000): loss= 6822.32141944154\n",
      "Losgistic Regression(     100/10000): loss= 4658.14651552443\n",
      "Losgistic Regression(     200/10000): loss= 4462.78669946749\n",
      "Losgistic Regression(     300/10000): loss= 4387.07828882383\n",
      "Losgistic Regression(     400/10000): loss= 4342.87670808852\n",
      "Losgistic Regression(     500/10000): loss= 4315.29621165487\n",
      "Losgistic Regression(     600/10000): loss= 4295.82231932856\n",
      "Losgistic Regression(     700/10000): loss= 4285.17993077222\n",
      "Losgistic Regression(     800/10000): loss= 4277.87565345289\n",
      "Losgistic Regression(     900/10000): loss= 4274.42331869931\n",
      "Losgistic Regression(    1000/10000): loss= 4271.95226413865\n",
      "Losgistic Regression(    1100/10000): loss= 4269.21908906051\n",
      "Losgistic Regression(    1200/10000): loss= 4267.67895752786\n",
      "Losgistic Regression(    1300/10000): loss= 4266.30631107955\n",
      "Losgistic Regression(    1400/10000): loss= 4266.13952294425\n",
      "Losgistic Regression(    1500/10000): loss= 4266.13130808456\n",
      "Totoal number of iterations =  1500\n",
      "Loss                        =  4266.13130808\n",
      "Time for  0th cross validation = 47.9852s\n",
      "Training Accuracy         = 0.8198\n",
      "Cross Validation Accuracy = 0.809852\n",
      "Losgistic Regression(       0/10000): loss= 6828.70474594403\n",
      "Losgistic Regression(     100/10000): loss= 4606.16256561438\n",
      "Losgistic Regression(     200/10000): loss= 4411.10670785557\n",
      "Losgistic Regression(     300/10000): loss= 4329.5481671434\n",
      "Losgistic Regression(     400/10000): loss= 4280.11357213024\n",
      "Losgistic Regression(     500/10000): loss= 4251.03511415833\n",
      "Losgistic Regression(     600/10000): loss= 4233.63661824286\n",
      "Losgistic Regression(     700/10000): loss= 4221.9039116508\n",
      "Losgistic Regression(     800/10000): loss= 4212.58043602563\n",
      "Losgistic Regression(     900/10000): loss= 4206.4440591356\n",
      "Losgistic Regression(    1000/10000): loss= 4202.34759705678\n",
      "Losgistic Regression(    1100/10000): loss= 4198.31772526547\n",
      "Losgistic Regression(    1200/10000): loss= 4194.97460170249\n",
      "Losgistic Regression(    1300/10000): loss= 4192.03122307332\n",
      "Losgistic Regression(    1400/10000): loss= 4189.09796364856\n",
      "Losgistic Regression(    1500/10000): loss= 4186.44169412284\n",
      "Losgistic Regression(    1600/10000): loss= 4184.61117371937\n",
      "Losgistic Regression(    1700/10000): loss= 4184.2616553546\n",
      "Losgistic Regression(    1800/10000): loss= 4183.77861750168\n",
      "Losgistic Regression(    1900/10000): loss= 4183.49609892192\n",
      "Losgistic Regression(    2000/10000): loss= 4183.18866841986\n",
      "Losgistic Regression(    2100/10000): loss= 4182.84498782538\n",
      "Losgistic Regression(    2200/10000): loss= 4182.45661519815\n",
      "Losgistic Regression(    2300/10000): loss= 4182.02717972744\n",
      "Losgistic Regression(    2400/10000): loss= 4181.56033110852\n",
      "Losgistic Regression(    2500/10000): loss= 4181.06195614258\n",
      "Losgistic Regression(    2600/10000): loss= 4180.53388559013\n",
      "Losgistic Regression(    2700/10000): loss= 4179.97761597432\n",
      "Losgistic Regression(    2800/10000): loss= 4179.66299337943\n",
      "Losgistic Regression(    2900/10000): loss= 4179.66064639863\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  4179.6606464\n",
      "Time for  1th cross validation = 92.1645s\n",
      "Training Accuracy         = 0.8239\n",
      "Cross Validation Accuracy = 0.813132\n",
      "Losgistic Regression(       0/10000): loss= 6826.49348941293\n",
      "Losgistic Regression(     100/10000): loss= 4575.40191199879\n",
      "Losgistic Regression(     200/10000): loss= 4395.02383986609\n",
      "Losgistic Regression(     300/10000): loss= 4317.90124872744\n",
      "Losgistic Regression(     400/10000): loss= 4270.57297137895\n",
      "Losgistic Regression(     500/10000): loss= 4237.16998883101\n",
      "Losgistic Regression(     600/10000): loss= 4218.77129602252\n",
      "Losgistic Regression(     700/10000): loss= 4205.45789045256\n",
      "Losgistic Regression(     800/10000): loss= 4197.1116396885\n",
      "Losgistic Regression(     900/10000): loss= 4193.64053308406\n",
      "Losgistic Regression(    1000/10000): loss= 4190.67834786559\n",
      "Losgistic Regression(    1100/10000): loss= 4187.72638619271\n",
      "Losgistic Regression(    1200/10000): loss= 4186.4469197209\n",
      "Losgistic Regression(    1300/10000): loss= 4186.26833309143\n",
      "Losgistic Regression(    1400/10000): loss= 4186.2554329748\n",
      "Totoal number of iterations =  1400\n",
      "Loss                        =  4186.25543297\n",
      "Time for  2th cross validation = 44.9504s\n",
      "Training Accuracy         = 0.8277\n",
      "Cross Validation Accuracy = 0.809456\n",
      "Losgistic Regression(       0/10000): loss= 6824.06230984767\n",
      "Losgistic Regression(     100/10000): loss= 4653.88587231966\n",
      "Losgistic Regression(     200/10000): loss= 4469.90404487886\n",
      "Losgistic Regression(     300/10000): loss= 4392.02591104022\n",
      "Losgistic Regression(     400/10000): loss= 4348.12113625595\n",
      "Losgistic Regression(     500/10000): loss= 4321.10744779862\n",
      "Losgistic Regression(     600/10000): loss= 4299.26792708951\n",
      "Losgistic Regression(     700/10000): loss= 4281.92802369654\n",
      "Losgistic Regression(     800/10000): loss= 4267.89865030509\n",
      "Losgistic Regression(     900/10000): loss= 4259.46450642324\n",
      "Losgistic Regression(    1000/10000): loss= 4253.78031457365\n",
      "Losgistic Regression(    1100/10000): loss= 4250.14338266834\n",
      "Losgistic Regression(    1200/10000): loss= 4247.73798045885\n",
      "Losgistic Regression(    1300/10000): loss= 4245.48730594249\n",
      "Losgistic Regression(    1400/10000): loss= 4243.46063189376\n",
      "Losgistic Regression(    1500/10000): loss= 4242.11164864088\n",
      "Losgistic Regression(    1600/10000): loss= 4240.38926063494\n",
      "Losgistic Regression(    1700/10000): loss= 4239.47070129951\n",
      "Losgistic Regression(    1800/10000): loss= 4238.6316964756\n",
      "Losgistic Regression(    1900/10000): loss= 4237.50065495632\n",
      "Losgistic Regression(    2000/10000): loss= 4237.27881871422\n",
      "Losgistic Regression(    2100/10000): loss= 4237.25694096051\n",
      "Totoal number of iterations =  2100\n",
      "Loss                        =  4237.25694096\n",
      "Time for  3th cross validation = 66.8339s\n",
      "Training Accuracy         = 0.8241\n",
      "Cross Validation Accuracy = 0.81164\n",
      "Losgistic Regression(       0/10000): loss= 6822.03580023684\n",
      "Losgistic Regression(     100/10000): loss= 4667.9156416593\n",
      "Losgistic Regression(     200/10000): loss= 4497.31266583698\n",
      "Losgistic Regression(     300/10000): loss= 4432.52299520895\n",
      "Losgistic Regression(     400/10000): loss= 4398.52700915552\n",
      "Losgistic Regression(     500/10000): loss= 4378.17686340663\n",
      "Losgistic Regression(     600/10000): loss= 4365.07823178627\n",
      "Losgistic Regression(     700/10000): loss= 4355.26240502475\n",
      "Losgistic Regression(     800/10000): loss= 4347.8668820935\n",
      "Losgistic Regression(     900/10000): loss= 4342.42016762999\n",
      "Losgistic Regression(    1000/10000): loss= 4337.67453781376\n",
      "Losgistic Regression(    1100/10000): loss= 4334.52857660295\n",
      "Losgistic Regression(    1200/10000): loss= 4331.92938840988\n",
      "Losgistic Regression(    1300/10000): loss= 4329.61470924317\n",
      "Losgistic Regression(    1400/10000): loss= 4327.84275607148\n",
      "Losgistic Regression(    1500/10000): loss= 4326.19496734859\n",
      "Losgistic Regression(    1600/10000): loss= 4324.74981976372\n",
      "Losgistic Regression(    1700/10000): loss= 4323.53315654154\n",
      "Losgistic Regression(    1800/10000): loss= 4322.30924450157\n",
      "Losgistic Regression(    1900/10000): loss= 4322.04887323687\n",
      "Losgistic Regression(    2000/10000): loss= 4321.898427704\n",
      "Losgistic Regression(    2100/10000): loss= 4321.75757121563\n",
      "Losgistic Regression(    2200/10000): loss= 4321.73622708903\n",
      "Totoal number of iterations =  2200\n",
      "Loss                        =  4321.73622709\n",
      "Time for  4th cross validation = 70.1543s\n",
      "Training Accuracy         = 0.8148\n",
      "Cross Validation Accuracy = 0.808876\n",
      "*************** ([0.81979999999999997, 0.82389999999999997, 0.82769999999999999, 0.82410000000000005, 0.81479999999999997], [0.80985200000000002, 0.81313199999999997, 0.80945599999999995, 0.81164000000000003, 0.80887600000000004])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.84109999999999996,\n",
       "   0.84240000000000004,\n",
       "   0.84740000000000004,\n",
       "   0.84499999999999997,\n",
       "   0.83530000000000004],\n",
       "  [0.81363200000000002,\n",
       "   0.81584000000000001,\n",
       "   0.81413999999999997,\n",
       "   0.81413999999999997,\n",
       "   0.81575200000000003]),\n",
       " ([0.84050000000000002,\n",
       "   0.8417,\n",
       "   0.84689999999999999,\n",
       "   0.84509999999999996,\n",
       "   0.83550000000000002],\n",
       "  [0.81368399999999996,\n",
       "   0.81589999999999996,\n",
       "   0.81435599999999997,\n",
       "   0.81452400000000003,\n",
       "   0.81611199999999995]),\n",
       " ([0.83950000000000002,\n",
       "   0.84160000000000001,\n",
       "   0.84689999999999999,\n",
       "   0.84470000000000001,\n",
       "   0.83479999999999999],\n",
       "  [0.81397600000000003,\n",
       "   0.81662800000000002,\n",
       "   0.81508000000000003,\n",
       "   0.81530800000000003,\n",
       "   0.81635199999999997]),\n",
       " ([0.83799999999999997,\n",
       "   0.83860000000000001,\n",
       "   0.84150000000000003,\n",
       "   0.84130000000000005,\n",
       "   0.8337],\n",
       "  [0.81418800000000002,\n",
       "   0.81781599999999999,\n",
       "   0.81410800000000005,\n",
       "   0.81677599999999995,\n",
       "   0.81650800000000001]),\n",
       " ([0.83120000000000005,\n",
       "   0.8347,\n",
       "   0.83979999999999999,\n",
       "   0.83509999999999995,\n",
       "   0.8286],\n",
       "  [0.81367199999999995,\n",
       "   0.81845199999999996,\n",
       "   0.81597600000000003,\n",
       "   0.816056,\n",
       "   0.81603199999999998]),\n",
       " ([0.81979999999999997,\n",
       "   0.82389999999999997,\n",
       "   0.82769999999999999,\n",
       "   0.82410000000000005,\n",
       "   0.81479999999999997],\n",
       "  [0.80985200000000002,\n",
       "   0.81313199999999997,\n",
       "   0.80945599999999995,\n",
       "   0.81164000000000003,\n",
       "   0.80887600000000004])]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_10000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(10000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_10000.append(tmp)\n",
    "\n",
    "accu_10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 3436.25369024268\n",
      "Losgistic Regression(     100/10000): loss= 2335.38408481608\n",
      "Losgistic Regression(     200/10000): loss= 2120.8244973092\n",
      "Losgistic Regression(     300/10000): loss= 2038.86023994235\n",
      "Losgistic Regression(     400/10000): loss= 1982.2365087908\n",
      "Losgistic Regression(     500/10000): loss= 1939.79190706255\n",
      "Losgistic Regression(     600/10000): loss= 1906.11170764866\n",
      "Losgistic Regression(     700/10000): loss= 1878.43608128396\n",
      "Losgistic Regression(     800/10000): loss= 1855.55657337835\n",
      "Losgistic Regression(     900/10000): loss= 1836.53791910732\n",
      "Losgistic Regression(    1000/10000): loss= 1820.35362213278\n",
      "Losgistic Regression(    1100/10000): loss= 1806.22017349487\n",
      "Losgistic Regression(    1200/10000): loss= 1793.66089585403\n",
      "Losgistic Regression(    1300/10000): loss= 1782.3396391158\n",
      "Losgistic Regression(    1400/10000): loss= 1772.28250224786\n",
      "Losgistic Regression(    1500/10000): loss= 1763.84201200582\n",
      "Losgistic Regression(    1600/10000): loss= 1756.14797405185\n",
      "Losgistic Regression(    1700/10000): loss= 1749.37774649725\n",
      "Losgistic Regression(    1800/10000): loss= 1743.38438097274\n",
      "Losgistic Regression(    1900/10000): loss= 1738.05332793769\n",
      "Losgistic Regression(    2000/10000): loss= 1733.30288252629\n",
      "Losgistic Regression(    2100/10000): loss= 1728.6669491873\n",
      "Losgistic Regression(    2200/10000): loss= 1724.71313486582\n",
      "Losgistic Regression(    2300/10000): loss= 1721.05883423618\n",
      "Losgistic Regression(    2400/10000): loss= 1717.46015870837\n",
      "Losgistic Regression(    2500/10000): loss= 1714.23393028509\n",
      "Losgistic Regression(    2600/10000): loss= 1710.85555287582\n",
      "Losgistic Regression(    2700/10000): loss= 1707.50737055236\n",
      "Losgistic Regression(    2800/10000): loss= 1704.44245565712\n",
      "Losgistic Regression(    2900/10000): loss= 1701.62099528256\n",
      "Losgistic Regression(    3000/10000): loss= 1698.94288307236\n",
      "Losgistic Regression(    3100/10000): loss= 1696.45479683125\n",
      "Losgistic Regression(    3200/10000): loss= 1694.03236116277\n",
      "Losgistic Regression(    3300/10000): loss= 1691.43441720798\n",
      "Losgistic Regression(    3400/10000): loss= 1688.77953872545\n",
      "Losgistic Regression(    3500/10000): loss= 1686.1727618056\n",
      "Losgistic Regression(    3600/10000): loss= 1683.71793622599\n",
      "Losgistic Regression(    3700/10000): loss= 1681.32244148538\n",
      "Losgistic Regression(    3800/10000): loss= 1678.74311790536\n",
      "Losgistic Regression(    3900/10000): loss= 1676.19505939456\n",
      "Losgistic Regression(    4000/10000): loss= 1673.65502000397\n",
      "Losgistic Regression(    4100/10000): loss= 1671.10698704082\n",
      "Losgistic Regression(    4200/10000): loss= 1668.55375639221\n",
      "Losgistic Regression(    4300/10000): loss= 1665.99071154689\n",
      "Losgistic Regression(    4400/10000): loss= 1663.45235648379\n",
      "Losgistic Regression(    4500/10000): loss= 1660.83935622069\n",
      "Losgistic Regression(    4600/10000): loss= 1658.02413424539\n",
      "Losgistic Regression(    4700/10000): loss= 1655.29260314267\n",
      "Losgistic Regression(    4800/10000): loss= 1652.68686157434\n",
      "Losgistic Regression(    4900/10000): loss= 1650.13133297363\n",
      "Losgistic Regression(    5000/10000): loss= 1647.61038421736\n",
      "Losgistic Regression(    5100/10000): loss= 1645.219824974\n",
      "Losgistic Regression(    5200/10000): loss= 1642.95360038188\n",
      "Losgistic Regression(    5300/10000): loss= 1640.80019243772\n",
      "Losgistic Regression(    5400/10000): loss= 1638.79672848377\n",
      "Losgistic Regression(    5500/10000): loss= 1636.90769960708\n",
      "Losgistic Regression(    5600/10000): loss= 1635.08066234283\n",
      "Losgistic Regression(    5700/10000): loss= 1633.32352242648\n",
      "Losgistic Regression(    5800/10000): loss= 1631.66756143926\n",
      "Losgistic Regression(    5900/10000): loss= 1630.11293168877\n",
      "Losgistic Regression(    6000/10000): loss= 1628.63562669533\n",
      "Losgistic Regression(    6100/10000): loss= 1626.97826547838\n",
      "Losgistic Regression(    6200/10000): loss= 1625.30031732047\n",
      "Losgistic Regression(    6300/10000): loss= 1623.74476216898\n",
      "Losgistic Regression(    6400/10000): loss= 1622.31841611641\n",
      "Losgistic Regression(    6500/10000): loss= 1620.99345711954\n",
      "Losgistic Regression(    6600/10000): loss= 1619.76420955088\n",
      "Losgistic Regression(    6700/10000): loss= 1618.6382589615\n",
      "Losgistic Regression(    6800/10000): loss= 1617.59108336269\n",
      "Losgistic Regression(    6900/10000): loss= 1616.6166074819\n",
      "Losgistic Regression(    7000/10000): loss= 1615.72035483453\n",
      "Losgistic Regression(    7100/10000): loss= 1614.87936867919\n",
      "Losgistic Regression(    7200/10000): loss= 1614.05606145282\n",
      "Losgistic Regression(    7300/10000): loss= 1613.24206791797\n",
      "Losgistic Regression(    7400/10000): loss= 1612.43660479524\n",
      "Losgistic Regression(    7500/10000): loss= 1611.64780354486\n",
      "Losgistic Regression(    7600/10000): loss= 1610.87080098175\n",
      "Losgistic Regression(    7700/10000): loss= 1610.10096938839\n",
      "Losgistic Regression(    7800/10000): loss= 1609.34844813343\n",
      "Losgistic Regression(    7900/10000): loss= 1608.57977475058\n",
      "Losgistic Regression(    8000/10000): loss= 1607.84025937449\n",
      "Losgistic Regression(    8100/10000): loss= 1607.1381016912\n",
      "Losgistic Regression(    8200/10000): loss= 1606.42927052921\n",
      "Losgistic Regression(    8300/10000): loss= 1605.74967416198\n",
      "Losgistic Regression(    8400/10000): loss= 1605.1175636036\n",
      "Losgistic Regression(    8500/10000): loss= 1604.5166218184\n",
      "Losgistic Regression(    8600/10000): loss= 1603.96545509868\n",
      "Losgistic Regression(    8700/10000): loss= 1603.48984665349\n",
      "Losgistic Regression(    8800/10000): loss= 1603.07331106498\n",
      "Losgistic Regression(    8900/10000): loss= 1602.6763294143\n",
      "Losgistic Regression(    9000/10000): loss= 1602.28838946694\n",
      "Losgistic Regression(    9100/10000): loss= 1601.93004422541\n",
      "Losgistic Regression(    9200/10000): loss= 1601.60299875908\n",
      "Losgistic Regression(    9300/10000): loss= 1601.29425135481\n",
      "Losgistic Regression(    9400/10000): loss= 1600.99559635413\n",
      "Losgistic Regression(    9500/10000): loss= 1600.67974700196\n",
      "Losgistic Regression(    9600/10000): loss= 1600.33069193013\n",
      "Losgistic Regression(    9700/10000): loss= 1599.96083422651\n",
      "Losgistic Regression(    9800/10000): loss= 1599.58224788871\n",
      "Losgistic Regression(    9900/10000): loss= 1599.1843110742\n",
      "Time for  0th cross validation = 165.499s\n",
      "Training Accuracy         =  0.859\n",
      "Cross Validation Accuracy = 0.805328\n",
      "Losgistic Regression(       0/10000): loss= 3438.20162231723\n",
      "Losgistic Regression(     100/10000): loss= 2368.58528188815\n",
      "Losgistic Regression(     200/10000): loss= 2164.74372176491\n",
      "Losgistic Regression(     300/10000): loss= 2073.60896950236\n",
      "Losgistic Regression(     400/10000): loss= 2015.82753076316\n",
      "Losgistic Regression(     500/10000): loss= 1972.01656844111\n",
      "Losgistic Regression(     600/10000): loss= 1935.76590789647\n",
      "Losgistic Regression(     700/10000): loss= 1906.62435938728\n",
      "Losgistic Regression(     800/10000): loss= 1880.85765430676\n",
      "Losgistic Regression(     900/10000): loss= 1858.27395243026\n",
      "Losgistic Regression(    1000/10000): loss= 1839.21455857314\n",
      "Losgistic Regression(    1100/10000): loss= 1823.02880658849\n",
      "Losgistic Regression(    1200/10000): loss= 1809.35474653064\n",
      "Losgistic Regression(    1300/10000): loss= 1797.53051749349\n",
      "Losgistic Regression(    1400/10000): loss= 1786.99054473857\n",
      "Losgistic Regression(    1500/10000): loss= 1777.60141462437\n",
      "Losgistic Regression(    1600/10000): loss= 1769.11835874655\n",
      "Losgistic Regression(    1700/10000): loss= 1761.36936313888\n",
      "Losgistic Regression(    1800/10000): loss= 1754.22562008187\n",
      "Losgistic Regression(    1900/10000): loss= 1747.68607661495\n",
      "Losgistic Regression(    2000/10000): loss= 1741.77243701924\n",
      "Losgistic Regression(    2100/10000): loss= 1736.41206927975\n",
      "Losgistic Regression(    2200/10000): loss= 1731.50786310019\n",
      "Losgistic Regression(    2300/10000): loss= 1727.01803223903\n",
      "Losgistic Regression(    2400/10000): loss= 1722.90743767241\n",
      "Losgistic Regression(    2500/10000): loss= 1719.11046455995\n",
      "Losgistic Regression(    2600/10000): loss= 1715.63535697168\n",
      "Losgistic Regression(    2700/10000): loss= 1712.47464807965\n",
      "Losgistic Regression(    2800/10000): loss= 1709.59614488648\n",
      "Losgistic Regression(    2900/10000): loss= 1706.96956332587\n",
      "Losgistic Regression(    3000/10000): loss= 1704.56439296061\n",
      "Losgistic Regression(    3100/10000): loss= 1702.31715594132\n",
      "Losgistic Regression(    3200/10000): loss= 1700.17977354203\n",
      "Losgistic Regression(    3300/10000): loss= 1698.15668007792\n",
      "Losgistic Regression(    3400/10000): loss= 1696.22931522308\n",
      "Losgistic Regression(    3500/10000): loss= 1694.3538354599\n",
      "Losgistic Regression(    3600/10000): loss= 1692.52409983294\n",
      "Losgistic Regression(    3700/10000): loss= 1690.75016641947\n",
      "Losgistic Regression(    3800/10000): loss= 1689.02229169686\n",
      "Losgistic Regression(    3900/10000): loss= 1687.28746799622\n",
      "Losgistic Regression(    4000/10000): loss= 1685.08018884668\n",
      "Losgistic Regression(    4100/10000): loss= 1682.54887153442\n",
      "Losgistic Regression(    4200/10000): loss= 1680.10585327617\n",
      "Losgistic Regression(    4300/10000): loss= 1677.74577045173\n",
      "Losgistic Regression(    4400/10000): loss= 1675.4692383963\n",
      "Losgistic Regression(    4500/10000): loss= 1673.28198497356\n",
      "Losgistic Regression(    4600/10000): loss= 1671.18268175914\n",
      "Losgistic Regression(    4700/10000): loss= 1669.16957228836\n",
      "Losgistic Regression(    4800/10000): loss= 1667.23911872816\n",
      "Losgistic Regression(    4900/10000): loss= 1665.38168759295\n",
      "Losgistic Regression(    5000/10000): loss= 1663.57326397155\n",
      "Losgistic Regression(    5100/10000): loss= 1661.75749268742\n",
      "Losgistic Regression(    5200/10000): loss= 1659.92137613105\n",
      "Losgistic Regression(    5300/10000): loss= 1658.11357907925\n",
      "Losgistic Regression(    5400/10000): loss= 1656.31972084087\n",
      "Losgistic Regression(    5500/10000): loss= 1654.52943335953\n",
      "Losgistic Regression(    5600/10000): loss= 1653.01629995564\n",
      "Losgistic Regression(    5700/10000): loss= 1651.89485893122\n",
      "Losgistic Regression(    5800/10000): loss= 1650.89531283267\n",
      "Losgistic Regression(    5900/10000): loss= 1649.61840069149\n",
      "Losgistic Regression(    6000/10000): loss= 1648.27512751248\n",
      "Losgistic Regression(    6100/10000): loss= 1647.10064081043\n",
      "Losgistic Regression(    6200/10000): loss= 1646.04799773933\n",
      "Losgistic Regression(    6300/10000): loss= 1645.0899047982\n",
      "Losgistic Regression(    6400/10000): loss= 1644.15655745671\n",
      "Losgistic Regression(    6500/10000): loss= 1643.37052790831\n",
      "Losgistic Regression(    6600/10000): loss= 1642.42015189747\n",
      "Losgistic Regression(    6700/10000): loss= 1641.61924846094\n",
      "Losgistic Regression(    6800/10000): loss= 1641.01288538831\n",
      "Losgistic Regression(    6900/10000): loss= 1640.45104537817\n",
      "Losgistic Regression(    7000/10000): loss= 1639.80637206489\n",
      "Losgistic Regression(    7100/10000): loss= 1639.24466280351\n",
      "Losgistic Regression(    7200/10000): loss= 1638.96348592661\n",
      "Losgistic Regression(    7300/10000): loss= 1638.87047553047\n",
      "Losgistic Regression(    7400/10000): loss= 1638.6621505768\n",
      "Losgistic Regression(    7500/10000): loss= 1638.37312355901\n",
      "Losgistic Regression(    7600/10000): loss= 1638.1857792475\n",
      "Losgistic Regression(    7700/10000): loss= 1637.9918398239\n",
      "Losgistic Regression(    7800/10000): loss= 1637.71635729861\n",
      "Losgistic Regression(    7900/10000): loss= 1637.38088580457\n",
      "Losgistic Regression(    8000/10000): loss= 1636.98332000494\n",
      "Losgistic Regression(    8100/10000): loss= 1636.72487654895\n",
      "Losgistic Regression(    8200/10000): loss= 1636.67619016226\n",
      "Losgistic Regression(    8300/10000): loss= 1636.67710643615\n",
      "Totoal number of iterations =  8300\n",
      "Loss                        =  1636.67710644\n",
      "Time for  1th cross validation = 136.193s\n",
      "Training Accuracy         = 0.8504\n",
      "Cross Validation Accuracy = 0.794028\n",
      "Losgistic Regression(       0/10000): loss= 3438.5671189856\n",
      "Losgistic Regression(     100/10000): loss= 2321.42623682973\n",
      "Losgistic Regression(     200/10000): loss= 2116.11296811218\n",
      "Losgistic Regression(     300/10000): loss= 2025.32160119401\n",
      "Losgistic Regression(     400/10000): loss= 1963.89415167178\n",
      "Losgistic Regression(     500/10000): loss= 1918.80401817087\n",
      "Losgistic Regression(     600/10000): loss= 1884.34006419215\n",
      "Losgistic Regression(     700/10000): loss= 1856.91242802465\n",
      "Losgistic Regression(     800/10000): loss= 1834.53866899363\n",
      "Losgistic Regression(     900/10000): loss= 1816.14046400515\n",
      "Losgistic Regression(    1000/10000): loss= 1800.8037905313\n",
      "Losgistic Regression(    1100/10000): loss= 1787.82976115422\n",
      "Losgistic Regression(    1200/10000): loss= 1775.30614739616\n",
      "Losgistic Regression(    1300/10000): loss= 1763.41608566995\n",
      "Losgistic Regression(    1400/10000): loss= 1752.7811813921\n",
      "Losgistic Regression(    1500/10000): loss= 1743.17986949519\n",
      "Losgistic Regression(    1600/10000): loss= 1734.44570324537\n",
      "Losgistic Regression(    1700/10000): loss= 1725.96870030846\n",
      "Losgistic Regression(    1800/10000): loss= 1718.06883724722\n",
      "Losgistic Regression(    1900/10000): loss= 1710.79779095635\n",
      "Losgistic Regression(    2000/10000): loss= 1704.04527320417\n",
      "Losgistic Regression(    2100/10000): loss= 1697.72330272646\n",
      "Losgistic Regression(    2200/10000): loss= 1691.77019650065\n",
      "Losgistic Regression(    2300/10000): loss= 1686.14135830559\n",
      "Losgistic Regression(    2400/10000): loss= 1680.04135800873\n",
      "Losgistic Regression(    2500/10000): loss= 1674.10630566887\n",
      "Losgistic Regression(    2600/10000): loss= 1668.43749949377\n",
      "Losgistic Regression(    2700/10000): loss= 1663.35760245051\n",
      "Losgistic Regression(    2800/10000): loss= 1658.29562372782\n",
      "Losgistic Regression(    2900/10000): loss= 1652.84433429432\n",
      "Losgistic Regression(    3000/10000): loss= 1647.6692041957\n",
      "Losgistic Regression(    3100/10000): loss= 1642.60707493996\n",
      "Losgistic Regression(    3200/10000): loss= 1637.29857404437\n",
      "Losgistic Regression(    3300/10000): loss= 1632.37465890759\n",
      "Losgistic Regression(    3400/10000): loss= 1627.7424635778\n",
      "Losgistic Regression(    3500/10000): loss= 1622.78543080524\n",
      "Losgistic Regression(    3600/10000): loss= 1617.92801569966\n",
      "Losgistic Regression(    3700/10000): loss= 1613.04864224201\n",
      "Losgistic Regression(    3800/10000): loss= 1607.82949095021\n",
      "Losgistic Regression(    3900/10000): loss= 1602.65740373859\n",
      "Losgistic Regression(    4000/10000): loss= 1597.56214543596\n",
      "Losgistic Regression(    4100/10000): loss= 1592.41323400268\n",
      "Losgistic Regression(    4200/10000): loss= 1587.39806479966\n",
      "Losgistic Regression(    4300/10000): loss= 1582.591886309\n",
      "Losgistic Regression(    4400/10000): loss= 1577.76057095712\n",
      "Losgistic Regression(    4500/10000): loss= 1572.9419642585\n",
      "Losgistic Regression(    4600/10000): loss= 1568.16594859583\n",
      "Losgistic Regression(    4700/10000): loss= 1563.18709922229\n",
      "Losgistic Regression(    4800/10000): loss= 1558.01501669423\n",
      "Losgistic Regression(    4900/10000): loss= 1552.92054496777\n",
      "Losgistic Regression(    5000/10000): loss= 1547.83525548711\n",
      "Losgistic Regression(    5100/10000): loss= 1542.6628161257\n",
      "Losgistic Regression(    5200/10000): loss= 1537.40714750125\n",
      "Losgistic Regression(    5300/10000): loss= 1532.10155604455\n",
      "Losgistic Regression(    5400/10000): loss= 1526.73696402422\n",
      "Losgistic Regression(    5500/10000): loss= 1521.20225720934\n",
      "Losgistic Regression(    5600/10000): loss= 1515.54324882685\n",
      "Losgistic Regression(    5700/10000): loss= 1509.92242738186\n",
      "Losgistic Regression(    5800/10000): loss= 1504.27679330881\n",
      "Losgistic Regression(    5900/10000): loss= 1498.50054665893\n",
      "Losgistic Regression(    6000/10000): loss= 1492.67709208187\n",
      "Losgistic Regression(    6100/10000): loss= 1486.87710278394\n",
      "Losgistic Regression(    6200/10000): loss= 1481.03438975966\n",
      "Losgistic Regression(    6300/10000): loss= 1475.10358840692\n",
      "Losgistic Regression(    6400/10000): loss= 1469.15539125857\n",
      "Losgistic Regression(    6500/10000): loss= 1463.2452489606\n",
      "Losgistic Regression(    6600/10000): loss= 1457.32826188421\n",
      "Losgistic Regression(    6700/10000): loss= 1451.39596584417\n",
      "Losgistic Regression(    6800/10000): loss= 1445.52397558467\n",
      "Losgistic Regression(    6900/10000): loss= 1439.71228211422\n",
      "Losgistic Regression(    7000/10000): loss= 1433.85713095119\n",
      "Losgistic Regression(    7100/10000): loss= 1427.98413844219\n",
      "Losgistic Regression(    7200/10000): loss= 1422.15328076399\n",
      "Losgistic Regression(    7300/10000): loss= 1416.32772975411\n",
      "Losgistic Regression(    7400/10000): loss= 1410.51414866942\n",
      "Losgistic Regression(    7500/10000): loss= 1404.76313139098\n",
      "Losgistic Regression(    7600/10000): loss= 1399.09187594771\n",
      "Losgistic Regression(    7700/10000): loss= 1393.45218213325\n",
      "Losgistic Regression(    7800/10000): loss= 1387.84032535804\n",
      "Losgistic Regression(    7900/10000): loss= 1382.29608105021\n",
      "Losgistic Regression(    8000/10000): loss= 1376.79109833637\n",
      "Losgistic Regression(    8100/10000): loss= 1371.30021525541\n",
      "Losgistic Regression(    8200/10000): loss= 1365.84828538222\n",
      "Losgistic Regression(    8300/10000): loss= 1360.46307720325\n",
      "Losgistic Regression(    8400/10000): loss= 1355.13269507566\n",
      "Losgistic Regression(    8500/10000): loss= 1349.84887288633\n",
      "Losgistic Regression(    8600/10000): loss= 1344.64857985853\n",
      "Losgistic Regression(    8700/10000): loss= 1339.55326307925\n",
      "Losgistic Regression(    8800/10000): loss= 1334.53389137172\n",
      "Losgistic Regression(    8900/10000): loss= 1329.59541897115\n",
      "Losgistic Regression(    9000/10000): loss= 1324.77718092465\n",
      "Losgistic Regression(    9100/10000): loss= 1320.06212530122\n",
      "Losgistic Regression(    9200/10000): loss= 1315.41128639641\n",
      "Losgistic Regression(    9300/10000): loss= 1310.83217881163\n",
      "Losgistic Regression(    9400/10000): loss= 1306.33970665892\n",
      "Losgistic Regression(    9500/10000): loss= 1301.91684568409\n",
      "Losgistic Regression(    9600/10000): loss= 1297.57311541336\n",
      "Losgistic Regression(    9700/10000): loss= 1293.32974610669\n",
      "Losgistic Regression(    9800/10000): loss= 1289.16648951204\n",
      "Losgistic Regression(    9900/10000): loss= 1285.07391652835\n",
      "Time for  2th cross validation = 164.273s\n",
      "Training Accuracy         = 0.8564\n",
      "Cross Validation Accuracy = 0.803584\n",
      "Losgistic Regression(       0/10000): loss= 3439.00528690578\n",
      "Losgistic Regression(     100/10000): loss= 2328.79488663219\n",
      "Losgistic Regression(     200/10000): loss= 2134.83791880897\n",
      "Losgistic Regression(     300/10000): loss= 2056.15492651242\n",
      "Losgistic Regression(     400/10000): loss= 2002.3375900623\n",
      "Losgistic Regression(     500/10000): loss= 1960.48019530323\n",
      "Losgistic Regression(     600/10000): loss= 1927.11519641434\n",
      "Losgistic Regression(     700/10000): loss= 1899.8995910392\n",
      "Losgistic Regression(     800/10000): loss= 1877.34095073868\n",
      "Losgistic Regression(     900/10000): loss= 1858.21333569949\n",
      "Losgistic Regression(    1000/10000): loss= 1841.62456294567\n",
      "Losgistic Regression(    1100/10000): loss= 1827.01552996407\n",
      "Losgistic Regression(    1200/10000): loss= 1813.41234259787\n",
      "Losgistic Regression(    1300/10000): loss= 1800.59989014435\n",
      "Losgistic Regression(    1400/10000): loss= 1789.27147701506\n",
      "Losgistic Regression(    1500/10000): loss= 1779.1144616045\n",
      "Losgistic Regression(    1600/10000): loss= 1769.77613424221\n",
      "Losgistic Regression(    1700/10000): loss= 1760.21836806269\n",
      "Losgistic Regression(    1800/10000): loss= 1751.47005986354\n",
      "Losgistic Regression(    1900/10000): loss= 1743.39776699156\n",
      "Losgistic Regression(    2000/10000): loss= 1735.97315824014\n",
      "Losgistic Regression(    2100/10000): loss= 1729.12281318908\n",
      "Losgistic Regression(    2200/10000): loss= 1722.8118760268\n",
      "Losgistic Regression(    2300/10000): loss= 1717.12401275328\n",
      "Losgistic Regression(    2400/10000): loss= 1712.07245333968\n",
      "Losgistic Regression(    2500/10000): loss= 1707.17641584497\n",
      "Losgistic Regression(    2600/10000): loss= 1702.71191740794\n",
      "Losgistic Regression(    2700/10000): loss= 1698.52239792609\n",
      "Losgistic Regression(    2800/10000): loss= 1694.51923742025\n",
      "Losgistic Regression(    2900/10000): loss= 1690.68521558929\n",
      "Losgistic Regression(    3000/10000): loss= 1687.03782045771\n",
      "Losgistic Regression(    3100/10000): loss= 1683.57223098584\n",
      "Losgistic Regression(    3200/10000): loss= 1680.21894426969\n",
      "Losgistic Regression(    3300/10000): loss= 1676.91171781211\n",
      "Losgistic Regression(    3400/10000): loss= 1673.65660664547\n",
      "Losgistic Regression(    3500/10000): loss= 1670.47717589862\n",
      "Losgistic Regression(    3600/10000): loss= 1667.40001255396\n",
      "Losgistic Regression(    3700/10000): loss= 1664.39879111987\n",
      "Losgistic Regression(    3800/10000): loss= 1661.39907559465\n",
      "Losgistic Regression(    3900/10000): loss= 1658.35272293618\n",
      "Losgistic Regression(    4000/10000): loss= 1655.26166318948\n",
      "Losgistic Regression(    4100/10000): loss= 1652.16695762223\n",
      "Losgistic Regression(    4200/10000): loss= 1649.0997920463\n",
      "Losgistic Regression(    4300/10000): loss= 1646.11684200565\n",
      "Losgistic Regression(    4400/10000): loss= 1643.24478911419\n",
      "Losgistic Regression(    4500/10000): loss= 1640.05012629495\n",
      "Losgistic Regression(    4600/10000): loss= 1637.16042069919\n",
      "Losgistic Regression(    4700/10000): loss= 1634.64068821454\n",
      "Losgistic Regression(    4800/10000): loss= 1631.82698602058\n",
      "Losgistic Regression(    4900/10000): loss= 1628.8929545992\n",
      "Losgistic Regression(    5000/10000): loss= 1626.25790198256\n",
      "Losgistic Regression(    5100/10000): loss= 1623.75834961872\n",
      "Losgistic Regression(    5200/10000): loss= 1620.83884331507\n",
      "Losgistic Regression(    5300/10000): loss= 1617.77758090965\n",
      "Losgistic Regression(    5400/10000): loss= 1614.7358296316\n",
      "Losgistic Regression(    5500/10000): loss= 1611.64864343722\n",
      "Losgistic Regression(    5600/10000): loss= 1608.14358319844\n",
      "Losgistic Regression(    5700/10000): loss= 1605.09606041397\n",
      "Losgistic Regression(    5800/10000): loss= 1602.73823990388\n",
      "Losgistic Regression(    5900/10000): loss= 1600.57808494445\n",
      "Losgistic Regression(    6000/10000): loss= 1598.38241222244\n",
      "Losgistic Regression(    6100/10000): loss= 1596.0684281714\n",
      "Losgistic Regression(    6200/10000): loss= 1593.48895484757\n",
      "Losgistic Regression(    6300/10000): loss= 1591.01166522968\n",
      "Losgistic Regression(    6400/10000): loss= 1588.7756359836\n",
      "Losgistic Regression(    6500/10000): loss= 1586.37124776157\n",
      "Losgistic Regression(    6600/10000): loss= 1583.92357556209\n",
      "Losgistic Regression(    6700/10000): loss= 1581.9741340755\n",
      "Losgistic Regression(    6800/10000): loss= 1580.32517723907\n",
      "Losgistic Regression(    6900/10000): loss= 1578.68085931451\n",
      "Losgistic Regression(    7000/10000): loss= 1576.85471427022\n",
      "Losgistic Regression(    7100/10000): loss= 1575.02624556886\n",
      "Losgistic Regression(    7200/10000): loss= 1573.15751167642\n",
      "Losgistic Regression(    7300/10000): loss= 1570.93326176416\n",
      "Losgistic Regression(    7400/10000): loss= 1568.76540718703\n",
      "Losgistic Regression(    7500/10000): loss= 1566.70076973592\n",
      "Losgistic Regression(    7600/10000): loss= 1564.590142311\n",
      "Losgistic Regression(    7700/10000): loss= 1562.79728661375\n",
      "Losgistic Regression(    7800/10000): loss= 1561.47557664877\n",
      "Losgistic Regression(    7900/10000): loss= 1560.38380467511\n",
      "Losgistic Regression(    8000/10000): loss= 1559.42323784201\n",
      "Losgistic Regression(    8100/10000): loss= 1558.65761334526\n",
      "Losgistic Regression(    8200/10000): loss= 1558.16933916356\n",
      "Losgistic Regression(    8300/10000): loss= 1557.77463254534\n",
      "Losgistic Regression(    8400/10000): loss= 1557.20797238477\n",
      "Losgistic Regression(    8500/10000): loss= 1556.43810449063\n",
      "Losgistic Regression(    8600/10000): loss= 1555.62346135706\n",
      "Losgistic Regression(    8700/10000): loss= 1555.02197719129\n",
      "Losgistic Regression(    8800/10000): loss= 1554.60813514468\n",
      "Losgistic Regression(    8900/10000): loss= 1554.36321079999\n",
      "Losgistic Regression(    9000/10000): loss= 1554.33950646614\n",
      "Losgistic Regression(    9100/10000): loss= 1554.34101456991\n",
      "Totoal number of iterations =  9100\n",
      "Loss                        =  1554.34101457\n",
      "Time for  3th cross validation = 150.926s\n",
      "Training Accuracy         = 0.8536\n",
      "Cross Validation Accuracy = 0.805448\n",
      "Losgistic Regression(       0/10000): loss= 3439.90417004716\n",
      "Losgistic Regression(     100/10000): loss= 2309.4106845367\n",
      "Losgistic Regression(     200/10000): loss= 2119.9554498385\n",
      "Losgistic Regression(     300/10000): loss= 2036.53170665932\n",
      "Losgistic Regression(     400/10000): loss= 1967.83224815826\n",
      "Losgistic Regression(     500/10000): loss= 1910.99633104121\n",
      "Losgistic Regression(     600/10000): loss= 1865.19065834839\n",
      "Losgistic Regression(     700/10000): loss= 1826.41836410906\n",
      "Losgistic Regression(     800/10000): loss= 1791.90585990756\n",
      "Losgistic Regression(     900/10000): loss= 1765.16853211037\n",
      "Losgistic Regression(    1000/10000): loss= 1743.91711534188\n",
      "Losgistic Regression(    1100/10000): loss= 1724.43052195393\n",
      "Losgistic Regression(    1200/10000): loss= 1709.13833290662\n",
      "Losgistic Regression(    1300/10000): loss= 1696.45394380393\n",
      "Losgistic Regression(    1400/10000): loss= 1685.1503601362\n",
      "Losgistic Regression(    1500/10000): loss= 1676.63195455905\n",
      "Losgistic Regression(    1600/10000): loss= 1670.44264393253\n",
      "Losgistic Regression(    1700/10000): loss= 1665.29396401083\n",
      "Losgistic Regression(    1800/10000): loss= 1661.28331054897\n",
      "Losgistic Regression(    1900/10000): loss= 1658.11596751886\n",
      "Losgistic Regression(    2000/10000): loss= 1654.43093624709\n",
      "Losgistic Regression(    2100/10000): loss= 1651.06223422191\n",
      "Losgistic Regression(    2200/10000): loss= 1648.33407817813\n",
      "Losgistic Regression(    2300/10000): loss= 1646.0570781432\n",
      "Losgistic Regression(    2400/10000): loss= 1644.20454394571\n",
      "Losgistic Regression(    2500/10000): loss= 1642.74085048374\n",
      "Losgistic Regression(    2600/10000): loss= 1641.36022774254\n",
      "Losgistic Regression(    2700/10000): loss= 1640.13042141143\n",
      "Losgistic Regression(    2800/10000): loss= 1639.11640498458\n",
      "Losgistic Regression(    2900/10000): loss= 1638.18001301199\n",
      "Losgistic Regression(    3000/10000): loss= 1637.32284849347\n",
      "Losgistic Regression(    3100/10000): loss= 1636.64130125888\n",
      "Losgistic Regression(    3200/10000): loss= 1635.9710374858\n",
      "Losgistic Regression(    3300/10000): loss= 1635.22662470988\n",
      "Losgistic Regression(    3400/10000): loss= 1634.42677057737\n",
      "Losgistic Regression(    3500/10000): loss= 1633.64813619322\n",
      "Losgistic Regression(    3600/10000): loss= 1632.84745839736\n",
      "Losgistic Regression(    3700/10000): loss= 1631.97933925738\n",
      "Losgistic Regression(    3800/10000): loss= 1631.05828776349\n",
      "Losgistic Regression(    3900/10000): loss= 1630.1552093748\n",
      "Losgistic Regression(    4000/10000): loss= 1629.24515175974\n",
      "Losgistic Regression(    4100/10000): loss= 1628.31733920084\n",
      "Losgistic Regression(    4200/10000): loss= 1627.43804679999\n",
      "Losgistic Regression(    4300/10000): loss= 1626.55744408434\n",
      "Losgistic Regression(    4400/10000): loss= 1625.63945193809\n",
      "Losgistic Regression(    4500/10000): loss= 1624.70331234986\n",
      "Losgistic Regression(    4600/10000): loss= 1623.75204915803\n",
      "Losgistic Regression(    4700/10000): loss= 1622.81460652108\n",
      "Losgistic Regression(    4800/10000): loss= 1621.92522557378\n",
      "Losgistic Regression(    4900/10000): loss= 1621.07603894497\n",
      "Losgistic Regression(    5000/10000): loss= 1620.24870608116\n",
      "Losgistic Regression(    5100/10000): loss= 1619.43592579019\n",
      "Losgistic Regression(    5200/10000): loss= 1618.62479678645\n",
      "Losgistic Regression(    5300/10000): loss= 1617.70064148697\n",
      "Losgistic Regression(    5400/10000): loss= 1616.7827626426\n",
      "Losgistic Regression(    5500/10000): loss= 1615.9042968322\n",
      "Losgistic Regression(    5600/10000): loss= 1615.05901294711\n",
      "Losgistic Regression(    5700/10000): loss= 1614.26708132382\n",
      "Losgistic Regression(    5800/10000): loss= 1613.54226281013\n",
      "Losgistic Regression(    5900/10000): loss= 1612.87738746566\n",
      "Losgistic Regression(    6000/10000): loss= 1612.26711102091\n",
      "Losgistic Regression(    6100/10000): loss= 1611.72849157714\n",
      "Losgistic Regression(    6200/10000): loss= 1611.26659568057\n",
      "Losgistic Regression(    6300/10000): loss= 1610.88213093624\n",
      "Losgistic Regression(    6400/10000): loss= 1610.57940785165\n",
      "Losgistic Regression(    6500/10000): loss= 1610.3540115578\n",
      "Losgistic Regression(    6600/10000): loss= 1610.2032302512\n",
      "Losgistic Regression(    6700/10000): loss= 1610.12381176973\n",
      "Losgistic Regression(    6800/10000): loss= 1610.11103211613\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  1610.11103212\n",
      "Time for  4th cross validation = 111.459s\n",
      "Training Accuracy         = 0.8542\n",
      "Cross Validation Accuracy = 0.805876\n",
      "*************** ([0.85899999999999999, 0.85040000000000004, 0.85640000000000005, 0.85360000000000003, 0.85419999999999996], [0.80532800000000004, 0.79402799999999996, 0.80358399999999996, 0.80544800000000005, 0.80587600000000004])\n",
      "Losgistic Regression(       0/10000): loss= 3436.25677405945\n",
      "Losgistic Regression(     100/10000): loss= 2335.86980196649\n",
      "Losgistic Regression(     200/10000): loss= 2121.80042804355\n",
      "Losgistic Regression(     300/10000): loss= 2040.23171350559\n",
      "Losgistic Regression(     400/10000): loss= 1984.00662050533\n",
      "Losgistic Regression(     500/10000): loss= 1941.95608750388\n",
      "Losgistic Regression(     600/10000): loss= 1908.66773481697\n",
      "Losgistic Regression(     700/10000): loss= 1881.37729513754\n",
      "Losgistic Regression(     800/10000): loss= 1858.85887471543\n",
      "Losgistic Regression(     900/10000): loss= 1840.17800325565\n",
      "Losgistic Regression(    1000/10000): loss= 1824.31236919769\n",
      "Losgistic Regression(    1100/10000): loss= 1810.48163889281\n",
      "Losgistic Regression(    1200/10000): loss= 1798.221789176\n",
      "Losgistic Regression(    1300/10000): loss= 1787.18793842039\n",
      "Losgistic Regression(    1400/10000): loss= 1777.39961121762\n",
      "Losgistic Regression(    1500/10000): loss= 1769.20695523461\n",
      "Losgistic Regression(    1600/10000): loss= 1761.74485837634\n",
      "Losgistic Regression(    1700/10000): loss= 1755.19190381914\n",
      "Losgistic Regression(    1800/10000): loss= 1749.40206834062\n",
      "Losgistic Regression(    1900/10000): loss= 1744.26100617087\n",
      "Losgistic Regression(    2000/10000): loss= 1739.68558049042\n",
      "Losgistic Regression(    2100/10000): loss= 1735.56010598771\n",
      "Losgistic Regression(    2200/10000): loss= 1731.77318562011\n",
      "Losgistic Regression(    2300/10000): loss= 1728.27439875978\n",
      "Losgistic Regression(    2400/10000): loss= 1724.83220212954\n",
      "Losgistic Regression(    2500/10000): loss= 1721.75074455585\n",
      "Losgistic Regression(    2600/10000): loss= 1718.89890369087\n",
      "Losgistic Regression(    2700/10000): loss= 1715.72021925446\n",
      "Losgistic Regression(    2800/10000): loss= 1712.80756308456\n",
      "Losgistic Regression(    2900/10000): loss= 1710.13340137883\n",
      "Losgistic Regression(    3000/10000): loss= 1707.62448826364\n",
      "Losgistic Regression(    3100/10000): loss= 1705.31691883618\n",
      "Losgistic Regression(    3200/10000): loss= 1703.21156040569\n",
      "Losgistic Regression(    3300/10000): loss= 1701.13386489059\n",
      "Losgistic Regression(    3400/10000): loss= 1698.73701365915\n",
      "Losgistic Regression(    3500/10000): loss= 1696.35347050221\n",
      "Losgistic Regression(    3600/10000): loss= 1694.04706969958\n",
      "Losgistic Regression(    3700/10000): loss= 1691.65874567493\n",
      "Losgistic Regression(    3800/10000): loss= 1689.28799833783\n",
      "Losgistic Regression(    3900/10000): loss= 1686.96754300216\n",
      "Losgistic Regression(    4000/10000): loss= 1684.6467338772\n",
      "Losgistic Regression(    4100/10000): loss= 1682.31898873522\n",
      "Losgistic Regression(    4200/10000): loss= 1679.99398233179\n",
      "Losgistic Regression(    4300/10000): loss= 1677.67903424363\n",
      "Losgistic Regression(    4400/10000): loss= 1675.39239181833\n",
      "Losgistic Regression(    4500/10000): loss= 1673.14952961517\n",
      "Losgistic Regression(    4600/10000): loss= 1670.94470312156\n",
      "Losgistic Regression(    4700/10000): loss= 1668.78688920491\n",
      "Losgistic Regression(    4800/10000): loss= 1666.4763413327\n",
      "Losgistic Regression(    4900/10000): loss= 1664.21406516244\n",
      "Losgistic Regression(    5000/10000): loss= 1661.98809809092\n",
      "Losgistic Regression(    5100/10000): loss= 1659.88843043356\n",
      "Losgistic Regression(    5200/10000): loss= 1657.88605141405\n",
      "Losgistic Regression(    5300/10000): loss= 1656.02326624131\n",
      "Losgistic Regression(    5400/10000): loss= 1654.33288531413\n",
      "Losgistic Regression(    5500/10000): loss= 1652.76118529306\n",
      "Losgistic Regression(    5600/10000): loss= 1651.24692586676\n",
      "Losgistic Regression(    5700/10000): loss= 1649.79406707945\n",
      "Losgistic Regression(    5800/10000): loss= 1648.44012669257\n",
      "Losgistic Regression(    5900/10000): loss= 1647.17691164055\n",
      "Losgistic Regression(    6000/10000): loss= 1645.9562912252\n",
      "Losgistic Regression(    6100/10000): loss= 1644.76142984364\n",
      "Losgistic Regression(    6200/10000): loss= 1643.60984896567\n",
      "Losgistic Regression(    6300/10000): loss= 1642.55656217674\n",
      "Losgistic Regression(    6400/10000): loss= 1641.3855636109\n",
      "Losgistic Regression(    6500/10000): loss= 1640.25367100707\n",
      "Losgistic Regression(    6600/10000): loss= 1639.20817870427\n",
      "Losgistic Regression(    6700/10000): loss= 1638.23895740241\n",
      "Losgistic Regression(    6800/10000): loss= 1637.33220216111\n",
      "Losgistic Regression(    6900/10000): loss= 1636.4989810444\n",
      "Losgistic Regression(    7000/10000): loss= 1635.73310621339\n",
      "Losgistic Regression(    7100/10000): loss= 1635.00625166989\n",
      "Losgistic Regression(    7200/10000): loss= 1634.28312651377\n",
      "Losgistic Regression(    7300/10000): loss= 1633.57174158869\n",
      "Losgistic Regression(    7400/10000): loss= 1632.89953866676\n",
      "Losgistic Regression(    7500/10000): loss= 1632.25469956931\n",
      "Losgistic Regression(    7600/10000): loss= 1631.64461395293\n",
      "Losgistic Regression(    7700/10000): loss= 1631.05275368144\n",
      "Losgistic Regression(    7800/10000): loss= 1630.42750304601\n",
      "Losgistic Regression(    7900/10000): loss= 1629.84587856403\n",
      "Losgistic Regression(    8000/10000): loss= 1629.31451757153\n",
      "Losgistic Regression(    8100/10000): loss= 1628.85183903196\n",
      "Losgistic Regression(    8200/10000): loss= 1628.41803797457\n",
      "Losgistic Regression(    8300/10000): loss= 1628.01695796498\n",
      "Losgistic Regression(    8400/10000): loss= 1627.63229254191\n",
      "Losgistic Regression(    8500/10000): loss= 1627.23841024447\n",
      "Losgistic Regression(    8600/10000): loss= 1626.9186794582\n",
      "Losgistic Regression(    8700/10000): loss= 1626.66981572356\n",
      "Losgistic Regression(    8800/10000): loss= 1626.4630315708\n",
      "Losgistic Regression(    8900/10000): loss= 1626.27517178574\n",
      "Losgistic Regression(    9000/10000): loss= 1626.09341700962\n",
      "Losgistic Regression(    9100/10000): loss= 1625.91973534606\n",
      "Losgistic Regression(    9200/10000): loss= 1625.79044833181\n",
      "Losgistic Regression(    9300/10000): loss= 1625.66250573245\n",
      "Losgistic Regression(    9400/10000): loss= 1625.50616954028\n",
      "Losgistic Regression(    9500/10000): loss= 1625.36115503191\n",
      "Losgistic Regression(    9600/10000): loss= 1625.21831705335\n",
      "Losgistic Regression(    9700/10000): loss= 1625.07325198847\n",
      "Losgistic Regression(    9800/10000): loss= 1624.94536353975\n",
      "Losgistic Regression(    9900/10000): loss= 1624.80086214702\n",
      "Time for  0th cross validation = 163.531s\n",
      "Training Accuracy         = 0.8584\n",
      "Cross Validation Accuracy = 0.805716\n",
      "Losgistic Regression(       0/10000): loss= 3438.20461692028\n",
      "Losgistic Regression(     100/10000): loss= 2369.10240163811\n",
      "Losgistic Regression(     200/10000): loss= 2165.75408897889\n",
      "Losgistic Regression(     300/10000): loss= 2075.09826115574\n",
      "Losgistic Regression(     400/10000): loss= 2017.71951411503\n",
      "Losgistic Regression(     500/10000): loss= 1974.32738494594\n",
      "Losgistic Regression(     600/10000): loss= 1938.49099446413\n",
      "Losgistic Regression(     700/10000): loss= 1909.75877037112\n",
      "Losgistic Regression(     800/10000): loss= 1884.43328787432\n",
      "Losgistic Regression(     900/10000): loss= 1862.245503604\n",
      "Losgistic Regression(    1000/10000): loss= 1843.56697801453\n",
      "Losgistic Regression(    1100/10000): loss= 1827.75083138911\n",
      "Losgistic Regression(    1200/10000): loss= 1814.4364878671\n",
      "Losgistic Regression(    1300/10000): loss= 1802.94863606913\n",
      "Losgistic Regression(    1400/10000): loss= 1792.73068745835\n",
      "Losgistic Regression(    1500/10000): loss= 1783.64808584376\n",
      "Losgistic Regression(    1600/10000): loss= 1775.46323095904\n",
      "Losgistic Regression(    1700/10000): loss= 1768.01234931218\n",
      "Losgistic Regression(    1800/10000): loss= 1761.15434017505\n",
      "Losgistic Regression(    1900/10000): loss= 1754.90612867594\n",
      "Losgistic Regression(    2000/10000): loss= 1749.28699331786\n",
      "Losgistic Regression(    2100/10000): loss= 1744.20454826549\n",
      "Losgistic Regression(    2200/10000): loss= 1739.55674123883\n",
      "Losgistic Regression(    2300/10000): loss= 1735.31823986773\n",
      "Losgistic Regression(    2400/10000): loss= 1731.43691314594\n",
      "Losgistic Regression(    2500/10000): loss= 1727.84871214146\n",
      "Losgistic Regression(    2600/10000): loss= 1724.56541652757\n",
      "Losgistic Regression(    2700/10000): loss= 1721.58235816886\n",
      "Losgistic Regression(    2800/10000): loss= 1718.86166778866\n",
      "Losgistic Regression(    2900/10000): loss= 1716.38185102627\n",
      "Losgistic Regression(    3000/10000): loss= 1714.11686545998\n",
      "Losgistic Regression(    3100/10000): loss= 1712.00784216631\n",
      "Losgistic Regression(    3200/10000): loss= 1710.00732694955\n",
      "Losgistic Regression(    3300/10000): loss= 1708.11312760268\n",
      "Losgistic Regression(    3400/10000): loss= 1706.30956322188\n",
      "Losgistic Regression(    3500/10000): loss= 1704.54578605668\n",
      "Losgistic Regression(    3600/10000): loss= 1702.83304060095\n",
      "Losgistic Regression(    3700/10000): loss= 1701.18149678249\n",
      "Losgistic Regression(    3800/10000): loss= 1699.57714659854\n",
      "Losgistic Regression(    3900/10000): loss= 1698.0198602073\n",
      "Losgistic Regression(    4000/10000): loss= 1696.50662801712\n",
      "Losgistic Regression(    4100/10000): loss= 1694.93789555558\n",
      "Losgistic Regression(    4200/10000): loss= 1692.72750166909\n",
      "Losgistic Regression(    4300/10000): loss= 1690.50171349774\n",
      "Losgistic Regression(    4400/10000): loss= 1688.36609697662\n",
      "Losgistic Regression(    4500/10000): loss= 1686.34802117035\n",
      "Losgistic Regression(    4600/10000): loss= 1684.41949619112\n",
      "Losgistic Regression(    4700/10000): loss= 1682.55608482756\n",
      "Losgistic Regression(    4800/10000): loss= 1680.78911941895\n",
      "Losgistic Regression(    4900/10000): loss= 1679.14334804781\n",
      "Losgistic Regression(    5000/10000): loss= 1677.59001905163\n",
      "Losgistic Regression(    5100/10000): loss= 1676.09611702748\n",
      "Losgistic Regression(    5200/10000): loss= 1674.57017994031\n",
      "Losgistic Regression(    5300/10000): loss= 1673.04104617239\n",
      "Losgistic Regression(    5400/10000): loss= 1671.52497867699\n",
      "Losgistic Regression(    5500/10000): loss= 1669.98504125882\n",
      "Losgistic Regression(    5600/10000): loss= 1668.77193223079\n",
      "Losgistic Regression(    5700/10000): loss= 1667.93896049891\n",
      "Losgistic Regression(    5800/10000): loss= 1667.19216926031\n",
      "Losgistic Regression(    5900/10000): loss= 1666.16716729588\n",
      "Losgistic Regression(    6000/10000): loss= 1665.109395439\n",
      "Losgistic Regression(    6100/10000): loss= 1664.18351500628\n",
      "Losgistic Regression(    6200/10000): loss= 1663.35751310715\n",
      "Losgistic Regression(    6300/10000): loss= 1662.59466534645\n",
      "Losgistic Regression(    6400/10000): loss= 1661.87261649635\n",
      "Losgistic Regression(    6500/10000): loss= 1661.27834100281\n",
      "Losgistic Regression(    6600/10000): loss= 1660.56053223174\n",
      "Losgistic Regression(    6700/10000): loss= 1659.99122441812\n",
      "Losgistic Regression(    6800/10000): loss= 1659.57439612135\n",
      "Losgistic Regression(    6900/10000): loss= 1659.1551211978\n",
      "Losgistic Regression(    7000/10000): loss= 1658.64419381797\n",
      "Losgistic Regression(    7100/10000): loss= 1658.24785490398\n",
      "Losgistic Regression(    7200/10000): loss= 1658.14096729252\n",
      "Losgistic Regression(    7300/10000): loss= 1658.14235005809\n",
      "Totoal number of iterations =  7300\n",
      "Loss                        =  1658.14235006\n",
      "Time for  1th cross validation = 119.649s\n",
      "Training Accuracy         = 0.8492\n",
      "Cross Validation Accuracy = 0.794768\n",
      "Losgistic Regression(       0/10000): loss= 3438.57003306499\n",
      "Losgistic Regression(     100/10000): loss= 2321.9325393893\n",
      "Losgistic Regression(     200/10000): loss= 2117.08580333844\n",
      "Losgistic Regression(     300/10000): loss= 2026.71425373974\n",
      "Losgistic Regression(     400/10000): loss= 1965.69545935213\n",
      "Losgistic Regression(     500/10000): loss= 1920.99839778236\n",
      "Losgistic Regression(     600/10000): loss= 1886.90874307486\n",
      "Losgistic Regression(     700/10000): loss= 1859.82954648904\n",
      "Losgistic Regression(     800/10000): loss= 1837.7862932511\n",
      "Losgistic Regression(     900/10000): loss= 1819.71808306069\n",
      "Losgistic Regression(    1000/10000): loss= 1804.70047136261\n",
      "Losgistic Regression(    1100/10000): loss= 1792.03412143283\n",
      "Losgistic Regression(    1200/10000): loss= 1779.82454708483\n",
      "Losgistic Regression(    1300/10000): loss= 1768.23802634108\n",
      "Losgistic Regression(    1400/10000): loss= 1757.91507158973\n",
      "Losgistic Regression(    1500/10000): loss= 1748.66523493717\n",
      "Losgistic Regression(    1600/10000): loss= 1740.29538967969\n",
      "Losgistic Regression(    1700/10000): loss= 1732.34890890956\n",
      "Losgistic Regression(    1800/10000): loss= 1724.84229396815\n",
      "Losgistic Regression(    1900/10000): loss= 1717.96683519455\n",
      "Losgistic Regression(    2000/10000): loss= 1711.6149155481\n",
      "Losgistic Regression(    2100/10000): loss= 1705.69411013213\n",
      "Losgistic Regression(    2200/10000): loss= 1700.14557127645\n",
      "Losgistic Regression(    2300/10000): loss= 1694.92691739829\n",
      "Losgistic Regression(    2400/10000): loss= 1689.78873670895\n",
      "Losgistic Regression(    2500/10000): loss= 1684.46845672906\n",
      "Losgistic Regression(    2600/10000): loss= 1679.39226644483\n",
      "Losgistic Regression(    2700/10000): loss= 1674.80651747311\n",
      "Losgistic Regression(    2800/10000): loss= 1670.19795218693\n",
      "Losgistic Regression(    2900/10000): loss= 1665.23698146726\n",
      "Losgistic Regression(    3000/10000): loss= 1660.60927778751\n",
      "Losgistic Regression(    3100/10000): loss= 1656.00750851585\n",
      "Losgistic Regression(    3200/10000): loss= 1651.23034876555\n",
      "Losgistic Regression(    3300/10000): loss= 1646.87908573046\n",
      "Losgistic Regression(    3400/10000): loss= 1642.71128536844\n",
      "Losgistic Regression(    3500/10000): loss= 1638.24713594737\n",
      "Losgistic Regression(    3600/10000): loss= 1633.90338749287\n",
      "Losgistic Regression(    3700/10000): loss= 1629.45863693281\n",
      "Losgistic Regression(    3800/10000): loss= 1624.69929240448\n",
      "Losgistic Regression(    3900/10000): loss= 1619.98178610782\n",
      "Losgistic Regression(    4000/10000): loss= 1615.36362360688\n",
      "Losgistic Regression(    4100/10000): loss= 1610.68811500775\n",
      "Losgistic Regression(    4200/10000): loss= 1606.13888277486\n",
      "Losgistic Regression(    4300/10000): loss= 1601.75039488196\n",
      "Losgistic Regression(    4400/10000): loss= 1597.40536794894\n",
      "Losgistic Regression(    4500/10000): loss= 1593.00407583391\n",
      "Losgistic Regression(    4600/10000): loss= 1588.56061520212\n",
      "Losgistic Regression(    4700/10000): loss= 1584.0839761891\n",
      "Losgistic Regression(    4800/10000): loss= 1579.58156558987\n",
      "Losgistic Regression(    4900/10000): loss= 1574.89016712996\n",
      "Losgistic Regression(    5000/10000): loss= 1570.17988941911\n",
      "Losgistic Regression(    5100/10000): loss= 1565.44904006939\n",
      "Losgistic Regression(    5200/10000): loss= 1560.62691750104\n",
      "Losgistic Regression(    5300/10000): loss= 1555.78252943817\n",
      "Losgistic Regression(    5400/10000): loss= 1550.90371879168\n",
      "Losgistic Regression(    5500/10000): loss= 1545.85612363749\n",
      "Losgistic Regression(    5600/10000): loss= 1540.73363111031\n",
      "Losgistic Regression(    5700/10000): loss= 1535.67506776233\n",
      "Losgistic Regression(    5800/10000): loss= 1530.54667843945\n",
      "Losgistic Regression(    5900/10000): loss= 1525.27728621454\n",
      "Losgistic Regression(    6000/10000): loss= 1520.00369232475\n",
      "Losgistic Regression(    6100/10000): loss= 1514.74588342375\n",
      "Losgistic Regression(    6200/10000): loss= 1509.40978793809\n",
      "Losgistic Regression(    6300/10000): loss= 1504.00303441639\n",
      "Losgistic Regression(    6400/10000): loss= 1498.61829691527\n",
      "Losgistic Regression(    6500/10000): loss= 1493.25473075319\n",
      "Losgistic Regression(    6600/10000): loss= 1487.82385605029\n",
      "Losgistic Regression(    6700/10000): loss= 1482.3963301437\n",
      "Losgistic Regression(    6800/10000): loss= 1477.02736596239\n",
      "Losgistic Regression(    6900/10000): loss= 1471.65481250419\n",
      "Losgistic Regression(    7000/10000): loss= 1466.22989403218\n",
      "Losgistic Regression(    7100/10000): loss= 1460.80665666794\n",
      "Losgistic Regression(    7200/10000): loss= 1455.3871928663\n",
      "Losgistic Regression(    7300/10000): loss= 1449.93291726762\n",
      "Losgistic Regression(    7400/10000): loss= 1444.51669879306\n",
      "Losgistic Regression(    7500/10000): loss= 1439.15824808687\n",
      "Losgistic Regression(    7600/10000): loss= 1433.84640557236\n",
      "Losgistic Regression(    7700/10000): loss= 1428.56030348499\n",
      "Losgistic Regression(    7800/10000): loss= 1423.30535952637\n",
      "Losgistic Regression(    7900/10000): loss= 1418.10808040588\n",
      "Losgistic Regression(    8000/10000): loss= 1412.90818447764\n",
      "Losgistic Regression(    8100/10000): loss= 1407.72180528211\n",
      "Losgistic Regression(    8200/10000): loss= 1402.59208450402\n",
      "Losgistic Regression(    8300/10000): loss= 1397.50937175095\n",
      "Losgistic Regression(    8400/10000): loss= 1392.44698416876\n",
      "Losgistic Regression(    8500/10000): loss= 1387.41760263588\n",
      "Losgistic Regression(    8600/10000): loss= 1382.48356426352\n",
      "Losgistic Regression(    8700/10000): loss= 1377.62679967\n",
      "Losgistic Regression(    8800/10000): loss= 1372.84156442324\n",
      "Losgistic Regression(    8900/10000): loss= 1368.15143098485\n",
      "Losgistic Regression(    9000/10000): loss= 1363.57234419351\n",
      "Losgistic Regression(    9100/10000): loss= 1359.08139712783\n",
      "Losgistic Regression(    9200/10000): loss= 1354.65702984486\n",
      "Losgistic Regression(    9300/10000): loss= 1350.32387030114\n",
      "Losgistic Regression(    9400/10000): loss= 1346.09577851685\n",
      "Losgistic Regression(    9500/10000): loss= 1341.9488302014\n",
      "Losgistic Regression(    9600/10000): loss= 1337.89832891546\n",
      "Losgistic Regression(    9700/10000): loss= 1333.98468213105\n",
      "Losgistic Regression(    9800/10000): loss= 1330.20219492108\n",
      "Losgistic Regression(    9900/10000): loss= 1326.52920589225\n",
      "Time for  2th cross validation = 163.619s\n",
      "Training Accuracy         = 0.8562\n",
      "Cross Validation Accuracy = 0.804136\n",
      "Losgistic Regression(       0/10000): loss= 3439.00821969135\n",
      "Losgistic Regression(     100/10000): loss= 2329.29758573702\n",
      "Losgistic Regression(     200/10000): loss= 2135.78046361515\n",
      "Losgistic Regression(     300/10000): loss= 2057.45646452031\n",
      "Losgistic Regression(     400/10000): loss= 2003.97925525691\n",
      "Losgistic Regression(     500/10000): loss= 1962.45759271285\n",
      "Losgistic Regression(     600/10000): loss= 1929.4244879401\n",
      "Losgistic Regression(     700/10000): loss= 1902.53943486734\n",
      "Losgistic Regression(     800/10000): loss= 1880.30672482938\n",
      "Losgistic Regression(     900/10000): loss= 1861.49058420775\n",
      "Losgistic Regression(    1000/10000): loss= 1845.21073963081\n",
      "Losgistic Regression(    1100/10000): loss= 1830.92104984872\n",
      "Losgistic Regression(    1200/10000): loss= 1817.65581131719\n",
      "Losgistic Regression(    1300/10000): loss= 1805.15962629651\n",
      "Losgistic Regression(    1400/10000): loss= 1794.15679097796\n",
      "Losgistic Regression(    1500/10000): loss= 1784.34263315446\n",
      "Losgistic Regression(    1600/10000): loss= 1775.49437989831\n",
      "Losgistic Regression(    1700/10000): loss= 1766.39081283313\n",
      "Losgistic Regression(    1800/10000): loss= 1757.99740594018\n",
      "Losgistic Regression(    1900/10000): loss= 1750.26070778678\n",
      "Losgistic Regression(    2000/10000): loss= 1743.15834594247\n",
      "Losgistic Regression(    2100/10000): loss= 1736.60585090714\n",
      "Losgistic Regression(    2200/10000): loss= 1730.56380414446\n",
      "Losgistic Regression(    2300/10000): loss= 1725.16846868092\n",
      "Losgistic Regression(    2400/10000): loss= 1720.42106507963\n",
      "Losgistic Regression(    2500/10000): loss= 1716.12504139908\n",
      "Losgistic Regression(    2600/10000): loss= 1711.97895999584\n",
      "Losgistic Regression(    2700/10000): loss= 1708.10147465179\n",
      "Losgistic Regression(    2800/10000): loss= 1704.37586766379\n",
      "Losgistic Regression(    2900/10000): loss= 1700.8123556411\n",
      "Losgistic Regression(    3000/10000): loss= 1697.42379062033\n",
      "Losgistic Regression(    3100/10000): loss= 1694.20085951526\n",
      "Losgistic Regression(    3200/10000): loss= 1691.06194727568\n",
      "Losgistic Regression(    3300/10000): loss= 1687.95397883493\n",
      "Losgistic Regression(    3400/10000): loss= 1684.8951132836\n",
      "Losgistic Regression(    3500/10000): loss= 1681.91861498871\n",
      "Losgistic Regression(    3600/10000): loss= 1679.0529797122\n",
      "Losgistic Regression(    3700/10000): loss= 1676.27294928684\n",
      "Losgistic Regression(    3800/10000): loss= 1673.47762114219\n",
      "Losgistic Regression(    3900/10000): loss= 1670.61384124106\n",
      "Losgistic Regression(    4000/10000): loss= 1667.71137819159\n",
      "Losgistic Regression(    4100/10000): loss= 1664.8356028878\n",
      "Losgistic Regression(    4200/10000): loss= 1662.007482203\n",
      "Losgistic Regression(    4300/10000): loss= 1659.32515676189\n",
      "Losgistic Regression(    4400/10000): loss= 1656.71877187353\n",
      "Losgistic Regression(    4500/10000): loss= 1654.07306114743\n",
      "Losgistic Regression(    4600/10000): loss= 1651.85485034523\n",
      "Losgistic Regression(    4700/10000): loss= 1649.85977595954\n",
      "Losgistic Regression(    4800/10000): loss= 1647.5542203731\n",
      "Losgistic Regression(    4900/10000): loss= 1645.13799370036\n",
      "Losgistic Regression(    5000/10000): loss= 1643.01286699169\n",
      "Losgistic Regression(    5100/10000): loss= 1640.8743753121\n",
      "Losgistic Regression(    5200/10000): loss= 1638.28439881869\n",
      "Losgistic Regression(    5300/10000): loss= 1635.65133100961\n",
      "Losgistic Regression(    5400/10000): loss= 1633.44657878141\n",
      "Losgistic Regression(    5500/10000): loss= 1631.36931345363\n",
      "Losgistic Regression(    5600/10000): loss= 1629.1280157145\n",
      "Losgistic Regression(    5700/10000): loss= 1627.20923155233\n",
      "Losgistic Regression(    5800/10000): loss= 1625.3928377992\n",
      "Losgistic Regression(    5900/10000): loss= 1623.65548670725\n",
      "Losgistic Regression(    6000/10000): loss= 1621.85047080185\n",
      "Losgistic Regression(    6100/10000): loss= 1619.79884955384\n",
      "Losgistic Regression(    6200/10000): loss= 1617.52793159462\n",
      "Losgistic Regression(    6300/10000): loss= 1615.48252675286\n",
      "Losgistic Regression(    6400/10000): loss= 1613.66244572497\n",
      "Losgistic Regression(    6500/10000): loss= 1611.82483497397\n",
      "Losgistic Regression(    6600/10000): loss= 1610.09266160776\n",
      "Losgistic Regression(    6700/10000): loss= 1608.69701249245\n",
      "Losgistic Regression(    6800/10000): loss= 1607.5286783341\n",
      "Losgistic Regression(    6900/10000): loss= 1606.28998321218\n",
      "Losgistic Regression(    7000/10000): loss= 1604.71231990626\n",
      "Losgistic Regression(    7100/10000): loss= 1603.24150545648\n",
      "Losgistic Regression(    7200/10000): loss= 1601.75990065172\n",
      "Losgistic Regression(    7300/10000): loss= 1600.03605813079\n",
      "Losgistic Regression(    7400/10000): loss= 1598.44825473082\n",
      "Losgistic Regression(    7500/10000): loss= 1596.96127220995\n",
      "Losgistic Regression(    7600/10000): loss= 1595.55085599714\n",
      "Losgistic Regression(    7700/10000): loss= 1594.47712155468\n",
      "Losgistic Regression(    7800/10000): loss= 1593.74548997875\n",
      "Losgistic Regression(    7900/10000): loss= 1593.21132573515\n",
      "Losgistic Regression(    8000/10000): loss= 1592.79496829149\n",
      "Losgistic Regression(    8100/10000): loss= 1592.53559320405\n",
      "Losgistic Regression(    8200/10000): loss= 1592.50712248332\n",
      "Losgistic Regression(    8300/10000): loss= 1592.50923349577\n",
      "Totoal number of iterations =  8300\n",
      "Loss                        =  1592.5092335\n",
      "Time for  3th cross validation = 136.526s\n",
      "Training Accuracy         = 0.8522\n",
      "Cross Validation Accuracy = 0.806132\n",
      "Losgistic Regression(       0/10000): loss= 3439.90708551067\n",
      "Losgistic Regression(     100/10000): loss= 2309.95104822929\n",
      "Losgistic Regression(     200/10000): loss= 2120.93563297214\n",
      "Losgistic Regression(     300/10000): loss= 2037.93697981962\n",
      "Losgistic Regression(     400/10000): loss= 1969.67457196636\n",
      "Losgistic Regression(     500/10000): loss= 1913.30372613139\n",
      "Losgistic Regression(     600/10000): loss= 1867.96695649531\n",
      "Losgistic Regression(     700/10000): loss= 1829.69827513709\n",
      "Losgistic Regression(     800/10000): loss= 1795.65365182776\n",
      "Losgistic Regression(     900/10000): loss= 1769.35390113315\n",
      "Losgistic Regression(    1000/10000): loss= 1748.54267132111\n",
      "Losgistic Regression(    1100/10000): loss= 1729.4652395887\n",
      "Losgistic Regression(    1200/10000): loss= 1714.55158369588\n",
      "Losgistic Regression(    1300/10000): loss= 1702.22236455208\n",
      "Losgistic Regression(    1400/10000): loss= 1691.24964038682\n",
      "Losgistic Regression(    1500/10000): loss= 1683.04955344894\n",
      "Losgistic Regression(    1600/10000): loss= 1677.17260776669\n",
      "Losgistic Regression(    1700/10000): loss= 1672.31254635567\n",
      "Losgistic Regression(    1800/10000): loss= 1668.52623628611\n",
      "Losgistic Regression(    1900/10000): loss= 1665.5526217232\n",
      "Losgistic Regression(    2000/10000): loss= 1662.10933448615\n",
      "Losgistic Regression(    2100/10000): loss= 1658.91516684462\n",
      "Losgistic Regression(    2200/10000): loss= 1656.35501266318\n",
      "Losgistic Regression(    2300/10000): loss= 1654.24195302266\n",
      "Losgistic Regression(    2400/10000): loss= 1652.5231772661\n",
      "Losgistic Regression(    2500/10000): loss= 1651.18887648292\n",
      "Losgistic Regression(    2600/10000): loss= 1649.94402777217\n",
      "Losgistic Regression(    2700/10000): loss= 1648.843060734\n",
      "Losgistic Regression(    2800/10000): loss= 1647.95252066922\n",
      "Losgistic Regression(    2900/10000): loss= 1647.12441897271\n",
      "Losgistic Regression(    3000/10000): loss= 1646.38329895769\n",
      "Losgistic Regression(    3100/10000): loss= 1645.82798583243\n",
      "Losgistic Regression(    3200/10000): loss= 1645.27817194751\n",
      "Losgistic Regression(    3300/10000): loss= 1644.64258500615\n",
      "Losgistic Regression(    3400/10000): loss= 1643.96526939089\n",
      "Losgistic Regression(    3500/10000): loss= 1643.31280701519\n",
      "Losgistic Regression(    3600/10000): loss= 1642.61483281612\n",
      "Losgistic Regression(    3700/10000): loss= 1641.83575593225\n",
      "Losgistic Regression(    3800/10000): loss= 1640.99890740445\n",
      "Losgistic Regression(    3900/10000): loss= 1640.17217969568\n",
      "Losgistic Regression(    4000/10000): loss= 1639.33237660003\n",
      "Losgistic Regression(    4100/10000): loss= 1638.48777884861\n",
      "Losgistic Regression(    4200/10000): loss= 1637.70793621861\n",
      "Losgistic Regression(    4300/10000): loss= 1636.91924864549\n",
      "Losgistic Regression(    4400/10000): loss= 1636.09667729033\n",
      "Losgistic Regression(    4500/10000): loss= 1635.26973430884\n",
      "Losgistic Regression(    4600/10000): loss= 1634.43468499506\n",
      "Losgistic Regression(    4700/10000): loss= 1633.62695431787\n",
      "Losgistic Regression(    4800/10000): loss= 1632.88335845178\n",
      "Losgistic Regression(    4900/10000): loss= 1632.17802078268\n",
      "Losgistic Regression(    5000/10000): loss= 1631.49471719145\n",
      "Losgistic Regression(    5100/10000): loss= 1630.84031862479\n",
      "Losgistic Regression(    5200/10000): loss= 1630.18745429657\n",
      "Losgistic Regression(    5300/10000): loss= 1629.53687278384\n",
      "Losgistic Regression(    5400/10000): loss= 1628.92349039961\n",
      "Losgistic Regression(    5500/10000): loss= 1628.32743617832\n",
      "Losgistic Regression(    5600/10000): loss= 1627.74827020751\n",
      "Losgistic Regression(    5700/10000): loss= 1627.22933179252\n",
      "Losgistic Regression(    5800/10000): loss= 1626.79314752793\n",
      "Losgistic Regression(    5900/10000): loss= 1626.41608602516\n",
      "Losgistic Regression(    6000/10000): loss= 1626.09426818402\n",
      "Losgistic Regression(    6100/10000): loss= 1625.74693481307\n",
      "Losgistic Regression(    6200/10000): loss= 1625.46418677314\n",
      "Losgistic Regression(    6300/10000): loss= 1625.25249249129\n",
      "Losgistic Regression(    6400/10000): loss= 1625.10194784387\n",
      "Losgistic Regression(    6500/10000): loss= 1625.01944499603\n",
      "Losgistic Regression(    6600/10000): loss= 1625.00425467614\n",
      "Totoal number of iterations =  6600\n",
      "Loss                        =  1625.00425468\n",
      "Time for  4th cross validation = 110.797s\n",
      "Training Accuracy         = 0.8536\n",
      "Cross Validation Accuracy = 0.806352\n",
      "*************** ([0.85840000000000005, 0.84919999999999995, 0.85619999999999996, 0.85219999999999996, 0.85360000000000003], [0.80571599999999999, 0.79476800000000003, 0.80413599999999996, 0.80613199999999996, 0.80635199999999996])\n",
      "Losgistic Regression(       0/10000): loss= 3436.26751253314\n",
      "Losgistic Regression(     100/10000): loss= 2337.5558073351\n",
      "Losgistic Regression(     200/10000): loss= 2125.17913082784\n",
      "Losgistic Regression(     300/10000): loss= 2044.97074319175\n",
      "Losgistic Regression(     400/10000): loss= 1990.11631406029\n",
      "Losgistic Regression(     500/10000): loss= 1949.41211671717\n",
      "Losgistic Regression(     600/10000): loss= 1917.44771847646\n",
      "Losgistic Regression(     700/10000): loss= 1891.45479592646\n",
      "Losgistic Regression(     800/10000): loss= 1870.14586854592\n",
      "Losgistic Regression(     900/10000): loss= 1852.59143163127\n",
      "Losgistic Regression(    1000/10000): loss= 1837.7796408099\n",
      "Losgistic Regression(    1100/10000): loss= 1824.95485551434\n",
      "Losgistic Regression(    1200/10000): loss= 1813.69045856918\n",
      "Losgistic Regression(    1300/10000): loss= 1803.59800255552\n",
      "Losgistic Regression(    1400/10000): loss= 1794.67611719477\n",
      "Losgistic Regression(    1500/10000): loss= 1787.28027137509\n",
      "Losgistic Regression(    1600/10000): loss= 1780.56356309085\n",
      "Losgistic Regression(    1700/10000): loss= 1774.70174587548\n",
      "Losgistic Regression(    1800/10000): loss= 1769.55908179607\n",
      "Losgistic Regression(    1900/10000): loss= 1765.01521146766\n",
      "Losgistic Regression(    2000/10000): loss= 1760.98820183547\n",
      "Losgistic Regression(    2100/10000): loss= 1757.3822880719\n",
      "Losgistic Regression(    2200/10000): loss= 1754.1069285495\n",
      "Losgistic Regression(    2300/10000): loss= 1751.1192004145\n",
      "Losgistic Regression(    2400/10000): loss= 1748.3929848104\n",
      "Losgistic Regression(    2500/10000): loss= 1745.89495983265\n",
      "Losgistic Regression(    2600/10000): loss= 1743.57913845054\n",
      "Losgistic Regression(    2700/10000): loss= 1741.42812346689\n",
      "Losgistic Regression(    2800/10000): loss= 1739.4137290452\n",
      "Losgistic Regression(    2900/10000): loss= 1737.5105249713\n",
      "Losgistic Regression(    3000/10000): loss= 1735.72725329705\n",
      "Losgistic Regression(    3100/10000): loss= 1734.03377184859\n",
      "Losgistic Regression(    3200/10000): loss= 1732.32921563178\n",
      "Losgistic Regression(    3300/10000): loss= 1730.5343788837\n",
      "Losgistic Regression(    3400/10000): loss= 1728.79346513998\n",
      "Losgistic Regression(    3500/10000): loss= 1727.08700757062\n",
      "Losgistic Regression(    3600/10000): loss= 1725.39180328939\n",
      "Losgistic Regression(    3700/10000): loss= 1723.75262546489\n",
      "Losgistic Regression(    3800/10000): loss= 1722.11066018862\n",
      "Losgistic Regression(    3900/10000): loss= 1720.53186023233\n",
      "Losgistic Regression(    4000/10000): loss= 1719.00791294485\n",
      "Losgistic Regression(    4100/10000): loss= 1717.49787522363\n",
      "Losgistic Regression(    4200/10000): loss= 1715.96064405979\n",
      "Losgistic Regression(    4300/10000): loss= 1714.3555226398\n",
      "Losgistic Regression(    4400/10000): loss= 1712.57863559735\n",
      "Losgistic Regression(    4500/10000): loss= 1710.88218081118\n",
      "Losgistic Regression(    4600/10000): loss= 1709.30351942117\n",
      "Losgistic Regression(    4700/10000): loss= 1707.77620071408\n",
      "Losgistic Regression(    4800/10000): loss= 1706.34430620891\n",
      "Losgistic Regression(    4900/10000): loss= 1704.96367072435\n",
      "Losgistic Regression(    5000/10000): loss= 1703.7121191936\n",
      "Losgistic Regression(    5100/10000): loss= 1702.62459246527\n",
      "Losgistic Regression(    5200/10000): loss= 1701.74505552549\n",
      "Losgistic Regression(    5300/10000): loss= 1700.92962271244\n",
      "Losgistic Regression(    5400/10000): loss= 1700.14443681267\n",
      "Losgistic Regression(    5500/10000): loss= 1699.42650951106\n",
      "Losgistic Regression(    5600/10000): loss= 1698.73453776682\n",
      "Losgistic Regression(    5700/10000): loss= 1698.15396624485\n",
      "Losgistic Regression(    5800/10000): loss= 1697.62115007732\n",
      "Losgistic Regression(    5900/10000): loss= 1697.07875067546\n",
      "Losgistic Regression(    6000/10000): loss= 1696.5549464312\n",
      "Losgistic Regression(    6100/10000): loss= 1695.90122189305\n",
      "Losgistic Regression(    6200/10000): loss= 1695.15145277051\n",
      "Losgistic Regression(    6300/10000): loss= 1694.45481180725\n",
      "Losgistic Regression(    6400/10000): loss= 1693.82104012752\n",
      "Losgistic Regression(    6500/10000): loss= 1693.14785820498\n",
      "Losgistic Regression(    6600/10000): loss= 1692.3723740132\n",
      "Losgistic Regression(    6700/10000): loss= 1691.75605627488\n",
      "Losgistic Regression(    6800/10000): loss= 1691.19497861859\n",
      "Losgistic Regression(    6900/10000): loss= 1690.6585893496\n",
      "Losgistic Regression(    7000/10000): loss= 1690.26589021276\n",
      "Losgistic Regression(    7100/10000): loss= 1689.87789735216\n",
      "Losgistic Regression(    7200/10000): loss= 1689.63008272053\n",
      "Losgistic Regression(    7300/10000): loss= 1689.37724240887\n",
      "Losgistic Regression(    7400/10000): loss= 1689.10587460061\n",
      "Losgistic Regression(    7500/10000): loss= 1688.91067521711\n",
      "Losgistic Regression(    7600/10000): loss= 1688.64190386385\n",
      "Losgistic Regression(    7700/10000): loss= 1688.36022292434\n",
      "Losgistic Regression(    7800/10000): loss= 1688.1177347987\n",
      "Losgistic Regression(    7900/10000): loss= 1687.86706248697\n",
      "Losgistic Regression(    8000/10000): loss= 1687.60482618144\n",
      "Losgistic Regression(    8100/10000): loss= 1687.36414830918\n",
      "Losgistic Regression(    8200/10000): loss= 1687.11443815302\n",
      "Losgistic Regression(    8300/10000): loss= 1686.80847474498\n",
      "Losgistic Regression(    8400/10000): loss= 1686.54832863766\n",
      "Losgistic Regression(    8500/10000): loss= 1686.27387272952\n",
      "Losgistic Regression(    8600/10000): loss= 1686.06220884635\n",
      "Losgistic Regression(    8700/10000): loss= 1685.79235956292\n",
      "Losgistic Regression(    8800/10000): loss= 1685.53044819971\n",
      "Losgistic Regression(    8900/10000): loss= 1685.28264975921\n",
      "Losgistic Regression(    9000/10000): loss= 1685.10573609743\n",
      "Losgistic Regression(    9100/10000): loss= 1684.98395291735\n",
      "Losgistic Regression(    9200/10000): loss= 1684.916228562\n",
      "Losgistic Regression(    9300/10000): loss= 1684.88836503624\n",
      "Losgistic Regression(    9400/10000): loss= 1684.8887160946\n",
      "Totoal number of iterations =  9400\n",
      "Loss                        =  1684.88871609\n",
      "Time for  0th cross validation = 155.615s\n",
      "Training Accuracy         = 0.8584\n",
      "Cross Validation Accuracy = 0.807548\n",
      "Losgistic Regression(       0/10000): loss= 3438.21504473377\n",
      "Losgistic Regression(     100/10000): loss= 2370.89738775485\n",
      "Losgistic Regression(     200/10000): loss= 2169.25544617623\n",
      "Losgistic Regression(     300/10000): loss= 2080.2466440885\n",
      "Losgistic Regression(     400/10000): loss= 2024.24321136758\n",
      "Losgistic Regression(     500/10000): loss= 1982.2871922674\n",
      "Losgistic Regression(     600/10000): loss= 1947.86318057244\n",
      "Losgistic Regression(     700/10000): loss= 1920.51327440353\n",
      "Losgistic Regression(     800/10000): loss= 1896.68043842181\n",
      "Losgistic Regression(     900/10000): loss= 1875.80615087689\n",
      "Losgistic Regression(    1000/10000): loss= 1858.40026790619\n",
      "Losgistic Regression(    1100/10000): loss= 1843.81373205108\n",
      "Losgistic Regression(    1200/10000): loss= 1831.6683871348\n",
      "Losgistic Regression(    1300/10000): loss= 1821.30738955143\n",
      "Losgistic Regression(    1400/10000): loss= 1812.13185094988\n",
      "Losgistic Regression(    1500/10000): loss= 1804.03788703442\n",
      "Losgistic Regression(    1600/10000): loss= 1796.84230363358\n",
      "Losgistic Regression(    1700/10000): loss= 1790.37836443436\n",
      "Losgistic Regression(    1800/10000): loss= 1784.44395231988\n",
      "Losgistic Regression(    1900/10000): loss= 1779.21645255498\n",
      "Losgistic Regression(    2000/10000): loss= 1774.53820595052\n",
      "Losgistic Regression(    2100/10000): loss= 1770.24744730811\n",
      "Losgistic Regression(    2200/10000): loss= 1766.3176688662\n",
      "Losgistic Regression(    2300/10000): loss= 1762.74187508388\n",
      "Losgistic Regression(    2400/10000): loss= 1759.4707490632\n",
      "Losgistic Regression(    2500/10000): loss= 1756.46878027757\n",
      "Losgistic Regression(    2600/10000): loss= 1753.74029443282\n",
      "Losgistic Regression(    2700/10000): loss= 1751.29227569549\n",
      "Losgistic Regression(    2800/10000): loss= 1749.02517007261\n",
      "Losgistic Regression(    2900/10000): loss= 1746.96476385942\n",
      "Losgistic Regression(    3000/10000): loss= 1745.11750250984\n",
      "Losgistic Regression(    3100/10000): loss= 1743.3975016497\n",
      "Losgistic Regression(    3200/10000): loss= 1741.71976329033\n",
      "Losgistic Regression(    3300/10000): loss= 1740.07128457342\n",
      "Losgistic Regression(    3400/10000): loss= 1738.38014054656\n",
      "Losgistic Regression(    3500/10000): loss= 1736.67267606563\n",
      "Losgistic Regression(    3600/10000): loss= 1735.0974126357\n",
      "Losgistic Regression(    3700/10000): loss= 1733.6631145744\n",
      "Losgistic Regression(    3800/10000): loss= 1732.35806971838\n",
      "Losgistic Regression(    3900/10000): loss= 1731.17184612799\n",
      "Losgistic Regression(    4000/10000): loss= 1730.09308854359\n",
      "Losgistic Regression(    4100/10000): loss= 1729.08048512409\n",
      "Losgistic Regression(    4200/10000): loss= 1728.12350801894\n",
      "Losgistic Regression(    4300/10000): loss= 1727.23760039126\n",
      "Losgistic Regression(    4400/10000): loss= 1726.41293151152\n",
      "Losgistic Regression(    4500/10000): loss= 1725.55912120177\n",
      "Losgistic Regression(    4600/10000): loss= 1724.53007132926\n",
      "Losgistic Regression(    4700/10000): loss= 1723.57854461498\n",
      "Losgistic Regression(    4800/10000): loss= 1722.73300305728\n",
      "Losgistic Regression(    4900/10000): loss= 1721.96697493281\n",
      "Losgistic Regression(    5000/10000): loss= 1721.14176361822\n",
      "Losgistic Regression(    5100/10000): loss= 1720.44117691015\n",
      "Losgistic Regression(    5200/10000): loss= 1719.85793848054\n",
      "Losgistic Regression(    5300/10000): loss= 1719.32844803124\n",
      "Losgistic Regression(    5400/10000): loss= 1718.81309121656\n",
      "Losgistic Regression(    5500/10000): loss= 1718.20437206812\n",
      "Losgistic Regression(    5600/10000): loss= 1717.76287832721\n",
      "Losgistic Regression(    5700/10000): loss= 1717.58342050635\n",
      "Losgistic Regression(    5800/10000): loss= 1717.50884318799\n",
      "Losgistic Regression(    5900/10000): loss= 1717.3874218192\n",
      "Losgistic Regression(    6000/10000): loss= 1717.13134960423\n",
      "Losgistic Regression(    6100/10000): loss= 1716.87615241543\n",
      "Losgistic Regression(    6200/10000): loss= 1716.75321920519\n",
      "Losgistic Regression(    6300/10000): loss= 1716.75503473593\n",
      "Totoal number of iterations =  6300\n",
      "Loss                        =  1716.75503474\n",
      "Time for  1th cross validation = 103.855s\n",
      "Training Accuracy         =   0.85\n",
      "Cross Validation Accuracy = 0.795696\n",
      "Losgistic Regression(       0/10000): loss= 3438.58018047882\n",
      "Losgistic Regression(     100/10000): loss= 2323.69058155292\n",
      "Losgistic Regression(     200/10000): loss= 2120.4550664169\n",
      "Losgistic Regression(     300/10000): loss= 2031.52830139024\n",
      "Losgistic Regression(     400/10000): loss= 1971.90669447069\n",
      "Losgistic Regression(     500/10000): loss= 1928.54995967354\n",
      "Losgistic Regression(     600/10000): loss= 1895.73175129222\n",
      "Losgistic Regression(     700/10000): loss= 1869.82307535958\n",
      "Losgistic Regression(     800/10000): loss= 1848.89026573364\n",
      "Losgistic Regression(     900/10000): loss= 1831.92126945559\n",
      "Losgistic Regression(    1000/10000): loss= 1817.95415258871\n",
      "Losgistic Regression(    1100/10000): loss= 1806.30524249018\n",
      "Losgistic Regression(    1200/10000): loss= 1795.23632599235\n",
      "Losgistic Regression(    1300/10000): loss= 1784.95232269677\n",
      "Losgistic Regression(    1400/10000): loss= 1775.99651972077\n",
      "Losgistic Regression(    1500/10000): loss= 1768.12445810737\n",
      "Losgistic Regression(    1600/10000): loss= 1761.14189362362\n",
      "Losgistic Regression(    1700/10000): loss= 1754.91375767468\n",
      "Losgistic Regression(    1800/10000): loss= 1749.01244119887\n",
      "Losgistic Regression(    1900/10000): loss= 1743.5226710385\n",
      "Losgistic Regression(    2000/10000): loss= 1738.52680619622\n",
      "Losgistic Regression(    2100/10000): loss= 1733.94981510068\n",
      "Losgistic Regression(    2200/10000): loss= 1729.7423940163\n",
      "Losgistic Regression(    2300/10000): loss= 1725.87233577428\n",
      "Losgistic Regression(    2400/10000): loss= 1722.25470933728\n",
      "Losgistic Regression(    2500/10000): loss= 1718.70241133161\n",
      "Losgistic Regression(    2600/10000): loss= 1715.27075609975\n",
      "Losgistic Regression(    2700/10000): loss= 1711.9633969617\n",
      "Losgistic Regression(    2800/10000): loss= 1708.50403879303\n",
      "Losgistic Regression(    2900/10000): loss= 1704.84563517532\n",
      "Losgistic Regression(    3000/10000): loss= 1701.54399406763\n",
      "Losgistic Regression(    3100/10000): loss= 1698.05448712036\n",
      "Losgistic Regression(    3200/10000): loss= 1694.82679794905\n",
      "Losgistic Regression(    3300/10000): loss= 1692.00857893542\n",
      "Losgistic Regression(    3400/10000): loss= 1689.00298364298\n",
      "Losgistic Regression(    3500/10000): loss= 1685.85353991128\n",
      "Losgistic Regression(    3600/10000): loss= 1682.78252272137\n",
      "Losgistic Regression(    3700/10000): loss= 1679.4206420608\n",
      "Losgistic Regression(    3800/10000): loss= 1675.95374321222\n",
      "Losgistic Regression(    3900/10000): loss= 1672.66524501126\n",
      "Losgistic Regression(    4000/10000): loss= 1669.47438015532\n",
      "Losgistic Regression(    4100/10000): loss= 1666.32805641128\n",
      "Losgistic Regression(    4200/10000): loss= 1663.30373834832\n",
      "Losgistic Regression(    4300/10000): loss= 1660.40524881191\n",
      "Losgistic Regression(    4400/10000): loss= 1657.47471454997\n",
      "Losgistic Regression(    4500/10000): loss= 1654.52155659985\n",
      "Losgistic Regression(    4600/10000): loss= 1651.6234448367\n",
      "Losgistic Regression(    4700/10000): loss= 1648.74275283234\n",
      "Losgistic Regression(    4800/10000): loss= 1645.80307876936\n",
      "Losgistic Regression(    4900/10000): loss= 1642.78825641147\n",
      "Losgistic Regression(    5000/10000): loss= 1639.83219307191\n",
      "Losgistic Regression(    5100/10000): loss= 1636.76292755039\n",
      "Losgistic Regression(    5200/10000): loss= 1633.46650553966\n",
      "Losgistic Regression(    5300/10000): loss= 1630.16780646397\n",
      "Losgistic Regression(    5400/10000): loss= 1626.86266566451\n",
      "Losgistic Regression(    5500/10000): loss= 1623.37694754925\n",
      "Losgistic Regression(    5600/10000): loss= 1619.70615767359\n",
      "Losgistic Regression(    5700/10000): loss= 1616.03468804021\n",
      "Losgistic Regression(    5800/10000): loss= 1612.25266995015\n",
      "Losgistic Regression(    5900/10000): loss= 1608.42432542993\n",
      "Losgistic Regression(    6000/10000): loss= 1604.63888291534\n",
      "Losgistic Regression(    6100/10000): loss= 1600.80138588937\n",
      "Losgistic Regression(    6200/10000): loss= 1596.87003608393\n",
      "Losgistic Regression(    6300/10000): loss= 1592.8452201049\n",
      "Losgistic Regression(    6400/10000): loss= 1588.9448702017\n",
      "Losgistic Regression(    6500/10000): loss= 1585.12286458343\n",
      "Losgistic Regression(    6600/10000): loss= 1581.29745115384\n",
      "Losgistic Regression(    6700/10000): loss= 1577.51572633312\n",
      "Losgistic Regression(    6800/10000): loss= 1573.75082071214\n",
      "Losgistic Regression(    6900/10000): loss= 1569.98816455299\n",
      "Losgistic Regression(    7000/10000): loss= 1566.18782652882\n",
      "Losgistic Regression(    7100/10000): loss= 1562.35780981528\n",
      "Losgistic Regression(    7200/10000): loss= 1558.64790382953\n",
      "Losgistic Regression(    7300/10000): loss= 1555.01162853727\n",
      "Losgistic Regression(    7400/10000): loss= 1551.40001392031\n",
      "Losgistic Regression(    7500/10000): loss= 1547.84619879141\n",
      "Losgistic Regression(    7600/10000): loss= 1544.31206507166\n",
      "Losgistic Regression(    7700/10000): loss= 1540.77889608474\n",
      "Losgistic Regression(    7800/10000): loss= 1537.2535956434\n",
      "Losgistic Regression(    7900/10000): loss= 1533.81149266062\n",
      "Losgistic Regression(    8000/10000): loss= 1530.41253867603\n",
      "Losgistic Regression(    8100/10000): loss= 1526.9267781226\n",
      "Losgistic Regression(    8200/10000): loss= 1523.47378462874\n",
      "Losgistic Regression(    8300/10000): loss= 1520.11696523286\n",
      "Losgistic Regression(    8400/10000): loss= 1516.77250014945\n",
      "Losgistic Regression(    8500/10000): loss= 1513.33628672074\n",
      "Losgistic Regression(    8600/10000): loss= 1509.85723238804\n",
      "Losgistic Regression(    8700/10000): loss= 1506.36303448312\n",
      "Losgistic Regression(    8800/10000): loss= 1502.88874659772\n",
      "Losgistic Regression(    8900/10000): loss= 1499.54454346596\n",
      "Losgistic Regression(    9000/10000): loss= 1496.340119162\n",
      "Losgistic Regression(    9100/10000): loss= 1493.19423960122\n",
      "Losgistic Regression(    9200/10000): loss= 1490.15872308473\n",
      "Losgistic Regression(    9300/10000): loss= 1487.22135792322\n",
      "Losgistic Regression(    9400/10000): loss= 1484.27098042966\n",
      "Losgistic Regression(    9500/10000): loss= 1481.4262243419\n",
      "Losgistic Regression(    9600/10000): loss= 1478.69485766537\n",
      "Losgistic Regression(    9700/10000): loss= 1476.11275859688\n",
      "Losgistic Regression(    9800/10000): loss= 1473.62650824524\n",
      "Losgistic Regression(    9900/10000): loss= 1471.23785175816\n",
      "Time for  2th cross validation = 163.206s\n",
      "Training Accuracy         = 0.8568\n",
      "Cross Validation Accuracy = 0.805732\n",
      "Losgistic Regression(       0/10000): loss= 3439.01843224387\n",
      "Losgistic Regression(     100/10000): loss= 2331.04340470093\n",
      "Losgistic Regression(     200/10000): loss= 2139.0445652405\n",
      "Losgistic Regression(     300/10000): loss= 2061.95399137107\n",
      "Losgistic Regression(     400/10000): loss= 2009.64104413037\n",
      "Losgistic Regression(     500/10000): loss= 1969.261768235\n",
      "Losgistic Regression(     600/10000): loss= 1937.35419860505\n",
      "Losgistic Regression(     700/10000): loss= 1911.58750456686\n",
      "Losgistic Regression(     800/10000): loss= 1890.44077962489\n",
      "Losgistic Regression(     900/10000): loss= 1872.66161014098\n",
      "Losgistic Regression(    1000/10000): loss= 1857.41729858973\n",
      "Losgistic Regression(    1100/10000): loss= 1844.18425578115\n",
      "Losgistic Regression(    1200/10000): loss= 1831.98777939159\n",
      "Losgistic Regression(    1300/10000): loss= 1820.54798007017\n",
      "Losgistic Regression(    1400/10000): loss= 1810.57911858735\n",
      "Losgistic Regression(    1500/10000): loss= 1801.8221548521\n",
      "Losgistic Regression(    1600/10000): loss= 1793.98512471739\n",
      "Losgistic Regression(    1700/10000): loss= 1786.77883927736\n",
      "Losgistic Regression(    1800/10000): loss= 1779.61449936325\n",
      "Losgistic Regression(    1900/10000): loss= 1773.06149977336\n",
      "Losgistic Regression(    2000/10000): loss= 1767.05805151213\n",
      "Losgistic Regression(    2100/10000): loss= 1761.53691903786\n",
      "Losgistic Regression(    2200/10000): loss= 1756.37148259028\n",
      "Losgistic Regression(    2300/10000): loss= 1751.71590115282\n",
      "Losgistic Regression(    2400/10000): loss= 1747.68312362128\n",
      "Losgistic Regression(    2500/10000): loss= 1744.25209515663\n",
      "Losgistic Regression(    2600/10000): loss= 1741.16695546344\n",
      "Losgistic Regression(    2700/10000): loss= 1738.18187062503\n",
      "Losgistic Regression(    2800/10000): loss= 1735.24839050546\n",
      "Losgistic Regression(    2900/10000): loss= 1732.17901949166\n",
      "Losgistic Regression(    3000/10000): loss= 1729.42507355813\n",
      "Losgistic Regression(    3100/10000): loss= 1726.94469501385\n",
      "Losgistic Regression(    3200/10000): loss= 1724.60205495318\n",
      "Losgistic Regression(    3300/10000): loss= 1722.28028520649\n",
      "Losgistic Regression(    3400/10000): loss= 1719.76719493751\n",
      "Losgistic Regression(    3500/10000): loss= 1717.42489657272\n",
      "Losgistic Regression(    3600/10000): loss= 1715.33791366154\n",
      "Losgistic Regression(    3700/10000): loss= 1713.34090564369\n",
      "Losgistic Regression(    3800/10000): loss= 1711.4047138624\n",
      "Losgistic Regression(    3900/10000): loss= 1709.31186717745\n",
      "Losgistic Regression(    4000/10000): loss= 1707.37643162967\n",
      "Losgistic Regression(    4100/10000): loss= 1705.54835040999\n",
      "Losgistic Regression(    4200/10000): loss= 1703.74059115922\n",
      "Losgistic Regression(    4300/10000): loss= 1702.08686553545\n",
      "Losgistic Regression(    4400/10000): loss= 1700.38508407867\n",
      "Losgistic Regression(    4500/10000): loss= 1698.7116161644\n",
      "Losgistic Regression(    4600/10000): loss= 1697.25436832721\n",
      "Losgistic Regression(    4700/10000): loss= 1695.74666846076\n",
      "Losgistic Regression(    4800/10000): loss= 1693.98149420872\n",
      "Losgistic Regression(    4900/10000): loss= 1692.27957523867\n",
      "Losgistic Regression(    5000/10000): loss= 1690.73216694639\n",
      "Losgistic Regression(    5100/10000): loss= 1689.10853589172\n",
      "Losgistic Regression(    5200/10000): loss= 1687.3754004212\n",
      "Losgistic Regression(    5300/10000): loss= 1685.72493729007\n",
      "Losgistic Regression(    5400/10000): loss= 1684.51091538279\n",
      "Losgistic Regression(    5500/10000): loss= 1683.49342891002\n",
      "Losgistic Regression(    5600/10000): loss= 1682.45281522271\n",
      "Losgistic Regression(    5700/10000): loss= 1681.61729102327\n",
      "Losgistic Regression(    5800/10000): loss= 1680.80849581751\n",
      "Losgistic Regression(    5900/10000): loss= 1679.94346627142\n",
      "Losgistic Regression(    6000/10000): loss= 1678.91966378777\n",
      "Losgistic Regression(    6100/10000): loss= 1677.72500675919\n",
      "Losgistic Regression(    6200/10000): loss= 1676.45644306978\n",
      "Losgistic Regression(    6300/10000): loss= 1675.22227952798\n",
      "Losgistic Regression(    6400/10000): loss= 1674.00508124127\n",
      "Losgistic Regression(    6500/10000): loss= 1672.85523525301\n",
      "Losgistic Regression(    6600/10000): loss= 1672.03403916065\n",
      "Losgistic Regression(    6700/10000): loss= 1671.21728801557\n",
      "Losgistic Regression(    6800/10000): loss= 1670.33535055779\n",
      "Losgistic Regression(    6900/10000): loss= 1669.34562325334\n",
      "Losgistic Regression(    7000/10000): loss= 1668.34985207243\n",
      "Losgistic Regression(    7100/10000): loss= 1667.38277051015\n",
      "Losgistic Regression(    7200/10000): loss= 1666.43107253843\n",
      "Losgistic Regression(    7300/10000): loss= 1665.14987789132\n",
      "Losgistic Regression(    7400/10000): loss= 1663.81323452154\n",
      "Losgistic Regression(    7500/10000): loss= 1662.94003794071\n",
      "Losgistic Regression(    7600/10000): loss= 1662.34314615922\n",
      "Losgistic Regression(    7700/10000): loss= 1661.86540390625\n",
      "Losgistic Regression(    7800/10000): loss= 1661.35273475702\n",
      "Losgistic Regression(    7900/10000): loss= 1661.01533390588\n",
      "Losgistic Regression(    8000/10000): loss= 1660.46500583188\n",
      "Losgistic Regression(    8100/10000): loss= 1659.79611150886\n",
      "Losgistic Regression(    8200/10000): loss= 1659.16671623153\n",
      "Losgistic Regression(    8300/10000): loss= 1658.56309371644\n",
      "Losgistic Regression(    8400/10000): loss= 1657.68821180662\n",
      "Losgistic Regression(    8500/10000): loss= 1656.88972446629\n",
      "Losgistic Regression(    8600/10000): loss= 1656.72114008787\n",
      "Losgistic Regression(    8700/10000): loss= 1656.72678047881\n",
      "Totoal number of iterations =  8700\n",
      "Loss                        =  1656.72678048\n",
      "Time for  3th cross validation = 142.903s\n",
      "Training Accuracy         = 0.8502\n",
      "Cross Validation Accuracy = 0.807272\n",
      "Losgistic Regression(       0/10000): loss= 3439.91723774427\n",
      "Losgistic Regression(     100/10000): loss= 2311.82745019293\n",
      "Losgistic Regression(     200/10000): loss= 2124.32947555038\n",
      "Losgistic Regression(     300/10000): loss= 2042.78847331054\n",
      "Losgistic Regression(     400/10000): loss= 1976.0265896052\n",
      "Losgistic Regression(     500/10000): loss= 1921.25394397609\n",
      "Losgistic Regression(     600/10000): loss= 1877.53843469183\n",
      "Losgistic Regression(     700/10000): loss= 1840.97509168119\n",
      "Losgistic Regression(     800/10000): loss= 1808.5011880889\n",
      "Losgistic Regression(     900/10000): loss= 1783.65917552101\n",
      "Losgistic Regression(    1000/10000): loss= 1764.29779866731\n",
      "Losgistic Regression(    1100/10000): loss= 1746.56486028317\n",
      "Losgistic Regression(    1200/10000): loss= 1732.9074070411\n",
      "Losgistic Regression(    1300/10000): loss= 1721.71801773251\n",
      "Losgistic Regression(    1400/10000): loss= 1711.7976042246\n",
      "Losgistic Regression(    1500/10000): loss= 1704.61669404389\n",
      "Losgistic Regression(    1600/10000): loss= 1699.69568032949\n",
      "Losgistic Regression(    1700/10000): loss= 1695.73922912764\n",
      "Losgistic Regression(    1800/10000): loss= 1692.72035014585\n",
      "Losgistic Regression(    1900/10000): loss= 1690.37378991373\n",
      "Losgistic Regression(    2000/10000): loss= 1687.76262153003\n",
      "Losgistic Regression(    2100/10000): loss= 1685.10223398279\n",
      "Losgistic Regression(    2200/10000): loss= 1683.01509384222\n",
      "Losgistic Regression(    2300/10000): loss= 1681.36009388316\n",
      "Losgistic Regression(    2400/10000): loss= 1680.08953969693\n",
      "Losgistic Regression(    2500/10000): loss= 1679.14508374428\n",
      "Losgistic Regression(    2600/10000): loss= 1678.27714733594\n",
      "Losgistic Regression(    2700/10000): loss= 1677.55512754801\n",
      "Losgistic Regression(    2800/10000): loss= 1677.00679828353\n",
      "Losgistic Regression(    2900/10000): loss= 1676.45955684482\n",
      "Losgistic Regression(    3000/10000): loss= 1676.03414779486\n",
      "Losgistic Regression(    3100/10000): loss= 1675.79182362242\n",
      "Losgistic Regression(    3200/10000): loss= 1675.48594108662\n",
      "Losgistic Regression(    3300/10000): loss= 1675.03674346215\n",
      "Losgistic Regression(    3400/10000): loss= 1674.52156843676\n",
      "Losgistic Regression(    3500/10000): loss= 1674.05992410279\n",
      "Losgistic Regression(    3600/10000): loss= 1673.56989221966\n",
      "Losgistic Regression(    3700/10000): loss= 1673.00428631351\n",
      "Losgistic Regression(    3800/10000): loss= 1672.3470499146\n",
      "Losgistic Regression(    3900/10000): loss= 1671.67467788048\n",
      "Losgistic Regression(    4000/10000): loss= 1670.98334121933\n",
      "Losgistic Regression(    4100/10000): loss= 1670.30624762119\n",
      "Losgistic Regression(    4200/10000): loss= 1669.75468471726\n",
      "Losgistic Regression(    4300/10000): loss= 1669.23332899469\n",
      "Losgistic Regression(    4400/10000): loss= 1668.68161303728\n",
      "Losgistic Regression(    4500/10000): loss= 1668.11967004152\n",
      "Losgistic Regression(    4600/10000): loss= 1667.58523937164\n",
      "Losgistic Regression(    4700/10000): loss= 1667.13162143604\n",
      "Losgistic Regression(    4800/10000): loss= 1666.7966825476\n",
      "Losgistic Regression(    4900/10000): loss= 1666.55362085597\n",
      "Losgistic Regression(    5000/10000): loss= 1666.29718940154\n",
      "Losgistic Regression(    5100/10000): loss= 1665.90767923865\n",
      "Losgistic Regression(    5200/10000): loss= 1665.48962465236\n",
      "Losgistic Regression(    5300/10000): loss= 1665.05378867005\n",
      "Losgistic Regression(    5400/10000): loss= 1664.66964034845\n",
      "Losgistic Regression(    5500/10000): loss= 1664.40136503511\n",
      "Losgistic Regression(    5600/10000): loss= 1664.19255719262\n",
      "Losgistic Regression(    5700/10000): loss= 1663.96684898778\n",
      "Losgistic Regression(    5800/10000): loss= 1663.82258715681\n",
      "Losgistic Regression(    5900/10000): loss= 1663.77044419674\n",
      "Losgistic Regression(    6000/10000): loss= 1663.77094935612\n",
      "Totoal number of iterations =  6000\n",
      "Loss                        =  1663.77094936\n",
      "Time for  4th cross validation = 98.7274s\n",
      "Training Accuracy         =  0.854\n",
      "Cross Validation Accuracy = 0.80766\n",
      "*************** ([0.85840000000000005, 0.84999999999999998, 0.85680000000000001, 0.85019999999999996, 0.85399999999999998], [0.80754800000000004, 0.79569599999999996, 0.805732, 0.80727199999999999, 0.80766000000000004])\n",
      "Losgistic Regression(       0/10000): loss= 3436.30490607044\n",
      "Losgistic Regression(     100/10000): loss= 2343.36656329851\n",
      "Losgistic Regression(     200/10000): loss= 2136.71508411647\n",
      "Losgistic Regression(     300/10000): loss= 2061.04392208202\n",
      "Losgistic Regression(     400/10000): loss= 2010.73164015807\n",
      "Losgistic Regression(     500/10000): loss= 1974.36960627158\n",
      "Losgistic Regression(     600/10000): loss= 1946.57187369872\n",
      "Losgistic Regression(     700/10000): loss= 1924.59653508861\n",
      "Losgistic Regression(     800/10000): loss= 1906.93127883111\n",
      "Losgistic Regression(     900/10000): loss= 1892.69220561865\n",
      "Losgistic Regression(    1000/10000): loss= 1881.00180997959\n",
      "Losgistic Regression(    1100/10000): loss= 1871.15441365405\n",
      "Losgistic Regression(    1200/10000): loss= 1862.67158427513\n",
      "Losgistic Regression(    1300/10000): loss= 1855.17433590828\n",
      "Losgistic Regression(    1400/10000): loss= 1848.53830303148\n",
      "Losgistic Regression(    1500/10000): loss= 1843.1835430857\n",
      "Losgistic Regression(    1600/10000): loss= 1838.41442522231\n",
      "Losgistic Regression(    1700/10000): loss= 1834.27624762362\n",
      "Losgistic Regression(    1800/10000): loss= 1830.62925790154\n",
      "Losgistic Regression(    1900/10000): loss= 1827.37444999318\n",
      "Losgistic Regression(    2000/10000): loss= 1824.46247673304\n",
      "Losgistic Regression(    2100/10000): loss= 1821.93125583143\n",
      "Losgistic Regression(    2200/10000): loss= 1819.60255270282\n",
      "Losgistic Regression(    2300/10000): loss= 1817.42994784465\n",
      "Losgistic Regression(    2400/10000): loss= 1815.51061555527\n",
      "Losgistic Regression(    2500/10000): loss= 1813.79875263657\n",
      "Losgistic Regression(    2600/10000): loss= 1812.18524970349\n",
      "Losgistic Regression(    2700/10000): loss= 1810.77841964046\n",
      "Losgistic Regression(    2800/10000): loss= 1809.530888673\n",
      "Losgistic Regression(    2900/10000): loss= 1808.3260538781\n",
      "Losgistic Regression(    3000/10000): loss= 1807.19966303708\n",
      "Losgistic Regression(    3100/10000): loss= 1806.14173099329\n",
      "Losgistic Regression(    3200/10000): loss= 1805.11732217344\n",
      "Losgistic Regression(    3300/10000): loss= 1804.21865257027\n",
      "Losgistic Regression(    3400/10000): loss= 1803.35067656188\n",
      "Losgistic Regression(    3500/10000): loss= 1802.48684186882\n",
      "Losgistic Regression(    3600/10000): loss= 1801.70427336516\n",
      "Losgistic Regression(    3700/10000): loss= 1800.96983085451\n",
      "Losgistic Regression(    3800/10000): loss= 1800.34968383533\n",
      "Losgistic Regression(    3900/10000): loss= 1799.83865875615\n",
      "Losgistic Regression(    4000/10000): loss= 1799.4081020841\n",
      "Losgistic Regression(    4100/10000): loss= 1798.96685333391\n",
      "Losgistic Regression(    4200/10000): loss= 1798.54097476468\n",
      "Losgistic Regression(    4300/10000): loss= 1798.02800448245\n",
      "Losgistic Regression(    4400/10000): loss= 1797.51559719341\n",
      "Losgistic Regression(    4500/10000): loss= 1797.03900596312\n",
      "Losgistic Regression(    4600/10000): loss= 1796.589215666\n",
      "Losgistic Regression(    4700/10000): loss= 1796.17625914529\n",
      "Losgistic Regression(    4800/10000): loss= 1795.77378704936\n",
      "Losgistic Regression(    4900/10000): loss= 1795.3506475589\n",
      "Losgistic Regression(    5000/10000): loss= 1794.94422863891\n",
      "Losgistic Regression(    5100/10000): loss= 1794.58392281144\n",
      "Losgistic Regression(    5200/10000): loss= 1794.20458137566\n",
      "Losgistic Regression(    5300/10000): loss= 1793.79971420418\n",
      "Losgistic Regression(    5400/10000): loss= 1793.40895144185\n",
      "Losgistic Regression(    5500/10000): loss= 1793.07230433995\n",
      "Losgistic Regression(    5600/10000): loss= 1792.76874512417\n",
      "Losgistic Regression(    5700/10000): loss= 1792.47876170185\n",
      "Losgistic Regression(    5800/10000): loss= 1792.19023831612\n",
      "Losgistic Regression(    5900/10000): loss= 1791.93866289205\n",
      "Losgistic Regression(    6000/10000): loss= 1791.68917625009\n",
      "Losgistic Regression(    6100/10000): loss= 1791.42708497241\n",
      "Losgistic Regression(    6200/10000): loss= 1791.15980167233\n",
      "Losgistic Regression(    6300/10000): loss= 1790.93995889569\n",
      "Losgistic Regression(    6400/10000): loss= 1790.7331858907\n",
      "Losgistic Regression(    6500/10000): loss= 1790.52240394451\n",
      "Losgistic Regression(    6600/10000): loss= 1790.31193230451\n",
      "Losgistic Regression(    6700/10000): loss= 1790.11628347984\n",
      "Losgistic Regression(    6800/10000): loss= 1789.91745760173\n",
      "Losgistic Regression(    6900/10000): loss= 1789.73409777061\n",
      "Losgistic Regression(    7000/10000): loss= 1789.53290155464\n",
      "Losgistic Regression(    7100/10000): loss= 1789.35556772219\n",
      "Losgistic Regression(    7200/10000): loss= 1789.20472639632\n",
      "Losgistic Regression(    7300/10000): loss= 1789.04618504757\n",
      "Losgistic Regression(    7400/10000): loss= 1788.8855506829\n",
      "Losgistic Regression(    7500/10000): loss= 1788.74839245281\n",
      "Losgistic Regression(    7600/10000): loss= 1788.61740905561\n",
      "Losgistic Regression(    7700/10000): loss= 1788.4816853747\n",
      "Losgistic Regression(    7800/10000): loss= 1788.35252832564\n",
      "Losgistic Regression(    7900/10000): loss= 1788.2216466093\n",
      "Losgistic Regression(    8000/10000): loss= 1788.09121321441\n",
      "Losgistic Regression(    8100/10000): loss= 1787.99676486138\n",
      "Losgistic Regression(    8200/10000): loss= 1787.89099167972\n",
      "Losgistic Regression(    8300/10000): loss= 1787.77099078584\n",
      "Losgistic Regression(    8400/10000): loss= 1787.67340507066\n",
      "Losgistic Regression(    8500/10000): loss= 1787.57077747789\n",
      "Losgistic Regression(    8600/10000): loss= 1787.4768011037\n",
      "Losgistic Regression(    8700/10000): loss= 1787.37396535664\n",
      "Losgistic Regression(    8800/10000): loss= 1787.28407635748\n",
      "Losgistic Regression(    8900/10000): loss= 1787.19803419303\n",
      "Losgistic Regression(    9000/10000): loss= 1787.1206128449\n",
      "Losgistic Regression(    9100/10000): loss= 1787.03692944181\n",
      "Losgistic Regression(    9200/10000): loss= 1786.95826398527\n",
      "Losgistic Regression(    9300/10000): loss= 1786.88334255766\n",
      "Losgistic Regression(    9400/10000): loss= 1786.81119378906\n",
      "Losgistic Regression(    9500/10000): loss= 1786.74882133117\n",
      "Losgistic Regression(    9600/10000): loss= 1786.67900531385\n",
      "Losgistic Regression(    9700/10000): loss= 1786.61536241809\n",
      "Losgistic Regression(    9800/10000): loss= 1786.55402227803\n",
      "Losgistic Regression(    9900/10000): loss= 1786.49507964094\n",
      "Time for  0th cross validation = 163.355s\n",
      "Training Accuracy         = 0.8546\n",
      "Cross Validation Accuracy = 0.8098\n",
      "Losgistic Regression(       0/10000): loss= 3438.2513564894\n",
      "Losgistic Regression(     100/10000): loss= 2377.0809299234\n",
      "Losgistic Regression(     200/10000): loss= 2181.24805766411\n",
      "Losgistic Regression(     300/10000): loss= 2097.71275571168\n",
      "Losgistic Regression(     400/10000): loss= 2046.16979431116\n",
      "Losgistic Regression(     500/10000): loss= 2008.95873228756\n",
      "Losgistic Regression(     600/10000): loss= 1979.03616553809\n",
      "Losgistic Regression(     700/10000): loss= 1956.03121173819\n",
      "Losgistic Regression(     800/10000): loss= 1936.88354448471\n",
      "Losgistic Regression(     900/10000): loss= 1919.98716890783\n",
      "Losgistic Regression(    1000/10000): loss= 1906.57801773924\n",
      "Losgistic Regression(    1100/10000): loss= 1895.64753773141\n",
      "Losgistic Regression(    1200/10000): loss= 1886.71930944593\n",
      "Losgistic Regression(    1300/10000): loss= 1879.37931402317\n",
      "Losgistic Regression(    1400/10000): loss= 1873.26812358631\n",
      "Losgistic Regression(    1500/10000): loss= 1868.01224931872\n",
      "Losgistic Regression(    1600/10000): loss= 1863.36523870328\n",
      "Losgistic Regression(    1700/10000): loss= 1858.84797002074\n",
      "Losgistic Regression(    1800/10000): loss= 1855.0668513525\n",
      "Losgistic Regression(    1900/10000): loss= 1851.72283878223\n",
      "Losgistic Regression(    2000/10000): loss= 1848.65616841872\n",
      "Losgistic Regression(    2100/10000): loss= 1845.76912793888\n",
      "Losgistic Regression(    2200/10000): loss= 1843.28892713395\n",
      "Losgistic Regression(    2300/10000): loss= 1841.07566207868\n",
      "Losgistic Regression(    2400/10000): loss= 1839.11619094849\n",
      "Losgistic Regression(    2500/10000): loss= 1837.37770877658\n",
      "Losgistic Regression(    2600/10000): loss= 1835.63954270735\n",
      "Losgistic Regression(    2700/10000): loss= 1834.19089789656\n",
      "Losgistic Regression(    2800/10000): loss= 1832.93609320974\n",
      "Losgistic Regression(    2900/10000): loss= 1831.50517449845\n",
      "Losgistic Regression(    3000/10000): loss= 1830.45308989045\n",
      "Losgistic Regression(    3100/10000): loss= 1829.69479077071\n",
      "Losgistic Regression(    3200/10000): loss= 1828.43993744278\n",
      "Losgistic Regression(    3300/10000): loss= 1827.56647796801\n",
      "Losgistic Regression(    3400/10000): loss= 1826.61543867666\n",
      "Losgistic Regression(    3500/10000): loss= 1825.92180038743\n",
      "Losgistic Regression(    3600/10000): loss= 1825.29753726796\n",
      "Losgistic Regression(    3700/10000): loss= 1825.23208925391\n",
      "Losgistic Regression(    3800/10000): loss= 1825.23522924852\n",
      "Totoal number of iterations =  3800\n",
      "Loss                        =  1825.23522925\n",
      "Time for  1th cross validation = 63.101s\n",
      "Training Accuracy         = 0.8418\n",
      "Cross Validation Accuracy = 0.797284\n",
      "Losgistic Regression(       0/10000): loss= 3438.61551582613\n",
      "Losgistic Regression(     100/10000): loss= 2329.75171846073\n",
      "Losgistic Regression(     200/10000): loss= 2131.97340361772\n",
      "Losgistic Regression(     300/10000): loss= 2047.87232953783\n",
      "Losgistic Regression(     400/10000): loss= 1992.8006041957\n",
      "Losgistic Regression(     500/10000): loss= 1953.75082143966\n",
      "Losgistic Regression(     600/10000): loss= 1924.94166844712\n",
      "Losgistic Regression(     700/10000): loss= 1902.6119017795\n",
      "Losgistic Regression(     800/10000): loss= 1885.01323672733\n",
      "Losgistic Regression(     900/10000): loss= 1871.28225283251\n",
      "Losgistic Regression(    1000/10000): loss= 1860.36439570067\n",
      "Losgistic Regression(    1100/10000): loss= 1851.61268381473\n",
      "Losgistic Regression(    1200/10000): loss= 1844.48918968289\n",
      "Losgistic Regression(    1300/10000): loss= 1838.58410199102\n",
      "Losgistic Regression(    1400/10000): loss= 1833.75840301355\n",
      "Losgistic Regression(    1500/10000): loss= 1829.85672091538\n",
      "Losgistic Regression(    1600/10000): loss= 1826.55769460252\n",
      "Losgistic Regression(    1700/10000): loss= 1823.74587310045\n",
      "Losgistic Regression(    1800/10000): loss= 1821.33143638612\n",
      "Losgistic Regression(    1900/10000): loss= 1819.21469848125\n",
      "Losgistic Regression(    2000/10000): loss= 1817.32789415595\n",
      "Losgistic Regression(    2100/10000): loss= 1815.60068198085\n",
      "Losgistic Regression(    2200/10000): loss= 1814.03692191394\n",
      "Losgistic Regression(    2300/10000): loss= 1812.61542441575\n",
      "Losgistic Regression(    2400/10000): loss= 1811.34898167303\n",
      "Losgistic Regression(    2500/10000): loss= 1810.1166897971\n",
      "Losgistic Regression(    2600/10000): loss= 1809.1015882712\n",
      "Losgistic Regression(    2700/10000): loss= 1808.0231831725\n",
      "Losgistic Regression(    2800/10000): loss= 1806.98314269186\n",
      "Losgistic Regression(    2900/10000): loss= 1805.82972308561\n",
      "Losgistic Regression(    3000/10000): loss= 1804.81763354799\n",
      "Losgistic Regression(    3100/10000): loss= 1804.00207775815\n",
      "Losgistic Regression(    3200/10000): loss= 1803.16662565133\n",
      "Losgistic Regression(    3300/10000): loss= 1802.49154865559\n",
      "Losgistic Regression(    3400/10000): loss= 1801.86769420557\n",
      "Losgistic Regression(    3500/10000): loss= 1801.27703815983\n",
      "Losgistic Regression(    3600/10000): loss= 1800.72354891793\n",
      "Losgistic Regression(    3700/10000): loss= 1800.2199620927\n",
      "Losgistic Regression(    3800/10000): loss= 1799.74016047886\n",
      "Losgistic Regression(    3900/10000): loss= 1799.20495959706\n",
      "Losgistic Regression(    4000/10000): loss= 1798.79059420482\n",
      "Losgistic Regression(    4100/10000): loss= 1798.2992625127\n",
      "Losgistic Regression(    4200/10000): loss= 1797.79610431044\n",
      "Losgistic Regression(    4300/10000): loss= 1797.22637120255\n",
      "Losgistic Regression(    4400/10000): loss= 1796.74203655387\n",
      "Losgistic Regression(    4500/10000): loss= 1796.35123515978\n",
      "Losgistic Regression(    4600/10000): loss= 1796.01087256679\n",
      "Losgistic Regression(    4700/10000): loss= 1795.64727164658\n",
      "Losgistic Regression(    4800/10000): loss= 1795.29765111698\n",
      "Losgistic Regression(    4900/10000): loss= 1795.05543878078\n",
      "Losgistic Regression(    5000/10000): loss= 1794.88056479865\n",
      "Losgistic Regression(    5100/10000): loss= 1794.72325236059\n",
      "Losgistic Regression(    5200/10000): loss= 1794.66606854573\n",
      "Losgistic Regression(    5300/10000): loss= 1794.50426717487\n",
      "Losgistic Regression(    5400/10000): loss= 1794.3133010673\n",
      "Losgistic Regression(    5500/10000): loss= 1794.07056699924\n",
      "Losgistic Regression(    5600/10000): loss= 1793.82273293637\n",
      "Losgistic Regression(    5700/10000): loss= 1793.62412680783\n",
      "Losgistic Regression(    5800/10000): loss= 1793.45687528736\n",
      "Losgistic Regression(    5900/10000): loss= 1793.15933406362\n",
      "Losgistic Regression(    6000/10000): loss= 1792.70009850996\n",
      "Losgistic Regression(    6100/10000): loss= 1792.36162498727\n",
      "Losgistic Regression(    6200/10000): loss= 1792.07906082977\n",
      "Losgistic Regression(    6300/10000): loss= 1791.84398119818\n",
      "Losgistic Regression(    6400/10000): loss= 1791.549252272\n",
      "Losgistic Regression(    6500/10000): loss= 1791.29894146673\n",
      "Losgistic Regression(    6600/10000): loss= 1791.00638384632\n",
      "Losgistic Regression(    6700/10000): loss= 1790.6722215017\n",
      "Losgistic Regression(    6800/10000): loss= 1790.23382194288\n",
      "Losgistic Regression(    6900/10000): loss= 1789.82570984612\n",
      "Losgistic Regression(    7000/10000): loss= 1789.48875802317\n",
      "Losgistic Regression(    7100/10000): loss= 1789.23867510488\n",
      "Losgistic Regression(    7200/10000): loss= 1789.04805574112\n",
      "Losgistic Regression(    7300/10000): loss= 1788.99803497911\n",
      "Losgistic Regression(    7400/10000): loss= 1788.95941637437\n",
      "Losgistic Regression(    7500/10000): loss= 1788.91083174616\n",
      "Losgistic Regression(    7600/10000): loss= 1788.81530374156\n",
      "Losgistic Regression(    7700/10000): loss= 1788.68726208902\n",
      "Losgistic Regression(    7800/10000): loss= 1788.44807015376\n",
      "Losgistic Regression(    7900/10000): loss= 1788.1053852851\n",
      "Losgistic Regression(    8000/10000): loss= 1787.68084772122\n",
      "Losgistic Regression(    8100/10000): loss= 1787.15454441489\n",
      "Losgistic Regression(    8200/10000): loss= 1786.64452079136\n",
      "Losgistic Regression(    8300/10000): loss= 1786.20535641892\n",
      "Losgistic Regression(    8400/10000): loss= 1785.74611140656\n",
      "Losgistic Regression(    8500/10000): loss= 1785.37176761671\n",
      "Losgistic Regression(    8600/10000): loss= 1785.04653425402\n",
      "Losgistic Regression(    8700/10000): loss= 1784.79085950168\n",
      "Losgistic Regression(    8800/10000): loss= 1784.58027117955\n",
      "Losgistic Regression(    8900/10000): loss= 1784.3593467893\n",
      "Losgistic Regression(    9000/10000): loss= 1784.16431713675\n",
      "Losgistic Regression(    9100/10000): loss= 1783.99147407136\n",
      "Losgistic Regression(    9200/10000): loss= 1783.84672663823\n",
      "Losgistic Regression(    9300/10000): loss= 1783.64032255005\n",
      "Losgistic Regression(    9400/10000): loss= 1783.44191401479\n",
      "Losgistic Regression(    9500/10000): loss= 1783.19479964137\n",
      "Losgistic Regression(    9600/10000): loss= 1782.98796246925\n",
      "Losgistic Regression(    9700/10000): loss= 1782.77642496867\n",
      "Losgistic Regression(    9800/10000): loss= 1782.56829451037\n",
      "Losgistic Regression(    9900/10000): loss= 1782.34750639312\n",
      "Time for  2th cross validation = 164.301s\n",
      "Training Accuracy         =  0.853\n",
      "Cross Validation Accuracy = 0.810392\n",
      "Losgistic Regression(       0/10000): loss= 3439.05399441729\n",
      "Losgistic Regression(     100/10000): loss= 2337.06120035486\n",
      "Losgistic Regression(     200/10000): loss= 2150.1893045916\n",
      "Losgistic Regression(     300/10000): loss= 2077.21449726043\n",
      "Losgistic Regression(     400/10000): loss= 2028.70998090459\n",
      "Losgistic Regression(     500/10000): loss= 1992.00027605154\n",
      "Losgistic Regression(     600/10000): loss= 1963.63039733518\n",
      "Losgistic Regression(     700/10000): loss= 1941.31963692848\n",
      "Losgistic Regression(     800/10000): loss= 1923.43831087322\n",
      "Losgistic Regression(     900/10000): loss= 1908.74580792227\n",
      "Losgistic Regression(    1000/10000): loss= 1896.51731314102\n",
      "Losgistic Regression(    1100/10000): loss= 1886.2978068372\n",
      "Losgistic Regression(    1200/10000): loss= 1877.5975426437\n",
      "Losgistic Regression(    1300/10000): loss= 1869.43385867307\n",
      "Losgistic Regression(    1400/10000): loss= 1862.34837778493\n",
      "Losgistic Regression(    1500/10000): loss= 1856.39198756759\n",
      "Losgistic Regression(    1600/10000): loss= 1851.12645215451\n",
      "Losgistic Regression(    1700/10000): loss= 1846.70672209336\n",
      "Losgistic Regression(    1800/10000): loss= 1842.66668705707\n",
      "Losgistic Regression(    1900/10000): loss= 1838.91763784747\n",
      "Losgistic Regression(    2000/10000): loss= 1835.96017833818\n",
      "Losgistic Regression(    2100/10000): loss= 1833.09379002808\n",
      "Losgistic Regression(    2200/10000): loss= 1830.40554738046\n",
      "Losgistic Regression(    2300/10000): loss= 1828.07577143095\n",
      "Losgistic Regression(    2400/10000): loss= 1826.12410937264\n",
      "Losgistic Regression(    2500/10000): loss= 1824.36944059055\n",
      "Losgistic Regression(    2600/10000): loss= 1822.66195228622\n",
      "Losgistic Regression(    2700/10000): loss= 1820.93809750673\n",
      "Losgistic Regression(    2800/10000): loss= 1819.32152592366\n",
      "Losgistic Regression(    2900/10000): loss= 1817.86094473589\n",
      "Losgistic Regression(    3000/10000): loss= 1816.3490125397\n",
      "Losgistic Regression(    3100/10000): loss= 1815.06584813575\n",
      "Losgistic Regression(    3200/10000): loss= 1813.81827262817\n",
      "Losgistic Regression(    3300/10000): loss= 1813.05012271019\n",
      "Losgistic Regression(    3400/10000): loss= 1812.17355804641\n",
      "Losgistic Regression(    3500/10000): loss= 1811.18500601806\n",
      "Losgistic Regression(    3600/10000): loss= 1810.41465643005\n",
      "Losgistic Regression(    3700/10000): loss= 1809.93214890906\n",
      "Losgistic Regression(    3800/10000): loss= 1809.4443392992\n",
      "Losgistic Regression(    3900/10000): loss= 1809.04830058564\n",
      "Losgistic Regression(    4000/10000): loss= 1808.51255721925\n",
      "Losgistic Regression(    4100/10000): loss= 1808.0574836276\n",
      "Losgistic Regression(    4200/10000): loss= 1807.68259004626\n",
      "Losgistic Regression(    4300/10000): loss= 1807.28145593622\n",
      "Losgistic Regression(    4400/10000): loss= 1806.84829446426\n",
      "Losgistic Regression(    4500/10000): loss= 1806.6136900719\n",
      "Losgistic Regression(    4600/10000): loss= 1806.34447763256\n",
      "Losgistic Regression(    4700/10000): loss= 1806.05640210522\n",
      "Losgistic Regression(    4800/10000): loss= 1806.00661104388\n",
      "Losgistic Regression(    4900/10000): loss= 1806.0095200129\n",
      "Totoal number of iterations =  4900\n",
      "Loss                        =  1806.00952001\n",
      "Time for  3th cross validation = 80.7491s\n",
      "Training Accuracy         = 0.8414\n",
      "Cross Validation Accuracy = 0.80814\n",
      "Losgistic Regression(       0/10000): loss= 3439.95258987496\n",
      "Losgistic Regression(     100/10000): loss= 2318.29884897182\n",
      "Losgistic Regression(     200/10000): loss= 2135.92035482531\n",
      "Losgistic Regression(     300/10000): loss= 2059.25610153006\n",
      "Losgistic Regression(     400/10000): loss= 1997.43610036848\n",
      "Losgistic Regression(     500/10000): loss= 1947.9485440179\n",
      "Losgistic Regression(     600/10000): loss= 1909.48535671109\n",
      "Losgistic Regression(     700/10000): loss= 1878.02855299703\n",
      "Losgistic Regression(     800/10000): loss= 1850.61905204763\n",
      "Losgistic Regression(     900/10000): loss= 1830.00959366074\n",
      "Losgistic Regression(    1000/10000): loss= 1814.82278855666\n",
      "Losgistic Regression(    1100/10000): loss= 1801.13668238729\n",
      "Losgistic Regression(    1200/10000): loss= 1790.74621660223\n",
      "Losgistic Regression(    1300/10000): loss= 1782.50053605677\n",
      "Losgistic Regression(    1400/10000): loss= 1775.54432628978\n",
      "Losgistic Regression(    1500/10000): loss= 1770.92527933797\n",
      "Losgistic Regression(    1600/10000): loss= 1768.37598046413\n",
      "Losgistic Regression(    1700/10000): loss= 1767.0127677321\n",
      "Losgistic Regression(    1800/10000): loss= 1765.70475416449\n",
      "Losgistic Regression(    1900/10000): loss= 1764.3857525346\n",
      "Losgistic Regression(    2000/10000): loss= 1763.44033620972\n",
      "Losgistic Regression(    2100/10000): loss= 1762.23147988218\n",
      "Losgistic Regression(    2200/10000): loss= 1760.91935653753\n",
      "Losgistic Regression(    2300/10000): loss= 1759.7694070661\n",
      "Losgistic Regression(    2400/10000): loss= 1758.94857121818\n",
      "Losgistic Regression(    2500/10000): loss= 1758.67263566681\n",
      "Losgistic Regression(    2600/10000): loss= 1758.32880054046\n",
      "Losgistic Regression(    2700/10000): loss= 1757.64070914639\n",
      "Losgistic Regression(    2800/10000): loss= 1756.91789511391\n",
      "Losgistic Regression(    2900/10000): loss= 1756.2943512759\n",
      "Losgistic Regression(    3000/10000): loss= 1755.51280461446\n",
      "Losgistic Regression(    3100/10000): loss= 1754.87168049539\n",
      "Losgistic Regression(    3200/10000): loss= 1754.49013606203\n",
      "Losgistic Regression(    3300/10000): loss= 1754.28745604161\n",
      "Losgistic Regression(    3400/10000): loss= 1753.80150047835\n",
      "Losgistic Regression(    3500/10000): loss= 1753.55053024248\n",
      "Losgistic Regression(    3600/10000): loss= 1753.12087573655\n",
      "Losgistic Regression(    3700/10000): loss= 1752.75638542855\n",
      "Losgistic Regression(    3800/10000): loss= 1752.51073014493\n",
      "Losgistic Regression(    3900/10000): loss= 1752.13246725615\n",
      "Losgistic Regression(    4000/10000): loss= 1751.77499171986\n",
      "Losgistic Regression(    4100/10000): loss= 1751.43502404173\n",
      "Losgistic Regression(    4200/10000): loss= 1750.99444264276\n",
      "Losgistic Regression(    4300/10000): loss= 1750.7365135523\n",
      "Losgistic Regression(    4400/10000): loss= 1749.80834571827\n",
      "Losgistic Regression(    4500/10000): loss= 1748.90292995276\n",
      "Losgistic Regression(    4600/10000): loss= 1748.47068267344\n",
      "Losgistic Regression(    4700/10000): loss= 1747.98483110977\n",
      "Losgistic Regression(    4800/10000): loss= 1747.81602037728\n",
      "Losgistic Regression(    4900/10000): loss= 1747.78589025037\n",
      "Losgistic Regression(    5000/10000): loss= 1747.78374638555\n",
      "Totoal number of iterations =  5000\n",
      "Loss                        =  1747.78374639\n",
      "Time for  4th cross validation = 82.996s\n",
      "Training Accuracy         =  0.849\n",
      "Cross Validation Accuracy = 0.809496\n",
      "*************** ([0.85460000000000003, 0.84179999999999999, 0.85299999999999998, 0.84140000000000004, 0.84899999999999998], [0.80979999999999996, 0.79728399999999999, 0.810392, 0.80813999999999997, 0.80949599999999999])\n",
      "Losgistic Regression(       0/10000): loss= 3436.43511793025\n",
      "Losgistic Regression(     100/10000): loss= 2362.83667692508\n",
      "Losgistic Regression(     200/10000): loss= 2174.2994631098\n",
      "Losgistic Regression(     300/10000): loss= 2112.16284705386\n",
      "Losgistic Regression(     400/10000): loss= 2074.38485128516\n",
      "Losgistic Regression(     500/10000): loss= 2048.81435631214\n",
      "Losgistic Regression(     600/10000): loss= 2030.79855288988\n",
      "Losgistic Regression(     700/10000): loss= 2017.37469801788\n",
      "Losgistic Regression(     800/10000): loss= 2007.14196542147\n",
      "Losgistic Regression(     900/10000): loss= 1999.51657498611\n",
      "Losgistic Regression(    1000/10000): loss= 1993.50810526049\n",
      "Losgistic Regression(    1100/10000): loss= 1988.46792710068\n",
      "Losgistic Regression(    1200/10000): loss= 1984.04714612785\n",
      "Losgistic Regression(    1300/10000): loss= 1980.66219549255\n",
      "Losgistic Regression(    1400/10000): loss= 1977.29474548119\n",
      "Losgistic Regression(    1500/10000): loss= 1974.24418387469\n",
      "Losgistic Regression(    1600/10000): loss= 1971.49059166504\n",
      "Losgistic Regression(    1700/10000): loss= 1969.34030107035\n",
      "Losgistic Regression(    1800/10000): loss= 1967.30520162792\n",
      "Losgistic Regression(    1900/10000): loss= 1966.00540460073\n",
      "Losgistic Regression(    2000/10000): loss= 1964.84567217985\n",
      "Losgistic Regression(    2100/10000): loss= 1963.6802050595\n",
      "Losgistic Regression(    2200/10000): loss= 1962.59008480312\n",
      "Losgistic Regression(    2300/10000): loss= 1961.59167182893\n",
      "Losgistic Regression(    2400/10000): loss= 1960.80054166616\n",
      "Losgistic Regression(    2500/10000): loss= 1959.9692554426\n",
      "Losgistic Regression(    2600/10000): loss= 1959.24640620886\n",
      "Losgistic Regression(    2700/10000): loss= 1958.64558848988\n",
      "Losgistic Regression(    2800/10000): loss= 1957.91786329848\n",
      "Losgistic Regression(    2900/10000): loss= 1957.42549362699\n",
      "Losgistic Regression(    3000/10000): loss= 1957.04421911662\n",
      "Losgistic Regression(    3100/10000): loss= 1956.70468248074\n",
      "Losgistic Regression(    3200/10000): loss= 1956.39545100225\n",
      "Losgistic Regression(    3300/10000): loss= 1956.07123297436\n",
      "Losgistic Regression(    3400/10000): loss= 1955.74200716491\n",
      "Losgistic Regression(    3500/10000): loss= 1955.41371406084\n",
      "Losgistic Regression(    3600/10000): loss= 1955.14706745593\n",
      "Losgistic Regression(    3700/10000): loss= 1954.84114912996\n",
      "Losgistic Regression(    3800/10000): loss= 1954.56801337342\n",
      "Losgistic Regression(    3900/10000): loss= 1954.21606763074\n",
      "Losgistic Regression(    4000/10000): loss= 1953.95109003167\n",
      "Losgistic Regression(    4100/10000): loss= 1953.72353636319\n",
      "Losgistic Regression(    4200/10000): loss= 1953.53707933511\n",
      "Losgistic Regression(    4300/10000): loss= 1953.48159025657\n",
      "Losgistic Regression(    4400/10000): loss= 1953.42683170903\n",
      "Losgistic Regression(    4500/10000): loss= 1953.3738916982\n",
      "Losgistic Regression(    4600/10000): loss= 1953.33770098703\n",
      "Losgistic Regression(    4700/10000): loss= 1953.30799791674\n",
      "Losgistic Regression(    4800/10000): loss= 1953.27938456856\n",
      "Losgistic Regression(    4900/10000): loss= 1953.2503834122\n",
      "Losgistic Regression(    5000/10000): loss= 1953.22077652387\n",
      "Losgistic Regression(    5100/10000): loss= 1953.19021453041\n",
      "Losgistic Regression(    5200/10000): loss= 1953.1582305067\n",
      "Losgistic Regression(    5300/10000): loss= 1953.12445720876\n",
      "Losgistic Regression(    5400/10000): loss= 1953.08912290957\n",
      "Losgistic Regression(    5500/10000): loss= 1953.05205978847\n",
      "Losgistic Regression(    5600/10000): loss= 1953.01338878581\n",
      "Losgistic Regression(    5700/10000): loss= 1952.97334710608\n",
      "Losgistic Regression(    5800/10000): loss= 1952.93199637304\n",
      "Losgistic Regression(    5900/10000): loss= 1952.88964997996\n",
      "Losgistic Regression(    6000/10000): loss= 1952.84610098887\n",
      "Losgistic Regression(    6100/10000): loss= 1952.80120102948\n",
      "Losgistic Regression(    6200/10000): loss= 1952.75547735683\n",
      "Losgistic Regression(    6300/10000): loss= 1952.7087143843\n",
      "Losgistic Regression(    6400/10000): loss= 1952.66108494592\n",
      "Losgistic Regression(    6500/10000): loss= 1952.61293213202\n",
      "Losgistic Regression(    6600/10000): loss= 1952.5641341076\n",
      "Losgistic Regression(    6700/10000): loss= 1952.51447580569\n",
      "Losgistic Regression(    6800/10000): loss= 1952.46419899781\n",
      "Losgistic Regression(    6900/10000): loss= 1952.41354084526\n",
      "Losgistic Regression(    7000/10000): loss= 1952.36274964654\n",
      "Losgistic Regression(    7100/10000): loss= 1952.31210158705\n",
      "Losgistic Regression(    7200/10000): loss= 1952.2613210961\n",
      "Losgistic Regression(    7300/10000): loss= 1952.21055126126\n",
      "Losgistic Regression(    7400/10000): loss= 1952.15980154781\n",
      "Losgistic Regression(    7500/10000): loss= 1952.109322616\n",
      "Losgistic Regression(    7600/10000): loss= 1952.0589856877\n",
      "Losgistic Regression(    7700/10000): loss= 1952.00896681587\n",
      "Losgistic Regression(    7800/10000): loss= 1951.97440075219\n",
      "Losgistic Regression(    7900/10000): loss= 1951.97435476533\n",
      "Totoal number of iterations =  7900\n",
      "Loss                        =  1951.97435477\n",
      "Time for  0th cross validation = 129.552s\n",
      "Training Accuracy         = 0.8396\n",
      "Cross Validation Accuracy = 0.810008\n",
      "Losgistic Regression(       0/10000): loss= 3438.37780136668\n",
      "Losgistic Regression(     100/10000): loss= 2397.81971038168\n",
      "Losgistic Regression(     200/10000): loss= 2220.4970632507\n",
      "Losgistic Regression(     300/10000): loss= 2152.81102629899\n",
      "Losgistic Regression(     400/10000): loss= 2113.48314703769\n",
      "Losgistic Regression(     500/10000): loss= 2088.65018616236\n",
      "Losgistic Regression(     600/10000): loss= 2070.31160248535\n",
      "Losgistic Regression(     700/10000): loss= 2056.90499481874\n",
      "Losgistic Regression(     800/10000): loss= 2046.70117257272\n",
      "Losgistic Regression(     900/10000): loss= 2039.14843658777\n",
      "Losgistic Regression(    1000/10000): loss= 2032.57530736451\n",
      "Losgistic Regression(    1100/10000): loss= 2026.46288348107\n",
      "Losgistic Regression(    1200/10000): loss= 2021.4669153089\n",
      "Losgistic Regression(    1300/10000): loss= 2018.52487683628\n",
      "Losgistic Regression(    1400/10000): loss= 2016.58639985624\n",
      "Losgistic Regression(    1500/10000): loss= 2014.70187012116\n",
      "Losgistic Regression(    1600/10000): loss= 2012.18930361829\n",
      "Losgistic Regression(    1700/10000): loss= 2009.61098313289\n",
      "Losgistic Regression(    1800/10000): loss= 2008.58238876135\n",
      "Losgistic Regression(    1900/10000): loss= 2007.47304757689\n",
      "Losgistic Regression(    2000/10000): loss= 2006.47355259225\n",
      "Losgistic Regression(    2100/10000): loss= 2005.15523665955\n",
      "Losgistic Regression(    2200/10000): loss= 2003.78195714739\n",
      "Losgistic Regression(    2300/10000): loss= 2002.59091403718\n",
      "Losgistic Regression(    2400/10000): loss= 2001.95329314656\n",
      "Losgistic Regression(    2500/10000): loss= 2001.2759744699\n",
      "Losgistic Regression(    2600/10000): loss= 2001.26001175595\n",
      "Totoal number of iterations =  2600\n",
      "Loss                        =  2001.26001176\n",
      "Time for  1th cross validation = 42.8929s\n",
      "Training Accuracy         =  0.837\n",
      "Cross Validation Accuracy = 0.800488\n",
      "Losgistic Regression(       0/10000): loss= 3438.73856065215\n",
      "Losgistic Regression(     100/10000): loss= 2350.15922698008\n",
      "Losgistic Regression(     200/10000): loss= 2169.4084966787\n",
      "Losgistic Regression(     300/10000): loss= 2099.69201463657\n",
      "Losgistic Regression(     400/10000): loss= 2057.19839322505\n",
      "Losgistic Regression(     500/10000): loss= 2029.38411193551\n",
      "Losgistic Regression(     600/10000): loss= 2009.58630461138\n",
      "Losgistic Regression(     700/10000): loss= 1994.88234079673\n",
      "Losgistic Regression(     800/10000): loss= 1983.84506111853\n",
      "Losgistic Regression(     900/10000): loss= 1975.97812298713\n",
      "Losgistic Regression(    1000/10000): loss= 1970.14716299072\n",
      "Losgistic Regression(    1100/10000): loss= 1966.16173773936\n",
      "Losgistic Regression(    1200/10000): loss= 1963.21872053289\n",
      "Losgistic Regression(    1300/10000): loss= 1961.09776882781\n",
      "Losgistic Regression(    1400/10000): loss= 1959.15359805941\n",
      "Losgistic Regression(    1500/10000): loss= 1957.3923421089\n",
      "Losgistic Regression(    1600/10000): loss= 1955.78272522087\n",
      "Losgistic Regression(    1700/10000): loss= 1954.48167956268\n",
      "Losgistic Regression(    1800/10000): loss= 1953.10062997601\n",
      "Losgistic Regression(    1900/10000): loss= 1951.84751297366\n",
      "Losgistic Regression(    2000/10000): loss= 1951.0780162671\n",
      "Losgistic Regression(    2100/10000): loss= 1950.23791087031\n",
      "Losgistic Regression(    2200/10000): loss= 1949.43958335258\n",
      "Losgistic Regression(    2300/10000): loss= 1948.64352025985\n",
      "Losgistic Regression(    2400/10000): loss= 1947.93885415214\n",
      "Losgistic Regression(    2500/10000): loss= 1947.33527660275\n",
      "Losgistic Regression(    2600/10000): loss= 1946.67906245176\n",
      "Losgistic Regression(    2700/10000): loss= 1946.11808023026\n",
      "Losgistic Regression(    2800/10000): loss= 1945.81570142314\n",
      "Losgistic Regression(    2900/10000): loss= 1945.46360811754\n",
      "Losgistic Regression(    3000/10000): loss= 1945.20243104731\n",
      "Losgistic Regression(    3100/10000): loss= 1944.82427778991\n",
      "Losgistic Regression(    3200/10000): loss= 1944.41527022891\n",
      "Losgistic Regression(    3300/10000): loss= 1944.04482731806\n",
      "Losgistic Regression(    3400/10000): loss= 1943.6792311852\n",
      "Losgistic Regression(    3500/10000): loss= 1943.48875255651\n",
      "Losgistic Regression(    3600/10000): loss= 1943.29610239038\n",
      "Losgistic Regression(    3700/10000): loss= 1942.9910979134\n",
      "Losgistic Regression(    3800/10000): loss= 1942.63902737206\n",
      "Losgistic Regression(    3900/10000): loss= 1942.54052773695\n",
      "Losgistic Regression(    4000/10000): loss= 1942.47353726778\n",
      "Losgistic Regression(    4100/10000): loss= 1942.41732795827\n",
      "Losgistic Regression(    4200/10000): loss= 1942.36187764968\n",
      "Losgistic Regression(    4300/10000): loss= 1942.36093908947\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  1942.36093909\n",
      "Time for  2th cross validation = 70.796s\n",
      "Training Accuracy         = 0.8428\n",
      "Cross Validation Accuracy = 0.811464\n",
      "Losgistic Regression(       0/10000): loss= 3439.17782909768\n",
      "Losgistic Regression(     100/10000): loss= 2357.31064623942\n",
      "Losgistic Regression(     200/10000): loss= 2186.42850925518\n",
      "Losgistic Regression(     300/10000): loss= 2125.57660902118\n",
      "Losgistic Regression(     400/10000): loss= 2087.41508141153\n",
      "Losgistic Regression(     500/10000): loss= 2060.13869149171\n",
      "Losgistic Regression(     600/10000): loss= 2040.24646075519\n",
      "Losgistic Regression(     700/10000): loss= 2025.53620073917\n",
      "Losgistic Regression(     800/10000): loss= 2014.2691871439\n",
      "Losgistic Regression(     900/10000): loss= 2005.71135214943\n",
      "Losgistic Regression(    1000/10000): loss= 1999.28479009885\n",
      "Losgistic Regression(    1100/10000): loss= 1993.9979823554\n",
      "Losgistic Regression(    1200/10000): loss= 1989.64823904795\n",
      "Losgistic Regression(    1300/10000): loss= 1985.93029469219\n",
      "Losgistic Regression(    1400/10000): loss= 1982.77495834827\n",
      "Losgistic Regression(    1500/10000): loss= 1980.20224894657\n",
      "Losgistic Regression(    1600/10000): loss= 1978.31849497828\n",
      "Losgistic Regression(    1700/10000): loss= 1976.48485873842\n",
      "Losgistic Regression(    1800/10000): loss= 1974.95636312221\n",
      "Losgistic Regression(    1900/10000): loss= 1973.59945056792\n",
      "Losgistic Regression(    2000/10000): loss= 1972.47277672637\n",
      "Losgistic Regression(    2100/10000): loss= 1971.69900062245\n",
      "Losgistic Regression(    2200/10000): loss= 1971.0292367723\n",
      "Losgistic Regression(    2300/10000): loss= 1970.41888556731\n",
      "Losgistic Regression(    2400/10000): loss= 1969.70292556967\n",
      "Losgistic Regression(    2500/10000): loss= 1969.10474088896\n",
      "Losgistic Regression(    2600/10000): loss= 1968.50678606693\n",
      "Losgistic Regression(    2700/10000): loss= 1967.85787064973\n",
      "Losgistic Regression(    2800/10000): loss= 1967.21938168241\n",
      "Losgistic Regression(    2900/10000): loss= 1966.59375627689\n",
      "Losgistic Regression(    3000/10000): loss= 1966.12710601863\n",
      "Losgistic Regression(    3100/10000): loss= 1965.92202012561\n",
      "Losgistic Regression(    3200/10000): loss= 1965.77334624168\n",
      "Losgistic Regression(    3300/10000): loss= 1965.66772768287\n",
      "Losgistic Regression(    3400/10000): loss= 1965.57063122214\n",
      "Losgistic Regression(    3500/10000): loss= 1965.46898893856\n",
      "Losgistic Regression(    3600/10000): loss= 1965.35072240076\n",
      "Losgistic Regression(    3700/10000): loss= 1965.21356061424\n",
      "Losgistic Regression(    3800/10000): loss= 1965.14085472616\n",
      "Losgistic Regression(    3900/10000): loss= 1965.07803269186\n",
      "Losgistic Regression(    4000/10000): loss= 1964.98408943267\n",
      "Losgistic Regression(    4100/10000): loss= 1964.90960337618\n",
      "Losgistic Regression(    4200/10000): loss= 1964.83591063741\n",
      "Losgistic Regression(    4300/10000): loss= 1964.76015109391\n",
      "Losgistic Regression(    4400/10000): loss= 1964.68000070507\n",
      "Losgistic Regression(    4500/10000): loss= 1964.6010433887\n",
      "Losgistic Regression(    4600/10000): loss= 1964.51717986172\n",
      "Losgistic Regression(    4700/10000): loss= 1964.43767835649\n",
      "Losgistic Regression(    4800/10000): loss= 1964.35676822214\n",
      "Losgistic Regression(    4900/10000): loss= 1964.27355862403\n",
      "Losgistic Regression(    5000/10000): loss= 1964.19367999212\n",
      "Losgistic Regression(    5100/10000): loss= 1964.11232771171\n",
      "Losgistic Regression(    5200/10000): loss= 1964.02865795787\n",
      "Losgistic Regression(    5300/10000): loss= 1963.94454816859\n",
      "Losgistic Regression(    5400/10000): loss= 1963.859382753\n",
      "Losgistic Regression(    5500/10000): loss= 1963.78657640886\n",
      "Losgistic Regression(    5600/10000): loss= 1963.7861468627\n",
      "Totoal number of iterations =  5600\n",
      "Loss                        =  1963.78614686\n",
      "Time for  3th cross validation = 92.1409s\n",
      "Training Accuracy         = 0.8346\n",
      "Cross Validation Accuracy = 0.807668\n",
      "Losgistic Regression(       0/10000): loss= 3440.07569314414\n",
      "Losgistic Regression(     100/10000): loss= 2340.04584639612\n",
      "Losgistic Regression(     200/10000): loss= 2173.6630512212\n",
      "Losgistic Regression(     300/10000): loss= 2111.40159922639\n",
      "Losgistic Regression(     400/10000): loss= 2063.88571721465\n",
      "Losgistic Regression(     500/10000): loss= 2028.99454405234\n",
      "Losgistic Regression(     600/10000): loss= 2003.24994569784\n",
      "Losgistic Regression(     700/10000): loss= 1982.95356134161\n",
      "Losgistic Regression(     800/10000): loss= 1966.74795206592\n",
      "Losgistic Regression(     900/10000): loss= 1954.11334584739\n",
      "Losgistic Regression(    1000/10000): loss= 1946.01856067693\n",
      "Losgistic Regression(    1100/10000): loss= 1939.39712471985\n",
      "Losgistic Regression(    1200/10000): loss= 1933.01209210788\n",
      "Losgistic Regression(    1300/10000): loss= 1930.25626881486\n",
      "Losgistic Regression(    1400/10000): loss= 1928.02844175446\n",
      "Losgistic Regression(    1500/10000): loss= 1926.29416035217\n",
      "Losgistic Regression(    1600/10000): loss= 1925.8095520203\n",
      "Losgistic Regression(    1700/10000): loss= 1925.80596189201\n",
      "Totoal number of iterations =  1700\n",
      "Loss                        =  1925.80596189\n",
      "Time for  4th cross validation = 28.1989s\n",
      "Training Accuracy         = 0.8388\n",
      "Cross Validation Accuracy = 0.805544\n",
      "*************** ([0.83960000000000001, 0.83699999999999997, 0.84279999999999999, 0.83460000000000001, 0.83879999999999999], [0.81000799999999995, 0.80048799999999998, 0.81146399999999996, 0.80766800000000005, 0.80554400000000004])\n",
      "Losgistic Regression(       0/10000): loss= 3436.88854196188\n",
      "Losgistic Regression(     100/10000): loss= 2422.07502547281\n",
      "Losgistic Regression(     200/10000): loss= 2276.92427738244\n",
      "Losgistic Regression(     300/10000): loss= 2240.15253955927\n",
      "Losgistic Regression(     400/10000): loss= 2219.89722417542\n",
      "Losgistic Regression(     500/10000): loss= 2207.23536571765\n",
      "Losgistic Regression(     600/10000): loss= 2198.44291979959\n",
      "Losgistic Regression(     700/10000): loss= 2192.80095458832\n",
      "Losgistic Regression(     800/10000): loss= 2188.28019142184\n",
      "Losgistic Regression(     900/10000): loss= 2185.13644614844\n",
      "Losgistic Regression(    1000/10000): loss= 2182.43311329151\n",
      "Losgistic Regression(    1100/10000): loss= 2180.1077817887\n",
      "Losgistic Regression(    1200/10000): loss= 2178.21815489864\n",
      "Losgistic Regression(    1300/10000): loss= 2176.8520406411\n",
      "Losgistic Regression(    1400/10000): loss= 2175.58355205266\n",
      "Losgistic Regression(    1500/10000): loss= 2175.01052864813\n",
      "Losgistic Regression(    1600/10000): loss= 2174.79613645634\n",
      "Losgistic Regression(    1700/10000): loss= 2174.5943976486\n",
      "Losgistic Regression(    1800/10000): loss= 2174.48800237438\n",
      "Losgistic Regression(    1900/10000): loss= 2174.4844778533\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  2174.48447785\n",
      "Time for  0th cross validation = 31.3791s\n",
      "Training Accuracy         = 0.8216\n",
      "Cross Validation Accuracy = 0.802548\n",
      "Losgistic Regression(       0/10000): loss= 3438.81810800326\n",
      "Losgistic Regression(     100/10000): loss= 2461.14048511831\n",
      "Losgistic Regression(     200/10000): loss= 2329.2226596707\n",
      "Losgistic Regression(     300/10000): loss= 2290.2917151482\n",
      "Losgistic Regression(     400/10000): loss= 2269.25337219101\n",
      "Losgistic Regression(     500/10000): loss= 2256.05916332991\n",
      "Losgistic Regression(     600/10000): loss= 2247.58441059543\n",
      "Losgistic Regression(     700/10000): loss= 2242.00429615003\n",
      "Losgistic Regression(     800/10000): loss= 2238.17272390211\n",
      "Losgistic Regression(     900/10000): loss= 2235.40044380255\n",
      "Losgistic Regression(    1000/10000): loss= 2233.29905490685\n",
      "Losgistic Regression(    1100/10000): loss= 2232.56140104189\n",
      "Losgistic Regression(    1200/10000): loss= 2232.21822242884\n",
      "Losgistic Regression(    1300/10000): loss= 2231.94762310427\n",
      "Losgistic Regression(    1400/10000): loss= 2231.64889901358\n",
      "Losgistic Regression(    1500/10000): loss= 2231.27857636443\n",
      "Losgistic Regression(    1600/10000): loss= 2230.86712859216\n",
      "Losgistic Regression(    1700/10000): loss= 2230.44331280567\n",
      "Losgistic Regression(    1800/10000): loss= 2230.02637730451\n",
      "Losgistic Regression(    1900/10000): loss= 2229.59197805255\n",
      "Losgistic Regression(    2000/10000): loss= 2229.14927372669\n",
      "Losgistic Regression(    2100/10000): loss= 2228.7021664696\n",
      "Losgistic Regression(    2200/10000): loss= 2228.28312902109\n",
      "Losgistic Regression(    2300/10000): loss= 2227.87387457787\n",
      "Losgistic Regression(    2400/10000): loss= 2227.67991899212\n",
      "Losgistic Regression(    2500/10000): loss= 2227.67855641063\n",
      "Totoal number of iterations =  2500\n",
      "Loss                        =  2227.67855641\n",
      "Time for  1th cross validation = 41.0332s\n",
      "Training Accuracy         = 0.8178\n",
      "Cross Validation Accuracy = 0.797564\n",
      "Losgistic Regression(       0/10000): loss= 3439.16702762254\n",
      "Losgistic Regression(     100/10000): loss= 2412.7847749669\n",
      "Losgistic Regression(     200/10000): loss= 2273.21703185044\n",
      "Losgistic Regression(     300/10000): loss= 2231.56818032164\n",
      "Losgistic Regression(     400/10000): loss= 2209.72198623999\n",
      "Losgistic Regression(     500/10000): loss= 2194.7499355595\n",
      "Losgistic Regression(     600/10000): loss= 2183.87559348125\n",
      "Losgistic Regression(     700/10000): loss= 2176.07678910064\n",
      "Losgistic Regression(     800/10000): loss= 2171.23785499966\n",
      "Losgistic Regression(     900/10000): loss= 2167.92798615989\n",
      "Losgistic Regression(    1000/10000): loss= 2165.29665138148\n",
      "Losgistic Regression(    1100/10000): loss= 2163.49740313451\n",
      "Losgistic Regression(    1200/10000): loss= 2162.18065542125\n",
      "Losgistic Regression(    1300/10000): loss= 2161.83115950643\n",
      "Losgistic Regression(    1400/10000): loss= 2161.62846017097\n",
      "Losgistic Regression(    1500/10000): loss= 2161.40379560288\n",
      "Losgistic Regression(    1600/10000): loss= 2161.13969240353\n",
      "Losgistic Regression(    1700/10000): loss= 2160.83674495137\n",
      "Losgistic Regression(    1800/10000): loss= 2160.49735684051\n",
      "Losgistic Regression(    1900/10000): loss= 2160.12391406136\n",
      "Losgistic Regression(    2000/10000): loss= 2159.7208427634\n",
      "Losgistic Regression(    2100/10000): loss= 2159.30467062906\n",
      "Losgistic Regression(    2200/10000): loss= 2158.86981874956\n",
      "Losgistic Regression(    2300/10000): loss= 2158.41978082437\n",
      "Losgistic Regression(    2400/10000): loss= 2157.97322734457\n",
      "Losgistic Regression(    2500/10000): loss= 2157.55279966166\n",
      "Losgistic Regression(    2600/10000): loss= 2157.20192888207\n",
      "Losgistic Regression(    2700/10000): loss= 2157.19801166341\n",
      "Totoal number of iterations =  2700\n",
      "Loss                        =  2157.19801166\n",
      "Time for  2th cross validation = 44.6733s\n",
      "Training Accuracy         = 0.8218\n",
      "Cross Validation Accuracy = 0.803776\n",
      "Losgistic Regression(       0/10000): loss= 3439.60904650075\n",
      "Losgistic Regression(     100/10000): loss= 2419.29973869335\n",
      "Losgistic Regression(     200/10000): loss= 2285.98584155347\n",
      "Losgistic Regression(     300/10000): loss= 2249.70592358447\n",
      "Losgistic Regression(     400/10000): loss= 2228.31006707728\n",
      "Losgistic Regression(     500/10000): loss= 2213.05452610922\n",
      "Losgistic Regression(     600/10000): loss= 2203.68022946573\n",
      "Losgistic Regression(     700/10000): loss= 2196.73934379506\n",
      "Losgistic Regression(     800/10000): loss= 2192.22819796773\n",
      "Losgistic Regression(     900/10000): loss= 2189.1992513601\n",
      "Losgistic Regression(    1000/10000): loss= 2186.6212477953\n",
      "Losgistic Regression(    1100/10000): loss= 2184.35256203238\n",
      "Losgistic Regression(    1200/10000): loss= 2182.5870375483\n",
      "Losgistic Regression(    1300/10000): loss= 2181.11125298023\n",
      "Losgistic Regression(    1400/10000): loss= 2179.55999488206\n",
      "Losgistic Regression(    1500/10000): loss= 2178.40285971298\n",
      "Losgistic Regression(    1600/10000): loss= 2177.28072447775\n",
      "Losgistic Regression(    1700/10000): loss= 2176.05835065014\n",
      "Losgistic Regression(    1800/10000): loss= 2175.93807163478\n",
      "Losgistic Regression(    1900/10000): loss= 2175.93262110481\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  2175.9326211\n",
      "Time for  3th cross validation = 30.656s\n",
      "Training Accuracy         = 0.8216\n",
      "Cross Validation Accuracy = 0.803396\n",
      "Losgistic Regression(       0/10000): loss= 3440.50436362544\n",
      "Losgistic Regression(     100/10000): loss= 2406.51789928352\n",
      "Losgistic Regression(     200/10000): loss= 2277.60360730416\n",
      "Losgistic Regression(     300/10000): loss= 2242.20297676209\n",
      "Losgistic Regression(     400/10000): loss= 2219.83676774876\n",
      "Losgistic Regression(     500/10000): loss= 2203.38789389431\n",
      "Losgistic Regression(     600/10000): loss= 2192.3956300541\n",
      "Losgistic Regression(     700/10000): loss= 2185.80013400588\n",
      "Losgistic Regression(     800/10000): loss= 2180.33255372055\n",
      "Losgistic Regression(     900/10000): loss= 2174.86161835612\n",
      "Losgistic Regression(    1000/10000): loss= 2170.32367302994\n",
      "Losgistic Regression(    1100/10000): loss= 2166.66270667815\n",
      "Losgistic Regression(    1200/10000): loss= 2164.84608882104\n",
      "Losgistic Regression(    1300/10000): loss= 2163.39548136397\n",
      "Losgistic Regression(    1400/10000): loss= 2161.55143104348\n",
      "Losgistic Regression(    1500/10000): loss= 2160.37509144261\n",
      "Losgistic Regression(    1600/10000): loss= 2159.67272556972\n",
      "Losgistic Regression(    1700/10000): loss= 2159.67774996368\n",
      "Totoal number of iterations =  1700\n",
      "Loss                        =  2159.67774996\n",
      "Time for  4th cross validation = 27.5177s\n",
      "Training Accuracy         =  0.819\n",
      "Cross Validation Accuracy = 0.798812\n",
      "*************** ([0.8216, 0.81779999999999997, 0.82179999999999997, 0.8216, 0.81899999999999995], [0.80254800000000004, 0.79756400000000005, 0.80377600000000005, 0.803396, 0.79881199999999997])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.85899999999999999,\n",
       "   0.85040000000000004,\n",
       "   0.85640000000000005,\n",
       "   0.85360000000000003,\n",
       "   0.85419999999999996],\n",
       "  [0.80532800000000004,\n",
       "   0.79402799999999996,\n",
       "   0.80358399999999996,\n",
       "   0.80544800000000005,\n",
       "   0.80587600000000004]),\n",
       " ([0.85840000000000005,\n",
       "   0.84919999999999995,\n",
       "   0.85619999999999996,\n",
       "   0.85219999999999996,\n",
       "   0.85360000000000003],\n",
       "  [0.80571599999999999,\n",
       "   0.79476800000000003,\n",
       "   0.80413599999999996,\n",
       "   0.80613199999999996,\n",
       "   0.80635199999999996]),\n",
       " ([0.85840000000000005,\n",
       "   0.84999999999999998,\n",
       "   0.85680000000000001,\n",
       "   0.85019999999999996,\n",
       "   0.85399999999999998],\n",
       "  [0.80754800000000004,\n",
       "   0.79569599999999996,\n",
       "   0.805732,\n",
       "   0.80727199999999999,\n",
       "   0.80766000000000004]),\n",
       " ([0.85460000000000003,\n",
       "   0.84179999999999999,\n",
       "   0.85299999999999998,\n",
       "   0.84140000000000004,\n",
       "   0.84899999999999998],\n",
       "  [0.80979999999999996,\n",
       "   0.79728399999999999,\n",
       "   0.810392,\n",
       "   0.80813999999999997,\n",
       "   0.80949599999999999]),\n",
       " ([0.83960000000000001,\n",
       "   0.83699999999999997,\n",
       "   0.84279999999999999,\n",
       "   0.83460000000000001,\n",
       "   0.83879999999999999],\n",
       "  [0.81000799999999995,\n",
       "   0.80048799999999998,\n",
       "   0.81146399999999996,\n",
       "   0.80766800000000005,\n",
       "   0.80554400000000004]),\n",
       " ([0.8216,\n",
       "   0.81779999999999997,\n",
       "   0.82179999999999997,\n",
       "   0.8216,\n",
       "   0.81899999999999995],\n",
       "  [0.80254800000000004,\n",
       "   0.79756400000000005,\n",
       "   0.80377600000000005,\n",
       "   0.803396,\n",
       "   0.79881199999999997])]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_5000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(5000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_5000.append(tmp)\n",
    "\n",
    "accu_5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 1725.77375301506\n",
      "Losgistic Regression(     100/10000): loss= 1237.43155470564\n",
      "Losgistic Regression(     200/10000): loss= 1074.24030546807\n",
      "Losgistic Regression(     300/10000): loss= 1014.06618478128\n",
      "Losgistic Regression(     400/10000): loss= 978.027651243102\n",
      "Losgistic Regression(     500/10000): loss= 951.520269440068\n",
      "Losgistic Regression(     600/10000): loss= 930.424732596964\n",
      "Losgistic Regression(     700/10000): loss= 913.044139632886\n",
      "Losgistic Regression(     800/10000): loss= 897.968405972309\n",
      "Losgistic Regression(     900/10000): loss= 884.696716677702\n",
      "Losgistic Regression(    1000/10000): loss= 872.801885068041\n",
      "Losgistic Regression(    1100/10000): loss= 862.062953044757\n",
      "Losgistic Regression(    1200/10000): loss= 852.332872905332\n",
      "Losgistic Regression(    1300/10000): loss= 843.503209681073\n",
      "Losgistic Regression(    1400/10000): loss= 835.484573541811\n",
      "Losgistic Regression(    1500/10000): loss= 828.184729862094\n",
      "Losgistic Regression(    1600/10000): loss= 821.520489627183\n",
      "Losgistic Regression(    1700/10000): loss= 815.418109721283\n",
      "Losgistic Regression(    1800/10000): loss= 809.813549915975\n",
      "Losgistic Regression(    1900/10000): loss= 804.650222536568\n",
      "Losgistic Regression(    2000/10000): loss= 799.876595212611\n",
      "Losgistic Regression(    2100/10000): loss= 795.447664922853\n",
      "Losgistic Regression(    2200/10000): loss= 791.325175454427\n",
      "Losgistic Regression(    2300/10000): loss= 787.477573446029\n",
      "Losgistic Regression(    2400/10000): loss= 783.877770719885\n",
      "Losgistic Regression(    2500/10000): loss= 780.500490183363\n",
      "Losgistic Regression(    2600/10000): loss= 776.882791543116\n",
      "Losgistic Regression(    2700/10000): loss= 773.379428739551\n",
      "Losgistic Regression(    2800/10000): loss= 770.084397471544\n",
      "Losgistic Regression(    2900/10000): loss= 766.969566296655\n",
      "Losgistic Regression(    3000/10000): loss= 764.006662054039\n",
      "Losgistic Regression(    3100/10000): loss= 761.175156675755\n",
      "Losgistic Regression(    3200/10000): loss= 758.138573883386\n",
      "Losgistic Regression(    3300/10000): loss= 755.175267423992\n",
      "Losgistic Regression(    3400/10000): loss= 752.319615063159\n",
      "Losgistic Regression(    3500/10000): loss= 749.365087319731\n",
      "Losgistic Regression(    3600/10000): loss= 746.44336914406\n",
      "Losgistic Regression(    3700/10000): loss= 743.626879045995\n",
      "Losgistic Regression(    3800/10000): loss= 740.914104109728\n",
      "Losgistic Regression(    3900/10000): loss= 738.293929862056\n",
      "Losgistic Regression(    4000/10000): loss= 735.752506704935\n",
      "Losgistic Regression(    4100/10000): loss= 733.286619205147\n",
      "Losgistic Regression(    4200/10000): loss= 730.896850099576\n",
      "Losgistic Regression(    4300/10000): loss= 728.253524582533\n",
      "Losgistic Regression(    4400/10000): loss= 725.561596610137\n",
      "Losgistic Regression(    4500/10000): loss= 722.94852457766\n",
      "Losgistic Regression(    4600/10000): loss= 720.417118394179\n",
      "Losgistic Regression(    4700/10000): loss= 717.946482397499\n",
      "Losgistic Regression(    4800/10000): loss= 715.52950501773\n",
      "Losgistic Regression(    4900/10000): loss= 713.165691650749\n",
      "Losgistic Regression(    5000/10000): loss= 710.849843577383\n",
      "Losgistic Regression(    5100/10000): loss= 708.589209857254\n",
      "Losgistic Regression(    5200/10000): loss= 706.391011509459\n",
      "Losgistic Regression(    5300/10000): loss= 704.25751740066\n",
      "Losgistic Regression(    5400/10000): loss= 702.181769094512\n",
      "Losgistic Regression(    5500/10000): loss= 700.041696502665\n",
      "Losgistic Regression(    5600/10000): loss= 697.9094935969\n",
      "Losgistic Regression(    5700/10000): loss= 695.699824989111\n",
      "Losgistic Regression(    5800/10000): loss= 693.312628092548\n",
      "Losgistic Regression(    5900/10000): loss= 690.893272013875\n",
      "Losgistic Regression(    6000/10000): loss= 688.512197054916\n",
      "Losgistic Regression(    6100/10000): loss= 686.174491690843\n",
      "Losgistic Regression(    6200/10000): loss= 683.884796819837\n",
      "Losgistic Regression(    6300/10000): loss= 681.643077101081\n",
      "Losgistic Regression(    6400/10000): loss= 679.367761049828\n",
      "Losgistic Regression(    6500/10000): loss= 677.052377271445\n",
      "Losgistic Regression(    6600/10000): loss= 674.77780115829\n",
      "Losgistic Regression(    6700/10000): loss= 672.536897428978\n",
      "Losgistic Regression(    6800/10000): loss= 670.323362031155\n",
      "Losgistic Regression(    6900/10000): loss= 668.051025178809\n",
      "Losgistic Regression(    7000/10000): loss= 665.52554159041\n",
      "Losgistic Regression(    7100/10000): loss= 663.030429634338\n",
      "Losgistic Regression(    7200/10000): loss= 660.513070598005\n",
      "Losgistic Regression(    7300/10000): loss= 657.981269947697\n",
      "Losgistic Regression(    7400/10000): loss= 655.47730986618\n",
      "Losgistic Regression(    7500/10000): loss= 653.009404233245\n",
      "Losgistic Regression(    7600/10000): loss= 650.581825179534\n",
      "Losgistic Regression(    7700/10000): loss= 648.196209446445\n",
      "Losgistic Regression(    7800/10000): loss= 645.855818694734\n",
      "Losgistic Regression(    7900/10000): loss= 643.561894194413\n",
      "Losgistic Regression(    8000/10000): loss= 641.312000211746\n",
      "Losgistic Regression(    8100/10000): loss= 639.109534397726\n",
      "Losgistic Regression(    8200/10000): loss= 636.961683980917\n",
      "Losgistic Regression(    8300/10000): loss= 634.799290217609\n",
      "Losgistic Regression(    8400/10000): loss= 632.672937023156\n",
      "Losgistic Regression(    8500/10000): loss= 630.590091978443\n",
      "Losgistic Regression(    8600/10000): loss= 628.554168527897\n",
      "Losgistic Regression(    8700/10000): loss= 626.540138029335\n",
      "Losgistic Regression(    8800/10000): loss= 624.511967038077\n",
      "Losgistic Regression(    8900/10000): loss= 622.547146795716\n",
      "Losgistic Regression(    9000/10000): loss= 620.63701477058\n",
      "Losgistic Regression(    9100/10000): loss= 618.776701910284\n",
      "Losgistic Regression(    9200/10000): loss= 616.958496596126\n",
      "Losgistic Regression(    9300/10000): loss= 615.175616828867\n",
      "Losgistic Regression(    9400/10000): loss= 613.431504257603\n",
      "Losgistic Regression(    9500/10000): loss= 611.729954509764\n",
      "Losgistic Regression(    9600/10000): loss= 610.067269806176\n",
      "Losgistic Regression(    9700/10000): loss= 608.438213869229\n",
      "Losgistic Regression(    9800/10000): loss= 606.788785872125\n",
      "Losgistic Regression(    9900/10000): loss= 604.991952078666\n",
      "Time for  0th cross validation = 83.6812s\n",
      "Training Accuracy         = 0.8832\n",
      "Cross Validation Accuracy = 0.784008\n",
      "Losgistic Regression(       0/10000): loss= 1724.814857444\n",
      "Losgistic Regression(     100/10000): loss= 1222.98495004634\n",
      "Losgistic Regression(     200/10000): loss= 1061.3014782467\n",
      "Losgistic Regression(     300/10000): loss= 995.832498248115\n",
      "Losgistic Regression(     400/10000): loss= 957.500008870819\n",
      "Losgistic Regression(     500/10000): loss= 929.475588296635\n",
      "Losgistic Regression(     600/10000): loss= 907.092668403834\n",
      "Losgistic Regression(     700/10000): loss= 888.401725215759\n",
      "Losgistic Regression(     800/10000): loss= 872.003075358262\n",
      "Losgistic Regression(     900/10000): loss= 857.030257059228\n",
      "Losgistic Regression(    1000/10000): loss= 844.107573150108\n",
      "Losgistic Regression(    1100/10000): loss= 832.95617782461\n",
      "Losgistic Regression(    1200/10000): loss= 823.256515875825\n",
      "Losgistic Regression(    1300/10000): loss= 814.814958886843\n",
      "Losgistic Regression(    1400/10000): loss= 807.472951370465\n",
      "Losgistic Regression(    1500/10000): loss= 801.053124403236\n",
      "Losgistic Regression(    1600/10000): loss= 795.318164701772\n",
      "Losgistic Regression(    1700/10000): loss= 789.591597241278\n",
      "Losgistic Regression(    1800/10000): loss= 784.466658329429\n",
      "Losgistic Regression(    1900/10000): loss= 779.8535264438\n",
      "Losgistic Regression(    2000/10000): loss= 775.443700705197\n",
      "Losgistic Regression(    2100/10000): loss= 770.868423871056\n",
      "Losgistic Regression(    2200/10000): loss= 765.573891120385\n",
      "Losgistic Regression(    2300/10000): loss= 760.385648586441\n",
      "Losgistic Regression(    2400/10000): loss= 755.348842907565\n",
      "Losgistic Regression(    2500/10000): loss= 750.080095691769\n",
      "Losgistic Regression(    2600/10000): loss= 744.980182868051\n",
      "Losgistic Regression(    2700/10000): loss= 740.024923643562\n",
      "Losgistic Regression(    2800/10000): loss= 735.054359041118\n",
      "Losgistic Regression(    2900/10000): loss= 729.558994367887\n",
      "Losgistic Regression(    3000/10000): loss= 723.953472345845\n",
      "Losgistic Regression(    3100/10000): loss= 718.419251222667\n",
      "Losgistic Regression(    3200/10000): loss= 712.954975240717\n",
      "Losgistic Regression(    3300/10000): loss= 707.304650701405\n",
      "Losgistic Regression(    3400/10000): loss= 700.874556256969\n",
      "Losgistic Regression(    3500/10000): loss= 694.561349652008\n",
      "Losgistic Regression(    3600/10000): loss= 688.400376762565\n",
      "Losgistic Regression(    3700/10000): loss= 682.387240479151\n",
      "Losgistic Regression(    3800/10000): loss= 676.535024877311\n",
      "Losgistic Regression(    3900/10000): loss= 670.846812576379\n",
      "Losgistic Regression(    4000/10000): loss= 664.979154304175\n",
      "Losgistic Regression(    4100/10000): loss= 659.209811850983\n",
      "Losgistic Regression(    4200/10000): loss= 653.59597166036\n",
      "Losgistic Regression(    4300/10000): loss= 648.135088005125\n",
      "Losgistic Regression(    4400/10000): loss= 642.749350589757\n",
      "Losgistic Regression(    4500/10000): loss= 637.056588573402\n",
      "Losgistic Regression(    4600/10000): loss= 631.572459938828\n",
      "Losgistic Regression(    4700/10000): loss= 626.287463220662\n",
      "Losgistic Regression(    4800/10000): loss= 621.15719527941\n",
      "Losgistic Regression(    4900/10000): loss= 616.030213382713\n",
      "Losgistic Regression(    5000/10000): loss= 611.078347667441\n",
      "Losgistic Regression(    5100/10000): loss= 606.335190530468\n",
      "Losgistic Regression(    5200/10000): loss= 601.751281613006\n",
      "Losgistic Regression(    5300/10000): loss= 597.195891600591\n",
      "Losgistic Regression(    5400/10000): loss= 592.516441923868\n",
      "Losgistic Regression(    5500/10000): loss= 587.846702656705\n",
      "Losgistic Regression(    5600/10000): loss= 583.305059983862\n",
      "Losgistic Regression(    5700/10000): loss= 578.834417794333\n",
      "Losgistic Regression(    5800/10000): loss= 574.295683635531\n",
      "Losgistic Regression(    5900/10000): loss= 569.818943316322\n",
      "Losgistic Regression(    6000/10000): loss= 565.428230757643\n",
      "Losgistic Regression(    6100/10000): loss= 561.108056228409\n",
      "Losgistic Regression(    6200/10000): loss= 556.832768474383\n",
      "Losgistic Regression(    6300/10000): loss= 552.588778852667\n",
      "Losgistic Regression(    6400/10000): loss= 548.384891212929\n",
      "Losgistic Regression(    6500/10000): loss= 544.222968589352\n",
      "Losgistic Regression(    6600/10000): loss= 540.078257000474\n",
      "Losgistic Regression(    6700/10000): loss= 535.930315022805\n",
      "Losgistic Regression(    6800/10000): loss= 531.77179235835\n",
      "Losgistic Regression(    6900/10000): loss= 527.459437004537\n",
      "Losgistic Regression(    7000/10000): loss= 522.899960740485\n",
      "Losgistic Regression(    7100/10000): loss= 518.344690911204\n",
      "Losgistic Regression(    7200/10000): loss= 513.786045304396\n",
      "Losgistic Regression(    7300/10000): loss= 509.223738773628\n",
      "Losgistic Regression(    7400/10000): loss= 504.678093720595\n",
      "Losgistic Regression(    7500/10000): loss= 500.143637205879\n",
      "Losgistic Regression(    7600/10000): loss= 495.401950067069\n",
      "Losgistic Regression(    7700/10000): loss= 490.653275971835\n",
      "Losgistic Regression(    7800/10000): loss= 485.807551064417\n",
      "Losgistic Regression(    7900/10000): loss= 480.889902698203\n",
      "Losgistic Regression(    8000/10000): loss= 475.962834711058\n",
      "Losgistic Regression(    8100/10000): loss= 471.027588041421\n",
      "Losgistic Regression(    8200/10000): loss= 466.119638132812\n",
      "Losgistic Regression(    8300/10000): loss= 461.19537385493\n",
      "Losgistic Regression(    8400/10000): loss= 456.135543809476\n",
      "Losgistic Regression(    8500/10000): loss= 451.032259087595\n",
      "Losgistic Regression(    8600/10000): loss= 445.751552096507\n",
      "Losgistic Regression(    8700/10000): loss= 440.481865224388\n",
      "Losgistic Regression(    8800/10000): loss= 435.239819792648\n",
      "Losgistic Regression(    8900/10000): loss= 430.039296767203\n",
      "Losgistic Regression(    9000/10000): loss= 424.869067442884\n",
      "Losgistic Regression(    9100/10000): loss= 419.714280624596\n",
      "Losgistic Regression(    9200/10000): loss= 414.579045752933\n",
      "Losgistic Regression(    9300/10000): loss= 409.445309911269\n",
      "Losgistic Regression(    9400/10000): loss= 404.291528134814\n",
      "Losgistic Regression(    9500/10000): loss= 399.108641244243\n",
      "Losgistic Regression(    9600/10000): loss= 393.880742247812\n",
      "Losgistic Regression(    9700/10000): loss= 388.615332175891\n",
      "Losgistic Regression(    9800/10000): loss= 383.326030047509\n",
      "Losgistic Regression(    9900/10000): loss= 378.017292020486\n",
      "Time for  1th cross validation = 83.9296s\n",
      "Training Accuracy         = 0.8864\n",
      "Cross Validation Accuracy = 0.77524\n",
      "Losgistic Regression(       0/10000): loss= 1726.0657242736\n",
      "Losgistic Regression(     100/10000): loss= 1223.73950531529\n",
      "Losgistic Regression(     200/10000): loss= 1066.33070269407\n",
      "Losgistic Regression(     300/10000): loss= 1004.28207083148\n",
      "Losgistic Regression(     400/10000): loss= 964.047178934336\n",
      "Losgistic Regression(     500/10000): loss= 934.845327726354\n",
      "Losgistic Regression(     600/10000): loss= 912.259966041411\n",
      "Losgistic Regression(     700/10000): loss= 893.948881608084\n",
      "Losgistic Regression(     800/10000): loss= 878.604438063928\n",
      "Losgistic Regression(     900/10000): loss= 865.513317836607\n",
      "Losgistic Regression(    1000/10000): loss= 852.706721912075\n",
      "Losgistic Regression(    1100/10000): loss= 840.830457153392\n",
      "Losgistic Regression(    1200/10000): loss= 830.131906936437\n",
      "Losgistic Regression(    1300/10000): loss= 820.498432970123\n",
      "Losgistic Regression(    1400/10000): loss= 811.796445227663\n",
      "Losgistic Regression(    1500/10000): loss= 803.903679337103\n",
      "Losgistic Regression(    1600/10000): loss= 796.721658929592\n",
      "Losgistic Regression(    1700/10000): loss= 790.152829003292\n",
      "Losgistic Regression(    1800/10000): loss= 783.416218456375\n",
      "Losgistic Regression(    1900/10000): loss= 776.14274602673\n",
      "Losgistic Regression(    2000/10000): loss= 768.90507844094\n",
      "Losgistic Regression(    2100/10000): loss= 762.162396115243\n",
      "Losgistic Regression(    2200/10000): loss= 755.825441229267\n",
      "Losgistic Regression(    2300/10000): loss= 749.267243041033\n",
      "Losgistic Regression(    2400/10000): loss= 742.469730812116\n",
      "Losgistic Regression(    2500/10000): loss= 736.037964128169\n",
      "Losgistic Regression(    2600/10000): loss= 729.949191118883\n",
      "Losgistic Regression(    2700/10000): loss= 724.183728707354\n",
      "Losgistic Regression(    2800/10000): loss= 718.697398037917\n",
      "Losgistic Regression(    2900/10000): loss= 713.261907950068\n",
      "Losgistic Regression(    3000/10000): loss= 708.146919496125\n",
      "Losgistic Regression(    3100/10000): loss= 703.323966708978\n",
      "Losgistic Regression(    3200/10000): loss= 698.787912935346\n",
      "Losgistic Regression(    3300/10000): loss= 694.537969213512\n",
      "Losgistic Regression(    3400/10000): loss= 690.571347209487\n",
      "Losgistic Regression(    3500/10000): loss= 686.887398859487\n",
      "Losgistic Regression(    3600/10000): loss= 683.457865070139\n",
      "Losgistic Regression(    3700/10000): loss= 680.240080045844\n",
      "Losgistic Regression(    3800/10000): loss= 677.201132239424\n",
      "Losgistic Regression(    3900/10000): loss= 674.321836296713\n",
      "Losgistic Regression(    4000/10000): loss= 671.586424503399\n",
      "Losgistic Regression(    4100/10000): loss= 668.873699658915\n",
      "Losgistic Regression(    4200/10000): loss= 666.210140466743\n",
      "Losgistic Regression(    4300/10000): loss= 663.673635378751\n",
      "Losgistic Regression(    4400/10000): loss= 661.261275623352\n",
      "Losgistic Regression(    4500/10000): loss= 658.965449518948\n",
      "Losgistic Regression(    4600/10000): loss= 656.768861002683\n",
      "Losgistic Regression(    4700/10000): loss= 654.654306854772\n",
      "Losgistic Regression(    4800/10000): loss= 652.610757378876\n",
      "Losgistic Regression(    4900/10000): loss= 650.640948011163\n",
      "Losgistic Regression(    5000/10000): loss= 648.73650072762\n",
      "Losgistic Regression(    5100/10000): loss= 646.902270468455\n",
      "Losgistic Regression(    5200/10000): loss= 645.156534357518\n",
      "Losgistic Regression(    5300/10000): loss= 643.437758679442\n",
      "Losgistic Regression(    5400/10000): loss= 641.735454703755\n",
      "Losgistic Regression(    5500/10000): loss= 640.099153028292\n",
      "Losgistic Regression(    5600/10000): loss= 638.51480913222\n",
      "Losgistic Regression(    5700/10000): loss= 636.982316316371\n",
      "Losgistic Regression(    5800/10000): loss= 635.501390832766\n",
      "Losgistic Regression(    5900/10000): loss= 634.056971043687\n",
      "Losgistic Regression(    6000/10000): loss= 632.649502786175\n",
      "Losgistic Regression(    6100/10000): loss= 631.276139399031\n",
      "Losgistic Regression(    6200/10000): loss= 629.93310497766\n",
      "Losgistic Regression(    6300/10000): loss= 628.618788167835\n",
      "Losgistic Regression(    6400/10000): loss= 627.346212907812\n",
      "Losgistic Regression(    6500/10000): loss= 626.116473622508\n",
      "Losgistic Regression(    6600/10000): loss= 624.921971178796\n",
      "Losgistic Regression(    6700/10000): loss= 623.754369467213\n",
      "Losgistic Regression(    6800/10000): loss= 622.618982553011\n",
      "Losgistic Regression(    6900/10000): loss= 621.516792391457\n",
      "Losgistic Regression(    7000/10000): loss= 620.448192771629\n",
      "Losgistic Regression(    7100/10000): loss= 619.235393719311\n",
      "Losgistic Regression(    7200/10000): loss= 618.037322955008\n",
      "Losgistic Regression(    7300/10000): loss= 616.871611897913\n",
      "Losgistic Regression(    7400/10000): loss= 615.734434150066\n",
      "Losgistic Regression(    7500/10000): loss= 614.625260722922\n",
      "Losgistic Regression(    7600/10000): loss= 613.518588072561\n",
      "Losgistic Regression(    7700/10000): loss= 612.393334163346\n",
      "Losgistic Regression(    7800/10000): loss= 611.288189480565\n",
      "Losgistic Regression(    7900/10000): loss= 610.198762162843\n",
      "Losgistic Regression(    8000/10000): loss= 609.13057147693\n",
      "Losgistic Regression(    8100/10000): loss= 608.080526427522\n",
      "Losgistic Regression(    8200/10000): loss= 607.03929385651\n",
      "Losgistic Regression(    8300/10000): loss= 606.022315463991\n",
      "Losgistic Regression(    8400/10000): loss= 605.028156300611\n",
      "Losgistic Regression(    8500/10000): loss= 604.061043823138\n",
      "Losgistic Regression(    8600/10000): loss= 603.124245234683\n",
      "Losgistic Regression(    8700/10000): loss= 602.222979449997\n",
      "Losgistic Regression(    8800/10000): loss= 601.356529072053\n",
      "Losgistic Regression(    8900/10000): loss= 600.512237012553\n",
      "Losgistic Regression(    9000/10000): loss= 599.689082144395\n",
      "Losgistic Regression(    9100/10000): loss= 598.90238446385\n",
      "Losgistic Regression(    9200/10000): loss= 598.155084661028\n",
      "Losgistic Regression(    9300/10000): loss= 597.440939537806\n",
      "Losgistic Regression(    9400/10000): loss= 596.758357442787\n",
      "Losgistic Regression(    9500/10000): loss= 596.104637115267\n",
      "Losgistic Regression(    9600/10000): loss= 595.479042574099\n",
      "Losgistic Regression(    9700/10000): loss= 594.882756769566\n",
      "Losgistic Regression(    9800/10000): loss= 594.148478208819\n",
      "Losgistic Regression(    9900/10000): loss= 593.416288059089\n",
      "Time for  2th cross validation = 83.0282s\n",
      "Training Accuracy         =   0.88\n",
      "Cross Validation Accuracy = 0.769156\n",
      "Losgistic Regression(       0/10000): loss= 1725.43977258251\n",
      "Losgistic Regression(     100/10000): loss= 1249.44492223939\n",
      "Losgistic Regression(     200/10000): loss= 1109.97797742257\n",
      "Losgistic Regression(     300/10000): loss= 1048.96579032818\n",
      "Losgistic Regression(     400/10000): loss= 1008.30115009525\n",
      "Losgistic Regression(     500/10000): loss= 977.778117713839\n",
      "Losgistic Regression(     600/10000): loss= 953.90534880792\n",
      "Losgistic Regression(     700/10000): loss= 934.370150729569\n",
      "Losgistic Regression(     800/10000): loss= 914.370322546053\n",
      "Losgistic Regression(     900/10000): loss= 895.60573394289\n",
      "Losgistic Regression(    1000/10000): loss= 874.586676076435\n",
      "Losgistic Regression(    1100/10000): loss= 855.840169113347\n",
      "Losgistic Regression(    1200/10000): loss= 839.414731586339\n",
      "Losgistic Regression(    1300/10000): loss= 825.068345647464\n",
      "Losgistic Regression(    1400/10000): loss= 812.671995287228\n",
      "Losgistic Regression(    1500/10000): loss= 802.000126644201\n",
      "Losgistic Regression(    1600/10000): loss= 792.657246927466\n",
      "Losgistic Regression(    1700/10000): loss= 783.913168152112\n",
      "Losgistic Regression(    1800/10000): loss= 776.049076938202\n",
      "Losgistic Regression(    1900/10000): loss= 768.925996823395\n",
      "Losgistic Regression(    2000/10000): loss= 762.410554855213\n",
      "Losgistic Regression(    2100/10000): loss= 756.363249342327\n",
      "Losgistic Regression(    2200/10000): loss= 750.686321268076\n",
      "Losgistic Regression(    2300/10000): loss= 745.052726660917\n",
      "Losgistic Regression(    2400/10000): loss= 739.643139754849\n",
      "Losgistic Regression(    2500/10000): loss= 734.598430123359\n",
      "Losgistic Regression(    2600/10000): loss= 729.849514745378\n",
      "Losgistic Regression(    2700/10000): loss= 725.343769203752\n",
      "Losgistic Regression(    2800/10000): loss= 721.084754194799\n",
      "Losgistic Regression(    2900/10000): loss= 717.098378551176\n",
      "Losgistic Regression(    3000/10000): loss= 713.38576481519\n",
      "Losgistic Regression(    3100/10000): loss= 709.923517452849\n",
      "Losgistic Regression(    3200/10000): loss= 706.698540380458\n",
      "Losgistic Regression(    3300/10000): loss= 703.70410777254\n",
      "Losgistic Regression(    3400/10000): loss= 700.914363107316\n",
      "Losgistic Regression(    3500/10000): loss= 697.98076129573\n",
      "Losgistic Regression(    3600/10000): loss= 695.068856475273\n",
      "Losgistic Regression(    3700/10000): loss= 692.283341707344\n",
      "Losgistic Regression(    3800/10000): loss= 689.481309467266\n",
      "Losgistic Regression(    3900/10000): loss= 686.552306158779\n",
      "Losgistic Regression(    4000/10000): loss= 683.471845663905\n",
      "Losgistic Regression(    4100/10000): loss= 680.479516715718\n",
      "Losgistic Regression(    4200/10000): loss= 677.562684404134\n",
      "Losgistic Regression(    4300/10000): loss= 674.726920298921\n",
      "Losgistic Regression(    4400/10000): loss= 671.975098082174\n",
      "Losgistic Regression(    4500/10000): loss= 669.02184905725\n",
      "Losgistic Regression(    4600/10000): loss= 666.043302868272\n",
      "Losgistic Regression(    4700/10000): loss= 663.161005121292\n",
      "Losgistic Regression(    4800/10000): loss= 660.398621562062\n",
      "Losgistic Regression(    4900/10000): loss= 657.759832483856\n",
      "Losgistic Regression(    5000/10000): loss= 655.229104046108\n",
      "Losgistic Regression(    5100/10000): loss= 652.81776296261\n",
      "Losgistic Regression(    5200/10000): loss= 650.562288838893\n",
      "Losgistic Regression(    5300/10000): loss= 648.474564685224\n",
      "Losgistic Regression(    5400/10000): loss= 646.53525111976\n",
      "Losgistic Regression(    5500/10000): loss= 644.737958719747\n",
      "Losgistic Regression(    5600/10000): loss= 642.970532298552\n",
      "Losgistic Regression(    5700/10000): loss= 641.209850442101\n",
      "Losgistic Regression(    5800/10000): loss= 639.560163633605\n",
      "Losgistic Regression(    5900/10000): loss= 638.022024355789\n",
      "Losgistic Regression(    6000/10000): loss= 636.590521226558\n",
      "Losgistic Regression(    6100/10000): loss= 635.267254953552\n",
      "Losgistic Regression(    6200/10000): loss= 634.047631385683\n",
      "Losgistic Regression(    6300/10000): loss= 632.92692058053\n",
      "Losgistic Regression(    6400/10000): loss= 631.903396090547\n",
      "Losgistic Regression(    6500/10000): loss= 630.839557018502\n",
      "Losgistic Regression(    6600/10000): loss= 629.818284024017\n",
      "Losgistic Regression(    6700/10000): loss= 628.86147723534\n",
      "Losgistic Regression(    6800/10000): loss= 627.952801256819\n",
      "Losgistic Regression(    6900/10000): loss= 626.97605516279\n",
      "Losgistic Regression(    7000/10000): loss= 626.084956160543\n",
      "Losgistic Regression(    7100/10000): loss= 625.274144779175\n",
      "Losgistic Regression(    7200/10000): loss= 624.528337597234\n",
      "Losgistic Regression(    7300/10000): loss= 623.821441728598\n",
      "Losgistic Regression(    7400/10000): loss= 622.992306014599\n",
      "Losgistic Regression(    7500/10000): loss= 622.243632763374\n",
      "Losgistic Regression(    7600/10000): loss= 621.575425283489\n",
      "Losgistic Regression(    7700/10000): loss= 620.982551661067\n",
      "Losgistic Regression(    7800/10000): loss= 620.474860512094\n",
      "Losgistic Regression(    7900/10000): loss= 619.663179614826\n",
      "Losgistic Regression(    8000/10000): loss= 618.658850076437\n",
      "Losgistic Regression(    8100/10000): loss= 617.690731551498\n",
      "Losgistic Regression(    8200/10000): loss= 616.753257578426\n",
      "Losgistic Regression(    8300/10000): loss= 615.847236302373\n",
      "Losgistic Regression(    8400/10000): loss= 614.964353018956\n",
      "Losgistic Regression(    8500/10000): loss= 614.103779611669\n",
      "Losgistic Regression(    8600/10000): loss= 613.267521769267\n",
      "Losgistic Regression(    8700/10000): loss= 612.456333825669\n",
      "Losgistic Regression(    8800/10000): loss= 611.685479918843\n",
      "Losgistic Regression(    8900/10000): loss= 610.957107676977\n",
      "Losgistic Regression(    9000/10000): loss= 610.275552240386\n",
      "Losgistic Regression(    9100/10000): loss= 609.646599471584\n",
      "Losgistic Regression(    9200/10000): loss= 608.96534119153\n",
      "Losgistic Regression(    9300/10000): loss= 608.148181284579\n",
      "Losgistic Regression(    9400/10000): loss= 607.220431162699\n",
      "Losgistic Regression(    9500/10000): loss= 606.321874176936\n",
      "Losgistic Regression(    9600/10000): loss= 605.44968833129\n",
      "Losgistic Regression(    9700/10000): loss= 604.606098836588\n",
      "Losgistic Regression(    9800/10000): loss= 603.795181437835\n",
      "Losgistic Regression(    9900/10000): loss= 602.961330598426\n",
      "Time for  3th cross validation = 81.7174s\n",
      "Training Accuracy         = 0.8744\n",
      "Cross Validation Accuracy = 0.78068\n",
      "Losgistic Regression(       0/10000): loss= 1726.0408963008\n",
      "Losgistic Regression(     100/10000): loss= 1230.33901101375\n",
      "Losgistic Regression(     200/10000): loss= 1074.0319133613\n",
      "Losgistic Regression(     300/10000): loss= 1011.596930871\n",
      "Losgistic Regression(     400/10000): loss= 969.884363047395\n",
      "Losgistic Regression(     500/10000): loss= 938.379834861964\n",
      "Losgistic Regression(     600/10000): loss= 913.336786099266\n",
      "Losgistic Regression(     700/10000): loss= 892.837942937383\n",
      "Losgistic Regression(     800/10000): loss= 874.345943007701\n",
      "Losgistic Regression(     900/10000): loss= 855.758212851357\n",
      "Losgistic Regression(    1000/10000): loss= 839.23489827231\n",
      "Losgistic Regression(    1100/10000): loss= 824.114010984937\n",
      "Losgistic Regression(    1200/10000): loss= 810.47612911422\n",
      "Losgistic Regression(    1300/10000): loss= 798.234027308609\n",
      "Losgistic Regression(    1400/10000): loss= 787.16478540486\n",
      "Losgistic Regression(    1500/10000): loss= 775.25680812325\n",
      "Losgistic Regression(    1600/10000): loss= 764.237235215388\n",
      "Losgistic Regression(    1700/10000): loss= 754.040187972589\n",
      "Losgistic Regression(    1800/10000): loss= 744.10360974512\n",
      "Losgistic Regression(    1900/10000): loss= 734.546771624468\n",
      "Losgistic Regression(    2000/10000): loss= 725.593408167114\n",
      "Losgistic Regression(    2100/10000): loss= 717.206709472922\n",
      "Losgistic Regression(    2200/10000): loss= 709.25944453633\n",
      "Losgistic Regression(    2300/10000): loss= 701.644266599502\n",
      "Losgistic Regression(    2400/10000): loss= 694.352537642211\n",
      "Losgistic Regression(    2500/10000): loss= 687.417541291983\n",
      "Losgistic Regression(    2600/10000): loss= 680.825999633391\n",
      "Losgistic Regression(    2700/10000): loss= 674.38393618256\n",
      "Losgistic Regression(    2800/10000): loss= 667.906738640157\n",
      "Losgistic Regression(    2900/10000): loss= 661.592998959678\n",
      "Losgistic Regression(    3000/10000): loss= 655.431397919825\n",
      "Losgistic Regression(    3100/10000): loss= 649.405906062104\n",
      "Losgistic Regression(    3200/10000): loss= 643.271104917367\n",
      "Losgistic Regression(    3300/10000): loss= 637.088166628393\n",
      "Losgistic Regression(    3400/10000): loss= 630.909493558772\n",
      "Losgistic Regression(    3500/10000): loss= 624.696171791977\n",
      "Losgistic Regression(    3600/10000): loss= 618.432607545915\n",
      "Losgistic Regression(    3700/10000): loss= 612.110222058633\n",
      "Losgistic Regression(    3800/10000): loss= 605.714232874529\n",
      "Losgistic Regression(    3900/10000): loss= 599.231247887745\n",
      "Losgistic Regression(    4000/10000): loss= 592.667063446323\n",
      "Losgistic Regression(    4100/10000): loss= 586.024678937057\n",
      "Losgistic Regression(    4200/10000): loss= 579.189437482439\n",
      "Losgistic Regression(    4300/10000): loss= 572.161379349717\n",
      "Losgistic Regression(    4400/10000): loss= 565.05411901277\n",
      "Losgistic Regression(    4500/10000): loss= 557.610084391489\n",
      "Losgistic Regression(    4600/10000): loss= 550.075288444323\n",
      "Losgistic Regression(    4700/10000): loss= 542.35721125553\n",
      "Losgistic Regression(    4800/10000): loss= 534.417565785714\n",
      "Losgistic Regression(    4900/10000): loss= 526.446244857001\n",
      "Losgistic Regression(    5000/10000): loss= 518.470795445707\n",
      "Losgistic Regression(    5100/10000): loss= 510.496915308593\n",
      "Losgistic Regression(    5200/10000): loss= 502.526703900502\n",
      "Losgistic Regression(    5300/10000): loss= 494.560742029215\n",
      "Losgistic Regression(    5400/10000): loss= 486.609663320972\n",
      "Losgistic Regression(    5500/10000): loss= 478.689349730453\n",
      "Losgistic Regression(    5600/10000): loss= 470.673536853183\n",
      "Losgistic Regression(    5700/10000): loss= 462.498527011489\n",
      "Losgistic Regression(    5800/10000): loss= 454.282358124964\n",
      "Losgistic Regression(    5900/10000): loss= 445.973590781988\n",
      "Losgistic Regression(    6000/10000): loss= 437.672909941106\n",
      "Losgistic Regression(    6100/10000): loss= 429.366759359469\n",
      "Losgistic Regression(    6200/10000): loss= 421.046430235565\n",
      "Losgistic Regression(    6300/10000): loss= 412.731104182337\n",
      "Losgistic Regression(    6400/10000): loss= 404.432549330851\n",
      "Losgistic Regression(    6500/10000): loss= 396.145856871434\n",
      "Losgistic Regression(    6600/10000): loss= 387.874072730037\n",
      "Losgistic Regression(    6700/10000): loss= 379.616241033672\n",
      "Losgistic Regression(    6800/10000): loss= 371.373530680586\n",
      "Losgistic Regression(    6900/10000): loss= 363.163700661794\n",
      "Losgistic Regression(    7000/10000): loss= 354.994303691297\n",
      "Losgistic Regression(    7100/10000): loss= 346.861469803592\n",
      "Losgistic Regression(    7200/10000): loss= 338.764243424952\n",
      "Losgistic Regression(    7300/10000): loss= 330.694261129473\n",
      "Losgistic Regression(    7400/10000): loss= 322.657420325291\n",
      "Losgistic Regression(    7500/10000): loss= 314.653367436429\n",
      "Losgistic Regression(    7600/10000): loss= 306.683387462005\n",
      "Losgistic Regression(    7700/10000): loss= 298.725865245984\n",
      "Losgistic Regression(    7800/10000): loss= 290.709969202403\n",
      "Losgistic Regression(    7900/10000): loss= 282.711518543134\n",
      "Losgistic Regression(    8000/10000): loss= 274.73396309278\n",
      "Losgistic Regression(    8100/10000): loss= 266.781358434512\n",
      "Losgistic Regression(    8200/10000): loss= 258.863987519745\n",
      "Losgistic Regression(    8300/10000): loss= 250.987386794525\n",
      "Losgistic Regression(    8400/10000): loss= 243.160339512384\n",
      "Losgistic Regression(    8500/10000): loss= 235.38409776165\n",
      "Losgistic Regression(    8600/10000): loss= 227.666472553602\n",
      "Losgistic Regression(    8700/10000): loss= 220.004173171218\n",
      "Losgistic Regression(    8800/10000): loss= 212.385510800714\n",
      "Losgistic Regression(    8900/10000): loss= 204.738335357283\n",
      "Losgistic Regression(    9000/10000): loss= 196.988670758664\n",
      "Losgistic Regression(    9100/10000): loss= 189.233736530162\n",
      "Losgistic Regression(    9200/10000): loss= 181.562324866253\n",
      "Losgistic Regression(    9300/10000): loss= 174.061792636552\n",
      "Losgistic Regression(    9400/10000): loss= 166.616634806296\n",
      "Losgistic Regression(    9500/10000): loss= 159.051456357966\n",
      "Losgistic Regression(    9600/10000): loss= 151.456714621324\n",
      "Losgistic Regression(    9700/10000): loss= 144.028580835982\n",
      "Losgistic Regression(    9800/10000): loss= 136.857840198807\n",
      "Losgistic Regression(    9900/10000): loss= 129.790033332561\n",
      "Time for  4th cross validation = 79.3895s\n",
      "Training Accuracy         = 0.8868\n",
      "Cross Validation Accuracy = 0.778544\n",
      "*************** ([0.88319999999999999, 0.88639999999999997, 0.88, 0.87439999999999996, 0.88680000000000003], [0.78400800000000004, 0.77524000000000004, 0.76915599999999995, 0.78068000000000004, 0.77854400000000001])\n",
      "Losgistic Regression(       0/10000): loss= 1725.775272995\n",
      "Losgistic Regression(     100/10000): loss= 1237.79948562259\n",
      "Losgistic Regression(     200/10000): loss= 1075.05724797123\n",
      "Losgistic Regression(     300/10000): loss= 1015.2565691403\n",
      "Losgistic Regression(     400/10000): loss= 979.548164693301\n",
      "Losgistic Regression(     500/10000): loss= 953.373333485631\n",
      "Losgistic Regression(     600/10000): loss= 932.614697706418\n",
      "Losgistic Regression(     700/10000): loss= 915.5596552726\n",
      "Losgistic Regression(     800/10000): loss= 900.807786943038\n",
      "Losgistic Regression(     900/10000): loss= 887.857317407539\n",
      "Losgistic Regression(    1000/10000): loss= 876.280439299983\n",
      "Losgistic Regression(    1100/10000): loss= 865.86320069131\n",
      "Losgistic Regression(    1200/10000): loss= 856.458083774774\n",
      "Losgistic Regression(    1300/10000): loss= 847.952740919722\n",
      "Losgistic Regression(    1400/10000): loss= 840.254227674166\n",
      "Losgistic Regression(    1500/10000): loss= 833.269934369251\n",
      "Losgistic Regression(    1600/10000): loss= 826.915415047483\n",
      "Losgistic Regression(    1700/10000): loss= 821.114281784174\n",
      "Losgistic Regression(    1800/10000): loss= 815.800049914877\n",
      "Losgistic Regression(    1900/10000): loss= 810.916229475823\n",
      "Losgistic Regression(    2000/10000): loss= 806.412488005446\n",
      "Losgistic Regression(    2100/10000): loss= 802.246232193724\n",
      "Losgistic Regression(    2200/10000): loss= 798.377733539878\n",
      "Losgistic Regression(    2300/10000): loss= 794.781961542085\n",
      "Losgistic Regression(    2400/10000): loss= 791.428439141923\n",
      "Losgistic Regression(    2500/10000): loss= 788.290775749674\n",
      "Losgistic Regression(    2600/10000): loss= 785.348404603222\n",
      "Losgistic Regression(    2700/10000): loss= 782.121404944497\n",
      "Losgistic Regression(    2800/10000): loss= 779.094818370452\n",
      "Losgistic Regression(    2900/10000): loss= 776.243665097846\n",
      "Losgistic Regression(    3000/10000): loss= 773.536750590214\n",
      "Losgistic Regression(    3100/10000): loss= 770.958314334253\n",
      "Losgistic Regression(    3200/10000): loss= 768.490794700278\n",
      "Losgistic Regression(    3300/10000): loss= 766.075349029657\n",
      "Losgistic Regression(    3400/10000): loss= 763.485762498497\n",
      "Losgistic Regression(    3500/10000): loss= 760.989955162867\n",
      "Losgistic Regression(    3600/10000): loss= 758.583418013244\n",
      "Losgistic Regression(    3700/10000): loss= 756.069934880171\n",
      "Losgistic Regression(    3800/10000): loss= 753.627217032174\n",
      "Losgistic Regression(    3900/10000): loss= 751.277035616033\n",
      "Losgistic Regression(    4000/10000): loss= 749.000775069257\n",
      "Losgistic Regression(    4100/10000): loss= 746.794725425464\n",
      "Losgistic Regression(    4200/10000): loss= 744.676803167943\n",
      "Losgistic Regression(    4300/10000): loss= 742.639337927195\n",
      "Losgistic Regression(    4400/10000): loss= 740.673458367755\n",
      "Losgistic Regression(    4500/10000): loss= 738.581982844216\n",
      "Losgistic Regression(    4600/10000): loss= 736.372515437094\n",
      "Losgistic Regression(    4700/10000): loss= 734.220170166942\n",
      "Losgistic Regression(    4800/10000): loss= 732.123640251934\n",
      "Losgistic Regression(    4900/10000): loss= 730.067541172257\n",
      "Losgistic Regression(    5000/10000): loss= 728.049573331682\n",
      "Losgistic Regression(    5100/10000): loss= 726.07122400243\n",
      "Losgistic Regression(    5200/10000): loss= 724.157880028075\n",
      "Losgistic Regression(    5300/10000): loss= 722.315895979479\n",
      "Losgistic Regression(    5400/10000): loss= 720.534987744757\n",
      "Losgistic Regression(    5500/10000): loss= 718.797626739371\n",
      "Losgistic Regression(    5600/10000): loss= 717.091572329415\n",
      "Losgistic Regression(    5700/10000): loss= 715.407249947481\n",
      "Losgistic Regression(    5800/10000): loss= 713.751000602667\n",
      "Losgistic Regression(    5900/10000): loss= 712.134748703083\n",
      "Losgistic Regression(    6000/10000): loss= 710.564252288692\n",
      "Losgistic Regression(    6100/10000): loss= 708.961040713797\n",
      "Losgistic Regression(    6200/10000): loss= 707.155056009159\n",
      "Losgistic Regression(    6300/10000): loss= 705.409972496126\n",
      "Losgistic Regression(    6400/10000): loss= 703.674840475555\n",
      "Losgistic Regression(    6500/10000): loss= 701.835911653376\n",
      "Losgistic Regression(    6600/10000): loss= 699.990894870768\n",
      "Losgistic Regression(    6700/10000): loss= 698.091656756074\n",
      "Losgistic Regression(    6800/10000): loss= 695.982976166274\n",
      "Losgistic Regression(    6900/10000): loss= 693.906624579672\n",
      "Losgistic Regression(    7000/10000): loss= 691.743211110867\n",
      "Losgistic Regression(    7100/10000): loss= 689.539235900624\n",
      "Losgistic Regression(    7200/10000): loss= 687.354080690134\n",
      "Losgistic Regression(    7300/10000): loss= 685.225657544792\n",
      "Losgistic Regression(    7400/10000): loss= 683.165290399843\n",
      "Losgistic Regression(    7500/10000): loss= 681.167975667077\n",
      "Losgistic Regression(    7600/10000): loss= 679.210060029592\n",
      "Losgistic Regression(    7700/10000): loss= 677.265890157107\n",
      "Losgistic Regression(    7800/10000): loss= 675.345265663279\n",
      "Losgistic Regression(    7900/10000): loss= 673.491040953056\n",
      "Losgistic Regression(    8000/10000): loss= 671.701522445987\n",
      "Losgistic Regression(    8100/10000): loss= 669.976724955393\n",
      "Losgistic Regression(    8200/10000): loss= 668.29907219667\n",
      "Losgistic Regression(    8300/10000): loss= 666.660726483117\n",
      "Losgistic Regression(    8400/10000): loss= 665.052546317726\n",
      "Losgistic Regression(    8500/10000): loss= 663.49826070833\n",
      "Losgistic Regression(    8600/10000): loss= 662.009588401041\n",
      "Losgistic Regression(    8700/10000): loss= 660.594440913781\n",
      "Losgistic Regression(    8800/10000): loss= 659.247320558413\n",
      "Losgistic Regression(    8900/10000): loss= 657.95638063281\n",
      "Losgistic Regression(    9000/10000): loss= 656.697230673609\n",
      "Losgistic Regression(    9100/10000): loss= 655.454071258994\n",
      "Losgistic Regression(    9200/10000): loss= 654.245672139868\n",
      "Losgistic Regression(    9300/10000): loss= 653.08551472539\n",
      "Losgistic Regression(    9400/10000): loss= 651.966369852846\n",
      "Losgistic Regression(    9500/10000): loss= 650.880145531839\n",
      "Losgistic Regression(    9600/10000): loss= 649.827007389165\n",
      "Losgistic Regression(    9700/10000): loss= 648.779165269464\n",
      "Losgistic Regression(    9800/10000): loss= 647.727366526696\n",
      "Losgistic Regression(    9900/10000): loss= 646.656085591702\n",
      "Time for  0th cross validation = 79.6116s\n",
      "Training Accuracy         = 0.8832\n",
      "Cross Validation Accuracy = 0.78554\n",
      "Losgistic Regression(       0/10000): loss= 1724.81651699789\n",
      "Losgistic Regression(     100/10000): loss= 1223.34935891003\n",
      "Losgistic Regression(     200/10000): loss= 1062.09277313622\n",
      "Losgistic Regression(     300/10000): loss= 997.004107667449\n",
      "Losgistic Regression(     400/10000): loss= 959.035719610665\n",
      "Losgistic Regression(     500/10000): loss= 931.371133671158\n",
      "Losgistic Regression(     600/10000): loss= 909.331623432289\n",
      "Losgistic Regression(     700/10000): loss= 890.966691815915\n",
      "Losgistic Regression(     800/10000): loss= 875.055921227302\n",
      "Losgistic Regression(     900/10000): loss= 860.407667405488\n",
      "Losgistic Regression(    1000/10000): loss= 847.806341012235\n",
      "Losgistic Regression(    1100/10000): loss= 836.972807777147\n",
      "Losgistic Regression(    1200/10000): loss= 827.580397190652\n",
      "Losgistic Regression(    1300/10000): loss= 819.444865197958\n",
      "Losgistic Regression(    1400/10000): loss= 812.402509877449\n",
      "Losgistic Regression(    1500/10000): loss= 806.277520100243\n",
      "Losgistic Regression(    1600/10000): loss= 800.907837921917\n",
      "Losgistic Regression(    1700/10000): loss= 795.69498210429\n",
      "Losgistic Regression(    1800/10000): loss= 790.890872199192\n",
      "Losgistic Regression(    1900/10000): loss= 786.591417251727\n",
      "Losgistic Regression(    2000/10000): loss= 782.160881161218\n",
      "Losgistic Regression(    2100/10000): loss= 777.929239859167\n",
      "Losgistic Regression(    2200/10000): loss= 774.010796980638\n",
      "Losgistic Regression(    2300/10000): loss= 769.682516375074\n",
      "Losgistic Regression(    2400/10000): loss= 765.239307513781\n",
      "Losgistic Regression(    2500/10000): loss= 760.754662380663\n",
      "Losgistic Regression(    2600/10000): loss= 756.159751119076\n",
      "Losgistic Regression(    2700/10000): loss= 751.715991794467\n",
      "Losgistic Regression(    2800/10000): loss= 747.436377848723\n",
      "Losgistic Regression(    2900/10000): loss= 743.293619986648\n",
      "Losgistic Regression(    3000/10000): loss= 738.642692866542\n",
      "Losgistic Regression(    3100/10000): loss= 733.695808843672\n",
      "Losgistic Regression(    3200/10000): loss= 728.757685271786\n",
      "Losgistic Regression(    3300/10000): loss= 723.963014364574\n",
      "Losgistic Regression(    3400/10000): loss= 719.232323542653\n",
      "Losgistic Regression(    3500/10000): loss= 714.223646198797\n",
      "Losgistic Regression(    3600/10000): loss= 708.790441090452\n",
      "Losgistic Regression(    3700/10000): loss= 703.387868820886\n",
      "Losgistic Regression(    3800/10000): loss= 698.218341860227\n",
      "Losgistic Regression(    3900/10000): loss= 693.245725085981\n",
      "Losgistic Regression(    4000/10000): loss= 688.439968925217\n",
      "Losgistic Regression(    4100/10000): loss= 683.809207364844\n",
      "Losgistic Regression(    4200/10000): loss= 679.353009049905\n",
      "Losgistic Regression(    4300/10000): loss= 674.787302207875\n",
      "Losgistic Regression(    4400/10000): loss= 670.282666364083\n",
      "Losgistic Regression(    4500/10000): loss= 665.903516579453\n",
      "Losgistic Regression(    4600/10000): loss= 661.647570869803\n",
      "Losgistic Regression(    4700/10000): loss= 657.511133360748\n",
      "Losgistic Regression(    4800/10000): loss= 653.430119197149\n",
      "Losgistic Regression(    4900/10000): loss= 649.142688793653\n",
      "Losgistic Regression(    5000/10000): loss= 645.051433285895\n",
      "Losgistic Regression(    5100/10000): loss= 641.135431193979\n",
      "Losgistic Regression(    5200/10000): loss= 637.335681748969\n",
      "Losgistic Regression(    5300/10000): loss= 633.572305439717\n",
      "Losgistic Regression(    5400/10000): loss= 629.741938323488\n",
      "Losgistic Regression(    5500/10000): loss= 626.004361371342\n",
      "Losgistic Regression(    5600/10000): loss= 622.218813416447\n",
      "Losgistic Regression(    5700/10000): loss= 618.453078892375\n",
      "Losgistic Regression(    5800/10000): loss= 614.747676015672\n",
      "Losgistic Regression(    5900/10000): loss= 611.077957576086\n",
      "Losgistic Regression(    6000/10000): loss= 607.425930162404\n",
      "Losgistic Regression(    6100/10000): loss= 603.803620534054\n",
      "Losgistic Regression(    6200/10000): loss= 600.208899940773\n",
      "Losgistic Regression(    6300/10000): loss= 596.612733558828\n",
      "Losgistic Regression(    6400/10000): loss= 593.055360282949\n",
      "Losgistic Regression(    6500/10000): loss= 589.564670704755\n",
      "Losgistic Regression(    6600/10000): loss= 586.064886671487\n",
      "Losgistic Regression(    6700/10000): loss= 582.478904614424\n",
      "Losgistic Regression(    6800/10000): loss= 578.807667815052\n",
      "Losgistic Regression(    6900/10000): loss= 575.139770537637\n",
      "Losgistic Regression(    7000/10000): loss= 571.457331078017\n",
      "Losgistic Regression(    7100/10000): loss= 567.791989792294\n",
      "Losgistic Regression(    7200/10000): loss= 564.190084359913\n",
      "Losgistic Regression(    7300/10000): loss= 560.594023747242\n",
      "Losgistic Regression(    7400/10000): loss= 557.000330329675\n",
      "Losgistic Regression(    7500/10000): loss= 553.493435620425\n",
      "Losgistic Regression(    7600/10000): loss= 550.022585059118\n",
      "Losgistic Regression(    7700/10000): loss= 546.543312315894\n",
      "Losgistic Regression(    7800/10000): loss= 543.176677855272\n",
      "Losgistic Regression(    7900/10000): loss= 539.829639631784\n",
      "Losgistic Regression(    8000/10000): loss= 536.511912715939\n",
      "Losgistic Regression(    8100/10000): loss= 533.102453031655\n",
      "Losgistic Regression(    8200/10000): loss= 529.605874982239\n",
      "Losgistic Regression(    8300/10000): loss= 526.107643603748\n",
      "Losgistic Regression(    8400/10000): loss= 522.649664103714\n",
      "Losgistic Regression(    8500/10000): loss= 519.186301111433\n",
      "Losgistic Regression(    8600/10000): loss= 515.680347834085\n",
      "Losgistic Regression(    8700/10000): loss= 512.269813654403\n",
      "Losgistic Regression(    8800/10000): loss= 508.948761044703\n",
      "Losgistic Regression(    8900/10000): loss= 505.706879706238\n",
      "Losgistic Regression(    9000/10000): loss= 502.498736053514\n",
      "Losgistic Regression(    9100/10000): loss= 499.191257659525\n",
      "Losgistic Regression(    9200/10000): loss= 495.80351893355\n",
      "Losgistic Regression(    9300/10000): loss= 492.406080036687\n",
      "Losgistic Regression(    9400/10000): loss= 488.937774778576\n",
      "Losgistic Regression(    9500/10000): loss= 485.445418351685\n",
      "Losgistic Regression(    9600/10000): loss= 481.97563947292\n",
      "Losgistic Regression(    9700/10000): loss= 478.504847018547\n",
      "Losgistic Regression(    9800/10000): loss= 474.944333414014\n",
      "Losgistic Regression(    9900/10000): loss= 471.236586452744\n",
      "Time for  1th cross validation = 82.5535s\n",
      "Training Accuracy         =  0.884\n",
      "Cross Validation Accuracy = 0.776808\n",
      "Losgistic Regression(       0/10000): loss= 1726.06721756932\n",
      "Losgistic Regression(     100/10000): loss= 1224.13225543563\n",
      "Losgistic Regression(     200/10000): loss= 1067.14024532909\n",
      "Losgistic Regression(     300/10000): loss= 1005.50375790935\n",
      "Losgistic Regression(     400/10000): loss= 965.664995258297\n",
      "Losgistic Regression(     500/10000): loss= 936.840331079129\n",
      "Losgistic Regression(     600/10000): loss= 914.620320088978\n",
      "Losgistic Regression(     700/10000): loss= 896.664779632022\n",
      "Losgistic Regression(     800/10000): loss= 881.661319442159\n",
      "Losgistic Regression(     900/10000): loss= 868.900050872757\n",
      "Losgistic Regression(    1000/10000): loss= 856.61112351812\n",
      "Losgistic Regression(    1100/10000): loss= 845.075607892155\n",
      "Losgistic Regression(    1200/10000): loss= 834.711240997775\n",
      "Losgistic Regression(    1300/10000): loss= 825.408887775889\n",
      "Losgistic Regression(    1400/10000): loss= 817.041633410675\n",
      "Losgistic Regression(    1500/10000): loss= 809.484586673624\n",
      "Losgistic Regression(    1600/10000): loss= 802.641991717632\n",
      "Losgistic Regression(    1700/10000): loss= 796.410221159809\n",
      "Losgistic Regression(    1800/10000): loss= 790.426476461777\n",
      "Losgistic Regression(    1900/10000): loss= 784.153546478551\n",
      "Losgistic Regression(    2000/10000): loss= 777.804201655627\n",
      "Losgistic Regression(    2100/10000): loss= 771.477199159612\n",
      "Losgistic Regression(    2200/10000): loss= 765.535245504414\n",
      "Losgistic Regression(    2300/10000): loss= 759.950896384797\n",
      "Losgistic Regression(    2400/10000): loss= 754.140263532308\n",
      "Losgistic Regression(    2500/10000): loss= 748.182285206768\n",
      "Losgistic Regression(    2600/10000): loss= 742.555144351767\n",
      "Losgistic Regression(    2700/10000): loss= 737.226859808872\n",
      "Losgistic Regression(    2800/10000): loss= 732.167884820441\n",
      "Losgistic Regression(    2900/10000): loss= 727.398298496713\n",
      "Losgistic Regression(    3000/10000): loss= 722.933670697685\n",
      "Losgistic Regression(    3100/10000): loss= 718.462538433767\n",
      "Losgistic Regression(    3200/10000): loss= 714.276686488294\n",
      "Losgistic Regression(    3300/10000): loss= 710.351777217794\n",
      "Losgistic Regression(    3400/10000): loss= 706.667459394735\n",
      "Losgistic Regression(    3500/10000): loss= 703.243959374563\n",
      "Losgistic Regression(    3600/10000): loss= 700.083180727059\n",
      "Losgistic Regression(    3700/10000): loss= 697.136411310684\n",
      "Losgistic Regression(    3800/10000): loss= 694.350610363237\n",
      "Losgistic Regression(    3900/10000): loss= 691.701696996094\n",
      "Losgistic Regression(    4000/10000): loss= 689.189193898383\n",
      "Losgistic Regression(    4100/10000): loss= 686.811274597386\n",
      "Losgistic Regression(    4200/10000): loss= 684.566570106648\n",
      "Losgistic Regression(    4300/10000): loss= 682.444101033786\n",
      "Losgistic Regression(    4400/10000): loss= 680.320787047369\n",
      "Losgistic Regression(    4500/10000): loss= 678.241657987175\n",
      "Losgistic Regression(    4600/10000): loss= 676.249993790266\n",
      "Losgistic Regression(    4700/10000): loss= 674.349137416431\n",
      "Losgistic Regression(    4800/10000): loss= 672.544406148534\n",
      "Losgistic Regression(    4900/10000): loss= 670.807172736803\n",
      "Losgistic Regression(    5000/10000): loss= 669.125110757795\n",
      "Losgistic Regression(    5100/10000): loss= 667.514856793151\n",
      "Losgistic Regression(    5200/10000): loss= 666.009412988268\n",
      "Losgistic Regression(    5300/10000): loss= 664.605235988146\n",
      "Losgistic Regression(    5400/10000): loss= 663.283434330845\n",
      "Losgistic Regression(    5500/10000): loss= 662.019879168411\n",
      "Losgistic Regression(    5600/10000): loss= 660.778925295538\n",
      "Losgistic Regression(    5700/10000): loss= 659.570687589923\n",
      "Losgistic Regression(    5800/10000): loss= 658.393851513341\n",
      "Losgistic Regression(    5900/10000): loss= 657.237099793594\n",
      "Losgistic Regression(    6000/10000): loss= 656.116761522841\n",
      "Losgistic Regression(    6100/10000): loss= 654.961475447865\n",
      "Losgistic Regression(    6200/10000): loss= 653.769486676307\n",
      "Losgistic Regression(    6300/10000): loss= 652.641380143464\n",
      "Losgistic Regression(    6400/10000): loss= 651.570257462963\n",
      "Losgistic Regression(    6500/10000): loss= 650.510076377858\n",
      "Losgistic Regression(    6600/10000): loss= 649.446813185713\n",
      "Losgistic Regression(    6700/10000): loss= 648.394360042944\n",
      "Losgistic Regression(    6800/10000): loss= 647.377569796442\n",
      "Losgistic Regression(    6900/10000): loss= 646.384629881475\n",
      "Losgistic Regression(    7000/10000): loss= 645.436382885803\n",
      "Losgistic Regression(    7100/10000): loss= 644.554088916172\n",
      "Losgistic Regression(    7200/10000): loss= 643.720077552004\n",
      "Losgistic Regression(    7300/10000): loss= 642.910290041956\n",
      "Losgistic Regression(    7400/10000): loss= 642.141130468302\n",
      "Losgistic Regression(    7500/10000): loss= 641.385151924578\n",
      "Losgistic Regression(    7600/10000): loss= 640.529956035227\n",
      "Losgistic Regression(    7700/10000): loss= 639.686919766449\n",
      "Losgistic Regression(    7800/10000): loss= 638.854301576128\n",
      "Losgistic Regression(    7900/10000): loss= 638.025191967034\n",
      "Losgistic Regression(    8000/10000): loss= 637.196123340775\n",
      "Losgistic Regression(    8100/10000): loss= 636.383794067201\n",
      "Losgistic Regression(    8200/10000): loss= 635.591073674896\n",
      "Losgistic Regression(    8300/10000): loss= 634.828407212509\n",
      "Losgistic Regression(    8400/10000): loss= 634.089689728338\n",
      "Losgistic Regression(    8500/10000): loss= 633.376665537383\n",
      "Losgistic Regression(    8600/10000): loss= 632.668643043209\n",
      "Losgistic Regression(    8700/10000): loss= 631.969841987154\n",
      "Losgistic Regression(    8800/10000): loss= 631.236739538439\n",
      "Losgistic Regression(    8900/10000): loss= 630.544847761755\n",
      "Losgistic Regression(    9000/10000): loss= 629.868085361724\n",
      "Losgistic Regression(    9100/10000): loss= 629.203398891902\n",
      "Losgistic Regression(    9200/10000): loss= 628.588497530738\n",
      "Losgistic Regression(    9300/10000): loss= 628.046938289159\n",
      "Losgistic Regression(    9400/10000): loss= 627.553726703772\n",
      "Losgistic Regression(    9500/10000): loss= 627.092124437852\n",
      "Losgistic Regression(    9600/10000): loss= 626.648970411282\n",
      "Losgistic Regression(    9700/10000): loss= 626.217390741857\n",
      "Losgistic Regression(    9800/10000): loss= 625.783889736855\n",
      "Losgistic Regression(    9900/10000): loss= 625.327513289576\n",
      "Time for  2th cross validation = 80.853s\n",
      "Training Accuracy         = 0.8804\n",
      "Cross Validation Accuracy = 0.770928\n",
      "Losgistic Regression(       0/10000): loss= 1725.44136833601\n",
      "Losgistic Regression(     100/10000): loss= 1249.82612984231\n",
      "Losgistic Regression(     200/10000): loss= 1110.76450600815\n",
      "Losgistic Regression(     300/10000): loss= 1050.1396572981\n",
      "Losgistic Regression(     400/10000): loss= 1009.85186031506\n",
      "Losgistic Regression(     500/10000): loss= 979.694928256072\n",
      "Losgistic Regression(     600/10000): loss= 956.169382363926\n",
      "Losgistic Regression(     700/10000): loss= 936.965278559283\n",
      "Losgistic Regression(     800/10000): loss= 917.411876068588\n",
      "Losgistic Regression(     900/10000): loss= 899.174691425462\n",
      "Losgistic Regression(    1000/10000): loss= 878.593215865626\n",
      "Losgistic Regression(    1100/10000): loss= 860.237190165536\n",
      "Losgistic Regression(    1200/10000): loss= 844.198411196388\n",
      "Losgistic Regression(    1300/10000): loss= 830.221638515316\n",
      "Losgistic Regression(    1400/10000): loss= 818.200721581512\n",
      "Losgistic Regression(    1500/10000): loss= 807.89088104604\n",
      "Losgistic Regression(    1600/10000): loss= 798.860300828548\n",
      "Losgistic Regression(    1700/10000): loss= 790.662775508612\n",
      "Losgistic Regression(    1800/10000): loss= 783.060019052764\n",
      "Losgistic Regression(    1900/10000): loss= 776.160006907953\n",
      "Losgistic Regression(    2000/10000): loss= 769.863209200903\n",
      "Losgistic Regression(    2100/10000): loss= 764.059383683923\n",
      "Losgistic Regression(    2200/10000): loss= 758.64300848762\n",
      "Losgistic Regression(    2300/10000): loss= 753.567282803667\n",
      "Losgistic Regression(    2400/10000): loss= 748.533543390354\n",
      "Losgistic Regression(    2500/10000): loss= 743.875057804614\n",
      "Losgistic Regression(    2600/10000): loss= 739.507576468349\n",
      "Losgistic Regression(    2700/10000): loss= 735.392273227149\n",
      "Losgistic Regression(    2800/10000): loss= 731.545692355353\n",
      "Losgistic Regression(    2900/10000): loss= 727.996892820091\n",
      "Losgistic Regression(    3000/10000): loss= 724.723818426333\n",
      "Losgistic Regression(    3100/10000): loss= 721.699666132945\n",
      "Losgistic Regression(    3200/10000): loss= 718.926121461854\n",
      "Losgistic Regression(    3300/10000): loss= 716.370591720745\n",
      "Losgistic Regression(    3400/10000): loss= 714.010444344666\n",
      "Losgistic Regression(    3500/10000): loss= 711.41198279064\n",
      "Losgistic Regression(    3600/10000): loss= 708.980951330965\n",
      "Losgistic Regression(    3700/10000): loss= 706.686715823733\n",
      "Losgistic Regression(    3800/10000): loss= 704.491876100369\n",
      "Losgistic Regression(    3900/10000): loss= 702.374358225176\n",
      "Losgistic Regression(    4000/10000): loss= 700.143906865347\n",
      "Losgistic Regression(    4100/10000): loss= 697.670843751948\n",
      "Losgistic Regression(    4200/10000): loss= 695.291513195889\n",
      "Losgistic Regression(    4300/10000): loss= 692.970004526194\n",
      "Losgistic Regression(    4400/10000): loss= 690.696144876199\n",
      "Losgistic Regression(    4500/10000): loss= 688.472330304832\n",
      "Losgistic Regression(    4600/10000): loss= 686.145692108451\n",
      "Losgistic Regression(    4700/10000): loss= 683.7832850546\n",
      "Losgistic Regression(    4800/10000): loss= 681.522915811087\n",
      "Losgistic Regression(    4900/10000): loss= 679.358235543462\n",
      "Losgistic Regression(    5000/10000): loss= 677.305077884682\n",
      "Losgistic Regression(    5100/10000): loss= 675.358965469388\n",
      "Losgistic Regression(    5200/10000): loss= 673.519235439859\n",
      "Losgistic Regression(    5300/10000): loss= 671.785526668143\n",
      "Losgistic Regression(    5400/10000): loss= 670.164657347016\n",
      "Losgistic Regression(    5500/10000): loss= 668.658109591458\n",
      "Losgistic Regression(    5600/10000): loss= 667.283045690484\n",
      "Losgistic Regression(    5700/10000): loss= 666.011992590512\n",
      "Losgistic Regression(    5800/10000): loss= 664.825777213503\n",
      "Losgistic Regression(    5900/10000): loss= 663.737251244256\n",
      "Losgistic Regression(    6000/10000): loss= 662.73156151883\n",
      "Losgistic Regression(    6100/10000): loss= 661.656210844093\n",
      "Losgistic Regression(    6200/10000): loss= 660.662980869418\n",
      "Losgistic Regression(    6300/10000): loss= 659.767347953456\n",
      "Losgistic Regression(    6400/10000): loss= 658.939279330272\n",
      "Losgistic Regression(    6500/10000): loss= 658.159366922839\n",
      "Losgistic Regression(    6600/10000): loss= 657.407795905862\n",
      "Losgistic Regression(    6700/10000): loss= 656.685555828857\n",
      "Losgistic Regression(    6800/10000): loss= 655.999200839343\n",
      "Losgistic Regression(    6900/10000): loss= 655.35937158463\n",
      "Losgistic Regression(    7000/10000): loss= 654.751265375102\n",
      "Losgistic Regression(    7100/10000): loss= 654.184780760574\n",
      "Losgistic Regression(    7200/10000): loss= 653.65936783556\n",
      "Losgistic Regression(    7300/10000): loss= 653.182562080087\n",
      "Losgistic Regression(    7400/10000): loss= 652.71869239446\n",
      "Losgistic Regression(    7500/10000): loss= 652.234337709199\n",
      "Losgistic Regression(    7600/10000): loss= 651.607252646244\n",
      "Losgistic Regression(    7700/10000): loss= 650.684094590657\n",
      "Losgistic Regression(    7800/10000): loss= 649.759871668789\n",
      "Losgistic Regression(    7900/10000): loss= 648.841046716216\n",
      "Losgistic Regression(    8000/10000): loss= 647.965342933525\n",
      "Losgistic Regression(    8100/10000): loss= 647.127263414745\n",
      "Losgistic Regression(    8200/10000): loss= 646.308666267688\n",
      "Losgistic Regression(    8300/10000): loss= 645.499962127442\n",
      "Losgistic Regression(    8400/10000): loss= 644.685742889683\n",
      "Losgistic Regression(    8500/10000): loss= 643.895628021682\n",
      "Losgistic Regression(    8600/10000): loss= 643.114817181073\n",
      "Losgistic Regression(    8700/10000): loss= 642.30890092764\n",
      "Losgistic Regression(    8800/10000): loss= 641.53820261501\n",
      "Losgistic Regression(    8900/10000): loss= 640.845854629487\n",
      "Losgistic Regression(    9000/10000): loss= 640.19502569007\n",
      "Losgistic Regression(    9100/10000): loss= 639.600509257508\n",
      "Losgistic Regression(    9200/10000): loss= 639.092366086158\n",
      "Losgistic Regression(    9300/10000): loss= 638.623742246107\n",
      "Losgistic Regression(    9400/10000): loss= 638.170690974836\n",
      "Losgistic Regression(    9500/10000): loss= 637.789376087895\n",
      "Losgistic Regression(    9600/10000): loss= 637.441377461441\n",
      "Losgistic Regression(    9700/10000): loss= 637.110743922312\n",
      "Losgistic Regression(    9800/10000): loss= 636.828442818553\n",
      "Losgistic Regression(    9900/10000): loss= 636.567023880892\n",
      "Time for  3th cross validation = 80.9277s\n",
      "Training Accuracy         = 0.8716\n",
      "Cross Validation Accuracy = 0.781948\n",
      "Losgistic Regression(       0/10000): loss= 1726.04236710636\n",
      "Losgistic Regression(     100/10000): loss= 1230.69270454409\n",
      "Losgistic Regression(     200/10000): loss= 1074.80663958698\n",
      "Losgistic Regression(     300/10000): loss= 1012.75825400046\n",
      "Losgistic Regression(     400/10000): loss= 971.447969154807\n",
      "Losgistic Regression(     500/10000): loss= 940.331339742865\n",
      "Losgistic Regression(     600/10000): loss= 915.65255205285\n",
      "Losgistic Regression(     700/10000): loss= 895.504175918017\n",
      "Losgistic Regression(     800/10000): loss= 877.525479045076\n",
      "Losgistic Regression(     900/10000): loss= 859.465696738048\n",
      "Losgistic Regression(    1000/10000): loss= 843.486757705959\n",
      "Losgistic Regression(    1100/10000): loss= 828.94206243943\n",
      "Losgistic Regression(    1200/10000): loss= 815.712013648209\n",
      "Losgistic Regression(    1300/10000): loss= 803.862829102668\n",
      "Losgistic Regression(    1400/10000): loss= 793.203423184537\n",
      "Losgistic Regression(    1500/10000): loss= 782.789939399811\n",
      "Losgistic Regression(    1600/10000): loss= 772.379423423585\n",
      "Losgistic Regression(    1700/10000): loss= 762.73825160792\n",
      "Losgistic Regression(    1800/10000): loss= 753.64421794877\n",
      "Losgistic Regression(    1900/10000): loss= 744.682752222898\n",
      "Losgistic Regression(    2000/10000): loss= 736.329428786897\n",
      "Losgistic Regression(    2100/10000): loss= 728.538317680673\n",
      "Losgistic Regression(    2200/10000): loss= 721.177142398223\n",
      "Losgistic Regression(    2300/10000): loss= 714.162057448603\n",
      "Losgistic Regression(    2400/10000): loss= 707.485307511137\n",
      "Losgistic Regression(    2500/10000): loss= 701.161567229195\n",
      "Losgistic Regression(    2600/10000): loss= 695.137958970754\n",
      "Losgistic Regression(    2700/10000): loss= 689.08205539723\n",
      "Losgistic Regression(    2800/10000): loss= 683.2488872055\n",
      "Losgistic Regression(    2900/10000): loss= 677.609276726945\n",
      "Losgistic Regression(    3000/10000): loss= 672.152549737308\n",
      "Losgistic Regression(    3100/10000): loss= 666.83296284621\n",
      "Losgistic Regression(    3200/10000): loss= 661.608805888426\n",
      "Losgistic Regression(    3300/10000): loss= 656.416518835661\n",
      "Losgistic Regression(    3400/10000): loss= 650.955449971056\n",
      "Losgistic Regression(    3500/10000): loss= 645.397205303065\n",
      "Losgistic Regression(    3600/10000): loss= 639.79210548361\n",
      "Losgistic Regression(    3700/10000): loss= 634.113484059664\n",
      "Losgistic Regression(    3800/10000): loss= 628.35051904952\n",
      "Losgistic Regression(    3900/10000): loss= 622.514778267442\n",
      "Losgistic Regression(    4000/10000): loss= 616.618480298484\n",
      "Losgistic Regression(    4100/10000): loss= 610.652948269352\n",
      "Losgistic Regression(    4200/10000): loss= 604.604636421673\n",
      "Losgistic Regression(    4300/10000): loss= 598.443233371764\n",
      "Losgistic Regression(    4400/10000): loss= 592.223442827188\n",
      "Losgistic Regression(    4500/10000): loss= 585.841048372505\n",
      "Losgistic Regression(    4600/10000): loss= 579.295392092697\n",
      "Losgistic Regression(    4700/10000): loss= 572.470458988349\n",
      "Losgistic Regression(    4800/10000): loss= 565.576465771999\n",
      "Losgistic Regression(    4900/10000): loss= 558.674367441824\n",
      "Losgistic Regression(    5000/10000): loss= 551.633372541105\n",
      "Losgistic Regression(    5100/10000): loss= 544.649082862567\n",
      "Losgistic Regression(    5200/10000): loss= 537.665499506281\n",
      "Losgistic Regression(    5300/10000): loss= 530.660413679638\n",
      "Losgistic Regression(    5400/10000): loss= 523.650039154158\n",
      "Losgistic Regression(    5500/10000): loss= 516.688934264935\n",
      "Losgistic Regression(    5600/10000): loss= 509.820153051779\n",
      "Losgistic Regression(    5700/10000): loss= 503.019376600268\n",
      "Losgistic Regression(    5800/10000): loss= 496.187185011879\n",
      "Losgistic Regression(    5900/10000): loss= 489.291580183502\n",
      "Losgistic Regression(    6000/10000): loss= 482.416725098074\n",
      "Losgistic Regression(    6100/10000): loss= 475.560630985629\n",
      "Losgistic Regression(    6200/10000): loss= 468.725991022813\n",
      "Losgistic Regression(    6300/10000): loss= 461.829875139343\n",
      "Losgistic Regression(    6400/10000): loss= 454.929788533395\n",
      "Losgistic Regression(    6500/10000): loss= 447.988508379786\n",
      "Losgistic Regression(    6600/10000): loss= 441.053134130464\n",
      "Losgistic Regression(    6700/10000): loss= 434.130538072893\n",
      "Losgistic Regression(    6800/10000): loss= 427.214621666437\n",
      "Losgistic Regression(    6900/10000): loss= 420.357156687811\n",
      "Losgistic Regression(    7000/10000): loss= 413.585493952065\n",
      "Losgistic Regression(    7100/10000): loss= 406.869931923195\n",
      "Losgistic Regression(    7200/10000): loss= 400.200665426693\n",
      "Losgistic Regression(    7300/10000): loss= 393.560653414135\n",
      "Losgistic Regression(    7400/10000): loss= 386.969945360082\n",
      "Losgistic Regression(    7500/10000): loss= 380.492881131325\n",
      "Losgistic Regression(    7600/10000): loss= 374.116454652025\n",
      "Losgistic Regression(    7700/10000): loss= 367.73875959411\n",
      "Losgistic Regression(    7800/10000): loss= 361.328462152725\n",
      "Losgistic Regression(    7900/10000): loss= 354.902378933289\n",
      "Losgistic Regression(    8000/10000): loss= 348.480044483782\n",
      "Losgistic Regression(    8100/10000): loss= 342.067651218998\n",
      "Losgistic Regression(    8200/10000): loss= 335.647279330423\n",
      "Losgistic Regression(    8300/10000): loss= 329.234470908997\n",
      "Losgistic Regression(    8400/10000): loss= 322.872716713189\n",
      "Losgistic Regression(    8500/10000): loss= 316.588097548434\n",
      "Losgistic Regression(    8600/10000): loss= 310.416073275774\n",
      "Losgistic Regression(    8700/10000): loss= 304.355693848312\n",
      "Losgistic Regression(    8800/10000): loss= 298.374415041855\n",
      "Losgistic Regression(    8900/10000): loss= 292.396156885898\n",
      "Losgistic Regression(    9000/10000): loss= 286.392440402034\n",
      "Losgistic Regression(    9100/10000): loss= 280.352772204312\n",
      "Losgistic Regression(    9200/10000): loss= 274.2857965755\n",
      "Losgistic Regression(    9300/10000): loss= 268.232498330069\n",
      "Losgistic Regression(    9400/10000): loss= 262.235476788606\n",
      "Losgistic Regression(    9500/10000): loss= 256.330233517827\n",
      "Losgistic Regression(    9600/10000): loss= 250.463168603158\n",
      "Losgistic Regression(    9700/10000): loss= 244.560416582568\n",
      "Losgistic Regression(    9800/10000): loss= 238.663032903728\n",
      "Losgistic Regression(    9900/10000): loss= 232.876555682563\n",
      "Time for  4th cross validation = 81.2737s\n",
      "Training Accuracy         =  0.886\n",
      "Cross Validation Accuracy = 0.780192\n",
      "*************** ([0.88319999999999999, 0.88400000000000001, 0.88039999999999996, 0.87160000000000004, 0.88600000000000001], [0.78554000000000002, 0.77680800000000005, 0.77092799999999995, 0.78194799999999998, 0.780192])\n",
      "Losgistic Regression(       0/10000): loss= 1725.78056587255\n",
      "Losgistic Regression(     100/10000): loss= 1239.07644461936\n",
      "Losgistic Regression(     200/10000): loss= 1077.88026091558\n",
      "Losgistic Regression(     300/10000): loss= 1019.35573769087\n",
      "Losgistic Regression(     400/10000): loss= 984.775774923056\n",
      "Losgistic Regression(     500/10000): loss= 959.730658772812\n",
      "Losgistic Regression(     600/10000): loss= 940.107055248558\n",
      "Losgistic Regression(     700/10000): loss= 924.13644782797\n",
      "Losgistic Regression(     800/10000): loss= 910.459938720795\n",
      "Losgistic Regression(     900/10000): loss= 898.572825887378\n",
      "Losgistic Regression(    1000/10000): loss= 888.042623005712\n",
      "Losgistic Regression(    1100/10000): loss= 878.687969556373\n",
      "Losgistic Regression(    1200/10000): loss= 870.35462994593\n",
      "Losgistic Regression(    1300/10000): loss= 862.906361336751\n",
      "Losgistic Regression(    1400/10000): loss= 856.237810692363\n",
      "Losgistic Regression(    1500/10000): loss= 850.25187793777\n",
      "Losgistic Regression(    1600/10000): loss= 844.856008145075\n",
      "Losgistic Regression(    1700/10000): loss= 839.979742647806\n",
      "Losgistic Regression(    1800/10000): loss= 835.542434573382\n",
      "Losgistic Regression(    1900/10000): loss= 831.494206091028\n",
      "Losgistic Regression(    2000/10000): loss= 827.814187937587\n",
      "Losgistic Regression(    2100/10000): loss= 824.438862368978\n",
      "Losgistic Regression(    2200/10000): loss= 821.333385672163\n",
      "Losgistic Regression(    2300/10000): loss= 818.487245952812\n",
      "Losgistic Regression(    2400/10000): loss= 815.852777762021\n",
      "Losgistic Regression(    2500/10000): loss= 813.404793147458\n",
      "Losgistic Regression(    2600/10000): loss= 811.139038374307\n",
      "Losgistic Regression(    2700/10000): loss= 809.046058724041\n",
      "Losgistic Regression(    2800/10000): loss= 807.114276997949\n",
      "Losgistic Regression(    2900/10000): loss= 805.308146742891\n",
      "Losgistic Regression(    3000/10000): loss= 803.621470689627\n",
      "Losgistic Regression(    3100/10000): loss= 802.043881792647\n",
      "Losgistic Regression(    3200/10000): loss= 800.311171970264\n",
      "Losgistic Regression(    3300/10000): loss= 798.61755631771\n",
      "Losgistic Regression(    3400/10000): loss= 796.984478754787\n",
      "Losgistic Regression(    3500/10000): loss= 795.427984155912\n",
      "Losgistic Regression(    3600/10000): loss= 793.959210736603\n",
      "Losgistic Regression(    3700/10000): loss= 792.576890455688\n",
      "Losgistic Regression(    3800/10000): loss= 791.261781329128\n",
      "Losgistic Regression(    3900/10000): loss= 790.009065705553\n",
      "Losgistic Regression(    4000/10000): loss= 788.798428121199\n",
      "Losgistic Regression(    4100/10000): loss= 787.570520859419\n",
      "Losgistic Regression(    4200/10000): loss= 786.311252719678\n",
      "Losgistic Regression(    4300/10000): loss= 785.104084811113\n",
      "Losgistic Regression(    4400/10000): loss= 783.93086240715\n",
      "Losgistic Regression(    4500/10000): loss= 782.839183696378\n",
      "Losgistic Regression(    4600/10000): loss= 781.810710921492\n",
      "Losgistic Regression(    4700/10000): loss= 780.853223435559\n",
      "Losgistic Regression(    4800/10000): loss= 779.941290722733\n",
      "Losgistic Regression(    4900/10000): loss= 779.016079409583\n",
      "Losgistic Regression(    5000/10000): loss= 778.096797255165\n",
      "Losgistic Regression(    5100/10000): loss= 777.223004392085\n",
      "Losgistic Regression(    5200/10000): loss= 776.379595888977\n",
      "Losgistic Regression(    5300/10000): loss= 775.598895160451\n",
      "Losgistic Regression(    5400/10000): loss= 774.86265973106\n",
      "Losgistic Regression(    5500/10000): loss= 774.129629566246\n",
      "Losgistic Regression(    5600/10000): loss= 773.423533843865\n",
      "Losgistic Regression(    5700/10000): loss= 772.713472421998\n",
      "Losgistic Regression(    5800/10000): loss= 771.902402542763\n",
      "Losgistic Regression(    5900/10000): loss= 771.161652537226\n",
      "Losgistic Regression(    6000/10000): loss= 770.483522682291\n",
      "Losgistic Regression(    6100/10000): loss= 769.858205367769\n",
      "Losgistic Regression(    6200/10000): loss= 769.265164853726\n",
      "Losgistic Regression(    6300/10000): loss= 768.700853456423\n",
      "Losgistic Regression(    6400/10000): loss= 768.072026246037\n",
      "Losgistic Regression(    6500/10000): loss= 767.405364445939\n",
      "Losgistic Regression(    6600/10000): loss= 766.693098313721\n",
      "Losgistic Regression(    6700/10000): loss= 765.953206437012\n",
      "Losgistic Regression(    6800/10000): loss= 765.189668090111\n",
      "Losgistic Regression(    6900/10000): loss= 764.398988581147\n",
      "Losgistic Regression(    7000/10000): loss= 763.61568326461\n",
      "Losgistic Regression(    7100/10000): loss= 762.904283323402\n",
      "Losgistic Regression(    7200/10000): loss= 762.234917233063\n",
      "Losgistic Regression(    7300/10000): loss= 761.653789991937\n",
      "Losgistic Regression(    7400/10000): loss= 761.065713204502\n",
      "Losgistic Regression(    7500/10000): loss= 760.437960817592\n",
      "Losgistic Regression(    7600/10000): loss= 759.799972333978\n",
      "Losgistic Regression(    7700/10000): loss= 759.14399141697\n",
      "Losgistic Regression(    7800/10000): loss= 758.491310694796\n",
      "Losgistic Regression(    7900/10000): loss= 757.923925703967\n",
      "Losgistic Regression(    8000/10000): loss= 757.324900894945\n",
      "Losgistic Regression(    8100/10000): loss= 756.770560984719\n",
      "Losgistic Regression(    8200/10000): loss= 756.314019963768\n",
      "Losgistic Regression(    8300/10000): loss= 755.809271511409\n",
      "Losgistic Regression(    8400/10000): loss= 755.28721595165\n",
      "Losgistic Regression(    8500/10000): loss= 754.777750427969\n",
      "Losgistic Regression(    8600/10000): loss= 754.259413758329\n",
      "Losgistic Regression(    8700/10000): loss= 753.803516254934\n",
      "Losgistic Regression(    8800/10000): loss= 753.444545610283\n",
      "Losgistic Regression(    8900/10000): loss= 753.118976870119\n",
      "Losgistic Regression(    9000/10000): loss= 752.781534854801\n",
      "Losgistic Regression(    9100/10000): loss= 752.372599246686\n",
      "Losgistic Regression(    9200/10000): loss= 751.936605493129\n",
      "Losgistic Regression(    9300/10000): loss= 751.50963029908\n",
      "Losgistic Regression(    9400/10000): loss= 751.203525271118\n",
      "Losgistic Regression(    9500/10000): loss= 750.928308847147\n",
      "Losgistic Regression(    9600/10000): loss= 750.511334048677\n",
      "Losgistic Regression(    9700/10000): loss= 750.050302641206\n",
      "Losgistic Regression(    9800/10000): loss= 749.651748420093\n",
      "Losgistic Regression(    9900/10000): loss= 749.294534878476\n",
      "Time for  0th cross validation = 83.0543s\n",
      "Training Accuracy         = 0.8816\n",
      "Cross Validation Accuracy = 0.789588\n",
      "Losgistic Regression(       0/10000): loss= 1724.82229590019\n",
      "Losgistic Regression(     100/10000): loss= 1224.61431747928\n",
      "Losgistic Regression(     200/10000): loss= 1064.82870238259\n",
      "Losgistic Regression(     300/10000): loss= 1001.04395835549\n",
      "Losgistic Regression(     400/10000): loss= 964.316319619217\n",
      "Losgistic Regression(     500/10000): loss= 937.869059203012\n",
      "Losgistic Regression(     600/10000): loss= 916.980124138315\n",
      "Losgistic Regression(     700/10000): loss= 899.698996421594\n",
      "Losgistic Regression(     800/10000): loss= 885.005293381415\n",
      "Losgistic Regression(     900/10000): loss= 871.837938645472\n",
      "Losgistic Regression(    1000/10000): loss= 860.285681213658\n",
      "Losgistic Regression(    1100/10000): loss= 850.461020580162\n",
      "Losgistic Regression(    1200/10000): loss= 842.043448315129\n",
      "Losgistic Regression(    1300/10000): loss= 834.887317554441\n",
      "Losgistic Regression(    1400/10000): loss= 828.789358050462\n",
      "Losgistic Regression(    1500/10000): loss= 823.572779619077\n",
      "Losgistic Regression(    1600/10000): loss= 819.108516441818\n",
      "Losgistic Regression(    1700/10000): loss= 815.243698177656\n",
      "Losgistic Regression(    1800/10000): loss= 811.746500885549\n",
      "Losgistic Regression(    1900/10000): loss= 807.725599255393\n",
      "Losgistic Regression(    2000/10000): loss= 804.078276106937\n",
      "Losgistic Regression(    2100/10000): loss= 800.766664913274\n",
      "Losgistic Regression(    2200/10000): loss= 797.746087675304\n",
      "Losgistic Regression(    2300/10000): loss= 794.99284730289\n",
      "Losgistic Regression(    2400/10000): loss= 792.449617180891\n",
      "Losgistic Regression(    2500/10000): loss= 790.069507081943\n",
      "Losgistic Regression(    2600/10000): loss= 787.858465915446\n",
      "Losgistic Regression(    2700/10000): loss= 785.837573621939\n",
      "Losgistic Regression(    2800/10000): loss= 783.984440724469\n",
      "Losgistic Regression(    2900/10000): loss= 782.082207590142\n",
      "Losgistic Regression(    3000/10000): loss= 780.150641826463\n",
      "Losgistic Regression(    3100/10000): loss= 778.352570540888\n",
      "Losgistic Regression(    3200/10000): loss= 776.304573289921\n",
      "Losgistic Regression(    3300/10000): loss= 774.34081030916\n",
      "Losgistic Regression(    3400/10000): loss= 772.168717130312\n",
      "Losgistic Regression(    3500/10000): loss= 770.189299382866\n",
      "Losgistic Regression(    3600/10000): loss= 768.198170698355\n",
      "Losgistic Regression(    3700/10000): loss= 766.253652190856\n",
      "Losgistic Regression(    3800/10000): loss= 764.382994043899\n",
      "Losgistic Regression(    3900/10000): loss= 762.427148740748\n",
      "Losgistic Regression(    4000/10000): loss= 760.340504356036\n",
      "Losgistic Regression(    4100/10000): loss= 758.113797847188\n",
      "Losgistic Regression(    4200/10000): loss= 755.517370694636\n",
      "Losgistic Regression(    4300/10000): loss= 752.749923746933\n",
      "Losgistic Regression(    4400/10000): loss= 749.967455969184\n",
      "Losgistic Regression(    4500/10000): loss= 747.404395209879\n",
      "Losgistic Regression(    4600/10000): loss= 745.062191903319\n",
      "Losgistic Regression(    4700/10000): loss= 742.800161119479\n",
      "Losgistic Regression(    4800/10000): loss= 740.542088922025\n",
      "Losgistic Regression(    4900/10000): loss= 738.343109807947\n",
      "Losgistic Regression(    5000/10000): loss= 736.150198345377\n",
      "Losgistic Regression(    5100/10000): loss= 733.8520739028\n",
      "Losgistic Regression(    5200/10000): loss= 731.526805156837\n",
      "Losgistic Regression(    5300/10000): loss= 729.340227283312\n",
      "Losgistic Regression(    5400/10000): loss= 727.480324952093\n",
      "Losgistic Regression(    5500/10000): loss= 725.628402796353\n",
      "Losgistic Regression(    5600/10000): loss= 723.485162427811\n",
      "Losgistic Regression(    5700/10000): loss= 721.417876092098\n",
      "Losgistic Regression(    5800/10000): loss= 719.698839153317\n",
      "Losgistic Regression(    5900/10000): loss= 718.187170277802\n",
      "Losgistic Regression(    6000/10000): loss= 716.686184958549\n",
      "Losgistic Regression(    6100/10000): loss= 715.149888200548\n",
      "Losgistic Regression(    6200/10000): loss= 713.825360935957\n",
      "Losgistic Regression(    6300/10000): loss= 712.385002710232\n",
      "Losgistic Regression(    6400/10000): loss= 710.754253035909\n",
      "Losgistic Regression(    6500/10000): loss= 709.500371246053\n",
      "Losgistic Regression(    6600/10000): loss= 708.429723324671\n",
      "Losgistic Regression(    6700/10000): loss= 707.231817792754\n",
      "Losgistic Regression(    6800/10000): loss= 706.019918686812\n",
      "Losgistic Regression(    6900/10000): loss= 704.696774381814\n",
      "Losgistic Regression(    7000/10000): loss= 703.148364844079\n",
      "Losgistic Regression(    7100/10000): loss= 701.962747367195\n",
      "Losgistic Regression(    7200/10000): loss= 700.994929526507\n",
      "Losgistic Regression(    7300/10000): loss= 700.355434002166\n",
      "Losgistic Regression(    7400/10000): loss= 699.856511921211\n",
      "Losgistic Regression(    7500/10000): loss= 699.202282486335\n",
      "Losgistic Regression(    7600/10000): loss= 698.678571387784\n",
      "Losgistic Regression(    7700/10000): loss= 698.1796023867\n",
      "Losgistic Regression(    7800/10000): loss= 697.638122776683\n",
      "Losgistic Regression(    7900/10000): loss= 697.157320172073\n",
      "Losgistic Regression(    8000/10000): loss= 696.97543034096\n",
      "Losgistic Regression(    8100/10000): loss= 696.911066503568\n",
      "Losgistic Regression(    8200/10000): loss= 696.914228757235\n",
      "Totoal number of iterations =  8200\n",
      "Loss                        =  696.914228757\n",
      "Time for  1th cross validation = 67.7413s\n",
      "Training Accuracy         =  0.882\n",
      "Cross Validation Accuracy = 0.781796\n",
      "Losgistic Regression(       0/10000): loss= 1726.07241752705\n",
      "Losgistic Regression(     100/10000): loss= 1225.49486628735\n",
      "Losgistic Regression(     200/10000): loss= 1069.93894233674\n",
      "Losgistic Regression(     300/10000): loss= 1009.71424247759\n",
      "Losgistic Regression(     400/10000): loss= 971.227709699467\n",
      "Losgistic Regression(     500/10000): loss= 943.68166409249\n",
      "Losgistic Regression(     600/10000): loss= 922.691382451748\n",
      "Losgistic Regression(     700/10000): loss= 905.92284462176\n",
      "Losgistic Regression(     800/10000): loss= 892.054608326706\n",
      "Losgistic Regression(     900/10000): loss= 880.376878430199\n",
      "Losgistic Regression(    1000/10000): loss= 869.823411067926\n",
      "Losgistic Regression(    1100/10000): loss= 859.38516812558\n",
      "Losgistic Regression(    1200/10000): loss= 850.10737805291\n",
      "Losgistic Regression(    1300/10000): loss= 841.907181282327\n",
      "Losgistic Regression(    1400/10000): loss= 834.639132847135\n",
      "Losgistic Regression(    1500/10000): loss= 828.180697003914\n",
      "Losgistic Regression(    1600/10000): loss= 822.446781199965\n",
      "Losgistic Regression(    1700/10000): loss= 817.298161352792\n",
      "Losgistic Regression(    1800/10000): loss= 812.596249866124\n",
      "Losgistic Regression(    1900/10000): loss= 808.279087177029\n",
      "Losgistic Regression(    2000/10000): loss= 803.902511897051\n",
      "Losgistic Regression(    2100/10000): loss= 799.530831795236\n",
      "Losgistic Regression(    2200/10000): loss= 795.429733037868\n",
      "Losgistic Regression(    2300/10000): loss= 791.54006215897\n",
      "Losgistic Regression(    2400/10000): loss= 787.890682359321\n",
      "Losgistic Regression(    2500/10000): loss= 784.527189513716\n",
      "Losgistic Regression(    2600/10000): loss= 781.396722299738\n",
      "Losgistic Regression(    2700/10000): loss= 777.856387327501\n",
      "Losgistic Regression(    2800/10000): loss= 774.242443972114\n",
      "Losgistic Regression(    2900/10000): loss= 770.327406146347\n",
      "Losgistic Regression(    3000/10000): loss= 766.734597983234\n",
      "Losgistic Regression(    3100/10000): loss= 763.458931998276\n",
      "Losgistic Regression(    3200/10000): loss= 760.423865251662\n",
      "Losgistic Regression(    3300/10000): loss= 757.587492641912\n",
      "Losgistic Regression(    3400/10000): loss= 754.951980343234\n",
      "Losgistic Regression(    3500/10000): loss= 752.45907320759\n",
      "Losgistic Regression(    3600/10000): loss= 750.215376916122\n",
      "Losgistic Regression(    3700/10000): loss= 748.090475253\n",
      "Losgistic Regression(    3800/10000): loss= 746.029552764975\n",
      "Losgistic Regression(    3900/10000): loss= 744.187108411843\n",
      "Losgistic Regression(    4000/10000): loss= 742.575332167366\n",
      "Losgistic Regression(    4100/10000): loss= 741.208539473712\n",
      "Losgistic Regression(    4200/10000): loss= 739.863418072852\n",
      "Losgistic Regression(    4300/10000): loss= 738.413477745148\n",
      "Losgistic Regression(    4400/10000): loss= 737.105684526758\n",
      "Losgistic Regression(    4500/10000): loss= 735.935660513262\n",
      "Losgistic Regression(    4600/10000): loss= 734.839547406349\n",
      "Losgistic Regression(    4700/10000): loss= 734.017205045428\n",
      "Losgistic Regression(    4800/10000): loss= 733.404449300645\n",
      "Losgistic Regression(    4900/10000): loss= 732.755733871731\n",
      "Losgistic Regression(    5000/10000): loss= 732.076156465087\n",
      "Losgistic Regression(    5100/10000): loss= 731.438177155471\n",
      "Losgistic Regression(    5200/10000): loss= 730.823204151928\n",
      "Losgistic Regression(    5300/10000): loss= 730.277300554675\n",
      "Losgistic Regression(    5400/10000): loss= 729.636759312271\n",
      "Losgistic Regression(    5500/10000): loss= 728.907427798883\n",
      "Losgistic Regression(    5600/10000): loss= 728.268145624066\n",
      "Losgistic Regression(    5700/10000): loss= 727.652723619294\n",
      "Losgistic Regression(    5800/10000): loss= 727.075797935025\n",
      "Losgistic Regression(    5900/10000): loss= 726.517369054088\n",
      "Losgistic Regression(    6000/10000): loss= 725.872851604891\n",
      "Losgistic Regression(    6100/10000): loss= 725.110941248583\n",
      "Losgistic Regression(    6200/10000): loss= 724.474486584838\n",
      "Losgistic Regression(    6300/10000): loss= 723.999041336536\n",
      "Losgistic Regression(    6400/10000): loss= 723.570580112332\n",
      "Losgistic Regression(    6500/10000): loss= 723.244705182261\n",
      "Losgistic Regression(    6600/10000): loss= 722.96464960965\n",
      "Losgistic Regression(    6700/10000): loss= 722.78107941539\n",
      "Losgistic Regression(    6800/10000): loss= 722.528746502732\n",
      "Losgistic Regression(    6900/10000): loss= 722.286388259769\n",
      "Losgistic Regression(    7000/10000): loss= 722.246931249103\n",
      "Losgistic Regression(    7100/10000): loss= 722.249024218921\n",
      "Totoal number of iterations =  7100\n",
      "Loss                        =  722.249024219\n",
      "Time for  2th cross validation = 58.3343s\n",
      "Training Accuracy         = 0.8808\n",
      "Cross Validation Accuracy = 0.776188\n",
      "Losgistic Regression(       0/10000): loss= 1725.44692507244\n",
      "Losgistic Regression(     100/10000): loss= 1251.14932735084\n",
      "Losgistic Regression(     200/10000): loss= 1113.48516761705\n",
      "Losgistic Regression(     300/10000): loss= 1054.18943203284\n",
      "Losgistic Regression(     400/10000): loss= 1015.19004534061\n",
      "Losgistic Regression(     500/10000): loss= 986.275571215088\n",
      "Losgistic Regression(     600/10000): loss= 963.919837690285\n",
      "Losgistic Regression(     700/10000): loss= 945.821947033444\n",
      "Losgistic Regression(     800/10000): loss= 927.751420027749\n",
      "Losgistic Regression(     900/10000): loss= 910.73123515621\n",
      "Losgistic Regression(    1000/10000): loss= 892.103296250132\n",
      "Losgistic Regression(    1100/10000): loss= 875.020041833922\n",
      "Losgistic Regression(    1200/10000): loss= 860.217945936573\n",
      "Losgistic Regression(    1300/10000): loss= 847.428015655174\n",
      "Losgistic Regression(    1400/10000): loss= 836.586556030104\n",
      "Losgistic Regression(    1500/10000): loss= 827.408135972919\n",
      "Losgistic Regression(    1600/10000): loss= 819.390371454343\n",
      "Losgistic Regression(    1700/10000): loss= 812.2997863321\n",
      "Losgistic Regression(    1800/10000): loss= 805.945523252856\n",
      "Losgistic Regression(    1900/10000): loss= 799.864153335284\n",
      "Losgistic Regression(    2000/10000): loss= 794.356333841785\n",
      "Losgistic Regression(    2100/10000): loss= 789.375302651042\n",
      "Losgistic Regression(    2200/10000): loss= 784.877960581597\n",
      "Losgistic Regression(    2300/10000): loss= 780.891082640748\n",
      "Losgistic Regression(    2400/10000): loss= 777.358266755501\n",
      "Losgistic Regression(    2500/10000): loss= 774.223704891788\n",
      "Losgistic Regression(    2600/10000): loss= 771.049866963679\n",
      "Losgistic Regression(    2700/10000): loss= 768.117086075749\n",
      "Losgistic Regression(    2800/10000): loss= 765.463032354185\n",
      "Losgistic Regression(    2900/10000): loss= 763.094885778197\n",
      "Losgistic Regression(    3000/10000): loss= 761.055158355535\n",
      "Losgistic Regression(    3100/10000): loss= 759.220673415296\n",
      "Losgistic Regression(    3200/10000): loss= 757.63844162733\n",
      "Losgistic Regression(    3300/10000): loss= 756.353975652484\n",
      "Losgistic Regression(    3400/10000): loss= 755.072312218612\n",
      "Losgistic Regression(    3500/10000): loss= 753.714705213543\n",
      "Losgistic Regression(    3600/10000): loss= 752.492248063953\n",
      "Losgistic Regression(    3700/10000): loss= 751.431760982826\n",
      "Losgistic Regression(    3800/10000): loss= 750.424447408417\n",
      "Losgistic Regression(    3900/10000): loss= 749.43981619989\n",
      "Losgistic Regression(    4000/10000): loss= 748.588010402135\n",
      "Losgistic Regression(    4100/10000): loss= 747.803713852953\n",
      "Losgistic Regression(    4200/10000): loss= 747.089341755023\n",
      "Losgistic Regression(    4300/10000): loss= 746.369470800257\n",
      "Losgistic Regression(    4400/10000): loss= 745.667084811612\n",
      "Losgistic Regression(    4500/10000): loss= 745.019386790768\n",
      "Losgistic Regression(    4600/10000): loss= 744.054274312504\n",
      "Losgistic Regression(    4700/10000): loss= 743.099709792991\n",
      "Losgistic Regression(    4800/10000): loss= 742.171491203437\n",
      "Losgistic Regression(    4900/10000): loss= 741.278435102425\n",
      "Losgistic Regression(    5000/10000): loss= 740.482777915673\n",
      "Losgistic Regression(    5100/10000): loss= 739.848691264044\n",
      "Losgistic Regression(    5200/10000): loss= 739.334787411467\n",
      "Losgistic Regression(    5300/10000): loss= 738.753059025464\n",
      "Losgistic Regression(    5400/10000): loss= 738.308301764268\n",
      "Losgistic Regression(    5500/10000): loss= 737.940859609565\n",
      "Losgistic Regression(    5600/10000): loss= 737.614086638702\n",
      "Losgistic Regression(    5700/10000): loss= 737.144660643938\n",
      "Losgistic Regression(    5800/10000): loss= 736.673997893352\n",
      "Losgistic Regression(    5900/10000): loss= 736.165803955198\n",
      "Losgistic Regression(    6000/10000): loss= 735.487207550834\n",
      "Losgistic Regression(    6100/10000): loss= 734.743948428832\n",
      "Losgistic Regression(    6200/10000): loss= 734.166321444877\n",
      "Losgistic Regression(    6300/10000): loss= 733.437387704075\n",
      "Losgistic Regression(    6400/10000): loss= 732.59295478499\n",
      "Losgistic Regression(    6500/10000): loss= 731.767804756216\n",
      "Losgistic Regression(    6600/10000): loss= 730.807429791491\n",
      "Losgistic Regression(    6700/10000): loss= 729.874497612905\n",
      "Losgistic Regression(    6800/10000): loss= 729.031879416394\n",
      "Losgistic Regression(    6900/10000): loss= 727.991197710074\n",
      "Losgistic Regression(    7000/10000): loss= 726.816654353348\n",
      "Losgistic Regression(    7100/10000): loss= 725.663615493577\n",
      "Losgistic Regression(    7200/10000): loss= 724.682638922981\n",
      "Losgistic Regression(    7300/10000): loss= 723.776458691064\n",
      "Losgistic Regression(    7400/10000): loss= 722.923348967171\n",
      "Losgistic Regression(    7500/10000): loss= 722.183135852103\n",
      "Losgistic Regression(    7600/10000): loss= 721.587304488521\n",
      "Losgistic Regression(    7700/10000): loss= 721.007003076742\n",
      "Losgistic Regression(    7800/10000): loss= 720.39472554069\n",
      "Losgistic Regression(    7900/10000): loss= 719.761130617847\n",
      "Losgistic Regression(    8000/10000): loss= 719.104525816949\n",
      "Losgistic Regression(    8100/10000): loss= 718.465602433627\n",
      "Losgistic Regression(    8200/10000): loss= 717.702733536858\n",
      "Losgistic Regression(    8300/10000): loss= 717.013789549719\n",
      "Losgistic Regression(    8400/10000): loss= 716.489990769361\n",
      "Losgistic Regression(    8500/10000): loss= 716.031392411607\n",
      "Losgistic Regression(    8600/10000): loss= 715.570201240951\n",
      "Losgistic Regression(    8700/10000): loss= 714.990410706022\n",
      "Losgistic Regression(    8800/10000): loss= 714.337158247748\n",
      "Losgistic Regression(    8900/10000): loss= 713.761864424908\n",
      "Losgistic Regression(    9000/10000): loss= 713.213968122751\n",
      "Losgistic Regression(    9100/10000): loss= 712.70610987466\n",
      "Losgistic Regression(    9200/10000): loss= 712.248321747932\n",
      "Losgistic Regression(    9300/10000): loss= 711.788167196933\n",
      "Losgistic Regression(    9400/10000): loss= 711.23898284977\n",
      "Losgistic Regression(    9500/10000): loss= 710.712631788778\n",
      "Losgistic Regression(    9600/10000): loss= 710.164259772823\n",
      "Losgistic Regression(    9700/10000): loss= 709.542734317612\n",
      "Losgistic Regression(    9800/10000): loss= 708.907927754485\n",
      "Losgistic Regression(    9900/10000): loss= 708.324506586636\n",
      "Time for  3th cross validation = 81.4281s\n",
      "Training Accuracy         = 0.8696\n",
      "Cross Validation Accuracy = 0.785488\n",
      "Losgistic Regression(       0/10000): loss= 1726.04748874879\n",
      "Losgistic Regression(     100/10000): loss= 1231.92036185145\n",
      "Losgistic Regression(     200/10000): loss= 1077.48458935471\n",
      "Losgistic Regression(     300/10000): loss= 1016.76477531851\n",
      "Losgistic Regression(     400/10000): loss= 976.831144747524\n",
      "Losgistic Regression(     500/10000): loss= 947.026376324488\n",
      "Losgistic Regression(     600/10000): loss= 923.571595023161\n",
      "Losgistic Regression(     700/10000): loss= 904.597863279736\n",
      "Losgistic Regression(     800/10000): loss= 888.340338921599\n",
      "Losgistic Regression(     900/10000): loss= 872.068238365879\n",
      "Losgistic Regression(    1000/10000): loss= 857.360904006965\n",
      "Losgistic Regression(    1100/10000): loss= 844.600368674409\n",
      "Losgistic Regression(    1200/10000): loss= 833.068929668129\n",
      "Losgistic Regression(    1300/10000): loss= 822.650255475062\n",
      "Losgistic Regression(    1400/10000): loss= 813.337600297192\n",
      "Losgistic Regression(    1500/10000): loss= 805.231393001119\n",
      "Losgistic Regression(    1600/10000): loss= 798.23717465318\n",
      "Losgistic Regression(    1700/10000): loss= 790.752541475383\n",
      "Losgistic Regression(    1800/10000): loss= 783.76489811838\n",
      "Losgistic Regression(    1900/10000): loss= 777.313292941194\n",
      "Losgistic Regression(    2000/10000): loss= 771.324523458172\n",
      "Losgistic Regression(    2100/10000): loss= 765.649448190774\n",
      "Losgistic Regression(    2200/10000): loss= 760.164077265146\n",
      "Losgistic Regression(    2300/10000): loss= 754.808924024819\n",
      "Losgistic Regression(    2400/10000): loss= 749.523293910906\n",
      "Losgistic Regression(    2500/10000): loss= 744.543301378986\n",
      "Losgistic Regression(    2600/10000): loss= 739.951287071044\n",
      "Losgistic Regression(    2700/10000): loss= 735.763365138537\n",
      "Losgistic Regression(    2800/10000): loss= 731.876492778526\n",
      "Losgistic Regression(    2900/10000): loss= 728.058499716853\n",
      "Losgistic Regression(    3000/10000): loss= 724.431340561515\n",
      "Losgistic Regression(    3100/10000): loss= 720.956067798896\n",
      "Losgistic Regression(    3200/10000): loss= 717.537111091807\n",
      "Losgistic Regression(    3300/10000): loss= 714.002321816494\n",
      "Losgistic Regression(    3400/10000): loss= 710.249946855092\n",
      "Losgistic Regression(    3500/10000): loss= 706.466946205342\n",
      "Losgistic Regression(    3600/10000): loss= 702.765523502629\n",
      "Losgistic Regression(    3700/10000): loss= 699.057329666217\n",
      "Losgistic Regression(    3800/10000): loss= 695.247222233496\n",
      "Losgistic Regression(    3900/10000): loss= 691.398372434381\n",
      "Losgistic Regression(    4000/10000): loss= 687.514403470884\n",
      "Losgistic Regression(    4100/10000): loss= 683.720896142279\n",
      "Losgistic Regression(    4200/10000): loss= 679.977710536608\n",
      "Losgistic Regression(    4300/10000): loss= 676.104052566196\n",
      "Losgistic Regression(    4400/10000): loss= 672.198530454191\n",
      "Losgistic Regression(    4500/10000): loss= 668.316563983396\n",
      "Losgistic Regression(    4600/10000): loss= 664.527651696332\n",
      "Losgistic Regression(    4700/10000): loss= 660.796792922451\n",
      "Losgistic Regression(    4800/10000): loss= 657.099184328594\n",
      "Losgistic Regression(    4900/10000): loss= 653.450376782146\n",
      "Losgistic Regression(    5000/10000): loss= 649.928494761252\n",
      "Losgistic Regression(    5100/10000): loss= 646.544148043622\n",
      "Losgistic Regression(    5200/10000): loss= 643.191718994945\n",
      "Losgistic Regression(    5300/10000): loss= 639.763473572208\n",
      "Losgistic Regression(    5400/10000): loss= 636.42942008182\n",
      "Losgistic Regression(    5500/10000): loss= 633.165384991047\n",
      "Losgistic Regression(    5600/10000): loss= 629.97146993643\n",
      "Losgistic Regression(    5700/10000): loss= 626.56440804174\n",
      "Losgistic Regression(    5800/10000): loss= 623.041021788979\n",
      "Losgistic Regression(    5900/10000): loss= 619.396665977331\n",
      "Losgistic Regression(    6000/10000): loss= 615.761462377678\n",
      "Losgistic Regression(    6100/10000): loss= 612.312097032505\n",
      "Losgistic Regression(    6200/10000): loss= 608.935568347298\n",
      "Losgistic Regression(    6300/10000): loss= 605.549742197394\n",
      "Losgistic Regression(    6400/10000): loss= 602.070574693171\n",
      "Losgistic Regression(    6500/10000): loss= 598.460709685257\n",
      "Losgistic Regression(    6600/10000): loss= 594.9565036862\n",
      "Losgistic Regression(    6700/10000): loss= 591.823645813199\n",
      "Losgistic Regression(    6800/10000): loss= 588.860085871814\n",
      "Losgistic Regression(    6900/10000): loss= 585.911837017018\n",
      "Losgistic Regression(    7000/10000): loss= 582.783847321606\n",
      "Losgistic Regression(    7100/10000): loss= 579.580875016269\n",
      "Losgistic Regression(    7200/10000): loss= 576.501614209516\n",
      "Losgistic Regression(    7300/10000): loss= 573.523279985882\n",
      "Losgistic Regression(    7400/10000): loss= 570.386185064462\n",
      "Losgistic Regression(    7500/10000): loss= 567.074234782376\n",
      "Losgistic Regression(    7600/10000): loss= 563.695598648296\n",
      "Losgistic Regression(    7700/10000): loss= 560.56094940646\n",
      "Losgistic Regression(    7800/10000): loss= 557.855168703699\n",
      "Losgistic Regression(    7900/10000): loss= 555.322686414379\n",
      "Losgistic Regression(    8000/10000): loss= 552.644746651769\n",
      "Losgistic Regression(    8100/10000): loss= 549.989903025278\n",
      "Losgistic Regression(    8200/10000): loss= 547.634095356467\n",
      "Losgistic Regression(    8300/10000): loss= 545.568557300473\n",
      "Losgistic Regression(    8400/10000): loss= 543.270618896413\n",
      "Losgistic Regression(    8500/10000): loss= 540.421543033807\n",
      "Losgistic Regression(    8600/10000): loss= 537.398163520103\n",
      "Losgistic Regression(    8700/10000): loss= 534.401198881212\n",
      "Losgistic Regression(    8800/10000): loss= 531.61751968404\n",
      "Losgistic Regression(    8900/10000): loss= 528.756839282533\n",
      "Losgistic Regression(    9000/10000): loss= 525.798767550554\n",
      "Losgistic Regression(    9100/10000): loss= 522.997397798992\n",
      "Losgistic Regression(    9200/10000): loss= 520.532790731565\n",
      "Losgistic Regression(    9300/10000): loss= 518.227733599521\n",
      "Losgistic Regression(    9400/10000): loss= 515.907965243161\n",
      "Losgistic Regression(    9500/10000): loss= 513.505956853206\n",
      "Losgistic Regression(    9600/10000): loss= 511.147520390332\n",
      "Losgistic Regression(    9700/10000): loss= 508.955924325358\n",
      "Losgistic Regression(    9800/10000): loss= 507.097772614355\n",
      "Losgistic Regression(    9900/10000): loss= 505.29287292752\n",
      "Time for  4th cross validation = 81.3802s\n",
      "Training Accuracy         = 0.8836\n",
      "Cross Validation Accuracy = 0.784344\n",
      "*************** ([0.88160000000000005, 0.88200000000000001, 0.88080000000000003, 0.86960000000000004, 0.88360000000000005], [0.78958799999999996, 0.78179600000000005, 0.77618799999999999, 0.78548799999999996, 0.78434400000000004])\n",
      "Losgistic Regression(       0/10000): loss= 1725.79899674268\n",
      "Losgistic Regression(     100/10000): loss= 1243.46497328571\n",
      "Losgistic Regression(     200/10000): loss= 1087.44918080483\n",
      "Losgistic Regression(     300/10000): loss= 1033.09114967217\n",
      "Losgistic Regression(     400/10000): loss= 1002.20476392319\n",
      "Losgistic Regression(     500/10000): loss= 980.753019611808\n",
      "Losgistic Regression(     600/10000): loss= 964.581091515693\n",
      "Losgistic Regression(     700/10000): loss= 951.79738229357\n",
      "Losgistic Regression(     800/10000): loss= 941.214299551632\n",
      "Losgistic Regression(     900/10000): loss= 932.414420718683\n",
      "Losgistic Regression(    1000/10000): loss= 924.942929285134\n",
      "Losgistic Regression(    1100/10000): loss= 918.632380471554\n",
      "Losgistic Regression(    1200/10000): loss= 913.160573918665\n",
      "Losgistic Regression(    1300/10000): loss= 908.351573511357\n",
      "Losgistic Regression(    1400/10000): loss= 904.17962126163\n",
      "Losgistic Regression(    1500/10000): loss= 900.495181316916\n",
      "Losgistic Regression(    1600/10000): loss= 897.255011698786\n",
      "Losgistic Regression(    1700/10000): loss= 894.46773246005\n",
      "Losgistic Regression(    1800/10000): loss= 892.072614463078\n",
      "Losgistic Regression(    1900/10000): loss= 889.884054170937\n",
      "Losgistic Regression(    2000/10000): loss= 887.883267558013\n",
      "Losgistic Regression(    2100/10000): loss= 886.084368563479\n",
      "Losgistic Regression(    2200/10000): loss= 884.4640465854\n",
      "Losgistic Regression(    2300/10000): loss= 883.105087074809\n",
      "Losgistic Regression(    2400/10000): loss= 881.873222021066\n",
      "Losgistic Regression(    2500/10000): loss= 880.755902699147\n",
      "Losgistic Regression(    2600/10000): loss= 879.744351737825\n",
      "Losgistic Regression(    2700/10000): loss= 878.813659918976\n",
      "Losgistic Regression(    2800/10000): loss= 877.984904940009\n",
      "Losgistic Regression(    2900/10000): loss= 877.18809036118\n",
      "Losgistic Regression(    3000/10000): loss= 876.417358378106\n",
      "Losgistic Regression(    3100/10000): loss= 875.672464750104\n",
      "Losgistic Regression(    3200/10000): loss= 874.995503443351\n",
      "Losgistic Regression(    3300/10000): loss= 874.331221292796\n",
      "Losgistic Regression(    3400/10000): loss= 873.771257727142\n",
      "Losgistic Regression(    3500/10000): loss= 873.217601946992\n",
      "Losgistic Regression(    3600/10000): loss= 872.667034941413\n",
      "Losgistic Regression(    3700/10000): loss= 872.134649611435\n",
      "Losgistic Regression(    3800/10000): loss= 871.666700999647\n",
      "Losgistic Regression(    3900/10000): loss= 871.253064795045\n",
      "Losgistic Regression(    4000/10000): loss= 870.843488096327\n",
      "Losgistic Regression(    4100/10000): loss= 870.485235731907\n",
      "Losgistic Regression(    4200/10000): loss= 870.139480828383\n",
      "Losgistic Regression(    4300/10000): loss= 869.803850053402\n",
      "Losgistic Regression(    4400/10000): loss= 869.484188113848\n",
      "Losgistic Regression(    4500/10000): loss= 869.161557168936\n",
      "Losgistic Regression(    4600/10000): loss= 868.804721984413\n",
      "Losgistic Regression(    4700/10000): loss= 868.481178026018\n",
      "Losgistic Regression(    4800/10000): loss= 868.16781275534\n",
      "Losgistic Regression(    4900/10000): loss= 867.898683664048\n",
      "Losgistic Regression(    5000/10000): loss= 867.656474201623\n",
      "Losgistic Regression(    5100/10000): loss= 867.404053610636\n",
      "Losgistic Regression(    5200/10000): loss= 867.152329841874\n",
      "Losgistic Regression(    5300/10000): loss= 866.964262236778\n",
      "Losgistic Regression(    5400/10000): loss= 866.800985250463\n",
      "Losgistic Regression(    5500/10000): loss= 866.607695682238\n",
      "Losgistic Regression(    5600/10000): loss= 866.420400882091\n",
      "Losgistic Regression(    5700/10000): loss= 866.216344079766\n",
      "Losgistic Regression(    5800/10000): loss= 866.024780006052\n",
      "Losgistic Regression(    5900/10000): loss= 865.828384371057\n",
      "Losgistic Regression(    6000/10000): loss= 865.661728880854\n",
      "Losgistic Regression(    6100/10000): loss= 865.471944547983\n",
      "Losgistic Regression(    6200/10000): loss= 865.25972776908\n",
      "Losgistic Regression(    6300/10000): loss= 865.061401139261\n",
      "Losgistic Regression(    6400/10000): loss= 864.903454262965\n",
      "Losgistic Regression(    6500/10000): loss= 864.772354275806\n",
      "Losgistic Regression(    6600/10000): loss= 864.632274842138\n",
      "Losgistic Regression(    6700/10000): loss= 864.484235326331\n",
      "Losgistic Regression(    6800/10000): loss= 864.359016023619\n",
      "Losgistic Regression(    6900/10000): loss= 864.191682944625\n",
      "Losgistic Regression(    7000/10000): loss= 864.01173673941\n",
      "Losgistic Regression(    7100/10000): loss= 863.854421816271\n",
      "Losgistic Regression(    7200/10000): loss= 863.715714396967\n",
      "Losgistic Regression(    7300/10000): loss= 863.588996026005\n",
      "Losgistic Regression(    7400/10000): loss= 863.480757757871\n",
      "Losgistic Regression(    7500/10000): loss= 863.344649784974\n",
      "Losgistic Regression(    7600/10000): loss= 863.216575701267\n",
      "Losgistic Regression(    7700/10000): loss= 863.076188239847\n",
      "Losgistic Regression(    7800/10000): loss= 862.941508659638\n",
      "Losgistic Regression(    7900/10000): loss= 862.816032056826\n",
      "Losgistic Regression(    8000/10000): loss= 862.711486846472\n",
      "Losgistic Regression(    8100/10000): loss= 862.599985372075\n",
      "Losgistic Regression(    8200/10000): loss= 862.503515734557\n",
      "Losgistic Regression(    8300/10000): loss= 862.405180984532\n",
      "Losgistic Regression(    8400/10000): loss= 862.294016684601\n",
      "Losgistic Regression(    8500/10000): loss= 862.176228764969\n",
      "Losgistic Regression(    8600/10000): loss= 862.084941813902\n",
      "Losgistic Regression(    8700/10000): loss= 862.002883535174\n",
      "Losgistic Regression(    8800/10000): loss= 861.912721475852\n",
      "Losgistic Regression(    8900/10000): loss= 861.830812043506\n",
      "Losgistic Regression(    9000/10000): loss= 861.751874647394\n",
      "Losgistic Regression(    9100/10000): loss= 861.661856789978\n",
      "Losgistic Regression(    9200/10000): loss= 861.57208003701\n",
      "Losgistic Regression(    9300/10000): loss= 861.481149314465\n",
      "Losgistic Regression(    9400/10000): loss= 861.396135198296\n",
      "Losgistic Regression(    9500/10000): loss= 861.319857655024\n",
      "Losgistic Regression(    9600/10000): loss= 861.252200329659\n",
      "Losgistic Regression(    9700/10000): loss= 861.177293251995\n",
      "Losgistic Regression(    9800/10000): loss= 861.10781195718\n",
      "Losgistic Regression(    9900/10000): loss= 861.041299805381\n",
      "Time for  0th cross validation = 80.0237s\n",
      "Training Accuracy         = 0.8708\n",
      "Cross Validation Accuracy = 0.79714\n",
      "Losgistic Regression(       0/10000): loss= 1724.8424192068\n",
      "Losgistic Regression(     100/10000): loss= 1228.96250701568\n",
      "Losgistic Regression(     200/10000): loss= 1074.11944086632\n",
      "Losgistic Regression(     300/10000): loss= 1014.62769258242\n",
      "Losgistic Regression(     400/10000): loss= 981.898001823514\n",
      "Losgistic Regression(     500/10000): loss= 959.239631441532\n",
      "Losgistic Regression(     600/10000): loss= 941.813965240215\n",
      "Losgistic Regression(     700/10000): loss= 927.742412755788\n",
      "Losgistic Regression(     800/10000): loss= 916.016199263197\n",
      "Losgistic Regression(     900/10000): loss= 906.137803003709\n",
      "Losgistic Regression(    1000/10000): loss= 897.839112928563\n",
      "Losgistic Regression(    1100/10000): loss= 890.912662532104\n",
      "Losgistic Regression(    1200/10000): loss= 885.210704924219\n",
      "Losgistic Regression(    1300/10000): loss= 880.37563307611\n",
      "Losgistic Regression(    1400/10000): loss= 876.239595991317\n",
      "Losgistic Regression(    1500/10000): loss= 872.728953631304\n",
      "Losgistic Regression(    1600/10000): loss= 869.795076509553\n",
      "Losgistic Regression(    1700/10000): loss= 867.297863245005\n",
      "Losgistic Regression(    1800/10000): loss= 865.108588104531\n",
      "Losgistic Regression(    1900/10000): loss= 863.188731557776\n",
      "Losgistic Regression(    2000/10000): loss= 861.536383355359\n",
      "Losgistic Regression(    2100/10000): loss= 860.060501681281\n",
      "Losgistic Regression(    2200/10000): loss= 858.772996622229\n",
      "Losgistic Regression(    2300/10000): loss= 857.647500094238\n",
      "Losgistic Regression(    2400/10000): loss= 856.66044159612\n",
      "Losgistic Regression(    2500/10000): loss= 855.778020783573\n",
      "Losgistic Regression(    2600/10000): loss= 854.984265171937\n",
      "Losgistic Regression(    2700/10000): loss= 854.308647455975\n",
      "Losgistic Regression(    2800/10000): loss= 853.714701662589\n",
      "Losgistic Regression(    2900/10000): loss= 853.149712239132\n",
      "Losgistic Regression(    3000/10000): loss= 852.64233270528\n",
      "Losgistic Regression(    3100/10000): loss= 852.159454351375\n",
      "Losgistic Regression(    3200/10000): loss= 851.766790922903\n",
      "Losgistic Regression(    3300/10000): loss= 851.420371843217\n",
      "Losgistic Regression(    3400/10000): loss= 851.108047061237\n",
      "Losgistic Regression(    3500/10000): loss= 850.801997194697\n",
      "Losgistic Regression(    3600/10000): loss= 850.496967209959\n",
      "Losgistic Regression(    3700/10000): loss= 850.195619122666\n",
      "Losgistic Regression(    3800/10000): loss= 849.928548207016\n",
      "Losgistic Regression(    3900/10000): loss= 849.705092467731\n",
      "Losgistic Regression(    4000/10000): loss= 849.502525407582\n",
      "Losgistic Regression(    4100/10000): loss= 849.291248362863\n",
      "Losgistic Regression(    4200/10000): loss= 849.108976194509\n",
      "Losgistic Regression(    4300/10000): loss= 848.924187098309\n",
      "Losgistic Regression(    4400/10000): loss= 848.744031502528\n",
      "Losgistic Regression(    4500/10000): loss= 848.582318728577\n",
      "Losgistic Regression(    4600/10000): loss= 848.432938965227\n",
      "Losgistic Regression(    4700/10000): loss= 848.284118802763\n",
      "Losgistic Regression(    4800/10000): loss= 848.12384152616\n",
      "Losgistic Regression(    4900/10000): loss= 847.951454809247\n",
      "Losgistic Regression(    5000/10000): loss= 847.766527287278\n",
      "Losgistic Regression(    5100/10000): loss= 847.602941783583\n",
      "Losgistic Regression(    5200/10000): loss= 847.478139496402\n",
      "Losgistic Regression(    5300/10000): loss= 847.366924284903\n",
      "Losgistic Regression(    5400/10000): loss= 847.269864938295\n",
      "Losgistic Regression(    5500/10000): loss= 847.173566441555\n",
      "Losgistic Regression(    5600/10000): loss= 847.067112408374\n",
      "Losgistic Regression(    5700/10000): loss= 846.977624545061\n",
      "Losgistic Regression(    5800/10000): loss= 846.846401751716\n",
      "Losgistic Regression(    5900/10000): loss= 846.776765749296\n",
      "Losgistic Regression(    6000/10000): loss= 846.72122325166\n",
      "Losgistic Regression(    6100/10000): loss= 846.648033528613\n",
      "Losgistic Regression(    6200/10000): loss= 846.580604901034\n",
      "Losgistic Regression(    6300/10000): loss= 846.4889267818\n",
      "Losgistic Regression(    6400/10000): loss= 846.404869301503\n",
      "Losgistic Regression(    6500/10000): loss= 846.33333957167\n",
      "Losgistic Regression(    6600/10000): loss= 846.229883297835\n",
      "Losgistic Regression(    6700/10000): loss= 846.127151192494\n",
      "Losgistic Regression(    6800/10000): loss= 846.050397921278\n",
      "Losgistic Regression(    6900/10000): loss= 846.008788734307\n",
      "Losgistic Regression(    7000/10000): loss= 845.962159507359\n",
      "Losgistic Regression(    7100/10000): loss= 845.904692576605\n",
      "Losgistic Regression(    7200/10000): loss= 845.825200511151\n",
      "Losgistic Regression(    7300/10000): loss= 845.75291845133\n",
      "Losgistic Regression(    7400/10000): loss= 845.715313705714\n",
      "Losgistic Regression(    7500/10000): loss= 845.674027170978\n",
      "Losgistic Regression(    7600/10000): loss= 845.607649291769\n",
      "Losgistic Regression(    7700/10000): loss= 845.554610054086\n",
      "Losgistic Regression(    7800/10000): loss= 845.483939272432\n",
      "Losgistic Regression(    7900/10000): loss= 845.412182243677\n",
      "Losgistic Regression(    8000/10000): loss= 845.356785011631\n",
      "Losgistic Regression(    8100/10000): loss= 845.29301814693\n",
      "Losgistic Regression(    8200/10000): loss= 845.252678740749\n",
      "Losgistic Regression(    8300/10000): loss= 845.212775983103\n",
      "Losgistic Regression(    8400/10000): loss= 845.165794533276\n",
      "Losgistic Regression(    8500/10000): loss= 845.120541045759\n",
      "Losgistic Regression(    8600/10000): loss= 845.058449422886\n",
      "Losgistic Regression(    8700/10000): loss= 844.997304833031\n",
      "Losgistic Regression(    8800/10000): loss= 844.946291399358\n",
      "Losgistic Regression(    8900/10000): loss= 844.911388057238\n",
      "Losgistic Regression(    9000/10000): loss= 844.895566064973\n",
      "Losgistic Regression(    9100/10000): loss= 844.88489881353\n",
      "Losgistic Regression(    9200/10000): loss= 844.874155909726\n",
      "Losgistic Regression(    9300/10000): loss= 844.863772335293\n",
      "Losgistic Regression(    9400/10000): loss= 844.854180172452\n",
      "Losgistic Regression(    9500/10000): loss= 844.845090856395\n",
      "Losgistic Regression(    9600/10000): loss= 844.83564315769\n",
      "Losgistic Regression(    9700/10000): loss= 844.826653376216\n",
      "Losgistic Regression(    9800/10000): loss= 844.82653061175\n",
      "Totoal number of iterations =  9800\n",
      "Loss                        =  844.826530612\n",
      "Time for  1th cross validation = 78.3034s\n",
      "Training Accuracy         = 0.8744\n",
      "Cross Validation Accuracy = 0.790588\n",
      "Losgistic Regression(       0/10000): loss= 1726.09052483154\n",
      "Losgistic Regression(     100/10000): loss= 1230.17853290651\n",
      "Losgistic Regression(     200/10000): loss= 1079.46241352269\n",
      "Losgistic Regression(     300/10000): loss= 1023.87739614833\n",
      "Losgistic Regression(     400/10000): loss= 989.770739586209\n",
      "Losgistic Regression(     500/10000): loss= 966.260797082002\n",
      "Losgistic Regression(     600/10000): loss= 949.046782260174\n",
      "Losgistic Regression(     700/10000): loss= 935.747269451854\n",
      "Losgistic Regression(     800/10000): loss= 925.170874115064\n",
      "Losgistic Regression(     900/10000): loss= 916.47906870137\n",
      "Losgistic Regression(    1000/10000): loss= 909.290542702364\n",
      "Losgistic Regression(    1100/10000): loss= 903.32219725506\n",
      "Losgistic Regression(    1200/10000): loss= 897.638111035003\n",
      "Losgistic Regression(    1300/10000): loss= 892.558177849086\n",
      "Losgistic Regression(    1400/10000): loss= 888.299129552412\n",
      "Losgistic Regression(    1500/10000): loss= 884.673327534616\n",
      "Losgistic Regression(    1600/10000): loss= 881.574059411803\n",
      "Losgistic Regression(    1700/10000): loss= 878.8702520501\n",
      "Losgistic Regression(    1800/10000): loss= 876.494655418192\n",
      "Losgistic Regression(    1900/10000): loss= 874.279485922418\n",
      "Losgistic Regression(    2000/10000): loss= 872.211698894544\n",
      "Losgistic Regression(    2100/10000): loss= 870.404349540091\n",
      "Losgistic Regression(    2200/10000): loss= 868.99811468842\n",
      "Losgistic Regression(    2300/10000): loss= 867.576691112221\n",
      "Losgistic Regression(    2400/10000): loss= 866.181931431772\n",
      "Losgistic Regression(    2500/10000): loss= 864.954260241231\n",
      "Losgistic Regression(    2600/10000): loss= 863.878019534462\n",
      "Losgistic Regression(    2700/10000): loss= 862.981413078676\n",
      "Losgistic Regression(    2800/10000): loss= 862.194445215007\n",
      "Losgistic Regression(    2900/10000): loss= 861.336664265266\n",
      "Losgistic Regression(    3000/10000): loss= 860.520793188511\n",
      "Losgistic Regression(    3100/10000): loss= 859.88520025161\n",
      "Losgistic Regression(    3200/10000): loss= 859.33118033325\n",
      "Losgistic Regression(    3300/10000): loss= 858.713016399685\n",
      "Losgistic Regression(    3400/10000): loss= 858.094418444004\n",
      "Losgistic Regression(    3500/10000): loss= 857.638204153728\n",
      "Losgistic Regression(    3600/10000): loss= 857.287490685144\n",
      "Losgistic Regression(    3700/10000): loss= 857.057666583443\n",
      "Losgistic Regression(    3800/10000): loss= 856.760738734405\n",
      "Losgistic Regression(    3900/10000): loss= 856.467970926657\n",
      "Losgistic Regression(    4000/10000): loss= 856.118684414767\n",
      "Losgistic Regression(    4100/10000): loss= 855.886880183221\n",
      "Losgistic Regression(    4200/10000): loss= 855.72132645316\n",
      "Losgistic Regression(    4300/10000): loss= 855.5405851771\n",
      "Losgistic Regression(    4400/10000): loss= 855.356356725239\n",
      "Losgistic Regression(    4500/10000): loss= 855.192128484225\n",
      "Losgistic Regression(    4600/10000): loss= 854.940516396178\n",
      "Losgistic Regression(    4700/10000): loss= 854.678631094709\n",
      "Losgistic Regression(    4800/10000): loss= 854.509237402067\n",
      "Losgistic Regression(    4900/10000): loss= 854.440778942948\n",
      "Losgistic Regression(    5000/10000): loss= 854.441891792977\n",
      "Totoal number of iterations =  5000\n",
      "Loss                        =  854.441891793\n",
      "Time for  2th cross validation = 40.0047s\n",
      "Training Accuracy         = 0.8712\n",
      "Cross Validation Accuracy = 0.7839\n",
      "Losgistic Regression(       0/10000): loss= 1725.46627475255\n",
      "Losgistic Regression(     100/10000): loss= 1255.70016441706\n",
      "Losgistic Regression(     200/10000): loss= 1122.72490942613\n",
      "Losgistic Regression(     300/10000): loss= 1067.82603800295\n",
      "Losgistic Regression(     400/10000): loss= 1033.00666618691\n",
      "Losgistic Regression(     500/10000): loss= 1008.03102590861\n",
      "Losgistic Regression(     600/10000): loss= 989.252395642177\n",
      "Losgistic Regression(     700/10000): loss= 974.432309179666\n",
      "Losgistic Regression(     800/10000): loss= 961.100906746629\n",
      "Losgistic Regression(     900/10000): loss= 947.533997988228\n",
      "Losgistic Regression(    1000/10000): loss= 933.290842763874\n",
      "Losgistic Regression(    1100/10000): loss= 920.654806579567\n",
      "Losgistic Regression(    1200/10000): loss= 908.99183551521\n",
      "Losgistic Regression(    1300/10000): loss= 899.339752556916\n",
      "Losgistic Regression(    1400/10000): loss= 891.907185104555\n",
      "Losgistic Regression(    1500/10000): loss= 885.614507573038\n",
      "Losgistic Regression(    1600/10000): loss= 879.863953299146\n",
      "Losgistic Regression(    1700/10000): loss= 874.99528192842\n",
      "Losgistic Regression(    1800/10000): loss= 870.721500948182\n",
      "Losgistic Regression(    1900/10000): loss= 866.530561935184\n",
      "Losgistic Regression(    2000/10000): loss= 862.608168447497\n",
      "Losgistic Regression(    2100/10000): loss= 859.470658582194\n",
      "Losgistic Regression(    2200/10000): loss= 857.208949805174\n",
      "Losgistic Regression(    2300/10000): loss= 855.417815854299\n",
      "Losgistic Regression(    2400/10000): loss= 854.055334559741\n",
      "Losgistic Regression(    2500/10000): loss= 852.950739205548\n",
      "Losgistic Regression(    2600/10000): loss= 852.005452164479\n",
      "Losgistic Regression(    2700/10000): loss= 851.53588893528\n",
      "Losgistic Regression(    2800/10000): loss= 851.487085906038\n",
      "Losgistic Regression(    2900/10000): loss= 851.486996175348\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  851.486996175\n",
      "Time for  3th cross validation = 23.2629s\n",
      "Training Accuracy         = 0.8552\n",
      "Cross Validation Accuracy = 0.78924\n",
      "Losgistic Regression(       0/10000): loss= 1726.06532334358\n",
      "Losgistic Regression(     100/10000): loss= 1236.14885347198\n",
      "Losgistic Regression(     200/10000): loss= 1086.5801853589\n",
      "Losgistic Regression(     300/10000): loss= 1030.2654719416\n",
      "Losgistic Regression(     400/10000): loss= 994.824931533699\n",
      "Losgistic Regression(     500/10000): loss= 969.101023431867\n",
      "Losgistic Regression(     600/10000): loss= 949.405988074478\n",
      "Losgistic Regression(     700/10000): loss= 933.941710028095\n",
      "Losgistic Regression(     800/10000): loss= 921.41189012914\n",
      "Losgistic Regression(     900/10000): loss= 910.579336005022\n",
      "Losgistic Regression(    1000/10000): loss= 900.061571843982\n",
      "Losgistic Regression(    1100/10000): loss= 890.707266236863\n",
      "Losgistic Regression(    1200/10000): loss= 883.107119247108\n",
      "Losgistic Regression(    1300/10000): loss= 876.84639460241\n",
      "Losgistic Regression(    1400/10000): loss= 871.695824753656\n",
      "Losgistic Regression(    1500/10000): loss= 867.577050020174\n",
      "Losgistic Regression(    1600/10000): loss= 864.344943559195\n",
      "Losgistic Regression(    1700/10000): loss= 861.648145090514\n",
      "Losgistic Regression(    1800/10000): loss= 859.449224666592\n",
      "Losgistic Regression(    1900/10000): loss= 857.747296051236\n",
      "Losgistic Regression(    2000/10000): loss= 856.32135535049\n",
      "Losgistic Regression(    2100/10000): loss= 854.866351556659\n",
      "Losgistic Regression(    2200/10000): loss= 853.331312493314\n",
      "Losgistic Regression(    2300/10000): loss= 851.964117566148\n",
      "Losgistic Regression(    2400/10000): loss= 850.772807654144\n",
      "Losgistic Regression(    2500/10000): loss= 849.2421703326\n",
      "Losgistic Regression(    2600/10000): loss= 847.636092064047\n",
      "Losgistic Regression(    2700/10000): loss= 846.401615367233\n",
      "Losgistic Regression(    2800/10000): loss= 845.626339509484\n",
      "Losgistic Regression(    2900/10000): loss= 845.199329078286\n",
      "Losgistic Regression(    3000/10000): loss= 844.915822552224\n",
      "Losgistic Regression(    3100/10000): loss= 844.62911036182\n",
      "Losgistic Regression(    3200/10000): loss= 844.176727467463\n",
      "Losgistic Regression(    3300/10000): loss= 843.464318537966\n",
      "Losgistic Regression(    3400/10000): loss= 842.501975936824\n",
      "Losgistic Regression(    3500/10000): loss= 841.56800908557\n",
      "Losgistic Regression(    3600/10000): loss= 840.9259654193\n",
      "Losgistic Regression(    3700/10000): loss= 840.137413758613\n",
      "Losgistic Regression(    3800/10000): loss= 839.23515521781\n",
      "Losgistic Regression(    3900/10000): loss= 838.313934710848\n",
      "Losgistic Regression(    4000/10000): loss= 837.891467545815\n",
      "Losgistic Regression(    4100/10000): loss= 837.553510044974\n",
      "Losgistic Regression(    4200/10000): loss= 837.161646115408\n",
      "Losgistic Regression(    4300/10000): loss= 837.160005597052\n",
      "Totoal number of iterations =  4300\n",
      "Loss                        =  837.160005597\n",
      "Time for  4th cross validation = 34.4017s\n",
      "Training Accuracy         = 0.8736\n",
      "Cross Validation Accuracy = 0.793172\n",
      "*************** ([0.87080000000000002, 0.87439999999999996, 0.87119999999999997, 0.85519999999999996, 0.87360000000000004], [0.79713999999999996, 0.79058799999999996, 0.78390000000000004, 0.78924000000000005, 0.79317199999999999])\n",
      "Losgistic Regression(       0/10000): loss= 1725.86317676019\n",
      "Losgistic Regression(     100/10000): loss= 1258.02257105804\n",
      "Losgistic Regression(     200/10000): loss= 1117.77206862883\n",
      "Losgistic Regression(     300/10000): loss= 1075.038016174\n",
      "Losgistic Regression(     400/10000): loss= 1053.6243292897\n",
      "Losgistic Regression(     500/10000): loss= 1040.2974747196\n",
      "Losgistic Regression(     600/10000): loss= 1030.7659114854\n",
      "Losgistic Regression(     700/10000): loss= 1023.67930652328\n",
      "Losgistic Regression(     800/10000): loss= 1018.64084131147\n",
      "Losgistic Regression(     900/10000): loss= 1014.75117269134\n",
      "Losgistic Regression(    1000/10000): loss= 1011.75860918403\n",
      "Losgistic Regression(    1100/10000): loss= 1009.34233852828\n",
      "Losgistic Regression(    1200/10000): loss= 1007.1632734975\n",
      "Losgistic Regression(    1300/10000): loss= 1005.39643720941\n",
      "Losgistic Regression(    1400/10000): loss= 1003.82312079098\n",
      "Losgistic Regression(    1500/10000): loss= 1002.51478207188\n",
      "Losgistic Regression(    1600/10000): loss= 1001.40167789613\n",
      "Losgistic Regression(    1700/10000): loss= 1000.5771533867\n",
      "Losgistic Regression(    1800/10000): loss= 999.807863275301\n",
      "Losgistic Regression(    1900/10000): loss= 999.168470909723\n",
      "Losgistic Regression(    2000/10000): loss= 998.586987987782\n",
      "Losgistic Regression(    2100/10000): loss= 997.99090759315\n",
      "Losgistic Regression(    2200/10000): loss= 997.539722269344\n",
      "Losgistic Regression(    2300/10000): loss= 997.025887664533\n",
      "Losgistic Regression(    2400/10000): loss= 996.568920511781\n",
      "Losgistic Regression(    2500/10000): loss= 996.093643045032\n",
      "Losgistic Regression(    2600/10000): loss= 995.67385651661\n",
      "Losgistic Regression(    2700/10000): loss= 995.283219771023\n",
      "Losgistic Regression(    2800/10000): loss= 994.873573975429\n",
      "Losgistic Regression(    2900/10000): loss= 994.484556622437\n",
      "Losgistic Regression(    3000/10000): loss= 994.105696731707\n",
      "Losgistic Regression(    3100/10000): loss= 993.756561753915\n",
      "Losgistic Regression(    3200/10000): loss= 993.559530699958\n",
      "Losgistic Regression(    3300/10000): loss= 993.521789522697\n",
      "Losgistic Regression(    3400/10000): loss= 993.4881886941\n",
      "Losgistic Regression(    3500/10000): loss= 993.450024307966\n",
      "Losgistic Regression(    3600/10000): loss= 993.406662122956\n",
      "Losgistic Regression(    3700/10000): loss= 993.357361973532\n",
      "Losgistic Regression(    3800/10000): loss= 993.30149293945\n",
      "Losgistic Regression(    3900/10000): loss= 993.238688577667\n",
      "Losgistic Regression(    4000/10000): loss= 993.168951748443\n",
      "Losgistic Regression(    4100/10000): loss= 993.153746501529\n",
      "Losgistic Regression(    4200/10000): loss= 993.153338315805\n",
      "Totoal number of iterations =  4200\n",
      "Loss                        =  993.153338316\n",
      "Time for  0th cross validation = 33.6376s\n",
      "Training Accuracy         = 0.8444\n",
      "Cross Validation Accuracy = 0.794224\n",
      "Losgistic Regression(       0/10000): loss= 1724.91249263041\n",
      "Losgistic Regression(     100/10000): loss= 1243.42048104811\n",
      "Losgistic Regression(     200/10000): loss= 1103.61235727625\n",
      "Losgistic Regression(     300/10000): loss= 1056.08519612249\n",
      "Losgistic Regression(     400/10000): loss= 1033.18780270874\n",
      "Losgistic Regression(     500/10000): loss= 1018.9883386976\n",
      "Losgistic Regression(     600/10000): loss= 1008.46523038071\n",
      "Losgistic Regression(     700/10000): loss= 1000.37421711857\n",
      "Losgistic Regression(     800/10000): loss= 993.842039158496\n",
      "Losgistic Regression(     900/10000): loss= 988.619050645563\n",
      "Losgistic Regression(    1000/10000): loss= 984.504819529528\n",
      "Losgistic Regression(    1100/10000): loss= 981.237496050812\n",
      "Losgistic Regression(    1200/10000): loss= 978.499029790103\n",
      "Losgistic Regression(    1300/10000): loss= 976.055340245308\n",
      "Losgistic Regression(    1400/10000): loss= 974.022451099015\n",
      "Losgistic Regression(    1500/10000): loss= 972.396349205805\n",
      "Losgistic Regression(    1600/10000): loss= 971.332524907097\n",
      "Losgistic Regression(    1700/10000): loss= 970.074434168718\n",
      "Losgistic Regression(    1800/10000): loss= 969.069825944072\n",
      "Losgistic Regression(    1900/10000): loss= 968.576307217151\n",
      "Losgistic Regression(    2000/10000): loss= 968.072246740538\n",
      "Losgistic Regression(    2100/10000): loss= 967.566056548985\n",
      "Losgistic Regression(    2200/10000): loss= 967.106745717744\n",
      "Losgistic Regression(    2300/10000): loss= 966.81182001203\n",
      "Losgistic Regression(    2400/10000): loss= 966.445342906229\n",
      "Losgistic Regression(    2500/10000): loss= 966.143661172804\n",
      "Losgistic Regression(    2600/10000): loss= 965.866055916278\n",
      "Losgistic Regression(    2700/10000): loss= 965.634811352725\n",
      "Losgistic Regression(    2800/10000): loss= 965.382485486789\n",
      "Losgistic Regression(    2900/10000): loss= 965.170203831558\n",
      "Losgistic Regression(    3000/10000): loss= 965.097631490302\n",
      "Losgistic Regression(    3100/10000): loss= 965.034264558271\n",
      "Losgistic Regression(    3200/10000): loss= 964.993263152474\n",
      "Losgistic Regression(    3300/10000): loss= 964.954414797576\n",
      "Losgistic Regression(    3400/10000): loss= 964.915324912549\n",
      "Losgistic Regression(    3500/10000): loss= 964.877265306878\n",
      "Losgistic Regression(    3600/10000): loss= 964.836703711773\n",
      "Losgistic Regression(    3700/10000): loss= 964.792989737443\n",
      "Losgistic Regression(    3800/10000): loss= 964.746634671798\n",
      "Losgistic Regression(    3900/10000): loss= 964.697285186836\n",
      "Losgistic Regression(    4000/10000): loss= 964.645025428096\n",
      "Losgistic Regression(    4100/10000): loss= 964.589884406431\n",
      "Losgistic Regression(    4200/10000): loss= 964.532195299117\n",
      "Losgistic Regression(    4300/10000): loss= 964.471838129107\n",
      "Losgistic Regression(    4400/10000): loss= 964.409136167059\n",
      "Losgistic Regression(    4500/10000): loss= 964.343828909403\n",
      "Losgistic Regression(    4600/10000): loss= 964.276749640352\n",
      "Losgistic Regression(    4700/10000): loss= 964.207326560787\n",
      "Losgistic Regression(    4800/10000): loss= 964.135809813043\n",
      "Losgistic Regression(    4900/10000): loss= 964.123827084957\n",
      "Losgistic Regression(    5000/10000): loss= 964.123643355598\n",
      "Totoal number of iterations =  5000\n",
      "Loss                        =  964.123643356\n",
      "Time for  1th cross validation = 39.897s\n",
      "Training Accuracy         = 0.8504\n",
      "Cross Validation Accuracy = 0.796292\n",
      "Losgistic Regression(       0/10000): loss= 1726.15357812805\n",
      "Losgistic Regression(     100/10000): loss= 1245.74267861601\n",
      "Losgistic Regression(     200/10000): loss= 1109.94751205473\n",
      "Losgistic Regression(     300/10000): loss= 1067.39702936261\n",
      "Losgistic Regression(     400/10000): loss= 1044.28776791808\n",
      "Losgistic Regression(     500/10000): loss= 1029.96614507008\n",
      "Losgistic Regression(     600/10000): loss= 1020.37981060765\n",
      "Losgistic Regression(     700/10000): loss= 1013.53967887292\n",
      "Losgistic Regression(     800/10000): loss= 1008.32959127879\n",
      "Losgistic Regression(     900/10000): loss= 1004.48166619655\n",
      "Losgistic Regression(    1000/10000): loss= 1001.73769513398\n",
      "Losgistic Regression(    1100/10000): loss= 999.330656891525\n",
      "Losgistic Regression(    1200/10000): loss= 997.496247140783\n",
      "Losgistic Regression(    1300/10000): loss= 996.088350172024\n",
      "Losgistic Regression(    1400/10000): loss= 994.745000396524\n",
      "Losgistic Regression(    1500/10000): loss= 993.783620025825\n",
      "Losgistic Regression(    1600/10000): loss= 992.859616345723\n",
      "Losgistic Regression(    1700/10000): loss= 992.369600259051\n",
      "Losgistic Regression(    1800/10000): loss= 991.898455605899\n",
      "Losgistic Regression(    1900/10000): loss= 991.453076429416\n",
      "Losgistic Regression(    2000/10000): loss= 991.004584194981\n",
      "Losgistic Regression(    2100/10000): loss= 990.684397984421\n",
      "Losgistic Regression(    2200/10000): loss= 990.406782346362\n",
      "Losgistic Regression(    2300/10000): loss= 990.17125827513\n",
      "Losgistic Regression(    2400/10000): loss= 990.060497777945\n",
      "Losgistic Regression(    2500/10000): loss= 989.945684138942\n",
      "Losgistic Regression(    2600/10000): loss= 989.893908246348\n",
      "Losgistic Regression(    2700/10000): loss= 989.849023091868\n",
      "Losgistic Regression(    2800/10000): loss= 989.802983802965\n",
      "Losgistic Regression(    2900/10000): loss= 989.756710379686\n",
      "Losgistic Regression(    3000/10000): loss= 989.705823325906\n",
      "Losgistic Regression(    3100/10000): loss= 989.653957977417\n",
      "Losgistic Regression(    3200/10000): loss= 989.602440847499\n",
      "Losgistic Regression(    3300/10000): loss= 989.547364274735\n",
      "Losgistic Regression(    3400/10000): loss= 989.491171556965\n",
      "Losgistic Regression(    3500/10000): loss= 989.436848712984\n",
      "Losgistic Regression(    3600/10000): loss= 989.379624897088\n",
      "Losgistic Regression(    3700/10000): loss= 989.322166252006\n",
      "Losgistic Regression(    3800/10000): loss= 989.26394974688\n",
      "Losgistic Regression(    3900/10000): loss= 989.204820861549\n",
      "Losgistic Regression(    4000/10000): loss= 989.144637644295\n",
      "Losgistic Regression(    4100/10000): loss= 989.08850723874\n",
      "Losgistic Regression(    4200/10000): loss= 989.031658839851\n",
      "Losgistic Regression(    4300/10000): loss= 988.973550779039\n",
      "Losgistic Regression(    4400/10000): loss= 988.915962832413\n",
      "Losgistic Regression(    4500/10000): loss= 988.858270930059\n",
      "Losgistic Regression(    4600/10000): loss= 988.80080837098\n",
      "Losgistic Regression(    4700/10000): loss= 988.744805453517\n",
      "Losgistic Regression(    4800/10000): loss= 988.687977106997\n",
      "Losgistic Regression(    4900/10000): loss= 988.630401449995\n",
      "Losgistic Regression(    5000/10000): loss= 988.573570869665\n",
      "Losgistic Regression(    5100/10000): loss= 988.515632917792\n",
      "Losgistic Regression(    5200/10000): loss= 988.458250522131\n",
      "Losgistic Regression(    5300/10000): loss= 988.400593612084\n",
      "Losgistic Regression(    5400/10000): loss= 988.342532072655\n",
      "Losgistic Regression(    5500/10000): loss= 988.28440116802\n",
      "Losgistic Regression(    5600/10000): loss= 988.266208572894\n",
      "Losgistic Regression(    5700/10000): loss= 988.266097121205\n",
      "Totoal number of iterations =  5700\n",
      "Loss                        =  988.266097121\n",
      "Time for  2th cross validation = 45.711s\n",
      "Training Accuracy         = 0.8528\n",
      "Cross Validation Accuracy = 0.788756\n",
      "Losgistic Regression(       0/10000): loss= 1725.53365425221\n",
      "Losgistic Regression(     100/10000): loss= 1270.85115702397\n",
      "Losgistic Regression(     200/10000): loss= 1152.17109455675\n",
      "Losgistic Regression(     300/10000): loss= 1109.64234637338\n",
      "Losgistic Regression(     400/10000): loss= 1085.90129204696\n",
      "Losgistic Regression(     500/10000): loss= 1070.18961089456\n",
      "Losgistic Regression(     600/10000): loss= 1058.91277760885\n",
      "Losgistic Regression(     700/10000): loss= 1050.70495120264\n",
      "Losgistic Regression(     800/10000): loss= 1044.40439225223\n",
      "Losgistic Regression(     900/10000): loss= 1039.58677465523\n",
      "Losgistic Regression(    1000/10000): loss= 1034.7547561204\n",
      "Losgistic Regression(    1100/10000): loss= 1028.65366301218\n",
      "Losgistic Regression(    1200/10000): loss= 1023.08781697131\n",
      "Losgistic Regression(    1300/10000): loss= 1020.08931893643\n",
      "Losgistic Regression(    1400/10000): loss= 1017.12168956449\n",
      "Losgistic Regression(    1500/10000): loss= 1014.83919446231\n",
      "Losgistic Regression(    1600/10000): loss= 1012.2691604068\n",
      "Losgistic Regression(    1700/10000): loss= 1010.74598005271\n",
      "Losgistic Regression(    1800/10000): loss= 1010.36392819695\n",
      "Losgistic Regression(    1900/10000): loss= 1010.28400609783\n",
      "Losgistic Regression(    2000/10000): loss= 1010.28119953622\n",
      "Totoal number of iterations =  2000\n",
      "Loss                        =  1010.28119954\n",
      "Time for  3th cross validation = 16.132s\n",
      "Training Accuracy         = 0.8368\n",
      "Cross Validation Accuracy = 0.79016\n",
      "Losgistic Regression(       0/10000): loss= 1726.12742700976\n",
      "Losgistic Regression(     100/10000): loss= 1250.22484354635\n",
      "Losgistic Regression(     200/10000): loss= 1115.54864069555\n",
      "Losgistic Regression(     300/10000): loss= 1072.03741827418\n",
      "Losgistic Regression(     400/10000): loss= 1047.98027980937\n",
      "Losgistic Regression(     500/10000): loss= 1031.59417785595\n",
      "Losgistic Regression(     600/10000): loss= 1020.07275943645\n",
      "Losgistic Regression(     700/10000): loss= 1011.38178238294\n",
      "Losgistic Regression(     800/10000): loss= 1004.68196240893\n",
      "Losgistic Regression(     900/10000): loss= 999.543475615582\n",
      "Losgistic Regression(    1000/10000): loss= 995.631900341518\n",
      "Losgistic Regression(    1100/10000): loss= 992.547603204356\n",
      "Losgistic Regression(    1200/10000): loss= 990.157988623512\n",
      "Losgistic Regression(    1300/10000): loss= 988.323005463385\n",
      "Losgistic Regression(    1400/10000): loss= 986.891564552197\n",
      "Losgistic Regression(    1500/10000): loss= 985.672595665704\n",
      "Losgistic Regression(    1600/10000): loss= 984.81617648566\n",
      "Losgistic Regression(    1700/10000): loss= 984.13852492944\n",
      "Losgistic Regression(    1800/10000): loss= 983.474282140323\n",
      "Losgistic Regression(    1900/10000): loss= 982.916792028621\n",
      "Losgistic Regression(    2000/10000): loss= 982.36457045209\n",
      "Losgistic Regression(    2100/10000): loss= 981.815363327131\n",
      "Losgistic Regression(    2200/10000): loss= 981.423499438799\n",
      "Losgistic Regression(    2300/10000): loss= 981.105347443756\n",
      "Losgistic Regression(    2400/10000): loss= 980.759767880127\n",
      "Losgistic Regression(    2500/10000): loss= 980.471481936473\n",
      "Losgistic Regression(    2600/10000): loss= 980.239006054775\n",
      "Losgistic Regression(    2700/10000): loss= 979.971188300447\n",
      "Losgistic Regression(    2800/10000): loss= 979.766631125158\n",
      "Losgistic Regression(    2900/10000): loss= 979.521860251717\n",
      "Losgistic Regression(    3000/10000): loss= 979.305274882403\n",
      "Losgistic Regression(    3100/10000): loss= 979.124520636221\n",
      "Losgistic Regression(    3200/10000): loss= 978.924639123091\n",
      "Losgistic Regression(    3300/10000): loss= 978.75773065549\n",
      "Losgistic Regression(    3400/10000): loss= 978.719211180597\n",
      "Losgistic Regression(    3500/10000): loss= 978.683561720698\n",
      "Losgistic Regression(    3600/10000): loss= 978.652757768462\n",
      "Losgistic Regression(    3700/10000): loss= 978.638263304874\n",
      "Losgistic Regression(    3800/10000): loss= 978.637912545913\n",
      "Totoal number of iterations =  3800\n",
      "Loss                        =  978.637912546\n",
      "Time for  4th cross validation = 30.4057s\n",
      "Training Accuracy         = 0.8516\n",
      "Cross Validation Accuracy = 0.797012\n",
      "*************** ([0.84440000000000004, 0.85040000000000004, 0.8528, 0.83679999999999999, 0.85160000000000002], [0.79422400000000004, 0.796292, 0.78875600000000001, 0.79015999999999997, 0.79701200000000005])\n",
      "Losgistic Regression(       0/10000): loss= 1726.08666456175\n",
      "Losgistic Regression(     100/10000): loss= 1300.36354249297\n",
      "Losgistic Regression(     200/10000): loss= 1193.96309696313\n",
      "Losgistic Regression(     300/10000): loss= 1170.89932138868\n",
      "Losgistic Regression(     400/10000): loss= 1161.43866121067\n",
      "Losgistic Regression(     500/10000): loss= 1155.69324715\n",
      "Losgistic Regression(     600/10000): loss= 1151.47518230298\n",
      "Losgistic Regression(     700/10000): loss= 1148.77003268003\n",
      "Losgistic Regression(     800/10000): loss= 1146.52796771531\n",
      "Losgistic Regression(     900/10000): loss= 1144.81985049757\n",
      "Losgistic Regression(    1000/10000): loss= 1143.71936119527\n",
      "Losgistic Regression(    1100/10000): loss= 1143.47525326723\n",
      "Losgistic Regression(    1200/10000): loss= 1143.29286327698\n",
      "Losgistic Regression(    1300/10000): loss= 1143.06063295362\n",
      "Losgistic Regression(    1400/10000): loss= 1142.78862026471\n",
      "Losgistic Regression(    1500/10000): loss= 1142.51627428924\n",
      "Losgistic Regression(    1600/10000): loss= 1142.2477815417\n",
      "Losgistic Regression(    1700/10000): loss= 1141.99400855458\n",
      "Losgistic Regression(    1800/10000): loss= 1141.73914454107\n",
      "Losgistic Regression(    1900/10000): loss= 1141.48351870918\n",
      "Losgistic Regression(    2000/10000): loss= 1141.23047346604\n",
      "Losgistic Regression(    2100/10000): loss= 1140.97048919962\n",
      "Losgistic Regression(    2200/10000): loss= 1140.71978445041\n",
      "Losgistic Regression(    2300/10000): loss= 1140.67507534059\n",
      "Losgistic Regression(    2400/10000): loss= 1140.67413089524\n",
      "Totoal number of iterations =  2400\n",
      "Loss                        =  1140.6741309\n",
      "Time for  0th cross validation = 19.3453s\n",
      "Training Accuracy         = 0.8216\n",
      "Cross Validation Accuracy = 0.788336\n",
      "Losgistic Regression(       0/10000): loss= 1725.156502464\n",
      "Losgistic Regression(     100/10000): loss= 1285.35489178162\n",
      "Losgistic Regression(     200/10000): loss= 1177.96082576228\n",
      "Losgistic Regression(     300/10000): loss= 1149.49045709893\n",
      "Losgistic Regression(     400/10000): loss= 1138.69844189958\n",
      "Losgistic Regression(     500/10000): loss= 1132.80886863098\n",
      "Losgistic Regression(     600/10000): loss= 1128.24425835467\n",
      "Losgistic Regression(     700/10000): loss= 1124.73181046334\n",
      "Losgistic Regression(     800/10000): loss= 1123.27928390893\n",
      "Losgistic Regression(     900/10000): loss= 1122.79140179055\n",
      "Losgistic Regression(    1000/10000): loss= 1122.29036288874\n",
      "Losgistic Regression(    1100/10000): loss= 1121.63389867757\n",
      "Losgistic Regression(    1200/10000): loss= 1120.87186278395\n",
      "Losgistic Regression(    1300/10000): loss= 1120.08258373612\n",
      "Losgistic Regression(    1400/10000): loss= 1119.29196963102\n",
      "Losgistic Regression(    1500/10000): loss= 1118.56015279564\n",
      "Losgistic Regression(    1600/10000): loss= 1117.82506287215\n",
      "Losgistic Regression(    1700/10000): loss= 1117.16483460254\n",
      "Losgistic Regression(    1800/10000): loss= 1116.65152178656\n",
      "Losgistic Regression(    1900/10000): loss= 1116.59782065764\n",
      "Losgistic Regression(    2000/10000): loss= 1116.59054899036\n",
      "Totoal number of iterations =  2000\n",
      "Loss                        =  1116.59054899\n",
      "Time for  1th cross validation = 16.1112s\n",
      "Training Accuracy         = 0.8196\n",
      "Cross Validation Accuracy = 0.787564\n",
      "Losgistic Regression(       0/10000): loss= 1726.37314245924\n",
      "Losgistic Regression(     100/10000): loss= 1291.5998449402\n",
      "Losgistic Regression(     200/10000): loss= 1187.67228034385\n",
      "Losgistic Regression(     300/10000): loss= 1165.9394083683\n",
      "Losgistic Regression(     400/10000): loss= 1156.1225559305\n",
      "Losgistic Regression(     500/10000): loss= 1149.87543554343\n",
      "Losgistic Regression(     600/10000): loss= 1145.29942075362\n",
      "Losgistic Regression(     700/10000): loss= 1141.80065853657\n",
      "Losgistic Regression(     800/10000): loss= 1139.6606556691\n",
      "Losgistic Regression(     900/10000): loss= 1137.84738671592\n",
      "Losgistic Regression(    1000/10000): loss= 1136.55680505432\n",
      "Losgistic Regression(    1100/10000): loss= 1135.88406109755\n",
      "Losgistic Regression(    1200/10000): loss= 1135.68556469792\n",
      "Losgistic Regression(    1300/10000): loss= 1135.48237833604\n",
      "Losgistic Regression(    1400/10000): loss= 1135.26271872986\n",
      "Losgistic Regression(    1500/10000): loss= 1135.00276326451\n",
      "Losgistic Regression(    1600/10000): loss= 1134.7361429932\n",
      "Losgistic Regression(    1700/10000): loss= 1134.45595753293\n",
      "Losgistic Regression(    1800/10000): loss= 1134.35442097325\n",
      "Losgistic Regression(    1900/10000): loss= 1134.3519344929\n",
      "Totoal number of iterations =  1900\n",
      "Loss                        =  1134.35193449\n",
      "Time for  2th cross validation = 15.2619s\n",
      "Training Accuracy         = 0.8152\n",
      "Cross Validation Accuracy = 0.783424\n",
      "Losgistic Regression(       0/10000): loss= 1725.76828329777\n",
      "Losgistic Regression(     100/10000): loss= 1315.2366841062\n",
      "Losgistic Regression(     200/10000): loss= 1227.39578182893\n",
      "Losgistic Regression(     300/10000): loss= 1205.76459811404\n",
      "Losgistic Regression(     400/10000): loss= 1195.53591226958\n",
      "Losgistic Regression(     500/10000): loss= 1188.98824507913\n",
      "Losgistic Regression(     600/10000): loss= 1184.6220803083\n",
      "Losgistic Regression(     700/10000): loss= 1181.66840385826\n",
      "Losgistic Regression(     800/10000): loss= 1179.40226563972\n",
      "Losgistic Regression(     900/10000): loss= 1177.75521243068\n",
      "Losgistic Regression(    1000/10000): loss= 1176.54740106676\n",
      "Losgistic Regression(    1100/10000): loss= 1175.60565978357\n",
      "Losgistic Regression(    1200/10000): loss= 1174.86631306072\n",
      "Losgistic Regression(    1300/10000): loss= 1174.71688447973\n",
      "Losgistic Regression(    1400/10000): loss= 1174.57927336495\n",
      "Losgistic Regression(    1500/10000): loss= 1174.45770700792\n",
      "Losgistic Regression(    1600/10000): loss= 1174.33563108329\n",
      "Losgistic Regression(    1700/10000): loss= 1174.33392473444\n",
      "Totoal number of iterations =  1700\n",
      "Loss                        =  1174.33392473\n",
      "Time for  3th cross validation = 13.6464s\n",
      "Training Accuracy         = 0.8136\n",
      "Cross Validation Accuracy = 0.786216\n",
      "Losgistic Regression(       0/10000): loss= 1726.34368453606\n",
      "Losgistic Regression(     100/10000): loss= 1291.5794835707\n",
      "Losgistic Regression(     200/10000): loss= 1189.82428375296\n",
      "Losgistic Regression(     300/10000): loss= 1167.03830682009\n",
      "Losgistic Regression(     400/10000): loss= 1157.69353067344\n",
      "Losgistic Regression(     500/10000): loss= 1151.60536923104\n",
      "Losgistic Regression(     600/10000): loss= 1147.02268754233\n",
      "Losgistic Regression(     700/10000): loss= 1143.81266720475\n",
      "Losgistic Regression(     800/10000): loss= 1141.44562298156\n",
      "Losgistic Regression(     900/10000): loss= 1139.53843563392\n",
      "Losgistic Regression(    1000/10000): loss= 1138.22016368661\n",
      "Losgistic Regression(    1100/10000): loss= 1137.79689205482\n",
      "Losgistic Regression(    1200/10000): loss= 1137.63480609147\n",
      "Losgistic Regression(    1300/10000): loss= 1137.47087113593\n",
      "Losgistic Regression(    1400/10000): loss= 1137.2728586454\n",
      "Losgistic Regression(    1500/10000): loss= 1137.03849401344\n",
      "Losgistic Regression(    1600/10000): loss= 1136.77887862976\n",
      "Losgistic Regression(    1700/10000): loss= 1136.54330243629\n",
      "Losgistic Regression(    1800/10000): loss= 1136.32112238779\n",
      "Losgistic Regression(    1900/10000): loss= 1136.11795657918\n",
      "Losgistic Regression(    2000/10000): loss= 1135.9268204533\n",
      "Losgistic Regression(    2100/10000): loss= 1135.86037722756\n",
      "Losgistic Regression(    2200/10000): loss= 1135.85808332919\n",
      "Totoal number of iterations =  2200\n",
      "Loss                        =  1135.85808333\n",
      "Time for  4th cross validation = 17.8453s\n",
      "Training Accuracy         = 0.8192\n",
      "Cross Validation Accuracy = 0.791076\n",
      "*************** ([0.8216, 0.8196, 0.81520000000000004, 0.81359999999999999, 0.81920000000000004], [0.78833600000000004, 0.78756400000000004, 0.78342400000000001, 0.78621600000000003, 0.791076])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.88319999999999999,\n",
       "   0.88639999999999997,\n",
       "   0.88,\n",
       "   0.87439999999999996,\n",
       "   0.88680000000000003],\n",
       "  [0.78400800000000004,\n",
       "   0.77524000000000004,\n",
       "   0.76915599999999995,\n",
       "   0.78068000000000004,\n",
       "   0.77854400000000001]),\n",
       " ([0.88319999999999999,\n",
       "   0.88400000000000001,\n",
       "   0.88039999999999996,\n",
       "   0.87160000000000004,\n",
       "   0.88600000000000001],\n",
       "  [0.78554000000000002,\n",
       "   0.77680800000000005,\n",
       "   0.77092799999999995,\n",
       "   0.78194799999999998,\n",
       "   0.780192]),\n",
       " ([0.88160000000000005,\n",
       "   0.88200000000000001,\n",
       "   0.88080000000000003,\n",
       "   0.86960000000000004,\n",
       "   0.88360000000000005],\n",
       "  [0.78958799999999996,\n",
       "   0.78179600000000005,\n",
       "   0.77618799999999999,\n",
       "   0.78548799999999996,\n",
       "   0.78434400000000004]),\n",
       " ([0.87080000000000002,\n",
       "   0.87439999999999996,\n",
       "   0.87119999999999997,\n",
       "   0.85519999999999996,\n",
       "   0.87360000000000004],\n",
       "  [0.79713999999999996,\n",
       "   0.79058799999999996,\n",
       "   0.78390000000000004,\n",
       "   0.78924000000000005,\n",
       "   0.79317199999999999]),\n",
       " ([0.84440000000000004,\n",
       "   0.85040000000000004,\n",
       "   0.8528,\n",
       "   0.83679999999999999,\n",
       "   0.85160000000000002],\n",
       "  [0.79422400000000004,\n",
       "   0.796292,\n",
       "   0.78875600000000001,\n",
       "   0.79015999999999997,\n",
       "   0.79701200000000005]),\n",
       " ([0.8216,\n",
       "   0.8196,\n",
       "   0.81520000000000004,\n",
       "   0.81359999999999999,\n",
       "   0.81920000000000004],\n",
       "  [0.78833600000000004,\n",
       "   0.78756400000000004,\n",
       "   0.78342400000000001,\n",
       "   0.78621600000000003,\n",
       "   0.791076])]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_2500 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(2500, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_2500.append(tmp)\n",
    "\n",
    "accu_2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losgistic Regression(       0/10000): loss= 692.02168650719\n",
      "Losgistic Regression(     100/10000): loss= 529.983357919194\n",
      "Losgistic Regression(     200/10000): loss= 448.861121341205\n",
      "Losgistic Regression(     300/10000): loss= 403.881813624913\n",
      "Losgistic Regression(     400/10000): loss= 377.543317717095\n",
      "Losgistic Regression(     500/10000): loss= 359.029957853866\n",
      "Losgistic Regression(     600/10000): loss= 344.698875916908\n",
      "Losgistic Regression(     700/10000): loss= 333.148166114654\n",
      "Losgistic Regression(     800/10000): loss= 323.582828803302\n",
      "Losgistic Regression(     900/10000): loss= 315.505541016704\n",
      "Losgistic Regression(    1000/10000): loss= 308.571558187071\n",
      "Losgistic Regression(    1100/10000): loss= 302.515268805259\n",
      "Losgistic Regression(    1200/10000): loss= 297.132600376463\n",
      "Losgistic Regression(    1300/10000): loss= 292.275267475195\n",
      "Losgistic Regression(    1400/10000): loss= 287.844959583579\n",
      "Losgistic Regression(    1500/10000): loss= 283.774537227454\n",
      "Losgistic Regression(    1600/10000): loss= 280.006580816188\n",
      "Losgistic Regression(    1700/10000): loss= 275.989327807963\n",
      "Losgistic Regression(    1800/10000): loss= 272.190270099556\n",
      "Losgistic Regression(    1900/10000): loss= 268.621496772294\n",
      "Losgistic Regression(    2000/10000): loss= 265.239433823676\n",
      "Losgistic Regression(    2100/10000): loss= 262.012677244256\n",
      "Losgistic Regression(    2200/10000): loss= 258.915592511772\n",
      "Losgistic Regression(    2300/10000): loss= 255.932411542768\n",
      "Losgistic Regression(    2400/10000): loss= 252.962996027102\n",
      "Losgistic Regression(    2500/10000): loss= 249.644591141931\n",
      "Losgistic Regression(    2600/10000): loss= 246.306020464386\n",
      "Losgistic Regression(    2700/10000): loss= 242.581430653251\n",
      "Losgistic Regression(    2800/10000): loss= 238.702869144437\n",
      "Losgistic Regression(    2900/10000): loss= 234.868692369182\n",
      "Losgistic Regression(    3000/10000): loss= 230.924592714222\n",
      "Losgistic Regression(    3100/10000): loss= 226.36702920405\n",
      "Losgistic Regression(    3200/10000): loss= 221.727199546061\n",
      "Losgistic Regression(    3300/10000): loss= 217.105509752377\n",
      "Losgistic Regression(    3400/10000): loss= 212.489563759754\n",
      "Losgistic Regression(    3500/10000): loss= 207.813650457642\n",
      "Losgistic Regression(    3600/10000): loss= 202.811302513616\n",
      "Losgistic Regression(    3700/10000): loss= 197.511851601435\n",
      "Losgistic Regression(    3800/10000): loss= 192.172114682069\n",
      "Losgistic Regression(    3900/10000): loss= 186.795460499802\n",
      "Losgistic Regression(    4000/10000): loss= 181.391946773122\n",
      "Losgistic Regression(    4100/10000): loss= 175.54059893143\n",
      "Losgistic Regression(    4200/10000): loss= 169.513667881115\n",
      "Losgistic Regression(    4300/10000): loss= 163.455159627514\n",
      "Losgistic Regression(    4400/10000): loss= 157.379861196324\n",
      "Losgistic Regression(    4500/10000): loss= 151.106228565337\n",
      "Losgistic Regression(    4600/10000): loss= 144.75000611563\n",
      "Losgistic Regression(    4700/10000): loss= 138.42062567377\n",
      "Losgistic Regression(    4800/10000): loss= 132.130567465499\n",
      "Losgistic Regression(    4900/10000): loss= 125.879388674214\n",
      "Losgistic Regression(    5000/10000): loss= 119.672038021833\n",
      "Losgistic Regression(    5100/10000): loss= 113.507238120256\n",
      "Losgistic Regression(    5200/10000): loss= 107.374817448803\n",
      "Losgistic Regression(    5300/10000): loss= 101.271251228069\n",
      "Losgistic Regression(    5400/10000): loss= 95.2032607124948\n",
      "Losgistic Regression(    5500/10000): loss= 89.1088436310155\n",
      "Losgistic Regression(    5600/10000): loss= 82.873955794874\n",
      "Losgistic Regression(    5700/10000): loss= 76.5893435160353\n",
      "Losgistic Regression(    5800/10000): loss= 70.1430522795845\n",
      "Losgistic Regression(    5900/10000): loss= 63.7457326088328\n",
      "Losgistic Regression(    6000/10000): loss= 57.4083464110788\n",
      "Losgistic Regression(    6100/10000): loss= 51.1304087661587\n",
      "Losgistic Regression(    6200/10000): loss= 44.9167266590826\n",
      "Losgistic Regression(    6300/10000): loss= 38.7646310913302\n",
      "Losgistic Regression(    6400/10000): loss= 32.672782369601\n",
      "Losgistic Regression(    6500/10000): loss= 26.4052657754953\n",
      "Losgistic Regression(    6600/10000): loss= 20.0308266497493\n",
      "Losgistic Regression(    6700/10000): loss= 13.7030416389331\n",
      "Losgistic Regression(    6800/10000): loss= 7.42029996295908\n",
      "Losgistic Regression(    6900/10000): loss= 1.18210401614384\n",
      "Losgistic Regression(    7000/10000): loss=-5.02578854886449\n",
      "Losgistic Regression(    7100/10000): loss=-11.1918808289585\n",
      "Losgistic Regression(    7200/10000): loss=-17.3337039010693\n",
      "Losgistic Regression(    7300/10000): loss=-23.4523806255212\n",
      "Losgistic Regression(    7400/10000): loss=-29.5517938663714\n",
      "Losgistic Regression(    7500/10000): loss=-35.6552079612132\n",
      "Losgistic Regression(    7600/10000): loss=-41.7655389594433\n",
      "Losgistic Regression(    7700/10000): loss=-47.8885514144733\n",
      "Losgistic Regression(    7800/10000): loss=-54.0259622685329\n",
      "Losgistic Regression(    7900/10000): loss=-60.1633281111363\n",
      "Losgistic Regression(    8000/10000): loss=-66.3061739859283\n",
      "Losgistic Regression(    8100/10000): loss=-72.4424297368805\n",
      "Losgistic Regression(    8200/10000): loss=-78.5714435452636\n",
      "Losgistic Regression(    8300/10000): loss=-84.7573699455431\n",
      "Losgistic Regression(    8400/10000): loss=-91.0069412616562\n",
      "Losgistic Regression(    8500/10000): loss=-97.2510331468751\n",
      "Losgistic Regression(    8600/10000): loss=-103.587924026993\n",
      "Losgistic Regression(    8700/10000): loss=-110.020279277412\n",
      "Losgistic Regression(    8800/10000): loss=-116.563184224181\n",
      "Losgistic Regression(    8900/10000): loss=-123.094694336708\n",
      "Losgistic Regression(    9000/10000): loss=-129.584132401583\n",
      "Losgistic Regression(    9100/10000): loss=-136.012926891661\n",
      "Losgistic Regression(    9200/10000): loss=-142.384678988016\n",
      "Losgistic Regression(    9300/10000): loss=-148.688395230924\n",
      "Losgistic Regression(    9400/10000): loss=-154.916758947018\n",
      "Losgistic Regression(    9500/10000): loss=-161.182768114046\n",
      "Losgistic Regression(    9600/10000): loss=-167.546638698547\n",
      "Losgistic Regression(    9700/10000): loss=-173.844528594598\n",
      "Losgistic Regression(    9800/10000): loss=-180.080098165705\n",
      "Losgistic Regression(    9900/10000): loss=-186.247528817362\n",
      "Time for  0th cross validation = 31.9072s\n",
      "Training Accuracy         =  0.944\n",
      "Cross Validation Accuracy = 0.735756\n",
      "Losgistic Regression(       0/10000): loss= 691.963949447064\n",
      "Losgistic Regression(     100/10000): loss= 517.968550182376\n",
      "Losgistic Regression(     200/10000): loss= 434.923460509036\n",
      "Losgistic Regression(     300/10000): loss= 387.214953061971\n",
      "Losgistic Regression(     400/10000): loss= 359.78430510325\n",
      "Losgistic Regression(     500/10000): loss= 341.053522030901\n",
      "Losgistic Regression(     600/10000): loss= 326.532391243624\n",
      "Losgistic Regression(     700/10000): loss= 314.640416278715\n",
      "Losgistic Regression(     800/10000): loss= 304.549436340835\n",
      "Losgistic Regression(     900/10000): loss= 295.735744260185\n",
      "Losgistic Regression(    1000/10000): loss= 287.82922444707\n",
      "Losgistic Regression(    1100/10000): loss= 279.367095212487\n",
      "Losgistic Regression(    1200/10000): loss= 270.696038198177\n",
      "Losgistic Regression(    1300/10000): loss= 262.762533575219\n",
      "Losgistic Regression(    1400/10000): loss= 255.533091251568\n",
      "Losgistic Regression(    1500/10000): loss= 248.957632383694\n",
      "Losgistic Regression(    1600/10000): loss= 242.946408430221\n",
      "Losgistic Regression(    1700/10000): loss= 237.199503509022\n",
      "Losgistic Regression(    1800/10000): loss= 231.701640487177\n",
      "Losgistic Regression(    1900/10000): loss= 226.416235551338\n",
      "Losgistic Regression(    2000/10000): loss= 221.247822723259\n",
      "Losgistic Regression(    2100/10000): loss= 216.094828876695\n",
      "Losgistic Regression(    2200/10000): loss= 210.54240496219\n",
      "Losgistic Regression(    2300/10000): loss= 205.070072415857\n",
      "Losgistic Regression(    2400/10000): loss= 199.696446373809\n",
      "Losgistic Regression(    2500/10000): loss= 194.436353720054\n",
      "Losgistic Regression(    2600/10000): loss= 189.309132312438\n",
      "Losgistic Regression(    2700/10000): loss= 184.3285630837\n",
      "Losgistic Regression(    2800/10000): loss= 179.505937817651\n",
      "Losgistic Regression(    2900/10000): loss= 174.843150456308\n",
      "Losgistic Regression(    3000/10000): loss= 170.333683622569\n",
      "Losgistic Regression(    3100/10000): loss= 165.561981808247\n",
      "Losgistic Regression(    3200/10000): loss= 160.622956523916\n",
      "Losgistic Regression(    3300/10000): loss= 155.842015820965\n",
      "Losgistic Regression(    3400/10000): loss= 151.215115497186\n",
      "Losgistic Regression(    3500/10000): loss= 146.73809027193\n",
      "Losgistic Regression(    3600/10000): loss= 142.404392975196\n",
      "Losgistic Regression(    3700/10000): loss= 138.208618840657\n",
      "Losgistic Regression(    3800/10000): loss= 134.017074618282\n",
      "Losgistic Regression(    3900/10000): loss= 129.685773674269\n",
      "Losgistic Regression(    4000/10000): loss= 125.458887143141\n",
      "Losgistic Regression(    4100/10000): loss= 121.316827092926\n",
      "Losgistic Regression(    4200/10000): loss= 117.084552548839\n",
      "Losgistic Regression(    4300/10000): loss= 112.774760148916\n",
      "Losgistic Regression(    4400/10000): loss= 108.532109297393\n",
      "Losgistic Regression(    4500/10000): loss= 104.188576933741\n",
      "Losgistic Regression(    4600/10000): loss= 99.3274613685628\n",
      "Losgistic Regression(    4700/10000): loss= 94.5142787749236\n",
      "Losgistic Regression(    4800/10000): loss= 89.7610609733463\n",
      "Losgistic Regression(    4900/10000): loss= 84.6504278508474\n",
      "Losgistic Regression(    5000/10000): loss= 79.573993851862\n",
      "Losgistic Regression(    5100/10000): loss= 74.5677771343127\n",
      "Losgistic Regression(    5200/10000): loss= 69.6454698250519\n",
      "Losgistic Regression(    5300/10000): loss= 64.8052193037033\n",
      "Losgistic Regression(    5400/10000): loss= 60.0411843607452\n",
      "Losgistic Regression(    5500/10000): loss= 55.2676176883622\n",
      "Losgistic Regression(    5600/10000): loss= 50.1734378769566\n",
      "Losgistic Regression(    5700/10000): loss= 45.1130813241505\n",
      "Losgistic Regression(    5800/10000): loss= 40.1206279894415\n",
      "Losgistic Regression(    5900/10000): loss= 35.2021617366469\n",
      "Losgistic Regression(    6000/10000): loss= 30.3153345785718\n",
      "Losgistic Regression(    6100/10000): loss= 25.4635229523115\n",
      "Losgistic Regression(    6200/10000): loss= 20.5668371080279\n",
      "Losgistic Regression(    6300/10000): loss= 15.567148163399\n",
      "Losgistic Regression(    6400/10000): loss= 10.4818893758031\n",
      "Losgistic Regression(    6500/10000): loss= 5.35771340821459\n",
      "Losgistic Regression(    6600/10000): loss= 0.123677031590135\n",
      "Losgistic Regression(    6700/10000): loss=-5.05610304987878\n",
      "Losgistic Regression(    6800/10000): loss=-10.3626562766438\n",
      "Losgistic Regression(    6900/10000): loss=-15.9137502563857\n",
      "Losgistic Regression(    7000/10000): loss=-21.5273276305919\n",
      "Losgistic Regression(    7100/10000): loss=-27.0880290656557\n",
      "Losgistic Regression(    7200/10000): loss=-32.6026185357839\n",
      "Losgistic Regression(    7300/10000): loss=-38.0780474321411\n",
      "Losgistic Regression(    7400/10000): loss=-43.5170668857869\n",
      "Losgistic Regression(    7500/10000): loss=-48.9154416429387\n",
      "Losgistic Regression(    7600/10000): loss=-54.2850754752649\n",
      "Losgistic Regression(    7700/10000): loss=-59.6403852606422\n",
      "Losgistic Regression(    7800/10000): loss=-64.979510003299\n",
      "Losgistic Regression(    7900/10000): loss=-70.2808930005456\n",
      "Losgistic Regression(    8000/10000): loss=-75.6313226398023\n",
      "Losgistic Regression(    8100/10000): loss=-81.0382055310375\n",
      "Losgistic Regression(    8200/10000): loss=-86.6734039041047\n",
      "Losgistic Regression(    8300/10000): loss=-92.3689410322817\n",
      "Losgistic Regression(    8400/10000): loss=-98.038391288342\n",
      "Losgistic Regression(    8500/10000): loss=-103.697631540933\n",
      "Losgistic Regression(    8600/10000): loss=-109.333194212235\n",
      "Losgistic Regression(    8700/10000): loss=-114.949835292882\n",
      "Losgistic Regression(    8800/10000): loss=-120.646278324041\n",
      "Losgistic Regression(    8900/10000): loss=-126.436689937832\n",
      "Losgistic Regression(    9000/10000): loss=-132.268429746651\n",
      "Losgistic Regression(    9100/10000): loss=-138.294412202538\n",
      "Losgistic Regression(    9200/10000): loss=-144.434199527754\n",
      "Losgistic Regression(    9300/10000): loss=-150.552831043578\n",
      "Losgistic Regression(    9400/10000): loss=-156.632479022463\n",
      "Losgistic Regression(    9500/10000): loss=-162.706451790387\n",
      "Losgistic Regression(    9600/10000): loss=-168.760016325898\n",
      "Losgistic Regression(    9700/10000): loss=-174.904605654788\n",
      "Losgistic Regression(    9800/10000): loss=-181.061729235232\n",
      "Losgistic Regression(    9900/10000): loss=-187.198530964329\n",
      "Time for  1th cross validation = 32.1332s\n",
      "Training Accuracy         =  0.948\n",
      "Cross Validation Accuracy = 0.759636\n",
      "Losgistic Regression(       0/10000): loss= 691.568934009718\n",
      "Losgistic Regression(     100/10000): loss= 521.234433411883\n",
      "Losgistic Regression(     200/10000): loss= 452.860911639271\n",
      "Losgistic Regression(     300/10000): loss= 413.415231912147\n",
      "Losgistic Regression(     400/10000): loss= 389.979885183556\n",
      "Losgistic Regression(     500/10000): loss= 373.849853800587\n",
      "Losgistic Regression(     600/10000): loss= 361.701303636928\n",
      "Losgistic Regression(     700/10000): loss= 352.130674344632\n",
      "Losgistic Regression(     800/10000): loss= 344.24174831467\n",
      "Losgistic Regression(     900/10000): loss= 337.401715722092\n",
      "Losgistic Regression(    1000/10000): loss= 331.263440043534\n",
      "Losgistic Regression(    1100/10000): loss= 325.62172724666\n",
      "Losgistic Regression(    1200/10000): loss= 319.160000084442\n",
      "Losgistic Regression(    1300/10000): loss= 312.680608150852\n",
      "Losgistic Regression(    1400/10000): loss= 306.600296246379\n",
      "Losgistic Regression(    1500/10000): loss= 300.862980335006\n",
      "Losgistic Regression(    1600/10000): loss= 295.401079812232\n",
      "Losgistic Regression(    1700/10000): loss= 290.177825819172\n",
      "Losgistic Regression(    1800/10000): loss= 284.495141563317\n",
      "Losgistic Regression(    1900/10000): loss= 278.476966073562\n",
      "Losgistic Regression(    2000/10000): loss= 272.641645361395\n",
      "Losgistic Regression(    2100/10000): loss= 267.072087640659\n",
      "Losgistic Regression(    2200/10000): loss= 261.793578500258\n",
      "Losgistic Regression(    2300/10000): loss= 256.781792580033\n",
      "Losgistic Regression(    2400/10000): loss= 251.962501723746\n",
      "Losgistic Regression(    2500/10000): loss= 247.288057849403\n",
      "Losgistic Regression(    2600/10000): loss= 241.856068761025\n",
      "Losgistic Regression(    2700/10000): loss= 235.984298754182\n",
      "Losgistic Regression(    2800/10000): loss= 230.282645835688\n",
      "Losgistic Regression(    2900/10000): loss= 224.732607159248\n",
      "Losgistic Regression(    3000/10000): loss= 218.296212644074\n",
      "Losgistic Regression(    3100/10000): loss= 211.359813951373\n",
      "Losgistic Regression(    3200/10000): loss= 204.297575994452\n",
      "Losgistic Regression(    3300/10000): loss= 197.351393009539\n",
      "Losgistic Regression(    3400/10000): loss= 190.544121176532\n",
      "Losgistic Regression(    3500/10000): loss= 183.875108812735\n",
      "Losgistic Regression(    3600/10000): loss= 177.358439825072\n",
      "Losgistic Regression(    3700/10000): loss= 171.017216619746\n",
      "Losgistic Regression(    3800/10000): loss= 164.491334987777\n",
      "Losgistic Regression(    3900/10000): loss= 157.993213374824\n",
      "Losgistic Regression(    4000/10000): loss= 151.603257233647\n",
      "Losgistic Regression(    4100/10000): loss= 145.300481067345\n",
      "Losgistic Regression(    4200/10000): loss= 139.077358737372\n",
      "Losgistic Regression(    4300/10000): loss= 132.673341401905\n",
      "Losgistic Regression(    4400/10000): loss= 126.2660967877\n",
      "Losgistic Regression(    4500/10000): loss= 119.893478277165\n",
      "Losgistic Regression(    4600/10000): loss= 113.549514352292\n",
      "Losgistic Regression(    4700/10000): loss= 107.244571502654\n",
      "Losgistic Regression(    4800/10000): loss= 100.968314633681\n",
      "Losgistic Regression(    4900/10000): loss= 94.7417614649726\n",
      "Losgistic Regression(    5000/10000): loss= 88.5500971095107\n",
      "Losgistic Regression(    5100/10000): loss= 82.2725448472615\n",
      "Losgistic Regression(    5200/10000): loss= 75.8189264139949\n",
      "Losgistic Regression(    5300/10000): loss= 69.4013394483292\n",
      "Losgistic Regression(    5400/10000): loss= 63.0352154689487\n",
      "Losgistic Regression(    5500/10000): loss= 56.6889964674364\n",
      "Losgistic Regression(    5600/10000): loss= 50.3704678466266\n",
      "Losgistic Regression(    5700/10000): loss= 44.0509306561198\n",
      "Losgistic Regression(    5800/10000): loss= 37.4237509674125\n",
      "Losgistic Regression(    5900/10000): loss= 30.834842680568\n",
      "Losgistic Regression(    6000/10000): loss= 24.2601082865889\n",
      "Losgistic Regression(    6100/10000): loss= 17.4201704583974\n",
      "Losgistic Regression(    6200/10000): loss= 10.2751553368905\n",
      "Losgistic Regression(    6300/10000): loss= 2.93411968120974\n",
      "Losgistic Regression(    6400/10000): loss=-4.40039268466127\n",
      "Losgistic Regression(    6500/10000): loss=-11.732086421107\n",
      "Losgistic Regression(    6600/10000): loss=-19.0366276710665\n",
      "Losgistic Regression(    6700/10000): loss=-26.3210486768152\n",
      "Losgistic Regression(    6800/10000): loss=-33.5783992071566\n",
      "Losgistic Regression(    6900/10000): loss=-40.8118638675456\n",
      "Losgistic Regression(    7000/10000): loss=-47.9997236129373\n",
      "Losgistic Regression(    7100/10000): loss=-55.1327660753311\n",
      "Losgistic Regression(    7200/10000): loss=-62.2205965571818\n",
      "Losgistic Regression(    7300/10000): loss=-69.2517158382717\n",
      "Losgistic Regression(    7400/10000): loss=-76.4047196179433\n",
      "Losgistic Regression(    7500/10000): loss=-83.6626895445614\n",
      "Losgistic Regression(    7600/10000): loss=-91.2892365587232\n",
      "Losgistic Regression(    7700/10000): loss=-98.8103431102695\n",
      "Losgistic Regression(    7800/10000): loss=-106.222701041419\n",
      "Losgistic Regression(    7900/10000): loss=-113.527682655896\n",
      "Losgistic Regression(    8000/10000): loss=-120.73781408205\n",
      "Losgistic Regression(    8100/10000): loss=-127.863710097674\n",
      "Losgistic Regression(    8200/10000): loss=-134.897837537204\n",
      "Losgistic Regression(    8300/10000): loss=-141.843033975683\n",
      "Losgistic Regression(    8400/10000): loss=-148.697445820218\n",
      "Losgistic Regression(    8500/10000): loss=-155.465992039125\n",
      "Losgistic Regression(    8600/10000): loss=-162.144743607242\n",
      "Losgistic Regression(    8700/10000): loss=-168.747108814487\n",
      "Losgistic Regression(    8800/10000): loss=-175.392849867172\n",
      "Losgistic Regression(    8900/10000): loss=-181.968330829302\n",
      "Losgistic Regression(    9000/10000): loss=-188.463580480216\n",
      "Losgistic Regression(    9100/10000): loss=-194.974665493555\n",
      "Losgistic Regression(    9200/10000): loss=-201.474588427671\n",
      "Losgistic Regression(    9300/10000): loss=-207.850814446346\n",
      "Losgistic Regression(    9400/10000): loss=-214.099542480876\n",
      "Losgistic Regression(    9500/10000): loss=-220.409860289028\n",
      "Losgistic Regression(    9600/10000): loss=-226.627850708227\n",
      "Losgistic Regression(    9700/10000): loss=-232.741214680408\n",
      "Losgistic Regression(    9800/10000): loss=-238.752644160606\n",
      "Losgistic Regression(    9900/10000): loss=-244.856219231888\n",
      "Time for  2th cross validation = 30.8741s\n",
      "Training Accuracy         =  0.931\n",
      "Cross Validation Accuracy = 0.737372\n",
      "Losgistic Regression(       0/10000): loss= 691.706602046285\n",
      "Losgistic Regression(     100/10000): loss= 511.989101618138\n",
      "Losgistic Regression(     200/10000): loss= 435.664603756968\n",
      "Losgistic Regression(     300/10000): loss= 392.564509424881\n",
      "Losgistic Regression(     400/10000): loss= 367.366572253557\n",
      "Losgistic Regression(     500/10000): loss= 349.418285911519\n",
      "Losgistic Regression(     600/10000): loss= 335.299997305472\n",
      "Losgistic Regression(     700/10000): loss= 322.962956859441\n",
      "Losgistic Regression(     800/10000): loss= 311.521044554984\n",
      "Losgistic Regression(     900/10000): loss= 301.385324801036\n",
      "Losgistic Regression(    1000/10000): loss= 292.203250227493\n",
      "Losgistic Regression(    1100/10000): loss= 283.773640009822\n",
      "Losgistic Regression(    1200/10000): loss= 275.407968104876\n",
      "Losgistic Regression(    1300/10000): loss= 266.447105744959\n",
      "Losgistic Regression(    1400/10000): loss= 257.91022515175\n",
      "Losgistic Regression(    1500/10000): loss= 249.777586387321\n",
      "Losgistic Regression(    1600/10000): loss= 242.039471607936\n",
      "Losgistic Regression(    1700/10000): loss= 234.06351987938\n",
      "Losgistic Regression(    1800/10000): loss= 226.099423405021\n",
      "Losgistic Regression(    1900/10000): loss= 217.685366318731\n",
      "Losgistic Regression(    2000/10000): loss= 209.001477663121\n",
      "Losgistic Regression(    2100/10000): loss= 199.5921308723\n",
      "Losgistic Regression(    2200/10000): loss= 189.778314598906\n",
      "Losgistic Regression(    2300/10000): loss= 180.221603539512\n",
      "Losgistic Regression(    2400/10000): loss= 170.069118437094\n",
      "Losgistic Regression(    2500/10000): loss= 160.174328381865\n",
      "Losgistic Regression(    2600/10000): loss= 150.55518678563\n",
      "Losgistic Regression(    2700/10000): loss= 141.211084357837\n",
      "Losgistic Regression(    2800/10000): loss= 132.139132433744\n",
      "Losgistic Regression(    2900/10000): loss= 123.032457867158\n",
      "Losgistic Regression(    3000/10000): loss= 113.904104066407\n",
      "Losgistic Regression(    3100/10000): loss= 104.546561291968\n",
      "Losgistic Regression(    3200/10000): loss= 95.3378497419556\n",
      "Losgistic Regression(    3300/10000): loss= 86.0815186405069\n",
      "Losgistic Regression(    3400/10000): loss= 77.025373670946\n",
      "Losgistic Regression(    3500/10000): loss= 68.15998310212\n",
      "Losgistic Regression(    3600/10000): loss= 58.994562838851\n",
      "Losgistic Regression(    3700/10000): loss= 49.9104625053748\n",
      "Losgistic Regression(    3800/10000): loss= 40.9295791756937\n",
      "Losgistic Regression(    3900/10000): loss= 32.0294511475104\n",
      "Losgistic Regression(    4000/10000): loss= 23.2042408946317\n",
      "Losgistic Regression(    4100/10000): loss= 14.4540457558822\n",
      "Losgistic Regression(    4200/10000): loss= 5.72978141188857\n",
      "Losgistic Regression(    4300/10000): loss=-3.13729168771041\n",
      "Losgistic Regression(    4400/10000): loss=-11.9101618388579\n",
      "Losgistic Regression(    4500/10000): loss=-20.5784965855077\n",
      "Losgistic Regression(    4600/10000): loss=-29.1864201072928\n",
      "Losgistic Regression(    4700/10000): loss=-37.8209812536444\n",
      "Losgistic Regression(    4800/10000): loss=-46.3768871790296\n",
      "Losgistic Regression(    4900/10000): loss=-55.1867225158961\n",
      "Losgistic Regression(    5000/10000): loss=-64.6100004770298\n",
      "Losgistic Regression(    5100/10000): loss=-74.0754436025194\n",
      "Losgistic Regression(    5200/10000): loss=-83.5291995515956\n",
      "Losgistic Regression(    5300/10000): loss=-93.1546915795169\n",
      "Losgistic Regression(    5400/10000): loss=-102.950407610099\n",
      "Losgistic Regression(    5500/10000): loss=-112.80602792232\n",
      "Losgistic Regression(    5600/10000): loss=-122.779695387036\n",
      "Losgistic Regression(    5700/10000): loss=-133.103839841931\n",
      "Losgistic Regression(    5800/10000): loss=-143.635264611982\n",
      "Losgistic Regression(    5900/10000): loss=-154.276475667387\n",
      "Losgistic Regression(    6000/10000): loss=-165.495694861715\n",
      "Losgistic Regression(    6100/10000): loss=-177.328597577286\n",
      "Losgistic Regression(    6200/10000): loss=-189.182948083955\n",
      "Losgistic Regression(    6300/10000): loss=-201.057522977245\n",
      "Losgistic Regression(    6400/10000): loss=-212.95343126354\n",
      "Losgistic Regression(    6500/10000): loss=-224.829170110516\n",
      "Losgistic Regression(    6600/10000): loss=-236.681639993827\n",
      "Losgistic Regression(    6700/10000): loss=-248.502521594735\n",
      "Losgistic Regression(    6800/10000): loss=-260.315091338845\n",
      "Losgistic Regression(    6900/10000): loss=-272.546940274116\n",
      "Losgistic Regression(    7000/10000): loss=-284.876277526292\n",
      "Losgistic Regression(    7100/10000): loss=-297.150727717941\n",
      "Losgistic Regression(    7200/10000): loss=-309.365949575243\n",
      "Losgistic Regression(    7300/10000): loss=-321.520110628482\n",
      "Losgistic Regression(    7400/10000): loss=-333.605927111525\n",
      "Losgistic Regression(    7500/10000): loss=-345.641707069602\n",
      "Losgistic Regression(    7600/10000): loss=-357.617362946598\n",
      "Losgistic Regression(    7700/10000): loss=-369.578324380271\n",
      "Losgistic Regression(    7800/10000): loss=-381.606101702755\n",
      "Losgistic Regression(    7900/10000): loss=-393.619486455251\n",
      "Losgistic Regression(    8000/10000): loss=-405.775732972169\n",
      "Losgistic Regression(    8100/10000): loss=-417.94576656274\n",
      "Losgistic Regression(    8200/10000): loss=-430.246564310129\n",
      "Losgistic Regression(    8300/10000): loss=-442.499733988645\n",
      "Losgistic Regression(    8400/10000): loss=-454.70519892623\n",
      "Losgistic Regression(    8500/10000): loss=-466.842153545102\n",
      "Losgistic Regression(    8600/10000): loss=-479.392670416173\n",
      "Losgistic Regression(    8700/10000): loss=-492.075032002005\n",
      "Losgistic Regression(    8800/10000): loss=-504.673474305194\n",
      "Losgistic Regression(    8900/10000): loss=-517.349795051556\n",
      "Losgistic Regression(    9000/10000): loss=-530.01134306882\n",
      "Losgistic Regression(    9100/10000): loss=-542.584176050878\n",
      "Losgistic Regression(    9200/10000): loss=-555.100070994685\n",
      "Losgistic Regression(    9300/10000): loss=-567.758210788586\n",
      "Losgistic Regression(    9400/10000): loss=-580.481819469542\n",
      "Losgistic Regression(    9500/10000): loss=-593.179432290626\n",
      "Losgistic Regression(    9600/10000): loss=-605.946879515617\n",
      "Losgistic Regression(    9700/10000): loss=-618.714380389586\n",
      "Losgistic Regression(    9800/10000): loss=-631.398970864512\n",
      "Losgistic Regression(    9900/10000): loss=-643.965893957604\n",
      "Time for  3th cross validation = 31.3074s\n",
      "Training Accuracy         =  0.942\n",
      "Cross Validation Accuracy = 0.737844\n",
      "Losgistic Regression(       0/10000): loss= 692.020799933278\n",
      "Losgistic Regression(     100/10000): loss= 517.759436318264\n",
      "Losgistic Regression(     200/10000): loss= 441.623575780799\n",
      "Losgistic Regression(     300/10000): loss= 399.307506555391\n",
      "Losgistic Regression(     400/10000): loss= 373.825268120459\n",
      "Losgistic Regression(     500/10000): loss= 355.773160833067\n",
      "Losgistic Regression(     600/10000): loss= 341.499587546336\n",
      "Losgistic Regression(     700/10000): loss= 329.500248684929\n",
      "Losgistic Regression(     800/10000): loss= 319.112279083389\n",
      "Losgistic Regression(     900/10000): loss= 309.93773920608\n",
      "Losgistic Regression(    1000/10000): loss= 301.708004128044\n",
      "Losgistic Regression(    1100/10000): loss= 294.26032639591\n",
      "Losgistic Regression(    1200/10000): loss= 287.476514240814\n",
      "Losgistic Regression(    1300/10000): loss= 281.264004504301\n",
      "Losgistic Regression(    1400/10000): loss= 275.548104219973\n",
      "Losgistic Regression(    1500/10000): loss= 270.265429832213\n",
      "Losgistic Regression(    1600/10000): loss= 265.35870649676\n",
      "Losgistic Regression(    1700/10000): loss= 260.782534296528\n",
      "Losgistic Regression(    1800/10000): loss= 255.698391269885\n",
      "Losgistic Regression(    1900/10000): loss= 250.51922674735\n",
      "Losgistic Regression(    2000/10000): loss= 245.565367221779\n",
      "Losgistic Regression(    2100/10000): loss= 240.816156149595\n",
      "Losgistic Regression(    2200/10000): loss= 236.25334282288\n",
      "Losgistic Regression(    2300/10000): loss= 231.864530960423\n",
      "Losgistic Regression(    2400/10000): loss= 227.6402900715\n",
      "Losgistic Regression(    2500/10000): loss= 223.567473003971\n",
      "Losgistic Regression(    2600/10000): loss= 219.63637227722\n",
      "Losgistic Regression(    2700/10000): loss= 215.836874822491\n",
      "Losgistic Regression(    2800/10000): loss= 212.155632313583\n",
      "Losgistic Regression(    2900/10000): loss= 208.081529172718\n",
      "Losgistic Regression(    3000/10000): loss= 203.848478104658\n",
      "Losgistic Regression(    3100/10000): loss= 199.712011408509\n",
      "Losgistic Regression(    3200/10000): loss= 195.672284382328\n",
      "Losgistic Regression(    3300/10000): loss= 191.731825372637\n",
      "Losgistic Regression(    3400/10000): loss= 187.730275397642\n",
      "Losgistic Regression(    3500/10000): loss= 183.411731682293\n",
      "Losgistic Regression(    3600/10000): loss= 179.086105783925\n",
      "Losgistic Regression(    3700/10000): loss= 174.490056067193\n",
      "Losgistic Regression(    3800/10000): loss= 170.034108622372\n",
      "Losgistic Regression(    3900/10000): loss= 165.463037896993\n",
      "Losgistic Regression(    4000/10000): loss= 160.729895854418\n",
      "Losgistic Regression(    4100/10000): loss= 155.807249918938\n",
      "Losgistic Regression(    4200/10000): loss= 151.030679763374\n",
      "Losgistic Regression(    4300/10000): loss= 146.395013475083\n",
      "Losgistic Regression(    4400/10000): loss= 141.554261282597\n",
      "Losgistic Regression(    4500/10000): loss= 136.78442325225\n",
      "Losgistic Regression(    4600/10000): loss= 132.127887172686\n",
      "Losgistic Regression(    4700/10000): loss= 127.283884490667\n",
      "Losgistic Regression(    4800/10000): loss= 122.322012896315\n",
      "Losgistic Regression(    4900/10000): loss= 117.422163530935\n",
      "Losgistic Regression(    5000/10000): loss= 112.618372722061\n",
      "Losgistic Regression(    5100/10000): loss= 107.900096319167\n",
      "Losgistic Regression(    5200/10000): loss= 103.272703953189\n",
      "Losgistic Regression(    5300/10000): loss= 98.718211896044\n",
      "Losgistic Regression(    5400/10000): loss= 94.2456691651739\n",
      "Losgistic Regression(    5500/10000): loss= 89.8489984072989\n",
      "Losgistic Regression(    5600/10000): loss= 85.5226332593926\n",
      "Losgistic Regression(    5700/10000): loss= 81.2604943761055\n",
      "Losgistic Regression(    5800/10000): loss= 77.0663459750414\n",
      "Losgistic Regression(    5900/10000): loss= 72.8452136898326\n",
      "Losgistic Regression(    6000/10000): loss= 68.5200723492377\n",
      "Losgistic Regression(    6100/10000): loss= 63.9497032868658\n",
      "Losgistic Regression(    6200/10000): loss= 59.4349479912077\n",
      "Losgistic Regression(    6300/10000): loss= 54.9717032706017\n",
      "Losgistic Regression(    6400/10000): loss= 50.553247476818\n",
      "Losgistic Regression(    6500/10000): loss= 46.1617818506773\n",
      "Losgistic Regression(    6600/10000): loss= 41.6697063828061\n",
      "Losgistic Regression(    6700/10000): loss= 37.1501341144839\n",
      "Losgistic Regression(    6800/10000): loss= 32.518108088059\n",
      "Losgistic Regression(    6900/10000): loss= 27.9118437967829\n",
      "Losgistic Regression(    7000/10000): loss= 23.3143967884694\n",
      "Losgistic Regression(    7100/10000): loss= 18.7387733431529\n",
      "Losgistic Regression(    7200/10000): loss= 14.1775389210089\n",
      "Losgistic Regression(    7300/10000): loss= 9.61013778588574\n",
      "Losgistic Regression(    7400/10000): loss= 5.04263557836655\n",
      "Losgistic Regression(    7500/10000): loss= 0.477912912925468\n",
      "Losgistic Regression(    7600/10000): loss=-4.08533846676874\n",
      "Losgistic Regression(    7700/10000): loss=-8.63923562410226\n",
      "Losgistic Regression(    7800/10000): loss=-13.1822208372438\n",
      "Losgistic Regression(    7900/10000): loss=-17.720340816035\n",
      "Losgistic Regression(    8000/10000): loss=-22.4409610909979\n",
      "Losgistic Regression(    8100/10000): loss=-27.1416248585494\n",
      "Losgistic Regression(    8200/10000): loss=-31.8189864273447\n",
      "Losgistic Regression(    8300/10000): loss=-36.4778236575031\n",
      "Losgistic Regression(    8400/10000): loss=-41.1191903916971\n",
      "Losgistic Regression(    8500/10000): loss=-45.7313134077689\n",
      "Losgistic Regression(    8600/10000): loss=-50.3280348501112\n",
      "Losgistic Regression(    8700/10000): loss=-54.9182688556143\n",
      "Losgistic Regression(    8800/10000): loss=-59.4913670742085\n",
      "Losgistic Regression(    8900/10000): loss=-64.140508283878\n",
      "Losgistic Regression(    9000/10000): loss=-68.8492483544749\n",
      "Losgistic Regression(    9100/10000): loss=-73.5486708403506\n",
      "Losgistic Regression(    9200/10000): loss=-78.2443379492713\n",
      "Losgistic Regression(    9300/10000): loss=-82.9392222354206\n",
      "Losgistic Regression(    9400/10000): loss=-87.6229434900284\n",
      "Losgistic Regression(    9500/10000): loss=-92.3139732823086\n",
      "Losgistic Regression(    9600/10000): loss=-97.004388452147\n",
      "Losgistic Regression(    9700/10000): loss=-101.688271868915\n",
      "Losgistic Regression(    9800/10000): loss=-106.376570590354\n",
      "Losgistic Regression(    9900/10000): loss=-111.063812288959\n",
      "Time for  4th cross validation = 31.0481s\n",
      "Training Accuracy         =  0.946\n",
      "Cross Validation Accuracy = 0.739404\n",
      "*************** ([0.94399999999999995, 0.94799999999999995, 0.93100000000000005, 0.94199999999999995, 0.94599999999999995], [0.73575599999999997, 0.75963599999999998, 0.73737200000000003, 0.73784400000000006, 0.73940399999999995])\n",
      "Losgistic Regression(       0/10000): loss= 692.022322883668\n",
      "Losgistic Regression(     100/10000): loss= 530.244082890699\n",
      "Losgistic Regression(     200/10000): loss= 449.459374407816\n",
      "Losgistic Regression(     300/10000): loss= 404.80551367435\n",
      "Losgistic Regression(     400/10000): loss= 378.77349817842\n",
      "Losgistic Regression(     500/10000): loss= 360.557728990852\n",
      "Losgistic Regression(     600/10000): loss= 346.518519028887\n",
      "Losgistic Regression(     700/10000): loss= 335.251161170844\n",
      "Losgistic Regression(     800/10000): loss= 325.960912783976\n",
      "Losgistic Regression(     900/10000): loss= 318.153128502817\n",
      "Losgistic Regression(    1000/10000): loss= 311.480109734271\n",
      "Losgistic Regression(    1100/10000): loss= 305.675622954445\n",
      "Losgistic Regression(    1200/10000): loss= 300.54030415154\n",
      "Losgistic Regression(    1300/10000): loss= 295.924042109435\n",
      "Losgistic Regression(    1400/10000): loss= 291.73453824578\n",
      "Losgistic Regression(    1500/10000): loss= 287.906262638902\n",
      "Losgistic Regression(    1600/10000): loss= 284.378632970203\n",
      "Losgistic Regression(    1700/10000): loss= 280.984090180399\n",
      "Losgistic Regression(    1800/10000): loss= 277.445574650485\n",
      "Losgistic Regression(    1900/10000): loss= 274.130950041555\n",
      "Losgistic Regression(    2000/10000): loss= 271.000413854891\n",
      "Losgistic Regression(    2100/10000): loss= 268.0202737057\n",
      "Losgistic Regression(    2200/10000): loss= 265.176479053767\n",
      "Losgistic Regression(    2300/10000): loss= 262.443350275484\n",
      "Losgistic Regression(    2400/10000): loss= 259.815747591176\n",
      "Losgistic Regression(    2500/10000): loss= 257.06130955293\n",
      "Losgistic Regression(    2600/10000): loss= 254.073646427782\n",
      "Losgistic Regression(    2700/10000): loss= 251.134255767641\n",
      "Losgistic Regression(    2800/10000): loss= 248.196409082953\n",
      "Losgistic Regression(    2900/10000): loss= 244.944680943353\n",
      "Losgistic Regression(    3000/10000): loss= 241.564149342176\n",
      "Losgistic Regression(    3100/10000): loss= 238.209245275716\n",
      "Losgistic Regression(    3200/10000): loss= 234.485696094175\n",
      "Losgistic Regression(    3300/10000): loss= 230.48316988516\n",
      "Losgistic Regression(    3400/10000): loss= 226.418698868813\n",
      "Losgistic Regression(    3500/10000): loss= 222.325122344581\n",
      "Losgistic Regression(    3600/10000): loss= 218.216799448239\n",
      "Losgistic Regression(    3700/10000): loss= 214.067072957759\n",
      "Losgistic Regression(    3800/10000): loss= 209.598274529081\n",
      "Losgistic Regression(    3900/10000): loss= 204.875520454147\n",
      "Losgistic Regression(    4000/10000): loss= 200.098341197251\n",
      "Losgistic Regression(    4100/10000): loss= 195.312343746621\n",
      "Losgistic Regression(    4200/10000): loss= 190.524155134631\n",
      "Losgistic Regression(    4300/10000): loss= 185.190001104465\n",
      "Losgistic Regression(    4400/10000): loss= 179.825582412921\n",
      "Losgistic Regression(    4500/10000): loss= 174.493894788774\n",
      "Losgistic Regression(    4600/10000): loss= 169.169377202414\n",
      "Losgistic Regression(    4700/10000): loss= 163.820035837116\n",
      "Losgistic Regression(    4800/10000): loss= 158.324307514284\n",
      "Losgistic Regression(    4900/10000): loss= 152.903269456376\n",
      "Losgistic Regression(    5000/10000): loss= 147.48275796013\n",
      "Losgistic Regression(    5100/10000): loss= 142.122784178579\n",
      "Losgistic Regression(    5200/10000): loss= 136.807151973802\n",
      "Losgistic Regression(    5300/10000): loss= 131.525924616948\n",
      "Losgistic Regression(    5400/10000): loss= 126.307272400329\n",
      "Losgistic Regression(    5500/10000): loss= 121.133144756459\n",
      "Losgistic Regression(    5600/10000): loss= 116.035343117829\n",
      "Losgistic Regression(    5700/10000): loss= 110.98143521004\n",
      "Losgistic Regression(    5800/10000): loss= 105.974453775793\n",
      "Losgistic Regression(    5900/10000): loss= 101.020225520188\n",
      "Losgistic Regression(    6000/10000): loss= 96.0699590720559\n",
      "Losgistic Regression(    6100/10000): loss= 91.0812180717309\n",
      "Losgistic Regression(    6200/10000): loss= 86.1988764901067\n",
      "Losgistic Regression(    6300/10000): loss= 81.253898973415\n",
      "Losgistic Regression(    6400/10000): loss= 76.2925756289342\n",
      "Losgistic Regression(    6500/10000): loss= 71.4594295874204\n",
      "Losgistic Regression(    6600/10000): loss= 66.7164513339284\n",
      "Losgistic Regression(    6700/10000): loss= 62.003643159503\n",
      "Losgistic Regression(    6800/10000): loss= 57.118645988151\n",
      "Losgistic Regression(    6900/10000): loss= 52.1714657745526\n",
      "Losgistic Regression(    7000/10000): loss= 47.2570627112097\n",
      "Losgistic Regression(    7100/10000): loss= 42.3987244083946\n",
      "Losgistic Regression(    7200/10000): loss= 37.5072591563649\n",
      "Losgistic Regression(    7300/10000): loss= 32.6423773999487\n",
      "Losgistic Regression(    7400/10000): loss= 27.7642171967972\n",
      "Losgistic Regression(    7500/10000): loss= 22.8720427498964\n",
      "Losgistic Regression(    7600/10000): loss= 18.054654996706\n",
      "Losgistic Regression(    7700/10000): loss= 13.2269448327886\n",
      "Losgistic Regression(    7800/10000): loss= 8.35370474154358\n",
      "Losgistic Regression(    7900/10000): loss= 3.45746390525563\n",
      "Losgistic Regression(    8000/10000): loss=-1.40244199665175\n",
      "Losgistic Regression(    8100/10000): loss=-6.26876430738498\n",
      "Losgistic Regression(    8200/10000): loss=-11.1007947874876\n",
      "Losgistic Regression(    8300/10000): loss=-15.874536366149\n",
      "Losgistic Regression(    8400/10000): loss=-20.6590281184192\n",
      "Losgistic Regression(    8500/10000): loss=-25.4342704043166\n",
      "Losgistic Regression(    8600/10000): loss=-30.1339165376555\n",
      "Losgistic Regression(    8700/10000): loss=-34.8423701322149\n",
      "Losgistic Regression(    8800/10000): loss=-39.6032274463085\n",
      "Losgistic Regression(    8900/10000): loss=-44.2997777577886\n",
      "Losgistic Regression(    9000/10000): loss=-48.9871125238007\n",
      "Losgistic Regression(    9100/10000): loss=-53.6668599537307\n",
      "Losgistic Regression(    9200/10000): loss=-58.3137916462444\n",
      "Losgistic Regression(    9300/10000): loss=-62.9039338978606\n",
      "Losgistic Regression(    9400/10000): loss=-67.3810302281422\n",
      "Losgistic Regression(    9500/10000): loss=-71.7869550619289\n",
      "Losgistic Regression(    9600/10000): loss=-76.165425626358\n",
      "Losgistic Regression(    9700/10000): loss=-80.6243896129785\n",
      "Losgistic Regression(    9800/10000): loss=-85.0888184555669\n",
      "Losgistic Regression(    9900/10000): loss=-89.5052503631128\n",
      "Time for  0th cross validation = 30.6125s\n",
      "Training Accuracy         =   0.94\n",
      "Cross Validation Accuracy = 0.738636\n",
      "Losgistic Regression(       0/10000): loss= 691.964598136934\n",
      "Losgistic Regression(     100/10000): loss= 518.245340473712\n",
      "Losgistic Regression(     200/10000): loss= 435.517402640865\n",
      "Losgistic Regression(     300/10000): loss= 388.13640551179\n",
      "Losgistic Regression(     400/10000): loss= 361.039361726647\n",
      "Losgistic Regression(     500/10000): loss= 342.64327530631\n",
      "Losgistic Regression(     600/10000): loss= 328.447977723925\n",
      "Losgistic Regression(     700/10000): loss= 316.862446694558\n",
      "Losgistic Regression(     800/10000): loss= 307.067301372105\n",
      "Losgistic Regression(     900/10000): loss= 298.545771101284\n",
      "Losgistic Regression(    1000/10000): loss= 290.998657691756\n",
      "Losgistic Regression(    1100/10000): loss= 283.299815614631\n",
      "Losgistic Regression(    1200/10000): loss= 275.017211030087\n",
      "Losgistic Regression(    1300/10000): loss= 267.45456774065\n",
      "Losgistic Regression(    1400/10000): loss= 260.597704437285\n",
      "Losgistic Regression(    1500/10000): loss= 254.400041657419\n",
      "Losgistic Regression(    1600/10000): loss= 248.758562464851\n",
      "Losgistic Regression(    1700/10000): loss= 243.566064520788\n",
      "Losgistic Regression(    1800/10000): loss= 238.698551679347\n",
      "Losgistic Regression(    1900/10000): loss= 233.857768854192\n",
      "Losgistic Regression(    2000/10000): loss= 229.060401587265\n",
      "Losgistic Regression(    2100/10000): loss= 224.338467955938\n",
      "Losgistic Regression(    2200/10000): loss= 219.50168231801\n",
      "Losgistic Regression(    2300/10000): loss= 214.444028269925\n",
      "Losgistic Regression(    2400/10000): loss= 209.491853148414\n",
      "Losgistic Regression(    2500/10000): loss= 204.660880335323\n",
      "Losgistic Regression(    2600/10000): loss= 199.98547966336\n",
      "Losgistic Regression(    2700/10000): loss= 195.461976278976\n",
      "Losgistic Regression(    2800/10000): loss= 191.104740185472\n",
      "Losgistic Regression(    2900/10000): loss= 186.914588620201\n",
      "Losgistic Regression(    3000/10000): loss= 182.871069817364\n",
      "Losgistic Regression(    3100/10000): loss= 178.970817671317\n",
      "Losgistic Regression(    3200/10000): loss= 174.567261384779\n",
      "Losgistic Regression(    3300/10000): loss= 170.264617304623\n",
      "Losgistic Regression(    3400/10000): loss= 166.114739341431\n",
      "Losgistic Regression(    3500/10000): loss= 162.144672707693\n",
      "Losgistic Regression(    3600/10000): loss= 158.334880116749\n",
      "Losgistic Regression(    3700/10000): loss= 154.675221666699\n",
      "Losgistic Regression(    3800/10000): loss= 151.149453912084\n",
      "Losgistic Regression(    3900/10000): loss= 147.73533931104\n",
      "Losgistic Regression(    4000/10000): loss= 144.418204334144\n",
      "Losgistic Regression(    4100/10000): loss= 141.085514175227\n",
      "Losgistic Regression(    4200/10000): loss= 137.610513894557\n",
      "Losgistic Regression(    4300/10000): loss= 134.21081030577\n",
      "Losgistic Regression(    4400/10000): loss= 130.904821495773\n",
      "Losgistic Regression(    4500/10000): loss= 127.664274047931\n",
      "Losgistic Regression(    4600/10000): loss= 124.406548746143\n",
      "Losgistic Regression(    4700/10000): loss= 120.918672994695\n",
      "Losgistic Regression(    4800/10000): loss= 117.128739823254\n",
      "Losgistic Regression(    4900/10000): loss= 113.387903977786\n",
      "Losgistic Regression(    5000/10000): loss= 109.713792783458\n",
      "Losgistic Regression(    5100/10000): loss= 106.114747990836\n",
      "Losgistic Regression(    5200/10000): loss= 102.546598061021\n",
      "Losgistic Regression(    5300/10000): loss= 98.6792791179203\n",
      "Losgistic Regression(    5400/10000): loss= 94.8346621338613\n",
      "Losgistic Regression(    5500/10000): loss= 90.9454925409407\n",
      "Losgistic Regression(    5600/10000): loss= 87.0980943467336\n",
      "Losgistic Regression(    5700/10000): loss= 83.3205265445785\n",
      "Losgistic Regression(    5800/10000): loss= 79.601216284455\n",
      "Losgistic Regression(    5900/10000): loss= 76.0261607438079\n",
      "Losgistic Regression(    6000/10000): loss= 72.1648828160051\n",
      "Losgistic Regression(    6100/10000): loss= 68.2847371138966\n",
      "Losgistic Regression(    6200/10000): loss= 64.5449199319426\n",
      "Losgistic Regression(    6300/10000): loss= 60.8660600605369\n",
      "Losgistic Regression(    6400/10000): loss= 57.2758296738814\n",
      "Losgistic Regression(    6500/10000): loss= 53.7293775760489\n",
      "Losgistic Regression(    6600/10000): loss= 50.1998318179031\n",
      "Losgistic Regression(    6700/10000): loss= 46.7426253739362\n",
      "Losgistic Regression(    6800/10000): loss= 43.2765146669278\n",
      "Losgistic Regression(    6900/10000): loss= 39.6625811374334\n",
      "Losgistic Regression(    7000/10000): loss= 36.0907511413789\n",
      "Losgistic Regression(    7100/10000): loss= 32.4883837542602\n",
      "Losgistic Regression(    7200/10000): loss= 28.702408262412\n",
      "Losgistic Regression(    7300/10000): loss= 24.9086587945238\n",
      "Losgistic Regression(    7400/10000): loss= 21.0602922772587\n",
      "Losgistic Regression(    7500/10000): loss= 17.1032302555044\n",
      "Losgistic Regression(    7600/10000): loss= 12.9786075064751\n",
      "Losgistic Regression(    7700/10000): loss= 8.90769694202789\n",
      "Losgistic Regression(    7800/10000): loss= 4.78482395995144\n",
      "Losgistic Regression(    7900/10000): loss= 0.635051223667173\n",
      "Losgistic Regression(    8000/10000): loss=-3.5228408454123\n",
      "Losgistic Regression(    8100/10000): loss=-7.69827396966333\n",
      "Losgistic Regression(    8200/10000): loss=-11.7203555774572\n",
      "Losgistic Regression(    8300/10000): loss=-15.6539297410146\n",
      "Losgistic Regression(    8400/10000): loss=-19.5970772395394\n",
      "Losgistic Regression(    8500/10000): loss=-23.4153946350108\n",
      "Losgistic Regression(    8600/10000): loss=-27.2765784208327\n",
      "Losgistic Regression(    8700/10000): loss=-31.2026781946174\n",
      "Losgistic Regression(    8800/10000): loss=-35.0790594218997\n",
      "Losgistic Regression(    8900/10000): loss=-39.0452473170501\n",
      "Losgistic Regression(    9000/10000): loss=-43.1487706006102\n",
      "Losgistic Regression(    9100/10000): loss=-47.1564821284559\n",
      "Losgistic Regression(    9200/10000): loss=-51.0805709328608\n",
      "Losgistic Regression(    9300/10000): loss=-54.8762764227989\n",
      "Losgistic Regression(    9400/10000): loss=-58.5889230848284\n",
      "Losgistic Regression(    9500/10000): loss=-62.2510480792276\n",
      "Losgistic Regression(    9600/10000): loss=-65.8638143149373\n",
      "Losgistic Regression(    9700/10000): loss=-69.4669868711156\n",
      "Losgistic Regression(    9800/10000): loss=-73.0341908580786\n",
      "Losgistic Regression(    9900/10000): loss=-76.5186981234218\n",
      "Time for  1th cross validation = 31.137s\n",
      "Training Accuracy         =  0.947\n",
      "Cross Validation Accuracy = 0.761612\n",
      "Losgistic Regression(       0/10000): loss= 691.569707340736\n",
      "Losgistic Regression(     100/10000): loss= 521.479457171315\n",
      "Losgistic Regression(     200/10000): loss= 453.364801897028\n",
      "Losgistic Regression(     300/10000): loss= 414.212734960755\n",
      "Losgistic Regression(     400/10000): loss= 391.069047580342\n",
      "Losgistic Regression(     500/10000): loss= 375.219930435098\n",
      "Losgistic Regression(     600/10000): loss= 363.355618631938\n",
      "Losgistic Regression(     700/10000): loss= 354.059426472869\n",
      "Losgistic Regression(     800/10000): loss= 346.431550031187\n",
      "Losgistic Regression(     900/10000): loss= 339.850464714783\n",
      "Losgistic Regression(    1000/10000): loss= 333.972094040198\n",
      "Losgistic Regression(    1100/10000): loss= 328.593514786542\n",
      "Losgistic Regression(    1200/10000): loss= 322.76161618043\n",
      "Losgistic Regression(    1300/10000): loss= 316.606841409217\n",
      "Losgistic Regression(    1400/10000): loss= 310.85502414739\n",
      "Losgistic Regression(    1500/10000): loss= 305.444648507184\n",
      "Losgistic Regression(    1600/10000): loss= 300.306788890227\n",
      "Losgistic Regression(    1700/10000): loss= 295.42376430706\n",
      "Losgistic Regression(    1800/10000): loss= 290.507139875548\n",
      "Losgistic Regression(    1900/10000): loss= 285.389379958795\n",
      "Losgistic Regression(    2000/10000): loss= 280.062830776984\n",
      "Losgistic Regression(    2100/10000): loss= 274.948252396113\n",
      "Losgistic Regression(    2200/10000): loss= 270.111664524823\n",
      "Losgistic Regression(    2300/10000): loss= 265.523805854994\n",
      "Losgistic Regression(    2400/10000): loss= 261.133463682555\n",
      "Losgistic Regression(    2500/10000): loss= 256.900533301479\n",
      "Losgistic Regression(    2600/10000): loss= 252.699552360283\n",
      "Losgistic Regression(    2700/10000): loss= 247.945264390331\n",
      "Losgistic Regression(    2800/10000): loss= 242.857383426194\n",
      "Losgistic Regression(    2900/10000): loss= 237.921617739285\n",
      "Losgistic Regression(    3000/10000): loss= 232.8634820421\n",
      "Losgistic Regression(    3100/10000): loss= 227.265054861435\n",
      "Losgistic Regression(    3200/10000): loss= 221.351090102879\n",
      "Losgistic Regression(    3300/10000): loss= 215.409085619152\n",
      "Losgistic Regression(    3400/10000): loss= 209.397356175957\n",
      "Losgistic Regression(    3500/10000): loss= 203.537432280962\n",
      "Losgistic Regression(    3600/10000): loss= 197.849471890346\n",
      "Losgistic Regression(    3700/10000): loss= 192.337300126874\n",
      "Losgistic Regression(    3800/10000): loss= 186.971715584054\n",
      "Losgistic Regression(    3900/10000): loss= 181.742779736271\n",
      "Losgistic Regression(    4000/10000): loss= 176.240780645448\n",
      "Losgistic Regression(    4100/10000): loss= 170.820947102611\n",
      "Losgistic Regression(    4200/10000): loss= 165.491696893418\n",
      "Losgistic Regression(    4300/10000): loss= 160.248976259738\n",
      "Losgistic Regression(    4400/10000): loss= 155.039507400511\n",
      "Losgistic Regression(    4500/10000): loss= 149.872941888839\n",
      "Losgistic Regression(    4600/10000): loss= 144.74094878894\n",
      "Losgistic Regression(    4700/10000): loss= 139.657960145756\n",
      "Losgistic Regression(    4800/10000): loss= 134.455478852255\n",
      "Losgistic Regression(    4900/10000): loss= 129.305206627331\n",
      "Losgistic Regression(    5000/10000): loss= 124.263365808947\n",
      "Losgistic Regression(    5100/10000): loss= 119.285070013459\n",
      "Losgistic Regression(    5200/10000): loss= 114.35696851511\n",
      "Losgistic Regression(    5300/10000): loss= 109.503284081184\n",
      "Losgistic Regression(    5400/10000): loss= 104.697266766771\n",
      "Losgistic Regression(    5500/10000): loss= 99.7857003831572\n",
      "Losgistic Regression(    5600/10000): loss= 94.7371384462676\n",
      "Losgistic Regression(    5700/10000): loss= 89.7216180956019\n",
      "Losgistic Regression(    5800/10000): loss= 84.5223202227888\n",
      "Losgistic Regression(    5900/10000): loss= 79.2421853895556\n",
      "Losgistic Regression(    6000/10000): loss= 73.754683166654\n",
      "Losgistic Regression(    6100/10000): loss= 68.0139590291239\n",
      "Losgistic Regression(    6200/10000): loss= 62.2718942293941\n",
      "Losgistic Regression(    6300/10000): loss= 56.4909085747623\n",
      "Losgistic Regression(    6400/10000): loss= 50.7484551619389\n",
      "Losgistic Regression(    6500/10000): loss= 45.0677935816802\n",
      "Losgistic Regression(    6600/10000): loss= 39.44854070003\n",
      "Losgistic Regression(    6700/10000): loss= 33.8613695311654\n",
      "Losgistic Regression(    6800/10000): loss= 28.2310572658861\n",
      "Losgistic Regression(    6900/10000): loss= 22.2266175901212\n",
      "Losgistic Regression(    7000/10000): loss= 16.1049532736572\n",
      "Losgistic Regression(    7100/10000): loss= 10.0728682241617\n",
      "Losgistic Regression(    7200/10000): loss= 4.17122580230678\n",
      "Losgistic Regression(    7300/10000): loss=-1.68034167867656\n",
      "Losgistic Regression(    7400/10000): loss=-7.44098216532905\n",
      "Losgistic Regression(    7500/10000): loss=-13.1020883775288\n",
      "Losgistic Regression(    7600/10000): loss=-18.6573169715753\n",
      "Losgistic Regression(    7700/10000): loss=-24.0437480263689\n",
      "Losgistic Regression(    7800/10000): loss=-29.3881574671537\n",
      "Losgistic Regression(    7900/10000): loss=-34.575634205947\n",
      "Losgistic Regression(    8000/10000): loss=-39.7370951089148\n",
      "Losgistic Regression(    8100/10000): loss=-44.868192520936\n",
      "Losgistic Regression(    8200/10000): loss=-50.2563374640942\n",
      "Losgistic Regression(    8300/10000): loss=-55.7554488174516\n",
      "Losgistic Regression(    8400/10000): loss=-61.2814890222188\n",
      "Losgistic Regression(    8500/10000): loss=-66.8144787790518\n",
      "Losgistic Regression(    8600/10000): loss=-72.3469418865564\n",
      "Losgistic Regression(    8700/10000): loss=-77.8392627551823\n",
      "Losgistic Regression(    8800/10000): loss=-83.2850720008589\n",
      "Losgistic Regression(    8900/10000): loss=-88.6391799310827\n",
      "Losgistic Regression(    9000/10000): loss=-93.8905992692898\n",
      "Losgistic Regression(    9100/10000): loss=-99.0035397400544\n",
      "Losgistic Regression(    9200/10000): loss=-104.01401901669\n",
      "Losgistic Regression(    9300/10000): loss=-108.901073952275\n",
      "Losgistic Regression(    9400/10000): loss=-113.726320131835\n",
      "Losgistic Regression(    9500/10000): loss=-118.491789198807\n",
      "Losgistic Regression(    9600/10000): loss=-123.19975676398\n",
      "Losgistic Regression(    9700/10000): loss=-127.862730754814\n",
      "Losgistic Regression(    9800/10000): loss=-132.457118564016\n",
      "Losgistic Regression(    9900/10000): loss=-136.947727635091\n",
      "Time for  2th cross validation = 31.4181s\n",
      "Training Accuracy         =  0.928\n",
      "Cross Validation Accuracy = 0.739488\n",
      "Losgistic Regression(       0/10000): loss= 691.707325662028\n",
      "Losgistic Regression(     100/10000): loss= 512.257118517144\n",
      "Losgistic Regression(     200/10000): loss= 436.215229431893\n",
      "Losgistic Regression(     300/10000): loss= 393.449655910208\n",
      "Losgistic Regression(     400/10000): loss= 368.571823510648\n",
      "Losgistic Regression(     500/10000): loss= 350.923313151256\n",
      "Losgistic Regression(     600/10000): loss= 337.098215933545\n",
      "Losgistic Regression(     700/10000): loss= 325.321370689701\n",
      "Losgistic Regression(     800/10000): loss= 314.228094111496\n",
      "Losgistic Regression(     900/10000): loss= 304.435843734171\n",
      "Losgistic Regression(    1000/10000): loss= 295.582978546133\n",
      "Losgistic Regression(    1100/10000): loss= 287.472157003685\n",
      "Losgistic Regression(    1200/10000): loss= 279.722077485034\n",
      "Losgistic Regression(    1300/10000): loss= 271.126388798671\n",
      "Losgistic Regression(    1400/10000): loss= 262.95713972136\n",
      "Losgistic Regression(    1500/10000): loss= 255.206027347989\n",
      "Losgistic Regression(    1600/10000): loss= 247.851207320106\n",
      "Losgistic Regression(    1700/10000): loss= 240.67250511232\n",
      "Losgistic Regression(    1800/10000): loss= 233.15205032266\n",
      "Losgistic Regression(    1900/10000): loss= 225.724012176882\n",
      "Losgistic Regression(    2000/10000): loss= 217.900837185281\n",
      "Losgistic Regression(    2100/10000): loss= 209.593237920577\n",
      "Losgistic Regression(    2200/10000): loss= 200.535444786176\n",
      "Losgistic Regression(    2300/10000): loss= 191.667045474096\n",
      "Losgistic Regression(    2400/10000): loss= 182.720568209042\n",
      "Losgistic Regression(    2500/10000): loss= 173.61642437858\n",
      "Losgistic Regression(    2600/10000): loss= 164.794804724139\n",
      "Losgistic Regression(    2700/10000): loss= 156.201687993839\n",
      "Losgistic Regression(    2800/10000): loss= 147.859757477079\n",
      "Losgistic Regression(    2900/10000): loss= 139.765904006398\n",
      "Losgistic Regression(    3000/10000): loss= 131.833820719503\n",
      "Losgistic Regression(    3100/10000): loss= 123.759307034112\n",
      "Losgistic Regression(    3200/10000): loss= 115.754641030566\n",
      "Losgistic Regression(    3300/10000): loss= 107.713381199684\n",
      "Losgistic Regression(    3400/10000): loss= 99.8861264016526\n",
      "Losgistic Regression(    3500/10000): loss= 92.0794568582925\n",
      "Losgistic Regression(    3600/10000): loss= 84.3662117008077\n",
      "Losgistic Regression(    3700/10000): loss= 76.8126436183943\n",
      "Losgistic Regression(    3800/10000): loss= 68.955275536512\n",
      "Losgistic Regression(    3900/10000): loss= 61.1477394028232\n",
      "Losgistic Regression(    4000/10000): loss= 53.4698379056411\n",
      "Losgistic Regression(    4100/10000): loss= 45.9008058601788\n",
      "Losgistic Regression(    4200/10000): loss= 38.4139904194437\n",
      "Losgistic Regression(    4300/10000): loss= 31.0215047611811\n",
      "Losgistic Regression(    4400/10000): loss= 23.8005388315211\n",
      "Losgistic Regression(    4500/10000): loss= 16.6629710467235\n",
      "Losgistic Regression(    4600/10000): loss= 9.590846438843\n",
      "Losgistic Regression(    4700/10000): loss= 2.4028024654316\n",
      "Losgistic Regression(    4800/10000): loss=-4.77295245296881\n",
      "Losgistic Regression(    4900/10000): loss=-11.9248515905905\n",
      "Losgistic Regression(    5000/10000): loss=-19.0764728874144\n",
      "Losgistic Regression(    5100/10000): loss=-26.5018366917335\n",
      "Losgistic Regression(    5200/10000): loss=-34.4372351469172\n",
      "Losgistic Regression(    5300/10000): loss=-42.6136386800762\n",
      "Losgistic Regression(    5400/10000): loss=-51.0931603778373\n",
      "Losgistic Regression(    5500/10000): loss=-59.7349026951574\n",
      "Losgistic Regression(    5600/10000): loss=-68.4834769816452\n",
      "Losgistic Regression(    5700/10000): loss=-77.425183230251\n",
      "Losgistic Regression(    5800/10000): loss=-86.4969900951359\n",
      "Losgistic Regression(    5900/10000): loss=-95.6447204108442\n",
      "Losgistic Regression(    6000/10000): loss=-104.865274575131\n",
      "Losgistic Regression(    6100/10000): loss=-114.084136628641\n",
      "Losgistic Regression(    6200/10000): loss=-123.219300915458\n",
      "Losgistic Regression(    6300/10000): loss=-132.510241385547\n",
      "Losgistic Regression(    6400/10000): loss=-141.848472701673\n",
      "Losgistic Regression(    6500/10000): loss=-151.573108048484\n",
      "Losgistic Regression(    6600/10000): loss=-161.335252931197\n",
      "Losgistic Regression(    6700/10000): loss=-171.127050796375\n",
      "Losgistic Regression(    6800/10000): loss=-180.959387620078\n",
      "Losgistic Regression(    6900/10000): loss=-190.658709251393\n",
      "Losgistic Regression(    7000/10000): loss=-200.324652730412\n",
      "Losgistic Regression(    7100/10000): loss=-210.004250975039\n",
      "Losgistic Regression(    7200/10000): loss=-219.742190047165\n",
      "Losgistic Regression(    7300/10000): loss=-229.335136640363\n",
      "Losgistic Regression(    7400/10000): loss=-238.976295874759\n",
      "Losgistic Regression(    7500/10000): loss=-248.551317797774\n",
      "Losgistic Regression(    7600/10000): loss=-258.089461209675\n",
      "Losgistic Regression(    7700/10000): loss=-267.833142835643\n",
      "Losgistic Regression(    7800/10000): loss=-277.693245877379\n",
      "Losgistic Regression(    7900/10000): loss=-287.445878721153\n",
      "Losgistic Regression(    8000/10000): loss=-297.076340846318\n",
      "Losgistic Regression(    8100/10000): loss=-306.615226555933\n",
      "Losgistic Regression(    8200/10000): loss=-316.012894438942\n",
      "Losgistic Regression(    8300/10000): loss=-325.2085403529\n",
      "Losgistic Regression(    8400/10000): loss=-334.331730705987\n",
      "Losgistic Regression(    8500/10000): loss=-343.462483528311\n",
      "Losgistic Regression(    8600/10000): loss=-352.551010933155\n",
      "Losgistic Regression(    8700/10000): loss=-361.450807227585\n",
      "Losgistic Regression(    8800/10000): loss=-370.237764102703\n",
      "Losgistic Regression(    8900/10000): loss=-379.075922128183\n",
      "Losgistic Regression(    9000/10000): loss=-387.888960000056\n",
      "Losgistic Regression(    9100/10000): loss=-397.113515965542\n",
      "Losgistic Regression(    9200/10000): loss=-406.295242856487\n",
      "Losgistic Regression(    9300/10000): loss=-415.375027697925\n",
      "Losgistic Regression(    9400/10000): loss=-424.416917270442\n",
      "Losgistic Regression(    9500/10000): loss=-433.37036368909\n",
      "Losgistic Regression(    9600/10000): loss=-442.359577826905\n",
      "Losgistic Regression(    9700/10000): loss=-451.573479662188\n",
      "Losgistic Regression(    9800/10000): loss=-460.504220744578\n",
      "Losgistic Regression(    9900/10000): loss=-469.022344389197\n",
      "Time for  3th cross validation = 31.2862s\n",
      "Training Accuracy         =   0.94\n",
      "Cross Validation Accuracy = 0.740492\n",
      "Losgistic Regression(       0/10000): loss= 692.021458023298\n",
      "Losgistic Regression(     100/10000): loss= 518.021422636273\n",
      "Losgistic Regression(     200/10000): loss= 442.199081567282\n",
      "Losgistic Regression(     300/10000): loss= 400.193542946546\n",
      "Losgistic Regression(     400/10000): loss= 375.011027598533\n",
      "Losgistic Regression(     500/10000): loss= 357.248937612831\n",
      "Losgistic Regression(     600/10000): loss= 343.260489771233\n",
      "Losgistic Regression(     700/10000): loss= 331.540001441599\n",
      "Losgistic Regression(     800/10000): loss= 321.427756296329\n",
      "Losgistic Regression(     900/10000): loss= 312.534456786129\n",
      "Losgistic Regression(    1000/10000): loss= 304.582123303461\n",
      "Losgistic Regression(    1100/10000): loss= 297.416375356103\n",
      "Losgistic Regression(    1200/10000): loss= 290.925879940141\n",
      "Losgistic Regression(    1300/10000): loss= 285.006962508137\n",
      "Losgistic Regression(    1400/10000): loss= 279.587926442942\n",
      "Losgistic Regression(    1500/10000): loss= 274.60038532527\n",
      "Losgistic Regression(    1600/10000): loss= 269.98527191271\n",
      "Losgistic Regression(    1700/10000): loss= 265.699057365436\n",
      "Losgistic Regression(    1800/10000): loss= 261.310430681368\n",
      "Losgistic Regression(    1900/10000): loss= 256.466578304345\n",
      "Losgistic Regression(    2000/10000): loss= 251.848365496337\n",
      "Losgistic Regression(    2100/10000): loss= 247.4372214543\n",
      "Losgistic Regression(    2200/10000): loss= 243.206580818132\n",
      "Losgistic Regression(    2300/10000): loss= 239.148799608748\n",
      "Losgistic Regression(    2400/10000): loss= 235.243601449261\n",
      "Losgistic Regression(    2500/10000): loss= 231.485173525571\n",
      "Losgistic Regression(    2600/10000): loss= 227.857260898104\n",
      "Losgistic Regression(    2700/10000): loss= 224.359117879473\n",
      "Losgistic Regression(    2800/10000): loss= 220.978356577681\n",
      "Losgistic Regression(    2900/10000): loss= 217.701007151773\n",
      "Losgistic Regression(    3000/10000): loss= 213.905815289417\n",
      "Losgistic Regression(    3100/10000): loss= 210.110675338819\n",
      "Losgistic Regression(    3200/10000): loss= 206.419394113616\n",
      "Losgistic Regression(    3300/10000): loss= 202.828922182426\n",
      "Losgistic Regression(    3400/10000): loss= 199.337837565294\n",
      "Losgistic Regression(    3500/10000): loss= 195.955745431246\n",
      "Losgistic Regression(    3600/10000): loss= 192.223677854265\n",
      "Losgistic Regression(    3700/10000): loss= 188.572948337466\n",
      "Losgistic Regression(    3800/10000): loss= 184.717021525884\n",
      "Losgistic Regression(    3900/10000): loss= 180.851319165542\n",
      "Losgistic Regression(    4000/10000): loss= 177.113320146009\n",
      "Losgistic Regression(    4100/10000): loss= 173.337895954108\n",
      "Losgistic Regression(    4200/10000): loss= 169.495485683797\n",
      "Losgistic Regression(    4300/10000): loss= 165.757565519177\n",
      "Losgistic Regression(    4400/10000): loss= 162.127481395027\n",
      "Losgistic Regression(    4500/10000): loss= 158.37681736118\n",
      "Losgistic Regression(    4600/10000): loss= 154.570922303646\n",
      "Losgistic Regression(    4700/10000): loss= 150.607101383598\n",
      "Losgistic Regression(    4800/10000): loss= 146.736943089761\n",
      "Losgistic Regression(    4900/10000): loss= 142.960977208248\n",
      "Losgistic Regression(    5000/10000): loss= 139.295218831051\n",
      "Losgistic Regression(    5100/10000): loss= 135.694598363681\n",
      "Losgistic Regression(    5200/10000): loss= 131.843038765195\n",
      "Losgistic Regression(    5300/10000): loss= 127.973540605704\n",
      "Losgistic Regression(    5400/10000): loss= 124.191277559719\n",
      "Losgistic Regression(    5500/10000): loss= 120.494019296972\n",
      "Losgistic Regression(    5600/10000): loss= 116.854479169078\n",
      "Losgistic Regression(    5700/10000): loss= 113.280454963123\n",
      "Losgistic Regression(    5800/10000): loss= 109.79496809412\n",
      "Losgistic Regression(    5900/10000): loss= 106.364767567779\n",
      "Losgistic Regression(    6000/10000): loss= 103.010003675274\n",
      "Losgistic Regression(    6100/10000): loss= 99.7403590040103\n",
      "Losgistic Regression(    6200/10000): loss= 96.5127983762611\n",
      "Losgistic Regression(    6300/10000): loss= 93.3861854800489\n",
      "Losgistic Regression(    6400/10000): loss= 90.3389579685602\n",
      "Losgistic Regression(    6500/10000): loss= 87.3076003195207\n",
      "Losgistic Regression(    6600/10000): loss= 84.1728793390464\n",
      "Losgistic Regression(    6700/10000): loss= 81.0053186368037\n",
      "Losgistic Regression(    6800/10000): loss= 77.8163704666501\n",
      "Losgistic Regression(    6900/10000): loss= 74.414512326324\n",
      "Losgistic Regression(    7000/10000): loss= 71.0058421121052\n",
      "Losgistic Regression(    7100/10000): loss= 67.6221096691548\n",
      "Losgistic Regression(    7200/10000): loss= 64.2677234965933\n",
      "Losgistic Regression(    7300/10000): loss= 60.9443859849422\n",
      "Losgistic Regression(    7400/10000): loss= 57.6541853136415\n",
      "Losgistic Regression(    7500/10000): loss= 54.3507965896246\n",
      "Losgistic Regression(    7600/10000): loss= 51.0234793990343\n",
      "Losgistic Regression(    7700/10000): loss= 47.7587253262471\n",
      "Losgistic Regression(    7800/10000): loss= 44.5181748871937\n",
      "Losgistic Regression(    7900/10000): loss= 41.2703365913861\n",
      "Losgistic Regression(    8000/10000): loss= 37.9960697335244\n",
      "Losgistic Regression(    8100/10000): loss= 34.7116974990247\n",
      "Losgistic Regression(    8200/10000): loss= 31.4142863126366\n",
      "Losgistic Regression(    8300/10000): loss= 28.1517180671069\n",
      "Losgistic Regression(    8400/10000): loss= 24.879246716342\n",
      "Losgistic Regression(    8500/10000): loss= 21.5955525520149\n",
      "Losgistic Regression(    8600/10000): loss= 18.282629388867\n",
      "Losgistic Regression(    8700/10000): loss= 14.9625466119408\n",
      "Losgistic Regression(    8800/10000): loss= 11.6349652218817\n",
      "Losgistic Regression(    8900/10000): loss= 8.32269499890186\n",
      "Losgistic Regression(    9000/10000): loss= 5.02628909293416\n",
      "Losgistic Regression(    9100/10000): loss= 1.7705512000586\n",
      "Losgistic Regression(    9200/10000): loss=-1.55723535268011\n",
      "Losgistic Regression(    9300/10000): loss=-4.91386369943795\n",
      "Losgistic Regression(    9400/10000): loss=-8.34709870897223\n",
      "Losgistic Regression(    9500/10000): loss=-11.7149263253015\n",
      "Losgistic Regression(    9600/10000): loss=-15.0656422727736\n",
      "Losgistic Regression(    9700/10000): loss=-18.4079096785764\n",
      "Losgistic Regression(    9800/10000): loss=-21.6828305248945\n",
      "Losgistic Regression(    9900/10000): loss=-24.9020629599682\n",
      "Time for  4th cross validation = 31.8249s\n",
      "Training Accuracy         =  0.943\n",
      "Cross Validation Accuracy = 0.742312\n",
      "*************** ([0.93999999999999995, 0.94699999999999995, 0.92800000000000005, 0.93999999999999995, 0.94299999999999995], [0.73863599999999996, 0.76161199999999996, 0.73948800000000003, 0.74049200000000004, 0.74231199999999997])\n",
      "Losgistic Regression(       0/10000): loss= 692.024538875273\n",
      "Losgistic Regression(     100/10000): loss= 531.146758700516\n",
      "Losgistic Regression(     200/10000): loss= 451.524603037748\n",
      "Losgistic Regression(     300/10000): loss= 407.976857299804\n",
      "Losgistic Regression(     400/10000): loss= 382.980468980066\n",
      "Losgistic Regression(     500/10000): loss= 365.755911137163\n",
      "Losgistic Regression(     600/10000): loss= 352.686174542224\n",
      "Losgistic Regression(     700/10000): loss= 342.34633802605\n",
      "Losgistic Regression(     800/10000): loss= 333.948114840886\n",
      "Losgistic Regression(     900/10000): loss= 327.002953448001\n",
      "Losgistic Regression(    1000/10000): loss= 321.147267370627\n",
      "Losgistic Regression(    1100/10000): loss= 316.142605212711\n",
      "Losgistic Regression(    1200/10000): loss= 311.776525145734\n",
      "Losgistic Regression(    1300/10000): loss= 307.931469993441\n",
      "Losgistic Regression(    1400/10000): loss= 304.509577513143\n",
      "Losgistic Regression(    1500/10000): loss= 301.43773256735\n",
      "Losgistic Regression(    1600/10000): loss= 298.642438300574\n",
      "Losgistic Regression(    1700/10000): loss= 296.080375488867\n",
      "Losgistic Regression(    1800/10000): loss= 293.718803952646\n",
      "Losgistic Regression(    1900/10000): loss= 291.526903857147\n",
      "Losgistic Regression(    2000/10000): loss= 289.441726841813\n",
      "Losgistic Regression(    2100/10000): loss= 287.179497547128\n",
      "Losgistic Regression(    2200/10000): loss= 285.042722705402\n",
      "Losgistic Regression(    2300/10000): loss= 283.039148434734\n",
      "Losgistic Regression(    2400/10000): loss= 281.146620418252\n",
      "Losgistic Regression(    2500/10000): loss= 279.356083822949\n",
      "Losgistic Regression(    2600/10000): loss= 277.624415445779\n",
      "Losgistic Regression(    2700/10000): loss= 275.935847180302\n",
      "Losgistic Regression(    2800/10000): loss= 274.339966708471\n",
      "Losgistic Regression(    2900/10000): loss= 272.686735265571\n",
      "Losgistic Regression(    3000/10000): loss= 270.917168830777\n",
      "Losgistic Regression(    3100/10000): loss= 269.161250684754\n",
      "Losgistic Regression(    3200/10000): loss= 267.440528838011\n",
      "Losgistic Regression(    3300/10000): loss= 265.703006429641\n",
      "Losgistic Regression(    3400/10000): loss= 263.975784637079\n",
      "Losgistic Regression(    3500/10000): loss= 262.279026605692\n",
      "Losgistic Regression(    3600/10000): loss= 260.615934962681\n",
      "Losgistic Regression(    3700/10000): loss= 258.955895400165\n",
      "Losgistic Regression(    3800/10000): loss= 257.274410548715\n",
      "Losgistic Regression(    3900/10000): loss= 255.632648937451\n",
      "Losgistic Regression(    4000/10000): loss= 254.057913794971\n",
      "Losgistic Regression(    4100/10000): loss= 252.583301397204\n",
      "Losgistic Regression(    4200/10000): loss= 250.727114923488\n",
      "Losgistic Regression(    4300/10000): loss= 248.936205608939\n",
      "Losgistic Regression(    4400/10000): loss= 247.14338268367\n",
      "Losgistic Regression(    4500/10000): loss= 245.48213949815\n",
      "Losgistic Regression(    4600/10000): loss= 243.592787790525\n",
      "Losgistic Regression(    4700/10000): loss= 241.477441178472\n",
      "Losgistic Regression(    4800/10000): loss= 239.525500454419\n",
      "Losgistic Regression(    4900/10000): loss= 237.414129593289\n",
      "Losgistic Regression(    5000/10000): loss= 235.392374664933\n",
      "Losgistic Regression(    5100/10000): loss= 232.856184521942\n",
      "Losgistic Regression(    5200/10000): loss= 230.461397923848\n",
      "Losgistic Regression(    5300/10000): loss= 228.051656614953\n",
      "Losgistic Regression(    5400/10000): loss= 225.948772202787\n",
      "Losgistic Regression(    5500/10000): loss= 224.083678912082\n",
      "Losgistic Regression(    5600/10000): loss= 222.501620048213\n",
      "Losgistic Regression(    5700/10000): loss= 220.552046187767\n",
      "Losgistic Regression(    5800/10000): loss= 218.473234706097\n",
      "Losgistic Regression(    5900/10000): loss= 215.931427816147\n",
      "Losgistic Regression(    6000/10000): loss= 213.185757411609\n",
      "Losgistic Regression(    6100/10000): loss= 210.409654466337\n",
      "Losgistic Regression(    6200/10000): loss= 207.845157093406\n",
      "Losgistic Regression(    6300/10000): loss= 205.13454746156\n",
      "Losgistic Regression(    6400/10000): loss= 202.84000326616\n",
      "Losgistic Regression(    6500/10000): loss= 200.281331667834\n",
      "Losgistic Regression(    6600/10000): loss= 197.6760525778\n",
      "Losgistic Regression(    6700/10000): loss= 195.36445781064\n",
      "Losgistic Regression(    6800/10000): loss= 192.934281840794\n",
      "Losgistic Regression(    6900/10000): loss= 190.758578565975\n",
      "Losgistic Regression(    7000/10000): loss= 188.643008066044\n",
      "Losgistic Regression(    7100/10000): loss= 186.591573340149\n",
      "Losgistic Regression(    7200/10000): loss= 184.528745120775\n",
      "Losgistic Regression(    7300/10000): loss= 182.575260465147\n",
      "Losgistic Regression(    7400/10000): loss= 180.690218943762\n",
      "Losgistic Regression(    7500/10000): loss= 178.709031990074\n",
      "Losgistic Regression(    7600/10000): loss= 176.642237402751\n",
      "Losgistic Regression(    7700/10000): loss= 174.726997124473\n",
      "Losgistic Regression(    7800/10000): loss= 172.811323817632\n",
      "Losgistic Regression(    7900/10000): loss= 170.909007089941\n",
      "Losgistic Regression(    8000/10000): loss= 169.193439404711\n",
      "Losgistic Regression(    8100/10000): loss= 167.491276316208\n",
      "Losgistic Regression(    8200/10000): loss= 165.990133380678\n",
      "Losgistic Regression(    8300/10000): loss= 164.282778425568\n",
      "Losgistic Regression(    8400/10000): loss= 162.616357848339\n",
      "Losgistic Regression(    8500/10000): loss= 160.454173237503\n",
      "Losgistic Regression(    8600/10000): loss= 158.377306342388\n",
      "Losgistic Regression(    8700/10000): loss= 156.381564416661\n",
      "Losgistic Regression(    8800/10000): loss= 154.053088525129\n",
      "Losgistic Regression(    8900/10000): loss= 152.072225553039\n",
      "Losgistic Regression(    9000/10000): loss= 149.883148491149\n",
      "Losgistic Regression(    9100/10000): loss= 147.982772391088\n",
      "Losgistic Regression(    9200/10000): loss= 145.821361227084\n",
      "Losgistic Regression(    9300/10000): loss= 143.609052140532\n",
      "Losgistic Regression(    9400/10000): loss= 141.669261559916\n",
      "Losgistic Regression(    9500/10000): loss= 140.223649951685\n",
      "Losgistic Regression(    9600/10000): loss= 138.949283686298\n",
      "Losgistic Regression(    9700/10000): loss= 137.635866539717\n",
      "Losgistic Regression(    9800/10000): loss= 136.185800301784\n",
      "Losgistic Regression(    9900/10000): loss= 134.260513600001\n",
      "Time for  0th cross validation = 31.8911s\n",
      "Training Accuracy         =  0.931\n",
      "Cross Validation Accuracy = 0.75012\n",
      "Losgistic Regression(       0/10000): loss= 691.966857006262\n",
      "Losgistic Regression(     100/10000): loss= 519.203982936664\n",
      "Losgistic Regression(     200/10000): loss= 437.564328297982\n",
      "Losgistic Regression(     300/10000): loss= 391.298690438588\n",
      "Losgistic Regression(     400/10000): loss= 365.332492328538\n",
      "Losgistic Regression(     500/10000): loss= 348.054539838285\n",
      "Losgistic Regression(     600/10000): loss= 334.931608326533\n",
      "Losgistic Regression(     700/10000): loss= 324.350791907485\n",
      "Losgistic Regression(     800/10000): loss= 315.525454801257\n",
      "Losgistic Regression(     900/10000): loss= 307.94573607932\n",
      "Losgistic Regression(    1000/10000): loss= 301.300170808401\n",
      "Losgistic Regression(    1100/10000): loss= 295.387478757695\n",
      "Losgistic Regression(    1200/10000): loss= 289.391624032992\n",
      "Losgistic Regression(    1300/10000): loss= 282.978517770786\n",
      "Losgistic Regression(    1400/10000): loss= 277.247466454495\n",
      "Losgistic Regression(    1500/10000): loss= 272.140106599754\n",
      "Losgistic Regression(    1600/10000): loss= 267.563719239486\n",
      "Losgistic Regression(    1700/10000): loss= 263.428463498173\n",
      "Losgistic Regression(    1800/10000): loss= 259.621111720305\n",
      "Losgistic Regression(    1900/10000): loss= 256.0281229574\n",
      "Losgistic Regression(    2000/10000): loss= 252.623421890985\n",
      "Losgistic Regression(    2100/10000): loss= 249.344726438162\n",
      "Losgistic Regression(    2200/10000): loss= 246.208818651707\n",
      "Losgistic Regression(    2300/10000): loss= 243.186494525846\n",
      "Losgistic Regression(    2400/10000): loss= 240.002758534376\n",
      "Losgistic Regression(    2500/10000): loss= 236.632268317232\n",
      "Losgistic Regression(    2600/10000): loss= 233.227983252404\n",
      "Losgistic Regression(    2700/10000): loss= 229.972260534034\n",
      "Losgistic Regression(    2800/10000): loss= 226.933370779737\n",
      "Losgistic Regression(    2900/10000): loss= 224.008759462852\n",
      "Losgistic Regression(    3000/10000): loss= 221.255600433746\n",
      "Losgistic Regression(    3100/10000): loss= 218.638822271476\n",
      "Losgistic Regression(    3200/10000): loss= 216.091730724178\n",
      "Losgistic Regression(    3300/10000): loss= 213.708683835769\n",
      "Losgistic Regression(    3400/10000): loss= 211.453158936873\n",
      "Losgistic Regression(    3500/10000): loss= 209.137292063893\n",
      "Losgistic Regression(    3600/10000): loss= 206.631727196773\n",
      "Losgistic Regression(    3700/10000): loss= 204.326021074852\n",
      "Losgistic Regression(    3800/10000): loss= 202.140820383208\n",
      "Losgistic Regression(    3900/10000): loss= 200.006653866421\n",
      "Losgistic Regression(    4000/10000): loss= 197.987221905614\n",
      "Losgistic Regression(    4100/10000): loss= 196.008898829502\n",
      "Losgistic Regression(    4200/10000): loss= 194.315311191255\n",
      "Losgistic Regression(    4300/10000): loss= 192.774831931449\n",
      "Losgistic Regression(    4400/10000): loss= 191.310873832132\n",
      "Losgistic Regression(    4500/10000): loss= 189.898973967797\n",
      "Losgistic Regression(    4600/10000): loss= 188.631052210654\n",
      "Losgistic Regression(    4700/10000): loss= 187.350472277121\n",
      "Losgistic Regression(    4800/10000): loss= 186.098762061045\n",
      "Losgistic Regression(    4900/10000): loss= 184.908941462458\n",
      "Losgistic Regression(    5000/10000): loss= 183.805447570519\n",
      "Losgistic Regression(    5100/10000): loss= 182.633823121092\n",
      "Losgistic Regression(    5200/10000): loss= 181.507564651455\n",
      "Losgistic Regression(    5300/10000): loss= 180.369557251184\n",
      "Losgistic Regression(    5400/10000): loss= 179.17232696394\n",
      "Losgistic Regression(    5500/10000): loss= 178.180707773381\n",
      "Losgistic Regression(    5600/10000): loss= 177.241658922918\n",
      "Losgistic Regression(    5700/10000): loss= 175.972495206524\n",
      "Losgistic Regression(    5800/10000): loss= 174.734243495204\n",
      "Losgistic Regression(    5900/10000): loss= 173.386640649095\n",
      "Losgistic Regression(    6000/10000): loss= 172.157881731528\n",
      "Losgistic Regression(    6100/10000): loss= 170.94617985381\n",
      "Losgistic Regression(    6200/10000): loss= 169.6895963011\n",
      "Losgistic Regression(    6300/10000): loss= 168.446219670409\n",
      "Losgistic Regression(    6400/10000): loss= 167.40327952222\n",
      "Losgistic Regression(    6500/10000): loss= 166.233643203948\n",
      "Losgistic Regression(    6600/10000): loss= 165.281424695902\n",
      "Losgistic Regression(    6700/10000): loss= 164.408812220921\n",
      "Losgistic Regression(    6800/10000): loss= 163.63315602003\n",
      "Losgistic Regression(    6900/10000): loss= 162.705461277474\n",
      "Losgistic Regression(    7000/10000): loss= 161.986764299906\n",
      "Losgistic Regression(    7100/10000): loss= 161.368637339573\n",
      "Losgistic Regression(    7200/10000): loss= 160.766783724607\n",
      "Losgistic Regression(    7300/10000): loss= 160.26004092196\n",
      "Losgistic Regression(    7400/10000): loss= 159.916068550723\n",
      "Losgistic Regression(    7500/10000): loss= 159.392327445816\n",
      "Losgistic Regression(    7600/10000): loss= 158.661742637983\n",
      "Losgistic Regression(    7700/10000): loss= 158.111643391485\n",
      "Losgistic Regression(    7800/10000): loss= 157.378871931646\n",
      "Losgistic Regression(    7900/10000): loss= 156.620871098288\n",
      "Losgistic Regression(    8000/10000): loss= 155.806991981369\n",
      "Losgistic Regression(    8100/10000): loss= 154.99592566619\n",
      "Losgistic Regression(    8200/10000): loss= 154.375041928744\n",
      "Losgistic Regression(    8300/10000): loss= 153.947204121653\n",
      "Losgistic Regression(    8400/10000): loss= 153.476721246206\n",
      "Losgistic Regression(    8500/10000): loss= 153.015036696721\n",
      "Losgistic Regression(    8600/10000): loss= 152.411523836293\n",
      "Losgistic Regression(    8700/10000): loss= 151.800830173589\n",
      "Losgistic Regression(    8800/10000): loss= 150.806847686163\n",
      "Losgistic Regression(    8900/10000): loss= 149.980142967139\n",
      "Losgistic Regression(    9000/10000): loss= 149.154798925362\n",
      "Losgistic Regression(    9100/10000): loss= 148.482039350042\n",
      "Losgistic Regression(    9200/10000): loss= 147.743423257501\n",
      "Losgistic Regression(    9300/10000): loss= 147.147018082944\n",
      "Losgistic Regression(    9400/10000): loss= 146.36806939982\n",
      "Losgistic Regression(    9500/10000): loss= 145.399659474952\n",
      "Losgistic Regression(    9600/10000): loss= 144.261556330789\n",
      "Losgistic Regression(    9700/10000): loss= 143.44384236041\n",
      "Losgistic Regression(    9800/10000): loss= 142.269784659585\n",
      "Losgistic Regression(    9900/10000): loss= 141.506590650045\n",
      "Time for  1th cross validation = 31.7393s\n",
      "Training Accuracy         =  0.942\n",
      "Cross Validation Accuracy = 0.766416\n",
      "Losgistic Regression(       0/10000): loss= 691.57240023575\n",
      "Losgistic Regression(     100/10000): loss= 522.3280246139\n",
      "Losgistic Regression(     200/10000): loss= 455.1017185204\n",
      "Losgistic Regression(     300/10000): loss= 416.949810672041\n",
      "Losgistic Regression(     400/10000): loss= 394.789986692352\n",
      "Losgistic Regression(     500/10000): loss= 379.883034722865\n",
      "Losgistic Regression(     600/10000): loss= 368.951303021102\n",
      "Losgistic Regression(     700/10000): loss= 360.540932732663\n",
      "Losgistic Regression(     800/10000): loss= 353.760733165435\n",
      "Losgistic Regression(     900/10000): loss= 348.012206075916\n",
      "Losgistic Regression(    1000/10000): loss= 342.971473759466\n",
      "Losgistic Regression(    1100/10000): loss= 338.432995146218\n",
      "Losgistic Regression(    1200/10000): loss= 334.23880349019\n",
      "Losgistic Regression(    1300/10000): loss= 329.472572737733\n",
      "Losgistic Regression(    1400/10000): loss= 324.722990368497\n",
      "Losgistic Regression(    1500/10000): loss= 320.329297259993\n",
      "Losgistic Regression(    1600/10000): loss= 316.223133805894\n",
      "Losgistic Regression(    1700/10000): loss= 312.393597065415\n",
      "Losgistic Regression(    1800/10000): loss= 308.831029816108\n",
      "Losgistic Regression(    1900/10000): loss= 305.485882996708\n",
      "Losgistic Regression(    2000/10000): loss= 302.3503987088\n",
      "Losgistic Regression(    2100/10000): loss= 298.993805100516\n",
      "Losgistic Regression(    2200/10000): loss= 295.769089644622\n",
      "Losgistic Regression(    2300/10000): loss= 292.803092143615\n",
      "Losgistic Regression(    2400/10000): loss= 290.062757727167\n",
      "Losgistic Regression(    2500/10000): loss= 287.045549013827\n",
      "Losgistic Regression(    2600/10000): loss= 284.267328723441\n",
      "Losgistic Regression(    2700/10000): loss= 281.703471361881\n",
      "Losgistic Regression(    2800/10000): loss= 279.338320135151\n",
      "Losgistic Regression(    2900/10000): loss= 277.021710120186\n",
      "Losgistic Regression(    3000/10000): loss= 274.511543696344\n",
      "Losgistic Regression(    3100/10000): loss= 271.882921402247\n",
      "Losgistic Regression(    3200/10000): loss= 269.162823269543\n",
      "Losgistic Regression(    3300/10000): loss= 266.440641637214\n",
      "Losgistic Regression(    3400/10000): loss= 262.790006824236\n",
      "Losgistic Regression(    3500/10000): loss= 258.924294604384\n",
      "Losgistic Regression(    3600/10000): loss= 255.381718253969\n",
      "Losgistic Regression(    3700/10000): loss= 252.079943457861\n",
      "Losgistic Regression(    3800/10000): loss= 249.02419712337\n",
      "Losgistic Regression(    3900/10000): loss= 246.074576917587\n",
      "Losgistic Regression(    4000/10000): loss= 243.223296961639\n",
      "Losgistic Regression(    4100/10000): loss= 240.537718797173\n",
      "Losgistic Regression(    4200/10000): loss= 237.631674740812\n",
      "Losgistic Regression(    4300/10000): loss= 234.755635513742\n",
      "Losgistic Regression(    4400/10000): loss= 231.925513763355\n",
      "Losgistic Regression(    4500/10000): loss= 229.253616923281\n",
      "Losgistic Regression(    4600/10000): loss= 226.447402055704\n",
      "Losgistic Regression(    4700/10000): loss= 223.657050557139\n",
      "Losgistic Regression(    4800/10000): loss= 220.614118224525\n",
      "Losgistic Regression(    4900/10000): loss= 217.708556738757\n",
      "Losgistic Regression(    5000/10000): loss= 214.652978774407\n",
      "Losgistic Regression(    5100/10000): loss= 211.274177851371\n",
      "Losgistic Regression(    5200/10000): loss= 207.927491158412\n",
      "Losgistic Regression(    5300/10000): loss= 204.911346422848\n",
      "Losgistic Regression(    5400/10000): loss= 201.909456959822\n",
      "Losgistic Regression(    5500/10000): loss= 198.981787296472\n",
      "Losgistic Regression(    5600/10000): loss= 196.260321896047\n",
      "Losgistic Regression(    5700/10000): loss= 193.562427909781\n",
      "Losgistic Regression(    5800/10000): loss= 191.137299502182\n",
      "Losgistic Regression(    5900/10000): loss= 188.919400665759\n",
      "Losgistic Regression(    6000/10000): loss= 186.455817256677\n",
      "Losgistic Regression(    6100/10000): loss= 184.178084181417\n",
      "Losgistic Regression(    6200/10000): loss= 181.853685181036\n",
      "Losgistic Regression(    6300/10000): loss= 179.357230430158\n",
      "Losgistic Regression(    6400/10000): loss= 177.092393267145\n",
      "Losgistic Regression(    6500/10000): loss= 174.783455063722\n",
      "Losgistic Regression(    6600/10000): loss= 172.244370002467\n",
      "Losgistic Regression(    6700/10000): loss= 169.643655604917\n",
      "Losgistic Regression(    6800/10000): loss= 166.756946258031\n",
      "Losgistic Regression(    6900/10000): loss= 163.990939267841\n",
      "Losgistic Regression(    7000/10000): loss= 161.168434154879\n",
      "Losgistic Regression(    7100/10000): loss= 158.452363756051\n",
      "Losgistic Regression(    7200/10000): loss= 155.851532764295\n",
      "Losgistic Regression(    7300/10000): loss= 153.319019174719\n",
      "Losgistic Regression(    7400/10000): loss= 151.443252534566\n",
      "Losgistic Regression(    7500/10000): loss= 149.332135120545\n",
      "Losgistic Regression(    7600/10000): loss= 147.46861505095\n",
      "Losgistic Regression(    7700/10000): loss= 145.406304333774\n",
      "Losgistic Regression(    7800/10000): loss= 143.422400428456\n",
      "Losgistic Regression(    7900/10000): loss= 141.68892659423\n",
      "Losgistic Regression(    8000/10000): loss= 139.119422791352\n",
      "Losgistic Regression(    8100/10000): loss= 137.174778601897\n",
      "Losgistic Regression(    8200/10000): loss= 134.91049717903\n",
      "Losgistic Regression(    8300/10000): loss= 132.677110740616\n",
      "Losgistic Regression(    8400/10000): loss= 130.838433914034\n",
      "Losgistic Regression(    8500/10000): loss= 128.992244758392\n",
      "Losgistic Regression(    8600/10000): loss= 127.38099132156\n",
      "Losgistic Regression(    8700/10000): loss= 125.906458658852\n",
      "Losgistic Regression(    8800/10000): loss= 124.554173887375\n",
      "Losgistic Regression(    8900/10000): loss= 123.313735956144\n",
      "Losgistic Regression(    9000/10000): loss= 122.138567260004\n",
      "Losgistic Regression(    9100/10000): loss= 120.563274040693\n",
      "Losgistic Regression(    9200/10000): loss= 119.117678469364\n",
      "Losgistic Regression(    9300/10000): loss= 117.500604161754\n",
      "Losgistic Regression(    9400/10000): loss= 116.032020036175\n",
      "Losgistic Regression(    9500/10000): loss= 114.865035740361\n",
      "Losgistic Regression(    9600/10000): loss= 113.981600431506\n",
      "Losgistic Regression(    9700/10000): loss= 113.027420391703\n",
      "Losgistic Regression(    9800/10000): loss= 112.017859793289\n",
      "Losgistic Regression(    9900/10000): loss= 111.079281862841\n",
      "Time for  2th cross validation = 31.5059s\n",
      "Training Accuracy         =  0.922\n",
      "Cross Validation Accuracy = 0.748204\n",
      "Losgistic Regression(       0/10000): loss= 691.709845438396\n",
      "Losgistic Regression(     100/10000): loss= 513.18547899407\n",
      "Losgistic Regression(     200/10000): loss= 438.113671647637\n",
      "Losgistic Regression(     300/10000): loss= 396.486722460064\n",
      "Losgistic Regression(     400/10000): loss= 372.688909419636\n",
      "Losgistic Regression(     500/10000): loss= 356.040516739411\n",
      "Losgistic Regression(     600/10000): loss= 343.186961678383\n",
      "Losgistic Regression(     700/10000): loss= 332.665466661025\n",
      "Losgistic Regression(     800/10000): loss= 323.317988527156\n",
      "Losgistic Regression(     900/10000): loss= 314.582877438278\n",
      "Losgistic Regression(    1000/10000): loss= 306.702187097838\n",
      "Losgistic Regression(    1100/10000): loss= 299.527255364269\n",
      "Losgistic Regression(    1200/10000): loss= 292.975908902264\n",
      "Losgistic Regression(    1300/10000): loss= 286.211918743835\n",
      "Losgistic Regression(    1400/10000): loss= 279.200669232021\n",
      "Losgistic Regression(    1500/10000): loss= 272.623925807372\n",
      "Losgistic Regression(    1600/10000): loss= 266.469079958813\n",
      "Losgistic Regression(    1700/10000): loss= 260.733832187432\n",
      "Losgistic Regression(    1800/10000): loss= 255.396103883889\n",
      "Losgistic Regression(    1900/10000): loss= 249.837583952657\n",
      "Losgistic Regression(    2000/10000): loss= 244.341301547909\n",
      "Losgistic Regression(    2100/10000): loss= 239.139750506023\n",
      "Losgistic Regression(    2200/10000): loss= 233.49077643669\n",
      "Losgistic Regression(    2300/10000): loss= 227.437467811869\n",
      "Losgistic Regression(    2400/10000): loss= 221.163084899379\n",
      "Losgistic Regression(    2500/10000): loss= 214.601795619728\n",
      "Losgistic Regression(    2600/10000): loss= 208.423477743963\n",
      "Losgistic Regression(    2700/10000): loss= 202.500463377329\n",
      "Losgistic Regression(    2800/10000): loss= 196.781726715077\n",
      "Losgistic Regression(    2900/10000): loss= 190.958810697872\n",
      "Losgistic Regression(    3000/10000): loss= 185.300169295536\n",
      "Losgistic Regression(    3100/10000): loss= 179.915206084456\n",
      "Losgistic Regression(    3200/10000): loss= 174.762892499756\n",
      "Losgistic Regression(    3300/10000): loss= 169.783233540911\n",
      "Losgistic Regression(    3400/10000): loss= 165.015634122336\n",
      "Losgistic Regression(    3500/10000): loss= 160.267789875639\n",
      "Losgistic Regression(    3600/10000): loss= 155.580621972829\n",
      "Losgistic Regression(    3700/10000): loss= 150.802953811562\n",
      "Losgistic Regression(    3800/10000): loss= 146.057768143535\n",
      "Losgistic Regression(    3900/10000): loss= 141.27382166979\n",
      "Losgistic Regression(    4000/10000): loss= 136.279485140321\n",
      "Losgistic Regression(    4100/10000): loss= 131.571803053647\n",
      "Losgistic Regression(    4200/10000): loss= 126.851657943605\n",
      "Losgistic Regression(    4300/10000): loss= 122.149052137307\n",
      "Losgistic Regression(    4400/10000): loss= 117.377688351326\n",
      "Losgistic Regression(    4500/10000): loss= 112.52347612831\n",
      "Losgistic Regression(    4600/10000): loss= 107.613416833\n",
      "Losgistic Regression(    4700/10000): loss= 102.87254963162\n",
      "Losgistic Regression(    4800/10000): loss= 98.2552415445198\n",
      "Losgistic Regression(    4900/10000): loss= 93.7868057201029\n",
      "Losgistic Regression(    5000/10000): loss= 89.3375548713202\n",
      "Losgistic Regression(    5100/10000): loss= 85.0412941667404\n",
      "Losgistic Regression(    5200/10000): loss= 80.7789699842131\n",
      "Losgistic Regression(    5300/10000): loss= 76.3464369545916\n",
      "Losgistic Regression(    5400/10000): loss= 71.9125121009147\n",
      "Losgistic Regression(    5500/10000): loss= 67.3281286052122\n",
      "Losgistic Regression(    5600/10000): loss= 62.9944471379609\n",
      "Losgistic Regression(    5700/10000): loss= 58.6915932095603\n",
      "Losgistic Regression(    5800/10000): loss= 54.2785372198886\n",
      "Losgistic Regression(    5900/10000): loss= 49.8406967493743\n",
      "Losgistic Regression(    6000/10000): loss= 45.6238942974568\n",
      "Losgistic Regression(    6100/10000): loss= 41.5572605062966\n",
      "Losgistic Regression(    6200/10000): loss= 37.6163254271631\n",
      "Losgistic Regression(    6300/10000): loss= 33.8748985282999\n",
      "Losgistic Regression(    6400/10000): loss= 29.9976297912094\n",
      "Losgistic Regression(    6500/10000): loss= 26.2618868792632\n",
      "Losgistic Regression(    6600/10000): loss= 22.6950518323911\n",
      "Losgistic Regression(    6700/10000): loss= 19.1566930017364\n",
      "Losgistic Regression(    6800/10000): loss= 15.843822958672\n",
      "Losgistic Regression(    6900/10000): loss= 12.5525668438538\n",
      "Losgistic Regression(    7000/10000): loss= 8.98544332371912\n",
      "Losgistic Regression(    7100/10000): loss= 5.34957674338834\n",
      "Losgistic Regression(    7200/10000): loss= 1.47335847732644\n",
      "Losgistic Regression(    7300/10000): loss=-2.7087280785046\n",
      "Losgistic Regression(    7400/10000): loss=-6.84299947075709\n",
      "Losgistic Regression(    7500/10000): loss=-11.2859759424812\n",
      "Losgistic Regression(    7600/10000): loss=-15.9607462972018\n",
      "Losgistic Regression(    7700/10000): loss=-20.7212797833765\n",
      "Losgistic Regression(    7800/10000): loss=-25.4672935998032\n",
      "Losgistic Regression(    7900/10000): loss=-29.7512786869687\n",
      "Losgistic Regression(    8000/10000): loss=-33.9875382284193\n",
      "Losgistic Regression(    8100/10000): loss=-37.2512441721085\n",
      "Losgistic Regression(    8200/10000): loss=-39.4796369725486\n",
      "Losgistic Regression(    8300/10000): loss=-41.0374088489394\n",
      "Losgistic Regression(    8400/10000): loss=-41.7536967653283\n",
      "Losgistic Regression(    8500/10000): loss=-42.8470681018213\n",
      "Losgistic Regression(    8600/10000): loss=-44.0468623337075\n",
      "Losgistic Regression(    8700/10000): loss=-45.4175186743748\n",
      "Losgistic Regression(    8800/10000): loss=-47.0234043274009\n",
      "Losgistic Regression(    8900/10000): loss=-48.9406767334172\n",
      "Losgistic Regression(    9000/10000): loss=-51.9175479783753\n",
      "Losgistic Regression(    9100/10000): loss=-54.676396557335\n",
      "Losgistic Regression(    9200/10000): loss=-56.7891206724353\n",
      "Losgistic Regression(    9300/10000): loss=-58.6670634836654\n",
      "Losgistic Regression(    9400/10000): loss=-60.4626891002775\n",
      "Losgistic Regression(    9500/10000): loss=-62.0049875649644\n",
      "Losgistic Regression(    9600/10000): loss=-63.9886755512337\n",
      "Losgistic Regression(    9700/10000): loss=-65.344520353103\n",
      "Losgistic Regression(    9800/10000): loss=-66.8168232002767\n",
      "Losgistic Regression(    9900/10000): loss=-68.2687694114034\n",
      "Time for  3th cross validation = 31.4016s\n",
      "Training Accuracy         =  0.935\n",
      "Cross Validation Accuracy = 0.750784\n",
      "Losgistic Regression(       0/10000): loss= 692.023749625848\n",
      "Losgistic Regression(     100/10000): loss= 518.928494467493\n",
      "Losgistic Regression(     200/10000): loss= 444.18496787749\n",
      "Losgistic Regression(     300/10000): loss= 403.236032203022\n",
      "Losgistic Regression(     400/10000): loss= 379.065324591848\n",
      "Losgistic Regression(     500/10000): loss= 362.276603351915\n",
      "Losgistic Regression(     600/10000): loss= 349.234807751679\n",
      "Losgistic Regression(     700/10000): loss= 338.433774929878\n",
      "Losgistic Regression(     800/10000): loss= 329.231638354003\n",
      "Losgistic Regression(     900/10000): loss= 321.248800515243\n",
      "Losgistic Regression(    1000/10000): loss= 314.184461007832\n",
      "Losgistic Regression(    1100/10000): loss= 307.934918336977\n",
      "Losgistic Regression(    1200/10000): loss= 302.375327789198\n",
      "Losgistic Regression(    1300/10000): loss= 297.369263365155\n",
      "Losgistic Regression(    1400/10000): loss= 292.866736443301\n",
      "Losgistic Regression(    1500/10000): loss= 288.772802587705\n",
      "Losgistic Regression(    1600/10000): loss= 285.036322105766\n",
      "Losgistic Regression(    1700/10000): loss= 281.602101197492\n",
      "Losgistic Regression(    1800/10000): loss= 278.439464553167\n",
      "Losgistic Regression(    1900/10000): loss= 275.519669221577\n",
      "Losgistic Regression(    2000/10000): loss= 271.967276302953\n",
      "Losgistic Regression(    2100/10000): loss= 268.549102438679\n",
      "Losgistic Regression(    2200/10000): loss= 265.28851041105\n",
      "Losgistic Regression(    2300/10000): loss= 262.155985051147\n",
      "Losgistic Regression(    2400/10000): loss= 259.162368808805\n",
      "Losgistic Regression(    2500/10000): loss= 256.300545744645\n",
      "Losgistic Regression(    2600/10000): loss= 253.570888369065\n",
      "Losgistic Regression(    2700/10000): loss= 250.950933826793\n",
      "Losgistic Regression(    2800/10000): loss= 248.407177271458\n",
      "Losgistic Regression(    2900/10000): loss= 245.950246853446\n",
      "Losgistic Regression(    3000/10000): loss= 243.599581919308\n",
      "Losgistic Regression(    3100/10000): loss= 241.352823545141\n",
      "Losgistic Regression(    3200/10000): loss= 239.214162508565\n",
      "Losgistic Regression(    3300/10000): loss= 237.026010812861\n",
      "Losgistic Regression(    3400/10000): loss= 234.494882139032\n",
      "Losgistic Regression(    3500/10000): loss= 232.089702239787\n",
      "Losgistic Regression(    3600/10000): loss= 229.764671954467\n",
      "Losgistic Regression(    3700/10000): loss= 227.502316594692\n",
      "Losgistic Regression(    3800/10000): loss= 225.318124817185\n",
      "Losgistic Regression(    3900/10000): loss= 223.327532873127\n",
      "Losgistic Regression(    4000/10000): loss= 221.418873949319\n",
      "Losgistic Regression(    4100/10000): loss= 219.702456772719\n",
      "Losgistic Regression(    4200/10000): loss= 218.057333368323\n",
      "Losgistic Regression(    4300/10000): loss= 216.516286873637\n",
      "Losgistic Regression(    4400/10000): loss= 214.747303773547\n",
      "Losgistic Regression(    4500/10000): loss= 212.970927237334\n",
      "Losgistic Regression(    4600/10000): loss= 211.297102282578\n",
      "Losgistic Regression(    4700/10000): loss= 209.817657149832\n",
      "Losgistic Regression(    4800/10000): loss= 208.397495197035\n",
      "Losgistic Regression(    4900/10000): loss= 207.073660761524\n",
      "Losgistic Regression(    5000/10000): loss= 205.630844985425\n",
      "Losgistic Regression(    5100/10000): loss= 204.110924975235\n",
      "Losgistic Regression(    5200/10000): loss= 202.759685164884\n",
      "Losgistic Regression(    5300/10000): loss= 201.501649462901\n",
      "Losgistic Regression(    5400/10000): loss= 200.164871160488\n",
      "Losgistic Regression(    5500/10000): loss= 198.711158643346\n",
      "Losgistic Regression(    5600/10000): loss= 197.370975092424\n",
      "Losgistic Regression(    5700/10000): loss= 196.065554713096\n",
      "Losgistic Regression(    5800/10000): loss= 194.742156167595\n",
      "Losgistic Regression(    5900/10000): loss= 193.616123385567\n",
      "Losgistic Regression(    6000/10000): loss= 192.437334787434\n",
      "Losgistic Regression(    6100/10000): loss= 191.308683998825\n",
      "Losgistic Regression(    6200/10000): loss= 190.182432724491\n",
      "Losgistic Regression(    6300/10000): loss= 188.991833005858\n",
      "Losgistic Regression(    6400/10000): loss= 187.970934193364\n",
      "Losgistic Regression(    6500/10000): loss= 186.961592293149\n",
      "Losgistic Regression(    6600/10000): loss= 185.915084307962\n",
      "Losgistic Regression(    6700/10000): loss= 185.030358004297\n",
      "Losgistic Regression(    6800/10000): loss= 184.033377022178\n",
      "Losgistic Regression(    6900/10000): loss= 183.021864919481\n",
      "Losgistic Regression(    7000/10000): loss= 182.084916324606\n",
      "Losgistic Regression(    7100/10000): loss= 181.153757251248\n",
      "Losgistic Regression(    7200/10000): loss= 180.156670372923\n",
      "Losgistic Regression(    7300/10000): loss= 179.104218018704\n",
      "Losgistic Regression(    7400/10000): loss= 178.167643842494\n",
      "Losgistic Regression(    7500/10000): loss= 177.140299774068\n",
      "Losgistic Regression(    7600/10000): loss= 176.252853321551\n",
      "Losgistic Regression(    7700/10000): loss= 175.510689716845\n",
      "Losgistic Regression(    7800/10000): loss= 174.794587465603\n",
      "Losgistic Regression(    7900/10000): loss= 173.79988388115\n",
      "Losgistic Regression(    8000/10000): loss= 172.805467756969\n",
      "Losgistic Regression(    8100/10000): loss= 171.866104911566\n",
      "Losgistic Regression(    8200/10000): loss= 171.089220037254\n",
      "Losgistic Regression(    8300/10000): loss= 170.557563709628\n",
      "Losgistic Regression(    8400/10000): loss= 169.646408906589\n",
      "Losgistic Regression(    8500/10000): loss= 168.944878934117\n",
      "Losgistic Regression(    8600/10000): loss= 168.494894094389\n",
      "Losgistic Regression(    8700/10000): loss= 167.811703837284\n",
      "Losgistic Regression(    8800/10000): loss= 167.719813111308\n",
      "Losgistic Regression(    8900/10000): loss= 167.72143420652\n",
      "Totoal number of iterations =  8900\n",
      "Loss                        =  167.721434207\n",
      "Time for  4th cross validation = 28.2543s\n",
      "Training Accuracy         =  0.932\n",
      "Cross Validation Accuracy = 0.75098\n",
      "*************** ([0.93100000000000005, 0.94199999999999995, 0.92200000000000004, 0.93500000000000005, 0.93200000000000005], [0.75012000000000001, 0.76641599999999999, 0.74820399999999998, 0.75078400000000001, 0.75097999999999998])\n",
      "Losgistic Regression(       0/10000): loss= 692.032255406233\n",
      "Losgistic Regression(     100/10000): loss= 534.231981715536\n",
      "Losgistic Regression(     200/10000): loss= 458.488804914881\n",
      "Losgistic Regression(     300/10000): loss= 418.478337083546\n",
      "Losgistic Regression(     400/10000): loss= 396.693045577261\n",
      "Losgistic Regression(     500/10000): loss= 382.445401825215\n",
      "Losgistic Regression(     600/10000): loss= 372.112876291553\n",
      "Losgistic Regression(     700/10000): loss= 364.354636246018\n",
      "Losgistic Regression(     800/10000): loss= 358.323726818357\n",
      "Losgistic Regression(     900/10000): loss= 353.555137291435\n",
      "Losgistic Regression(    1000/10000): loss= 349.780587243687\n",
      "Losgistic Regression(    1100/10000): loss= 346.774118892124\n",
      "Losgistic Regression(    1200/10000): loss= 344.298331505\n",
      "Losgistic Regression(    1300/10000): loss= 342.178251339044\n",
      "Losgistic Regression(    1400/10000): loss= 340.366278787606\n",
      "Losgistic Regression(    1500/10000): loss= 338.720331424221\n",
      "Losgistic Regression(    1600/10000): loss= 337.246947127112\n",
      "Losgistic Regression(    1700/10000): loss= 335.948275347584\n",
      "Losgistic Regression(    1800/10000): loss= 334.801292577518\n",
      "Losgistic Regression(    1900/10000): loss= 333.820134786438\n",
      "Losgistic Regression(    2000/10000): loss= 332.886612435506\n",
      "Losgistic Regression(    2100/10000): loss= 332.045121611211\n",
      "Losgistic Regression(    2200/10000): loss= 331.304012439164\n",
      "Losgistic Regression(    2300/10000): loss= 330.585180002578\n",
      "Losgistic Regression(    2400/10000): loss= 329.991722128055\n",
      "Losgistic Regression(    2500/10000): loss= 329.446080937098\n",
      "Losgistic Regression(    2600/10000): loss= 328.889100909345\n",
      "Losgistic Regression(    2700/10000): loss= 328.348734000623\n",
      "Losgistic Regression(    2800/10000): loss= 327.872028133184\n",
      "Losgistic Regression(    2900/10000): loss= 327.405613395757\n",
      "Losgistic Regression(    3000/10000): loss= 326.956811068551\n",
      "Losgistic Regression(    3100/10000): loss= 326.531931512069\n",
      "Losgistic Regression(    3200/10000): loss= 326.124064523627\n",
      "Losgistic Regression(    3300/10000): loss= 325.753781257206\n",
      "Losgistic Regression(    3400/10000): loss= 325.346544547721\n",
      "Losgistic Regression(    3500/10000): loss= 324.998576366831\n",
      "Losgistic Regression(    3600/10000): loss= 324.719537239694\n",
      "Losgistic Regression(    3700/10000): loss= 324.449438539868\n",
      "Losgistic Regression(    3800/10000): loss= 324.166473213142\n",
      "Losgistic Regression(    3900/10000): loss= 323.873018017512\n",
      "Losgistic Regression(    4000/10000): loss= 323.583305073471\n",
      "Losgistic Regression(    4100/10000): loss= 323.369294717379\n",
      "Losgistic Regression(    4200/10000): loss= 323.183430306701\n",
      "Losgistic Regression(    4300/10000): loss= 322.945349407153\n",
      "Losgistic Regression(    4400/10000): loss= 322.724370155478\n",
      "Losgistic Regression(    4500/10000): loss= 322.524371154427\n",
      "Losgistic Regression(    4600/10000): loss= 322.351729658125\n",
      "Losgistic Regression(    4700/10000): loss= 322.2340812345\n",
      "Losgistic Regression(    4800/10000): loss= 322.092547102525\n",
      "Losgistic Regression(    4900/10000): loss= 321.934433533132\n",
      "Losgistic Regression(    5000/10000): loss= 321.79473653035\n",
      "Losgistic Regression(    5100/10000): loss= 321.660557266327\n",
      "Losgistic Regression(    5200/10000): loss= 321.549828086128\n",
      "Losgistic Regression(    5300/10000): loss= 321.398814487574\n",
      "Losgistic Regression(    5400/10000): loss= 321.270649961894\n",
      "Losgistic Regression(    5500/10000): loss= 321.170820868545\n",
      "Losgistic Regression(    5600/10000): loss= 321.065411999317\n",
      "Losgistic Regression(    5700/10000): loss= 320.932758288263\n",
      "Losgistic Regression(    5800/10000): loss= 320.835011511102\n",
      "Losgistic Regression(    5900/10000): loss= 320.738278751875\n",
      "Losgistic Regression(    6000/10000): loss= 320.652946713557\n",
      "Losgistic Regression(    6100/10000): loss= 320.572703955006\n",
      "Losgistic Regression(    6200/10000): loss= 320.548849710593\n",
      "Losgistic Regression(    6300/10000): loss= 320.514065028909\n",
      "Losgistic Regression(    6400/10000): loss= 320.477423659802\n",
      "Losgistic Regression(    6500/10000): loss= 320.450901550792\n",
      "Losgistic Regression(    6600/10000): loss= 320.423503209808\n",
      "Losgistic Regression(    6700/10000): loss= 320.403268819428\n",
      "Losgistic Regression(    6800/10000): loss= 320.381326445407\n",
      "Losgistic Regression(    6900/10000): loss= 320.360618118669\n",
      "Losgistic Regression(    7000/10000): loss= 320.341115726658\n",
      "Losgistic Regression(    7100/10000): loss= 320.320785081273\n",
      "Losgistic Regression(    7200/10000): loss= 320.302116009261\n",
      "Losgistic Regression(    7300/10000): loss= 320.29062290848\n",
      "Losgistic Regression(    7400/10000): loss= 320.27157334695\n",
      "Losgistic Regression(    7500/10000): loss= 320.258379929982\n",
      "Losgistic Regression(    7600/10000): loss= 320.241407066846\n",
      "Losgistic Regression(    7700/10000): loss= 320.225034486024\n",
      "Losgistic Regression(    7800/10000): loss= 320.207657293703\n",
      "Losgistic Regression(    7900/10000): loss= 320.191797921006\n",
      "Losgistic Regression(    8000/10000): loss= 320.174479989311\n",
      "Losgistic Regression(    8100/10000): loss= 320.157676201824\n",
      "Losgistic Regression(    8200/10000): loss= 320.13988371059\n",
      "Losgistic Regression(    8300/10000): loss= 320.122773543295\n",
      "Losgistic Regression(    8400/10000): loss= 320.104293927842\n",
      "Losgistic Regression(    8500/10000): loss= 320.08636221388\n",
      "Losgistic Regression(    8600/10000): loss= 320.067481973039\n",
      "Losgistic Regression(    8700/10000): loss= 320.048214206586\n",
      "Losgistic Regression(    8800/10000): loss= 320.028579624796\n",
      "Losgistic Regression(    8900/10000): loss= 320.008857481932\n",
      "Losgistic Regression(    9000/10000): loss= 319.988929708572\n",
      "Losgistic Regression(    9100/10000): loss= 319.968946735816\n",
      "Losgistic Regression(    9200/10000): loss= 319.948955720924\n",
      "Losgistic Regression(    9300/10000): loss= 319.92814064701\n",
      "Losgistic Regression(    9400/10000): loss= 319.907327359879\n",
      "Losgistic Regression(    9500/10000): loss= 319.886137743761\n",
      "Losgistic Regression(    9600/10000): loss= 319.865051759649\n",
      "Losgistic Regression(    9700/10000): loss= 319.843602220721\n",
      "Losgistic Regression(    9800/10000): loss= 319.821965048144\n",
      "Losgistic Regression(    9900/10000): loss= 319.800250231441\n",
      "Time for  0th cross validation = 31.7513s\n",
      "Training Accuracy         =  0.896\n",
      "Cross Validation Accuracy = 0.769864\n",
      "Losgistic Regression(       0/10000): loss= 691.974722846127\n",
      "Losgistic Regression(     100/10000): loss= 522.475039229994\n",
      "Losgistic Regression(     200/10000): loss= 444.448789843303\n",
      "Losgistic Regression(     300/10000): loss= 401.772520281694\n",
      "Losgistic Regression(     400/10000): loss= 379.335216004559\n",
      "Losgistic Regression(     500/10000): loss= 365.391333549916\n",
      "Losgistic Regression(     600/10000): loss= 355.358882807224\n",
      "Losgistic Regression(     700/10000): loss= 347.573004839934\n",
      "Losgistic Regression(     800/10000): loss= 341.368582655993\n",
      "Losgistic Regression(     900/10000): loss= 336.219379890494\n",
      "Losgistic Regression(    1000/10000): loss= 331.795088411237\n",
      "Losgistic Regression(    1100/10000): loss= 327.957625917355\n",
      "Losgistic Regression(    1200/10000): loss= 324.532214515065\n",
      "Losgistic Regression(    1300/10000): loss= 321.532086571185\n",
      "Losgistic Regression(    1400/10000): loss= 318.944312794909\n",
      "Losgistic Regression(    1500/10000): loss= 316.715174713799\n",
      "Losgistic Regression(    1600/10000): loss= 314.777949765123\n",
      "Losgistic Regression(    1700/10000): loss= 313.155143320317\n",
      "Losgistic Regression(    1800/10000): loss= 311.680652570245\n",
      "Losgistic Regression(    1900/10000): loss= 310.356242967633\n",
      "Losgistic Regression(    2000/10000): loss= 309.161373395717\n",
      "Losgistic Regression(    2100/10000): loss= 307.803462886884\n",
      "Losgistic Regression(    2200/10000): loss= 306.56367261448\n",
      "Losgistic Regression(    2300/10000): loss= 305.365756402516\n",
      "Losgistic Regression(    2400/10000): loss= 304.453981583725\n",
      "Losgistic Regression(    2500/10000): loss= 303.57811996477\n",
      "Losgistic Regression(    2600/10000): loss= 302.710486793369\n",
      "Losgistic Regression(    2700/10000): loss= 301.85373679806\n",
      "Losgistic Regression(    2800/10000): loss= 301.066744875463\n",
      "Losgistic Regression(    2900/10000): loss= 300.429128625458\n",
      "Losgistic Regression(    3000/10000): loss= 299.921645216489\n",
      "Losgistic Regression(    3100/10000): loss= 299.49384677251\n",
      "Losgistic Regression(    3200/10000): loss= 299.138149685189\n",
      "Losgistic Regression(    3300/10000): loss= 298.664472162587\n",
      "Losgistic Regression(    3400/10000): loss= 298.167879511231\n",
      "Losgistic Regression(    3500/10000): loss= 297.753877139329\n",
      "Losgistic Regression(    3600/10000): loss= 297.507378267884\n",
      "Losgistic Regression(    3700/10000): loss= 297.406722172726\n",
      "Losgistic Regression(    3800/10000): loss= 297.238118464946\n",
      "Losgistic Regression(    3900/10000): loss= 297.017473567856\n",
      "Losgistic Regression(    4000/10000): loss= 296.82295089888\n",
      "Losgistic Regression(    4100/10000): loss= 296.581836664892\n",
      "Losgistic Regression(    4200/10000): loss= 296.328487653898\n",
      "Losgistic Regression(    4300/10000): loss= 296.130194360193\n",
      "Losgistic Regression(    4400/10000): loss= 295.920015489599\n",
      "Losgistic Regression(    4500/10000): loss= 295.842270171205\n",
      "Losgistic Regression(    4600/10000): loss= 295.724756472072\n",
      "Losgistic Regression(    4700/10000): loss= 295.463648328598\n",
      "Losgistic Regression(    4800/10000): loss= 295.204644392223\n",
      "Losgistic Regression(    4900/10000): loss= 294.827259765093\n",
      "Losgistic Regression(    5000/10000): loss= 294.683734782165\n",
      "Losgistic Regression(    5100/10000): loss= 294.462799205173\n",
      "Losgistic Regression(    5200/10000): loss= 294.250088433225\n",
      "Losgistic Regression(    5300/10000): loss= 294.178423289862\n",
      "Losgistic Regression(    5400/10000): loss= 294.09683835401\n",
      "Losgistic Regression(    5500/10000): loss= 294.096617457925\n",
      "Totoal number of iterations =  5500\n",
      "Loss                        =  294.096617458\n",
      "Time for  1th cross validation = 17.3121s\n",
      "Training Accuracy         =  0.917\n",
      "Cross Validation Accuracy = 0.774824\n",
      "Losgistic Regression(       0/10000): loss= 691.581777440834\n",
      "Losgistic Regression(     100/10000): loss= 525.224443542299\n",
      "Losgistic Regression(     200/10000): loss= 460.944340249038\n",
      "Losgistic Regression(     300/10000): loss= 426.023657659818\n",
      "Losgistic Regression(     400/10000): loss= 406.901976707294\n",
      "Losgistic Regression(     500/10000): loss= 394.821526485095\n",
      "Losgistic Regression(     600/10000): loss= 386.499374689415\n",
      "Losgistic Regression(     700/10000): loss= 380.501600263081\n",
      "Losgistic Regression(     800/10000): loss= 375.939110786759\n",
      "Losgistic Regression(     900/10000): loss= 372.338331034583\n",
      "Losgistic Regression(    1000/10000): loss= 369.289034619688\n",
      "Losgistic Regression(    1100/10000): loss= 366.654419584399\n",
      "Losgistic Regression(    1200/10000): loss= 364.279787160653\n",
      "Losgistic Regression(    1300/10000): loss= 362.175629747179\n",
      "Losgistic Regression(    1400/10000): loss= 360.245666212581\n",
      "Losgistic Regression(    1500/10000): loss= 358.486087572493\n",
      "Losgistic Regression(    1600/10000): loss= 356.872428285404\n",
      "Losgistic Regression(    1700/10000): loss= 355.360155688533\n",
      "Losgistic Regression(    1800/10000): loss= 353.949468153265\n",
      "Losgistic Regression(    1900/10000): loss= 352.614253787241\n",
      "Losgistic Regression(    2000/10000): loss= 351.365129562728\n",
      "Losgistic Regression(    2100/10000): loss= 350.206210387538\n",
      "Losgistic Regression(    2200/10000): loss= 349.173055960688\n",
      "Losgistic Regression(    2300/10000): loss= 348.222919846499\n",
      "Losgistic Regression(    2400/10000): loss= 347.309102843668\n",
      "Losgistic Regression(    2500/10000): loss= 346.48161112424\n",
      "Losgistic Regression(    2600/10000): loss= 345.692072913433\n",
      "Losgistic Regression(    2700/10000): loss= 345.011878695374\n",
      "Losgistic Regression(    2800/10000): loss= 344.359728564301\n",
      "Losgistic Regression(    2900/10000): loss= 343.755548707254\n",
      "Losgistic Regression(    3000/10000): loss= 343.196094216944\n",
      "Losgistic Regression(    3100/10000): loss= 342.704716746405\n",
      "Losgistic Regression(    3200/10000): loss= 342.238528246215\n",
      "Losgistic Regression(    3300/10000): loss= 341.795345662318\n",
      "Losgistic Regression(    3400/10000): loss= 341.405224767507\n",
      "Losgistic Regression(    3500/10000): loss= 341.103696253479\n",
      "Losgistic Regression(    3600/10000): loss= 340.817895509555\n",
      "Losgistic Regression(    3700/10000): loss= 340.528196953701\n",
      "Losgistic Regression(    3800/10000): loss= 340.244359998414\n",
      "Losgistic Regression(    3900/10000): loss= 339.984392269993\n",
      "Losgistic Regression(    4000/10000): loss= 339.809054347403\n",
      "Losgistic Regression(    4100/10000): loss= 339.651968773185\n",
      "Losgistic Regression(    4200/10000): loss= 339.480208844479\n",
      "Losgistic Regression(    4300/10000): loss= 339.338058261598\n",
      "Losgistic Regression(    4400/10000): loss= 339.181513039707\n",
      "Losgistic Regression(    4500/10000): loss= 339.021452916946\n",
      "Losgistic Regression(    4600/10000): loss= 338.876339453282\n",
      "Losgistic Regression(    4700/10000): loss= 338.750540306106\n",
      "Losgistic Regression(    4800/10000): loss= 338.618071592883\n",
      "Losgistic Regression(    4900/10000): loss= 338.517049180327\n",
      "Losgistic Regression(    5000/10000): loss= 338.422909063392\n",
      "Losgistic Regression(    5100/10000): loss= 338.337089290885\n",
      "Losgistic Regression(    5200/10000): loss= 338.2680424741\n",
      "Losgistic Regression(    5300/10000): loss= 338.194593402026\n",
      "Losgistic Regression(    5400/10000): loss= 338.102071109984\n",
      "Losgistic Regression(    5500/10000): loss= 337.985450646245\n",
      "Losgistic Regression(    5600/10000): loss= 337.931499817028\n",
      "Losgistic Regression(    5700/10000): loss= 337.878108015133\n",
      "Losgistic Regression(    5800/10000): loss= 337.809112154259\n",
      "Losgistic Regression(    5900/10000): loss= 337.742963631278\n",
      "Losgistic Regression(    6000/10000): loss= 337.678940780909\n",
      "Losgistic Regression(    6100/10000): loss= 337.476823550743\n",
      "Losgistic Regression(    6200/10000): loss= 337.31698202061\n",
      "Losgistic Regression(    6300/10000): loss= 337.112370170368\n",
      "Losgistic Regression(    6400/10000): loss= 336.900985416512\n",
      "Losgistic Regression(    6500/10000): loss= 336.809119064207\n",
      "Losgistic Regression(    6600/10000): loss= 336.670020548582\n",
      "Losgistic Regression(    6700/10000): loss= 336.579794918005\n",
      "Losgistic Regression(    6800/10000): loss= 336.57965062587\n",
      "Totoal number of iterations =  6800\n",
      "Loss                        =  336.579650626\n",
      "Time for  2th cross validation = 21.1029s\n",
      "Training Accuracy         =  0.889\n",
      "Cross Validation Accuracy = 0.761128\n",
      "Losgistic Regression(       0/10000): loss= 691.718619809344\n",
      "Losgistic Regression(     100/10000): loss= 516.354540217655\n",
      "Losgistic Regression(     200/10000): loss= 444.491644690766\n",
      "Losgistic Regression(     300/10000): loss= 406.522698785679\n",
      "Losgistic Regression(     400/10000): loss= 386.047190091791\n",
      "Losgistic Regression(     500/10000): loss= 372.390509889713\n",
      "Losgistic Regression(     600/10000): loss= 362.384037147815\n",
      "Losgistic Regression(     700/10000): loss= 354.587639385707\n",
      "Losgistic Regression(     800/10000): loss= 348.317443595209\n",
      "Losgistic Regression(     900/10000): loss= 343.203245064006\n",
      "Losgistic Regression(    1000/10000): loss= 338.651792698438\n",
      "Losgistic Regression(    1100/10000): loss= 333.657266833092\n",
      "Losgistic Regression(    1200/10000): loss= 329.47024948725\n",
      "Losgistic Regression(    1300/10000): loss= 326.041457335334\n",
      "Losgistic Regression(    1400/10000): loss= 322.990937739946\n",
      "Losgistic Regression(    1500/10000): loss= 320.082375057625\n",
      "Losgistic Regression(    1600/10000): loss= 317.260937673483\n",
      "Losgistic Regression(    1700/10000): loss= 313.901367177045\n",
      "Losgistic Regression(    1800/10000): loss= 310.627542512757\n",
      "Losgistic Regression(    1900/10000): loss= 307.772790763341\n",
      "Losgistic Regression(    2000/10000): loss= 305.493425249825\n",
      "Losgistic Regression(    2100/10000): loss= 303.154430781117\n",
      "Losgistic Regression(    2200/10000): loss= 300.774909152536\n",
      "Losgistic Regression(    2300/10000): loss= 298.708089041311\n",
      "Losgistic Regression(    2400/10000): loss= 296.95081412536\n",
      "Losgistic Regression(    2500/10000): loss= 295.356253303284\n",
      "Losgistic Regression(    2600/10000): loss= 293.911615130704\n",
      "Losgistic Regression(    2700/10000): loss= 292.675754611072\n",
      "Losgistic Regression(    2800/10000): loss= 291.429001097741\n",
      "Losgistic Regression(    2900/10000): loss= 289.927399496676\n",
      "Losgistic Regression(    3000/10000): loss= 288.546364328322\n",
      "Losgistic Regression(    3100/10000): loss= 287.119038180163\n",
      "Losgistic Regression(    3200/10000): loss= 285.790019774212\n",
      "Losgistic Regression(    3300/10000): loss= 284.871861383959\n",
      "Losgistic Regression(    3400/10000): loss= 284.291199259497\n",
      "Losgistic Regression(    3500/10000): loss= 283.830335148482\n",
      "Losgistic Regression(    3600/10000): loss= 282.989068721755\n",
      "Losgistic Regression(    3700/10000): loss= 281.791463850898\n",
      "Losgistic Regression(    3800/10000): loss= 280.713385882978\n",
      "Losgistic Regression(    3900/10000): loss= 279.794972678856\n",
      "Losgistic Regression(    4000/10000): loss= 279.529346056696\n",
      "Losgistic Regression(    4100/10000): loss= 279.471295414668\n",
      "Losgistic Regression(    4200/10000): loss= 279.474435624202\n",
      "Totoal number of iterations =  4200\n",
      "Loss                        =  279.474435624\n",
      "Time for  3th cross validation = 13.0744s\n",
      "Training Accuracy         =  0.898\n",
      "Cross Validation Accuracy = 0.769624\n",
      "Losgistic Regression(       0/10000): loss= 692.031729449412\n",
      "Losgistic Regression(     100/10000): loss= 522.026768029092\n",
      "Losgistic Regression(     200/10000): loss= 450.872163720412\n",
      "Losgistic Regression(     300/10000): loss= 413.320407782111\n",
      "Losgistic Regression(     400/10000): loss= 392.318139623993\n",
      "Losgistic Regression(     500/10000): loss= 378.45151255742\n",
      "Losgistic Regression(     600/10000): loss= 368.134237852003\n",
      "Losgistic Regression(     700/10000): loss= 359.964325864828\n",
      "Losgistic Regression(     800/10000): loss= 353.278782171325\n",
      "Losgistic Regression(     900/10000): loss= 347.688750405911\n",
      "Losgistic Regression(    1000/10000): loss= 343.015547533504\n",
      "Losgistic Regression(    1100/10000): loss= 339.069174687845\n",
      "Losgistic Regression(    1200/10000): loss= 335.654989855544\n",
      "Losgistic Regression(    1300/10000): loss= 332.783764953506\n",
      "Losgistic Regression(    1400/10000): loss= 330.260190485618\n",
      "Losgistic Regression(    1500/10000): loss= 328.010502983914\n",
      "Losgistic Regression(    1600/10000): loss= 326.062116076466\n",
      "Losgistic Regression(    1700/10000): loss= 324.340001401824\n",
      "Losgistic Regression(    1800/10000): loss= 322.820968758989\n",
      "Losgistic Regression(    1900/10000): loss= 321.440824935824\n",
      "Losgistic Regression(    2000/10000): loss= 320.304616731818\n",
      "Losgistic Regression(    2100/10000): loss= 319.24563502622\n",
      "Losgistic Regression(    2200/10000): loss= 318.269096500668\n",
      "Losgistic Regression(    2300/10000): loss= 317.397716736142\n",
      "Losgistic Regression(    2400/10000): loss= 316.621562037852\n",
      "Losgistic Regression(    2500/10000): loss= 315.895400889335\n",
      "Losgistic Regression(    2600/10000): loss= 315.264600161863\n",
      "Losgistic Regression(    2700/10000): loss= 314.700343766577\n",
      "Losgistic Regression(    2800/10000): loss= 314.204789468267\n",
      "Losgistic Regression(    2900/10000): loss= 313.759899902487\n",
      "Losgistic Regression(    3000/10000): loss= 313.34676073884\n",
      "Losgistic Regression(    3100/10000): loss= 312.900612752859\n",
      "Losgistic Regression(    3200/10000): loss= 312.188557767265\n",
      "Losgistic Regression(    3300/10000): loss= 311.412905663438\n",
      "Losgistic Regression(    3400/10000): loss= 310.788570643796\n",
      "Losgistic Regression(    3500/10000): loss= 310.081534982376\n",
      "Losgistic Regression(    3600/10000): loss= 309.451795016364\n",
      "Losgistic Regression(    3700/10000): loss= 308.910717688913\n",
      "Losgistic Regression(    3800/10000): loss= 308.586600642724\n",
      "Losgistic Regression(    3900/10000): loss= 308.285848965814\n",
      "Losgistic Regression(    4000/10000): loss= 307.978860058173\n",
      "Losgistic Regression(    4100/10000): loss= 307.57997489017\n",
      "Losgistic Regression(    4200/10000): loss= 307.140155936106\n",
      "Losgistic Regression(    4300/10000): loss= 306.702765751222\n",
      "Losgistic Regression(    4400/10000): loss= 306.410602990473\n",
      "Losgistic Regression(    4500/10000): loss= 306.120892210608\n",
      "Losgistic Regression(    4600/10000): loss= 305.89322179169\n",
      "Losgistic Regression(    4700/10000): loss= 305.733821426002\n",
      "Losgistic Regression(    4800/10000): loss= 305.509614382736\n",
      "Losgistic Regression(    4900/10000): loss= 305.411771921943\n",
      "Losgistic Regression(    5000/10000): loss= 305.304661002174\n",
      "Losgistic Regression(    5100/10000): loss= 305.202187688542\n",
      "Losgistic Regression(    5200/10000): loss= 305.113712191569\n",
      "Losgistic Regression(    5300/10000): loss= 304.98343786396\n",
      "Losgistic Regression(    5400/10000): loss= 304.899836929171\n",
      "Losgistic Regression(    5500/10000): loss= 304.810155773047\n",
      "Losgistic Regression(    5600/10000): loss= 304.71543668038\n",
      "Losgistic Regression(    5700/10000): loss= 304.677291186967\n",
      "Losgistic Regression(    5800/10000): loss= 304.672483927011\n",
      "Losgistic Regression(    5900/10000): loss= 304.672131233343\n",
      "Totoal number of iterations =  5900\n",
      "Loss                        =  304.672131233\n",
      "Time for  4th cross validation = 18.2783s\n",
      "Training Accuracy         =  0.907\n",
      "Cross Validation Accuracy = 0.762292\n",
      "*************** ([0.89600000000000002, 0.91700000000000004, 0.88900000000000001, 0.89800000000000002, 0.90700000000000003], [0.76986399999999999, 0.77482399999999996, 0.76112800000000003, 0.76962399999999997, 0.76229199999999997])\n",
      "Losgistic Regression(       0/10000): loss= 692.05912592773\n",
      "Losgistic Regression(     100/10000): loss= 544.269085679418\n",
      "Losgistic Regression(     200/10000): loss= 479.886243137128\n",
      "Losgistic Regression(     300/10000): loss= 448.78540868959\n",
      "Losgistic Regression(     400/10000): loss= 434.130939291728\n",
      "Losgistic Regression(     500/10000): loss= 425.574507990988\n",
      "Losgistic Regression(     600/10000): loss= 420.046066273545\n",
      "Losgistic Regression(     700/10000): loss= 416.307375744936\n",
      "Losgistic Regression(     800/10000): loss= 413.662085641974\n",
      "Losgistic Regression(     900/10000): loss= 411.816156964451\n",
      "Losgistic Regression(    1000/10000): loss= 410.257328707955\n",
      "Losgistic Regression(    1100/10000): loss= 409.205400085849\n",
      "Losgistic Regression(    1200/10000): loss= 408.24751316117\n",
      "Losgistic Regression(    1300/10000): loss= 407.518082692345\n",
      "Losgistic Regression(    1400/10000): loss= 406.896527414479\n",
      "Losgistic Regression(    1500/10000): loss= 406.312755026396\n",
      "Losgistic Regression(    1600/10000): loss= 405.788084159281\n",
      "Losgistic Regression(    1700/10000): loss= 405.302491663629\n",
      "Losgistic Regression(    1800/10000): loss= 404.864270652278\n",
      "Losgistic Regression(    1900/10000): loss= 404.495396301742\n",
      "Losgistic Regression(    2000/10000): loss= 404.247048459406\n",
      "Losgistic Regression(    2100/10000): loss= 403.916441220213\n",
      "Losgistic Regression(    2200/10000): loss= 403.694900119069\n",
      "Losgistic Regression(    2300/10000): loss= 403.475200977213\n",
      "Losgistic Regression(    2400/10000): loss= 403.233625572236\n",
      "Losgistic Regression(    2500/10000): loss= 403.060754638696\n",
      "Losgistic Regression(    2600/10000): loss= 402.880953280704\n",
      "Losgistic Regression(    2700/10000): loss= 402.830536614308\n",
      "Losgistic Regression(    2800/10000): loss= 402.791301771326\n",
      "Losgistic Regression(    2900/10000): loss= 402.762679954243\n",
      "Losgistic Regression(    3000/10000): loss= 402.735191164219\n",
      "Losgistic Regression(    3100/10000): loss= 402.704952075042\n",
      "Losgistic Regression(    3200/10000): loss= 402.675980832038\n",
      "Losgistic Regression(    3300/10000): loss= 402.645138142795\n",
      "Losgistic Regression(    3400/10000): loss= 402.612142689156\n",
      "Losgistic Regression(    3500/10000): loss= 402.576609826214\n",
      "Losgistic Regression(    3600/10000): loss= 402.538628357762\n",
      "Losgistic Regression(    3700/10000): loss= 402.499070326654\n",
      "Losgistic Regression(    3800/10000): loss= 402.457069714581\n",
      "Losgistic Regression(    3900/10000): loss= 402.412484150862\n",
      "Losgistic Regression(    4000/10000): loss= 402.367934382038\n",
      "Losgistic Regression(    4100/10000): loss= 402.327605733955\n",
      "Losgistic Regression(    4200/10000): loss= 402.283081713408\n",
      "Losgistic Regression(    4300/10000): loss= 402.242187840088\n",
      "Losgistic Regression(    4400/10000): loss= 402.199293455295\n",
      "Losgistic Regression(    4500/10000): loss= 402.154163473313\n",
      "Losgistic Regression(    4600/10000): loss= 402.109615544021\n",
      "Losgistic Regression(    4700/10000): loss= 402.064981927219\n",
      "Losgistic Regression(    4800/10000): loss= 402.020240701099\n",
      "Losgistic Regression(    4900/10000): loss= 401.974411318971\n",
      "Losgistic Regression(    5000/10000): loss= 401.932447449959\n",
      "Losgistic Regression(    5100/10000): loss= 401.932290855958\n",
      "Totoal number of iterations =  5100\n",
      "Loss                        =  401.932290856\n",
      "Time for  0th cross validation = 15.9317s\n",
      "Training Accuracy         =  0.858\n",
      "Cross Validation Accuracy = 0.77658\n",
      "Losgistic Regression(       0/10000): loss= 692.002113291426\n",
      "Losgistic Regression(     100/10000): loss= 533.045927475285\n",
      "Losgistic Regression(     200/10000): loss= 465.393943023057\n",
      "Losgistic Regression(     300/10000): loss= 432.082809838541\n",
      "Losgistic Regression(     400/10000): loss= 417.568565506157\n",
      "Losgistic Regression(     500/10000): loss= 410.113738887878\n",
      "Losgistic Regression(     600/10000): loss= 405.358687418833\n",
      "Losgistic Regression(     700/10000): loss= 401.835197561385\n",
      "Losgistic Regression(     800/10000): loss= 399.114220634845\n",
      "Losgistic Regression(     900/10000): loss= 396.898571401568\n",
      "Losgistic Regression(    1000/10000): loss= 395.133315999526\n",
      "Losgistic Regression(    1100/10000): loss= 393.73634708042\n",
      "Losgistic Regression(    1200/10000): loss= 392.591885843016\n",
      "Losgistic Regression(    1300/10000): loss= 391.649898625263\n",
      "Losgistic Regression(    1400/10000): loss= 390.842399005792\n",
      "Losgistic Regression(    1500/10000): loss= 390.098419967314\n",
      "Losgistic Regression(    1600/10000): loss= 389.517873815858\n",
      "Losgistic Regression(    1700/10000): loss= 389.090038144259\n",
      "Losgistic Regression(    1800/10000): loss= 388.62735320403\n",
      "Losgistic Regression(    1900/10000): loss= 388.345695500618\n",
      "Losgistic Regression(    2000/10000): loss= 388.100025026747\n",
      "Losgistic Regression(    2100/10000): loss= 387.766199376055\n",
      "Losgistic Regression(    2200/10000): loss= 387.566477155477\n",
      "Losgistic Regression(    2300/10000): loss= 387.394704657247\n",
      "Losgistic Regression(    2400/10000): loss= 387.191933854999\n",
      "Losgistic Regression(    2500/10000): loss= 387.072220551721\n",
      "Losgistic Regression(    2600/10000): loss= 386.920432527236\n",
      "Losgistic Regression(    2700/10000): loss= 386.873683875172\n",
      "Losgistic Regression(    2800/10000): loss= 386.831548431169\n",
      "Losgistic Regression(    2900/10000): loss= 386.792851175805\n",
      "Losgistic Regression(    3000/10000): loss= 386.753584129237\n",
      "Losgistic Regression(    3100/10000): loss= 386.718594725648\n",
      "Losgistic Regression(    3200/10000): loss= 386.714082978878\n",
      "Losgistic Regression(    3300/10000): loss= 386.713490990592\n",
      "Totoal number of iterations =  3300\n",
      "Loss                        =  386.713490991\n",
      "Time for  1th cross validation = 10.7337s\n",
      "Training Accuracy         =  0.872\n",
      "Cross Validation Accuracy = 0.779004\n",
      "Losgistic Regression(       0/10000): loss= 691.614430765508\n",
      "Losgistic Regression(     100/10000): loss= 534.583423515397\n",
      "Losgistic Regression(     200/10000): loss= 478.780751543609\n",
      "Losgistic Regression(     300/10000): loss= 451.98519081709\n",
      "Losgistic Regression(     400/10000): loss= 439.712319338279\n",
      "Losgistic Regression(     500/10000): loss= 432.838774679885\n",
      "Losgistic Regression(     600/10000): loss= 428.668233425023\n",
      "Losgistic Regression(     700/10000): loss= 426.009784698878\n",
      "Losgistic Regression(     800/10000): loss= 424.176682333278\n",
      "Losgistic Regression(     900/10000): loss= 423.00916093269\n",
      "Losgistic Regression(    1000/10000): loss= 421.992473106274\n",
      "Losgistic Regression(    1100/10000): loss= 421.169371179038\n",
      "Losgistic Regression(    1200/10000): loss= 420.394250303495\n",
      "Losgistic Regression(    1300/10000): loss= 419.728074582245\n",
      "Losgistic Regression(    1400/10000): loss= 419.134196656329\n",
      "Losgistic Regression(    1500/10000): loss= 418.533587762691\n",
      "Losgistic Regression(    1600/10000): loss= 418.068209968823\n",
      "Losgistic Regression(    1700/10000): loss= 417.607230081694\n",
      "Losgistic Regression(    1800/10000): loss= 417.179914165641\n",
      "Losgistic Regression(    1900/10000): loss= 416.818712612604\n",
      "Losgistic Regression(    2000/10000): loss= 416.527049829512\n",
      "Losgistic Regression(    2100/10000): loss= 416.287626980779\n",
      "Losgistic Regression(    2200/10000): loss= 416.043901942241\n",
      "Losgistic Regression(    2300/10000): loss= 415.987942569493\n",
      "Losgistic Regression(    2400/10000): loss= 415.932919307356\n",
      "Losgistic Regression(    2500/10000): loss= 415.886213633991\n",
      "Losgistic Regression(    2600/10000): loss= 415.846968855094\n",
      "Losgistic Regression(    2700/10000): loss= 415.81002083798\n",
      "Losgistic Regression(    2800/10000): loss= 415.771147668427\n",
      "Losgistic Regression(    2900/10000): loss= 415.729552411798\n",
      "Losgistic Regression(    3000/10000): loss= 415.683789980869\n",
      "Losgistic Regression(    3100/10000): loss= 415.634553534625\n",
      "Losgistic Regression(    3200/10000): loss= 415.581943223456\n",
      "Losgistic Regression(    3300/10000): loss= 415.526360972236\n",
      "Losgistic Regression(    3400/10000): loss= 415.468125844633\n",
      "Losgistic Regression(    3500/10000): loss= 415.410696643335\n",
      "Losgistic Regression(    3600/10000): loss= 415.354424265217\n",
      "Losgistic Regression(    3700/10000): loss= 415.295629581269\n",
      "Losgistic Regression(    3800/10000): loss= 415.234075937345\n",
      "Losgistic Regression(    3900/10000): loss= 415.172736535863\n",
      "Losgistic Regression(    4000/10000): loss= 415.111931394793\n",
      "Losgistic Regression(    4100/10000): loss= 415.049924262669\n",
      "Losgistic Regression(    4200/10000): loss= 414.991435173987\n",
      "Losgistic Regression(    4300/10000): loss= 414.963546820659\n",
      "Losgistic Regression(    4400/10000): loss= 414.963328608778\n",
      "Totoal number of iterations =  4400\n",
      "Loss                        =  414.963328609\n",
      "Time for  2th cross validation = 15.032s\n",
      "Training Accuracy         =  0.842\n",
      "Cross Validation Accuracy = 0.759964\n",
      "Losgistic Regression(       0/10000): loss= 691.749173943629\n",
      "Losgistic Regression(     100/10000): loss= 526.601630056708\n",
      "Losgistic Regression(     200/10000): loss= 463.90186696492\n",
      "Losgistic Regression(     300/10000): loss= 434.992473032814\n",
      "Losgistic Regression(     400/10000): loss= 421.652857861071\n",
      "Losgistic Regression(     500/10000): loss= 413.98442773519\n",
      "Losgistic Regression(     600/10000): loss= 408.84254925982\n",
      "Losgistic Regression(     700/10000): loss= 405.404635727264\n",
      "Losgistic Regression(     800/10000): loss= 402.864267071185\n",
      "Losgistic Regression(     900/10000): loss= 400.997256966557\n",
      "Losgistic Regression(    1000/10000): loss= 399.479283989158\n",
      "Losgistic Regression(    1100/10000): loss= 398.248383120007\n",
      "Losgistic Regression(    1200/10000): loss= 397.22852714205\n",
      "Losgistic Regression(    1300/10000): loss= 396.29294823973\n",
      "Losgistic Regression(    1400/10000): loss= 395.562913743393\n",
      "Losgistic Regression(    1500/10000): loss= 394.935287544577\n",
      "Losgistic Regression(    1600/10000): loss= 394.374929601511\n",
      "Losgistic Regression(    1700/10000): loss= 393.885284319073\n",
      "Losgistic Regression(    1800/10000): loss= 393.487300992207\n",
      "Losgistic Regression(    1900/10000): loss= 393.126774415099\n",
      "Losgistic Regression(    2000/10000): loss= 392.80559957941\n",
      "Losgistic Regression(    2100/10000): loss= 392.511438601824\n",
      "Losgistic Regression(    2200/10000): loss= 392.303487783216\n",
      "Losgistic Regression(    2300/10000): loss= 392.109722464222\n",
      "Losgistic Regression(    2400/10000): loss= 391.903800053858\n",
      "Losgistic Regression(    2500/10000): loss= 391.716517129968\n",
      "Losgistic Regression(    2600/10000): loss= 391.652994494679\n",
      "Losgistic Regression(    2700/10000): loss= 391.622555606486\n",
      "Losgistic Regression(    2800/10000): loss= 391.592703469047\n",
      "Losgistic Regression(    2900/10000): loss= 391.557133001098\n",
      "Losgistic Regression(    3000/10000): loss= 391.519172217874\n",
      "Losgistic Regression(    3100/10000): loss= 391.489258067362\n",
      "Losgistic Regression(    3200/10000): loss= 391.455008808706\n",
      "Losgistic Regression(    3300/10000): loss= 391.419509098029\n",
      "Losgistic Regression(    3400/10000): loss= 391.380275115375\n",
      "Losgistic Regression(    3500/10000): loss= 391.339002519801\n",
      "Losgistic Regression(    3600/10000): loss= 391.294406957832\n",
      "Losgistic Regression(    3700/10000): loss= 391.24819087449\n",
      "Losgistic Regression(    3800/10000): loss= 391.201066405662\n",
      "Losgistic Regression(    3900/10000): loss= 391.150667402655\n",
      "Losgistic Regression(    4000/10000): loss= 391.098184618053\n",
      "Losgistic Regression(    4100/10000): loss= 391.04417882607\n",
      "Losgistic Regression(    4200/10000): loss= 390.98845242756\n",
      "Losgistic Regression(    4300/10000): loss= 390.9597467304\n",
      "Losgistic Regression(    4400/10000): loss= 390.959568637489\n",
      "Totoal number of iterations =  4400\n",
      "Loss                        =  390.959568637\n",
      "Time for  3th cross validation = 13.9822s\n",
      "Training Accuracy         =  0.854\n",
      "Cross Validation Accuracy = 0.775084\n",
      "Losgistic Regression(       0/10000): loss= 692.059516809006\n",
      "Losgistic Regression(     100/10000): loss= 532.127177288546\n",
      "Losgistic Regression(     200/10000): loss= 471.443928015304\n",
      "Losgistic Regression(     300/10000): loss= 442.517164505323\n",
      "Losgistic Regression(     400/10000): loss= 428.298245349957\n",
      "Losgistic Regression(     500/10000): loss= 420.283074346855\n",
      "Losgistic Regression(     600/10000): loss= 415.004412307381\n",
      "Losgistic Regression(     700/10000): loss= 410.948672385029\n",
      "Losgistic Regression(     800/10000): loss= 407.857872039171\n",
      "Losgistic Regression(     900/10000): loss= 405.681380281042\n",
      "Losgistic Regression(    1000/10000): loss= 403.785681131872\n",
      "Losgistic Regression(    1100/10000): loss= 402.445488911684\n",
      "Losgistic Regression(    1200/10000): loss= 401.134739975795\n",
      "Losgistic Regression(    1300/10000): loss= 400.074737259502\n",
      "Losgistic Regression(    1400/10000): loss= 399.189657019202\n",
      "Losgistic Regression(    1500/10000): loss= 398.583999852742\n",
      "Losgistic Regression(    1600/10000): loss= 397.980814405163\n",
      "Losgistic Regression(    1700/10000): loss= 397.464552576176\n",
      "Losgistic Regression(    1800/10000): loss= 397.20034380817\n",
      "Losgistic Regression(    1900/10000): loss= 396.807735875928\n",
      "Losgistic Regression(    2000/10000): loss= 396.538329128312\n",
      "Losgistic Regression(    2100/10000): loss= 396.338645035601\n",
      "Losgistic Regression(    2200/10000): loss= 396.15951273298\n",
      "Losgistic Regression(    2300/10000): loss= 395.971207320273\n",
      "Losgistic Regression(    2400/10000): loss= 395.893064889981\n",
      "Losgistic Regression(    2500/10000): loss= 395.845716605686\n",
      "Losgistic Regression(    2600/10000): loss= 395.803297261518\n",
      "Losgistic Regression(    2700/10000): loss= 395.76097791578\n",
      "Losgistic Regression(    2800/10000): loss= 395.726951446438\n",
      "Losgistic Regression(    2900/10000): loss= 395.697908316099\n",
      "Losgistic Regression(    3000/10000): loss= 395.667279522654\n",
      "Losgistic Regression(    3100/10000): loss= 395.635257251963\n",
      "Losgistic Regression(    3200/10000): loss= 395.60103836923\n",
      "Losgistic Regression(    3300/10000): loss= 395.56645765119\n",
      "Losgistic Regression(    3400/10000): loss= 395.531357202486\n",
      "Losgistic Regression(    3500/10000): loss= 395.495217553948\n",
      "Losgistic Regression(    3600/10000): loss= 395.457945749741\n",
      "Losgistic Regression(    3700/10000): loss= 395.420242665271\n",
      "Losgistic Regression(    3800/10000): loss= 395.381933087732\n",
      "Losgistic Regression(    3900/10000): loss= 395.343168830967\n",
      "Losgistic Regression(    4000/10000): loss= 395.30406518878\n",
      "Losgistic Regression(    4100/10000): loss= 395.264858831902\n",
      "Losgistic Regression(    4200/10000): loss= 395.22784250382\n",
      "Losgistic Regression(    4300/10000): loss= 395.192661142394\n",
      "Losgistic Regression(    4400/10000): loss= 395.156901580743\n",
      "Losgistic Regression(    4500/10000): loss= 395.12211081839\n",
      "Losgistic Regression(    4600/10000): loss= 395.08829067276\n",
      "Losgistic Regression(    4700/10000): loss= 395.068591342024\n",
      "Losgistic Regression(    4800/10000): loss= 395.06845919079\n",
      "Totoal number of iterations =  4800\n",
      "Loss                        =  395.068459191\n",
      "Time for  4th cross validation = 17.6859s\n",
      "Training Accuracy         =  0.865\n",
      "Cross Validation Accuracy = 0.769916\n",
      "*************** ([0.85799999999999998, 0.872, 0.84199999999999997, 0.85399999999999998, 0.86499999999999999], [0.77658000000000005, 0.77900400000000003, 0.75996399999999997, 0.775084, 0.76991600000000004])\n",
      "Losgistic Regression(       0/10000): loss= 692.15269451823\n",
      "Losgistic Regression(     100/10000): loss= 571.158446818561\n",
      "Losgistic Regression(     200/10000): loss= 526.871468442973\n",
      "Losgistic Regression(     300/10000): loss= 508.372708318406\n",
      "Losgistic Regression(     400/10000): loss= 502.246229971392\n",
      "Losgistic Regression(     500/10000): loss= 499.678578208716\n",
      "Losgistic Regression(     600/10000): loss= 497.756799888517\n",
      "Losgistic Regression(     700/10000): loss= 496.671963390385\n",
      "Losgistic Regression(     800/10000): loss= 495.877429016682\n",
      "Losgistic Regression(     900/10000): loss= 495.762334506192\n",
      "Losgistic Regression(    1000/10000): loss= 495.605103691817\n",
      "Losgistic Regression(    1100/10000): loss= 495.43705496928\n",
      "Losgistic Regression(    1200/10000): loss= 495.245768788307\n",
      "Losgistic Regression(    1300/10000): loss= 495.022520558488\n",
      "Losgistic Regression(    1400/10000): loss= 494.782005255243\n",
      "Losgistic Regression(    1500/10000): loss= 494.714072797923\n",
      "Losgistic Regression(    1600/10000): loss= 494.712017906628\n",
      "Totoal number of iterations =  1600\n",
      "Loss                        =  494.712017907\n",
      "Time for  0th cross validation = 5.23146s\n",
      "Training Accuracy         =  0.803\n",
      "Cross Validation Accuracy = 0.766652\n",
      "Losgistic Regression(       0/10000): loss= 692.097492361762\n",
      "Losgistic Regression(     100/10000): loss= 561.206698069302\n",
      "Losgistic Regression(     200/10000): loss= 511.356941424488\n",
      "Losgistic Regression(     300/10000): loss= 490.78547076547\n",
      "Losgistic Regression(     400/10000): loss= 484.998842859899\n",
      "Losgistic Regression(     500/10000): loss= 482.514563714572\n",
      "Losgistic Regression(     600/10000): loss= 480.815072388472\n",
      "Losgistic Regression(     700/10000): loss= 479.757492922157\n",
      "Losgistic Regression(     800/10000): loss= 478.956080710328\n",
      "Losgistic Regression(     900/10000): loss= 478.72934918774\n",
      "Losgistic Regression(    1000/10000): loss= 478.717106476853\n",
      "Losgistic Regression(    1100/10000): loss= 478.707102551776\n",
      "Losgistic Regression(    1200/10000): loss= 478.697906674986\n",
      "Losgistic Regression(    1300/10000): loss= 478.688868497865\n",
      "Losgistic Regression(    1400/10000): loss= 478.678901116313\n",
      "Losgistic Regression(    1500/10000): loss= 478.670262384174\n",
      "Losgistic Regression(    1600/10000): loss= 478.661852762032\n",
      "Losgistic Regression(    1700/10000): loss= 478.653395304952\n",
      "Losgistic Regression(    1800/10000): loss= 478.644629429115\n",
      "Losgistic Regression(    1900/10000): loss= 478.636393137547\n",
      "Losgistic Regression(    2000/10000): loss= 478.627539067494\n",
      "Losgistic Regression(    2100/10000): loss= 478.620048115323\n",
      "Losgistic Regression(    2200/10000): loss= 478.612966896197\n",
      "Losgistic Regression(    2300/10000): loss= 478.605115028177\n",
      "Losgistic Regression(    2400/10000): loss= 478.598247792708\n",
      "Losgistic Regression(    2500/10000): loss= 478.591775544538\n",
      "Losgistic Regression(    2600/10000): loss= 478.585183233266\n",
      "Losgistic Regression(    2700/10000): loss= 478.578653335305\n",
      "Losgistic Regression(    2800/10000): loss= 478.572863718784\n",
      "Losgistic Regression(    2900/10000): loss= 478.568471777017\n",
      "Totoal number of iterations =  2900\n",
      "Loss                        =  478.568471777\n",
      "Time for  1th cross validation = 9.8831s\n",
      "Training Accuracy         =  0.814\n",
      "Cross Validation Accuracy = 0.76954\n",
      "Losgistic Regression(       0/10000): loss= 691.72813624626\n",
      "Losgistic Regression(     100/10000): loss= 559.749364857921\n",
      "Losgistic Regression(     200/10000): loss= 518.428173684912\n",
      "Losgistic Regression(     300/10000): loss= 502.332098564658\n",
      "Losgistic Regression(     400/10000): loss= 497.297150692099\n",
      "Losgistic Regression(     500/10000): loss= 494.949721032093\n",
      "Losgistic Regression(     600/10000): loss= 493.745576976932\n",
      "Losgistic Regression(     700/10000): loss= 493.064529319883\n",
      "Losgistic Regression(     800/10000): loss= 492.934912413949\n",
      "Losgistic Regression(     900/10000): loss= 492.924743794366\n",
      "Losgistic Regression(    1000/10000): loss= 492.917470641734\n",
      "Losgistic Regression(    1100/10000): loss= 492.911266526549\n",
      "Losgistic Regression(    1200/10000): loss= 492.904853136347\n",
      "Losgistic Regression(    1300/10000): loss= 492.898891347259\n",
      "Losgistic Regression(    1400/10000): loss= 492.892632594255\n",
      "Losgistic Regression(    1500/10000): loss= 492.889191825524\n",
      "Totoal number of iterations =  1500\n",
      "Loss                        =  492.889191826\n",
      "Time for  2th cross validation = 5.02489s\n",
      "Training Accuracy         =  0.795\n",
      "Cross Validation Accuracy = 0.75424\n",
      "Losgistic Regression(       0/10000): loss= 691.855569618879\n",
      "Losgistic Regression(     100/10000): loss= 554.243509911927\n",
      "Losgistic Regression(     200/10000): loss= 507.48536478956\n",
      "Losgistic Regression(     300/10000): loss= 489.424040524135\n",
      "Losgistic Regression(     400/10000): loss= 483.706749434954\n",
      "Losgistic Regression(     500/10000): loss= 481.262377438413\n",
      "Losgistic Regression(     600/10000): loss= 479.777774905372\n",
      "Losgistic Regression(     700/10000): loss= 478.901623717855\n",
      "Losgistic Regression(     800/10000): loss= 478.275114763973\n",
      "Losgistic Regression(     900/10000): loss= 477.947449981735\n",
      "Losgistic Regression(    1000/10000): loss= 477.940164066587\n",
      "Losgistic Regression(    1100/10000): loss= 477.934825526535\n",
      "Losgistic Regression(    1200/10000): loss= 477.93011277501\n",
      "Totoal number of iterations =  1200\n",
      "Loss                        =  477.930112775\n",
      "Time for  3th cross validation = 3.82811s\n",
      "Training Accuracy         =  0.813\n",
      "Cross Validation Accuracy = 0.760576\n",
      "Losgistic Regression(       0/10000): loss= 692.156278015194\n",
      "Losgistic Regression(     100/10000): loss= 559.77259796423\n",
      "Losgistic Regression(     200/10000): loss= 516.630960858453\n",
      "Losgistic Regression(     300/10000): loss= 498.506881165699\n",
      "Losgistic Regression(     400/10000): loss= 491.763758851327\n",
      "Losgistic Regression(     500/10000): loss= 488.490803842087\n",
      "Losgistic Regression(     600/10000): loss= 486.316915544351\n",
      "Losgistic Regression(     700/10000): loss= 484.879870301058\n",
      "Losgistic Regression(     800/10000): loss= 483.936475354637\n",
      "Losgistic Regression(     900/10000): loss= 483.268010801293\n",
      "Losgistic Regression(    1000/10000): loss= 483.145316247887\n",
      "Losgistic Regression(    1100/10000): loss= 483.139322902811\n",
      "Losgistic Regression(    1200/10000): loss= 483.134036303571\n",
      "Losgistic Regression(    1300/10000): loss= 483.129401970153\n",
      "Totoal number of iterations =  1300\n",
      "Loss                        =  483.12940197\n",
      "Time for  4th cross validation = 4.26068s\n",
      "Training Accuracy         =   0.81\n",
      "Cross Validation Accuracy = 0.764156\n",
      "*************** ([0.80300000000000005, 0.81399999999999995, 0.79500000000000004, 0.81299999999999994, 0.81000000000000005], [0.766652, 0.76954, 0.75424000000000002, 0.76057600000000003, 0.76415599999999995])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0.94399999999999995,\n",
       "   0.94799999999999995,\n",
       "   0.93100000000000005,\n",
       "   0.94199999999999995,\n",
       "   0.94599999999999995],\n",
       "  [0.73575599999999997,\n",
       "   0.75963599999999998,\n",
       "   0.73737200000000003,\n",
       "   0.73784400000000006,\n",
       "   0.73940399999999995]),\n",
       " ([0.93999999999999995,\n",
       "   0.94699999999999995,\n",
       "   0.92800000000000005,\n",
       "   0.93999999999999995,\n",
       "   0.94299999999999995],\n",
       "  [0.73863599999999996,\n",
       "   0.76161199999999996,\n",
       "   0.73948800000000003,\n",
       "   0.74049200000000004,\n",
       "   0.74231199999999997]),\n",
       " ([0.93100000000000005,\n",
       "   0.94199999999999995,\n",
       "   0.92200000000000004,\n",
       "   0.93500000000000005,\n",
       "   0.93200000000000005],\n",
       "  [0.75012000000000001,\n",
       "   0.76641599999999999,\n",
       "   0.74820399999999998,\n",
       "   0.75078400000000001,\n",
       "   0.75097999999999998]),\n",
       " ([0.89600000000000002,\n",
       "   0.91700000000000004,\n",
       "   0.88900000000000001,\n",
       "   0.89800000000000002,\n",
       "   0.90700000000000003],\n",
       "  [0.76986399999999999,\n",
       "   0.77482399999999996,\n",
       "   0.76112800000000003,\n",
       "   0.76962399999999997,\n",
       "   0.76229199999999997]),\n",
       " ([0.85799999999999998,\n",
       "   0.872,\n",
       "   0.84199999999999997,\n",
       "   0.85399999999999998,\n",
       "   0.86499999999999999],\n",
       "  [0.77658000000000005,\n",
       "   0.77900400000000003,\n",
       "   0.75996399999999997,\n",
       "   0.775084,\n",
       "   0.76991600000000004]),\n",
       " ([0.80300000000000005,\n",
       "   0.81399999999999995,\n",
       "   0.79500000000000004,\n",
       "   0.81299999999999994,\n",
       "   0.81000000000000005],\n",
       "  [0.766652,\n",
       "   0.76954,\n",
       "   0.75424000000000002,\n",
       "   0.76057600000000003,\n",
       "   0.76415599999999995])]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_1000 = []\n",
    "for lambda_ in np.logspace(-3, 1.5,num=6,base=4):\n",
    "    tmp = training(1000, lambda_)\n",
    "    print(\"***************\", tmp)\n",
    "    accu_1000.append(tmp)\n",
    "\n",
    "accu_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processing(accu_, lambdas):\n",
    "    tr_accs = [np.array(i[0]) for i in accu_]\n",
    "    cv_accs = [np.array(i[1]) for i in accu_]\n",
    "    tr_error = [1-np.mean(i) for i in tr_accs]\n",
    "    cv_error = [1-np.mean(i) for i in cv_accs]\n",
    "    plt.semilogx(lambdas, tr_error,'r*-', lambdas, cv_error, 'bs-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.015625  ,  0.05440941,  0.18946457,  0.65975396,  2.29739671,  8.        ])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-3, 1.5,num=6,base=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trs.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw8AAAGNCAYAAABXO00sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c291d8ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "accus = [accu_1000, accu_2500, accu_5000, accu_10000, accu_25000]\n",
    "sizes = [1000, 2500,5000,10000, 25000]\n",
    "trs = []\n",
    "cvs = []\n",
    "for accu_ in accus:\n",
    "    tr_accs = [np.array(i[0]) for i in accu_]\n",
    "    cv_accs = [np.array(i[1]) for i in accu_]\n",
    "    tr_error = [1-np.mean(i) for i in tr_accs]\n",
    "    cv_error = [1-np.mean(i) for i in cv_accs]\n",
    "    trs.append(tr_error)\n",
    "    cvs.append(cv_error)\n",
    "\n",
    "trs = np.array(trs)\n",
    "cvs = np.array(cvs)\n",
    "\n",
    "plt.semilogx(sizes, (trs.T)[0],'r*-', label='tr lambda=0.0156')\n",
    "plt.semilogx(sizes, (trs.T)[1],'rs-',label='tr lambda=0.0544')\n",
    "plt.semilogx(sizes, (trs.T)[2],'ro:', label='tr lambda=0.1895')\n",
    "plt.semilogx(sizes, (trs.T)[3],'k*-', label='tr lambda=0.6598')\n",
    "plt.semilogx(sizes, (trs.T)[4],'ks-', label='tr lambda=2.297')\n",
    "plt.semilogx(sizes, (trs.T)[5],'ko-', label='tr lambda=8')\n",
    "\n",
    "plt.semilogx(sizes, (cvs.T)[0],'b*-', label='cv lambda=0.0156')\n",
    "plt.semilogx(sizes, (cvs.T)[1],'bs-',label='cv lambda=0.0544')\n",
    "plt.semilogx(sizes, (cvs.T)[2],'bo:', label='cv lambda=0.1895')\n",
    "plt.semilogx(sizes, (cvs.T)[3],'g*-', label='cv lambda=0.6598')\n",
    "plt.semilogx(sizes, (cvs.T)[4],'gs-', label='cv lambda=2.297')\n",
    "plt.semilogx(sizes, (cvs.T)[5],'go:', label='cv lambda=8')\n",
    "\n",
    "plt.xlim([10**3, 3*10**4])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Learning curve or the logistic regression\")\n",
    "plt.xlabel(\"size of training data size\")\n",
    "plt.ylabel(\"Error prediction rate\")\n",
    "plt.savefig(\"LearningCurve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ", sizes, (cvs.T)[0], 'bs-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmcVXX9x/HXl0UFtBFFQEQRJRfcGSFJS5Pc0tTMjVwy\n9w2VJHMJLdfEFDX3XSwptVT8ZeGWayA6A5oKGoogLogbKWBs398f3zsOMzDj3OHOnHtnXs/H4zyY\ne+453/kMDy68Od8txBiRJEkqhDZZFyBJkloOg4UkSSoYg4UkSSoYg4UkSSoYg4UkSSoYg4UkSSoY\ng4UkSSoYg4UkSSoYg4UkSSoYg4UkSSqYRgWLEMJJIYRpIYT5IYTxIYT+9Vz7oxDCCyGET0MIX4QQ\nJoYQDl3OdeeHEN4LIcwLITwaQujTmNokSVJ28g4WIYSDgMuB84BtgJeAsSGELnXc8jFwIbAdsAVw\nO3B7CGGXpdr8JXAycBwwAJiba3OlfOuTJEnZCfluQhZCGA88H2M8Nfc6AO8AV8cYRzSwjQrg/2KM\n5+VevwdcFmMcmXv9DWAW8NMY4z15FShJkjKT1xOLEEJ7oBx4vOpcTMnkMWBgA9sYBGwEPJV73Rvo\nXqvN/wLPN7RNSZJUHNrleX0XoC3pacLSZgEb13VT7gnEu8DKwCLgxBjjE7m3uwOxjja751mfJEnK\nUL7Boi6BFA7q8jmwFbAqMAgYGUJ4K8b4dGPaDCGsCewGvA182ZiCJUlqpVYB1gfGxhg/LnTj+QaL\nj4DFQLda57uy7BOHr+S6S97KvXw5hNAXOAt4GviAFCK61WqjKzCxjiZ3A/6YZ+2SJKnaIcDdhW40\nr2ARY1yYG3g5CBgDXw3eHARcnUdTbUjdIsQYp4UQPsi18XKuzW8A3wKureP+twH+8Ic/sOmmm+bz\nI7R4Q4cOZeTIkVmXUa8samzK71motle0ncbcn+89+VxfCn8Ws1AKvy8t6TNayHZXpK3G3tsUn9HJ\nkydz6KGHQu7f0kJrTFfIFcCduYAxARgKdATuAAghjAJmxhjPzr0+E3gReJMUJvYEDgWOX6rNK4Ff\nhRCmkn7QC4CZwIN11PAlwKabbkq/fv0a8SO0XGVlZUX/e5JFjU35PQvV9oq205j7870nn+tL4c9i\nFkrh96UlfUYL2e6KtNXYe5vyM0oTDSXIO1jEGO/JrVlxPqn7YhKwW4xxdu6SnqQBmlU6kZ489ATm\nA1OAQ2KM9y3V5ogQQkfgRmB14Blgjxjjgvx/pNZt8ODBWZfwtbKosSm/Z6HaXtF2GnN/vveUwp+v\nYlcKv4ct6TNayHZXpK3G3luKn9G817EoBiGEfkBFRUVF0Sd/qbXae++9GTNmTNZlSKqlsrKS8vJy\ngPIYY2Wh23evEEmSVDAGC0lNohgeyUpqfgYLSU3CYCG1TgYLSZJUMAYLSZJUMAYLSZJUMAYLSZJU\nMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYL\nSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJU\nMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYL\nSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMAYLSZJUMO2yLmBF/OhHp7HK\nKqsD0KtXGY88clfGFUmS1LqVdLCYMeNKoF/u1d5ZliJJkrArRJIkFZDBQpIkFYzBQpIkFUyLCRZv\nvw333guLF2ddiSRJrVeLCRbt2sGBB0LfvnD77bBgQdYVSZLU+rSYYNGzJzz/fAoWRx4JffrA1VfD\nvHlZVyZJUuvRqGARQjgphDAthDA/hDA+hNC/nmuPDiE8HUL4JHc8Wvv6EMLtIYQltY6Hv66O9dY7\njY022puNNtqbXr3KGDAA7r8fXnkFdtwRfv5zWH99uPhi+OyzxvykkiQpH3kHixDCQcDlwHnANsBL\nwNgQQpc6btkRuBvYCdgOeAd4JISwdq3r/g50A7rnjsFfV8v991/J66+P4fXXx9RYHGuzzeCuu+A/\n/4H994fzz4deveDss+HDD/P5aSVJUj4a88RiKHBjjHFUjHEKcDwwDzhyeRfHGA+LMd4QY3w5xvgG\ncHTu+w6qden/YoyzY4wf5o45jaitht694brrYNo0OO44+P3vU8A45RSYMWNFW5ckSbXlFSxCCO2B\ncuDxqnMxxgg8BgxsYDOdgPbAJ7XO7xRCmBVCmBJCuC6EsEY+tdVn7bVhxAiYPh3OOgv++EfYcEP4\n2c9gypRCfRdJkpTvE4suQFtgVq3zs0jdFw1xKfAuKYxU+TtwOLAzcAap++ThEELIs756rbEGnHtu\nChiXXgpjx6bBngccAJWVhfxOkiS1ToWaFRKA+LUXhXAmcCCwb4zxqwmhMcZ7Yoz/F2N8NcY4BtgL\nGEAal1Fwq66aBnZOmwY33phCRXk57LEHPPNMU3xHSZJah3w3IfsIWEwaZLm0riz7FKOGEMIw0tOI\nQTHGV+u7NsY4LYTwEdAH+Gdd1w0dOpSysrIa5wYPHszgwV877hOAlVeGY45JXSL33ptmj3z3u7DD\nDmmg5+67Q2GfmUiS1HxGjx7N6NGja5ybM2eFhzDWK6QhEnncEMJ44PkY46m51wGYAVwdY7ysjnt+\nAZwN7BpjfKEB36MnMB3YJ8b4f8t5vx9QUVFRQb9+/Za5v7GWLIG//Q0uuiitibH11ilg7LcftG1b\nsG8jSVJmKisrKS8vByiPMRZ8IEBjukKuAI4NIRweQtgEuAHoCNwBEEIYFUK4uOriEMIZwAWkWSMz\nQgjdcken3PudQggjQgjfCiH0CiEMAh4A3gDGrsgPl682beCHP4Rx4+CJJ6BLl+rVPG+7zdU8JUn6\nOnkHixjjPcDpwPnARGBLYLcY4+zcJT2pOZDzBNIskPuA95Y6Ts+9vzjXxoPA68DNwAvAd2OMC/Ot\nrxBCgO99Dx59FCZMSOtiHHVUmkniap6SJNUt766QYtBUXSH1efXVNJPk7ruhc2cYOhROPBFWX71Z\nvr0kSQVRjF0hrdJmm8GoUWk1zwMOcDVPSZKWx2CRp7pW8xwyJK2PIUlSa2awaKSq1TxnzEhPLe6+\nO+2o6mqekqTWzGCxgjp3huHD09OKESPgkUdczVOS1HoZLApk1VXTgM633kqreU6c6GqekqTWx2BR\nYFWreU6ZkrpHZs6sXs3z4YehBCfhSJLUYAaLJtKuHQweDC+9BGPGwOLFsOeesM02cM896bUkSS2N\nwaKJVa3m+a9/wT//CV27wkEHwaabupqnJKnlMVg0kxBgp53S4M4JE2DzzatX87zqKpg7N+sKJUla\ncQaLDPTvD3/9a1rN83vfg9NPh/XXT5ufffZZ1tVJktR4BosM9e1bvZrngQfCBRfAeuvBWWfBrHo3\noZckqTgZLIpA795w7bXw9ttwwglwzTXpCYareUqSSo3Booh07542Oqu9mucRR7iapySpNBgsilDt\n1TwffTR1m+y/P1RUZF2dJEl1M1gUsaVX87zpJpg0CbbdFnbfHZ5+2sW2JEnFx2BRAlZeGY4+OnWH\njB4N770HO+4I3/mOq3lKkoqLwaKEtGsHBx+cVvN86KGaq3n++c+u5ilJyp7BogSFAHvtVXM1z4MP\nTqt53nqrq3lKkrJjsChhS6/m+cILsMUWqcvE1TwlSVkxWLQQ224Lf/lLWs1z551dzVOSlA2DRQvT\nty/ceSdMnVpzNc8zz3Q1T0lS0zNYtFDrr19zNc/rrkvnTj7Z1TwlSU3HYNHCVa3mOX06nHMO/OlP\n1at5Tp6cdXWSpJbGYNFKdO4Mv/pVChiXXQaPPQabbeZqnpKkwjJYtDKdOsFpp8Gbb9ZczXO33eCp\np1xsS5K0YtplXYCyUbWa5xFHwH33wSWXpKmr3/522gDtBz+A3XY7jOnT59TZRq9eZTzyyF3NVrMk\nqfgZLFq5qtU8DzooLQ9+0UVp8a2ttoIPP5zD+++PqefuvZutTklSabArREBabGvPPeG55+DJJ6Fb\nN3j//ayrkiSVGoOFagghbXA2dmxa/0KSpHwYLFSnVVbJugJJUqkxWKjR3n8fXnwx6yokScXEYKFG\nmz8f+veH734XHnwQlizJuiJJUtYMFmq03r3TxmeLF8O++8Imm8D118O8eVlXJknKitNNVadevcqo\nb0ppr15l7Lcf7LcfjB8Pl1+e9iIZPjztT3LSSWlJcUlS62GwUJ3yWfxqu+3g3nvhrbfgqqtg5EgY\nMQIOPRSGDoXNN2/CQiVJRcOuEBXUBhukYPHOO3D++fCPf8AWW8Duu8Ojj7pkuCS1dAYLNYnOneGX\nv4Rp0+Cuu2DWLNh1V9h6a7jzTliwIOsKJUlNwWChJrXSSqk7pLISHn8cevZM+5Osv37an+STT7Ku\nUJJUSAYLNYsQYOed4W9/g1dfTcuH/+Y3sO66MGRI2m1VklT6DBZqdn37ws03w4wZ8ItfwJ/+BN/8\nZppd8txzjsOQpFJmsFBmunaFX/86BYwbboDXXoMddoCBA9MMk0WLsq5QkpQvg4Uy16EDHHtsChYP\nPZReH3hgeopx1VXw+edZVyhJaiiDhYpGmzaw117wz39CRQVsvz0MG5bGYZxxBsycmXWFkqSvY7BQ\nUerXD/7wh7Tg1rHHwo03piXEDz0UJk7MujpJUl0MFipq666bVvCcORMuuwyefTaFjqoZJm58JknF\nxWChkrDaanDaaTB1Kvz5zzB3buo22WwzuOmmtNOqJCl7BguVlHbt0sDO8ePhmWdg003h+OOhV680\nw+TDD7OuUJJaN4OFSlIIaWrqX/8Kb7yRwsZll8F666UxGZMnZ12hJLVOBguVvD594Jpr0sZn556b\npqz27ZtW93ziCRfckqTmZLBQi7HGGnD22fD223D77SloDBoE5eVphsnChVlXKEktn8FCLc7KK6eN\nzl56CcaOhbXWgsMOS9NVR4yAzz7LukJJarkaFSxCCCeFEKaFEOaHEMaHEPrXc+3RIYSnQwif5I5H\nl3d9COH8EMJ7IYR5uWv6NKY2qUoIaav2sWPh3/9OXw8fnqawnnZa2tJdklRYeQeLEMJBwOXAecA2\nwEvA2BBClzpu2RG4G9gJ2A54B3gkhLD2Um3+EjgZOA4YAMzNtblSvvVJy7P55nDbbTB9Opx6Ktx1\nVxqbccABaYaJJKkwGvPEYihwY4xxVIxxCnA8MA84cnkXxxgPizHeEGN8Ocb4BnB07vsOWuqyU4EL\nYowPxRhfAQ4HegD7NqI+qU7du8OFF6aNz37/e5g0KW16tv32aYbJ4sVZVyhJpS2vYBFCaA+UA49X\nnYsxRuAxYGADm+kEtAc+ybXZG+heq83/As/n0aaUl06d4MQTYcoUeOABaNsWfvxj2HjjNMNk7tys\nK5Sk0pTvE4suQFtgVq3zs0jhoCEuBd4lhRFy98UVbFNqlLZtYZ994Omn4fnnYdtt0/iLdddNM0ze\ney/rCiWptLQrUDuBFA7qvyiEM4EDgR1jjAsK0aZUKAMGwJ/+lMZhXHVVenLxu9/B4MFw+umw5Zbp\nul13PYzp0+fU2U6vXmU88shdzVS1JBWXfIPFR8BioFut811Z9olDDSGEYcAZwKAY46tLvfUBKUR0\nq9VGV6DefSyHDh1KWVlZjXODBw9m8ODB9d0m1atXL7jiCjjvPLj55hQyRo2C738/BYzp0+fwxhtj\n6mlh72arVZLqM3r0aEaPHl3j3Jw5df/HqBBCzHNZwhDCeOD5GOOpudcBmAFcHWO8rI57fgGcDewa\nY3xhOe+/B1wWYxyZe/0NUsg4PMZ473Ku7wdUVFRU0K9fv7zql/K1cCHcdx9cfjlUVMBKK+3NggV1\nB4uNNtqb11+vL3hIUnYqKyspLy8HKI8xVha6/cbMCrkCODaEcHgIYRPgBqAjcAdACGFUCOHiqotD\nCGcAF5BmjcwIIXTLHZ2WavNK4FchhB+GELYARgEzgQcb80NJhdS+feoOeeEFePLJ9FqStHx5j7GI\nMd6TW7PifFL3xSRgtxjj7NwlPYFFS91yAmkWyH21mvpNrg1ijCNCCB2BG4HVgWeAPRowDkNqNiHA\njjvCOuukjc/q8sUXadBnjx7NV5skFYtGDd6MMV4HXFfHezvXet27gW3+Gvh1Y+qRisl776Xwsc46\naUDogAHQv3+acVJrSJAktTiFmhUiKWeDDdIW7hMmpO6Tiy+Gzz9P722ySXXQGDAAttoq7W0iSS2F\nwUIqsHbtYL/90gGwZAm8/np10JgwAUaPToNC27dP4aLqycaAAWmRrjZuDyipRBkspDz16lVGfVNK\n0/vV2rSBTTdNx09/ms79739p99WqoPHEE3BdrnNxtdVSt8nS3Sg9e6YxHpJU7PKebloMnG6qlmjO\nnDSddcKE6qcbM2em97p3rxk0+veHzp2zrVdSaWrq6aY+sZCKRFkZ7LxzOqq89171U40XXkhjN6rW\ntvnmN6vHagwYAFtvDR06ZFO7JFUxWEhFrEePtJfJPvuk10uWwNSpNcdr/OUvqWulXTvYYoua4zU2\n3TTthyJJzcVgIZWQNm1go43Sceih6dyCBfDKK9VdKM8+CzfdBDGmXVzLy2t2o/Tq5XgNSU3HYCGV\nuJVWgn790nH88enc559DZWV12Lj33rShGsBaa9Wc8tq/P3Tpkl39kloWg4XUAq22WloldMcdq8/N\nmlVzvMbVV8Mnn6T3Ntig5niNfv2gY8dsapdU2gwWUivRrRvstVc6IHWVvPVWddiYMAHGjIH589O4\njM02qzleY7PN0jiOuridvCQwWEitVgiw4YbpOPjgdG7RInj11eqgMWEC3HZbGjTaoUN6krH0eI0N\nNqger+F28pLAYCFpKe3apZVAt9oKjjkmnZs7FyZOrA4aDz4II0em99ZYozpozJ2bXd2SiofBQlK9\nOnWCHXZIR5WPPkpdKFXdKNdfD7Nn190GpCXM581z7IbU0hksJOWtSxfYY490QBqvseGGMG1a3fdM\nm5ZCyje+AWuv/fVHWZnTYqVSZLCQtMJCSBuq1adnz7TT6/vvwwcfpF/fey8tY/7++9U7wFZZZZXq\nkNG9e90BZK213LRNKiYGC0nNomNHOOywut+fOzcFjOUdH3wATz+dvv7oo5r3tW2bZrzUFTyqQkn3\n7mnND0lNy2AhqSh06gR9+qSjPgsWpDU56gohlZXp11mzYPHimveuuWbDumE6dWq6n1Nq6QwWkgoi\n3+3kG2ullWDdddNRn8WL4eOP6w4gU6fCM8+kr7/8sua9q61Wf/dL1dG5s+NApNoMFpIKotgWv2rb\nFrp2TcdWW9V9XYxpx9ilx37UPl56Kf06p9b6XyuvvPwAUvtc166F3wzOBclUrAwWklq1EGD11dOx\n6ab1Xzt//vLHf1R9/dxz6dfZs1NgqdKmTQoXX/cEpHv3FFYawgXJVKwMFpLUQB06pNVGN9ig/usW\nLoQPP6y7G+bf/4ZHHkmhZOHCmvd27tywcSBSsTJYSFKBtW8P66yTjvosWZI2gqsrgLz9Nowbl76e\nN6/mvY7tULEyWEhSRtq0SYuNdekCW2xR/7Wff14zdJxyytevdiplwWAhSSVgtdXSsdFG6fV55xks\nVJxcr06SJBWMwUKSJBWMXSGSVIKaa0EyKV8GC0kqQS5+pWJlV4gklbgYI2efdhpx6VW5pIwYLCSp\nxFVUVPD7a6+lsrIy61Ikg4Uklbp7r7+eyxct4t7rr8+6FMlgIUml6JLhw9l4zTXZo0cP3rjnHo4B\nXv+//2P3Pn3YuGtXLjn33KxLVCvl4E1JKmaLFsGbb8Jrr8HkyenX115j2OTJdP3ySx4A7s9dev+s\nWfwQOGP4cA4//vgMi1ZrZrCQpGKwYAH85z9fBYevjjfeSO9B2oJ1s81g221pf/jhHNW3Lw+ecAK8\n9dZXzYSyMo4aMiSjH0IyWEhS85o/H15/vcbTB157LYWKxYvTNWutBX37wne+A8cdl77u2xe6dVtm\n97FFbdpwS4cO/Hn11Tnos89YtGRJBj+UVM1gIUlN4YsvYMqUZZ9AvPUWVE0L7dEjBYZdd4XTTktf\nb7pp2pWsgXr360c45RQePOooRt96K72ffbaJfiCpYUIpznsOIfQDKioqKujXr1/W5UhqzT77LD19\nqP0EYvr06mvWW6/6qUPVsemmqWtDamaVlZWUl5cDlMcYCz5H2ScWktQQH3+87NOH116D995L74cA\nG2yQQsPBB1cHiE02gVVXzbZ2qRkZLCSpSozw4YfLDxAffpiuadsW+vRJoeFnP6sOEBtvDB06ZFu/\nVAQMFpJanxjh3XeXHyA+/TRd0759Cgt9+8JOO1UHiD59YOWVMy1fKmYGC0kt15IlMGPG8gPE55+n\na1ZZJXVX9O0Le+xRHSA23BDa+VeklC8/NZJK3+LFabZF7fAwZQrMm5eu6dSpOjTst1/11716pe4N\nSQVhsJBUOhYuhKlTlw0Qr78O//tfuqasLAWGbbaBQw6pDhA9e0IbdzGQmprBQlLBxRg5Z+hQLho5\nklBrQacG+fLLtOJk7QDxn/+kJa4B1lwzrUL57W/D0UdXB4ju3ZdZREpS8zFYSCq4qm28f3zYYVXz\n5Zdv3rzlLyL15ptpfATA2munwDBoEAwZUh0g1lqreX4YSXkxWEgquKW38S6/5Rb473+XXUCqahGp\nqkX61l03BYa99qq5iFTnztn+MJLyYrCQVBCXnHsud9xwAxt06MAqn33Gb4H97rqL3e+4g2mLF3ME\ncFYI0Lt3Cg0HHFBzEalvfCPjn0BSIRgsJDXep5/C88/DuHEMGzeOrp9/zgOzZ1dv471gAT/s1Ikz\n9tuPw08+GTbfHDp2zLRkSU3LYCGpYRYvTt0X48fDuHHpmDIlvbfmmrQfOJCjfvUrHrzhBpg586vb\nwjrrcNSoURkVLam5GSwkLd/HH3/1NIJx42DChLSoVNu2sOWW8L3vwdlnw8CBaTGp3EyMRXfc4Tbe\nUitmsJCUpnC++mp1iBg/Pk33hDT7YuDA6hCx7bZpsak6uI231Lo1atv0EMJJwDCgO/ASMCTG+EId\n1/YFzgfKgV7AaTHGq2tdcx5wXq1bp8QY+9bRptumSyti9uzqLo3x49PTiLlz0xLWW22VAsTAgbDd\ndmmwpetCSC1G0W2bHkI4CLgcOBaYAAwFxoYQNooxfrScWzoCbwL3ACPrafoVYBBQ9TfYonxrk7Qc\nixbByy/XHBvx5pvpve7dU4A499z0a3m5gyslrZDGdIUMBW6MMY4CCCEcD+wJHAmMqH1xjPFF4MXc\ntZfW0+6iGOPsRtQjaWmzZtUMES++mBaiat8+LXO9557VTyTWW8+nEZIKKq9gEUJoT+rSuLjqXIwx\nhhAeAwauYC3fDCG8C3wJjAPOijG+s4JtSi3bwoUwaVLNbo1p09J7PXqk8HD++enXbbaBDh2yrVdS\ni5fvE4suQFtgVq3zs4CNV6CO8cARwOvA2sCvgadDCJvHGOeuQLtSy/L++zUHWL74YtpXY6WVoF8/\n2Hff6rER666bdbWSWqFCzQoJQP6jQHNijGOXevlKCGECMB04ELh9BWuTStOCBTBxYnWIGDcOZsxI\n7627bgoQl1ySQsQ228DKK2dbrySRf7D4CFgMdKt1vivLPsVotBjjnBDCG0Cf+q4bOnQoZWVlNc4N\nHjyYwYMHF6oUqfnMnFkzRFRWpq3AV145TfE88MAUIrbbDtZZJ+tqJZWA0aNHM3r06Brn5syZ06Tf\nM+/ppiGE8cDzMcZTc68DMAO4OsZ42dfcOw0YWXu66XKuW5X0xOK8GOM1y3nf6aYqbV9+mYLD0mMj\nqlarXH/96u6MgQPT9M+VVsq0XEktR9FNNwWuAO4MIVRQPd20I3AHQAhhFDAzxnh27nV7oC+pu2Ql\nYJ0QwlbAFzHGN3PXXAY8RAoT6wC/IU03rRmzpCITY+ScoUO5aORIQl2zK2KEd96pOTaisjINvOzQ\nAfr3h5/8pDpMdO/evD+EJBVQ3sEixnhPCKELadGrbsAkYLelpor2pOYaFD2AiVSPwRiWO54Cdl7q\nnruBNYHZwLPAdjHGj/OtT2pOFRUV/P7aa/nxYYdV/Q8A5s+Hioqa3Rrvv5/e23DDFB4OOyz9uuWW\naRqoJLUQjRq8GWO8Driujvd2rvV6OtDma9pzUIRK0r3XXcflixZx77BhlG+xRQoRkyalRak6doQB\nA+CII6rHRnTtmnXJktSk3CtEytMl557LHTfcwAbAKh9/zG+B/Z58kt2fe45pbdpwxC67cNbFF6ct\nwtv5EZPUuvi3npSPefMY1rkzXRcs4IE5c7g/d/p+4IdrrMEZ55zD4ccfb/eGpFar3i4KSTmffw6X\nXQa9e9P+F7/gqH32Iay/fo1LQlkZRw0ZQntDhaRWzGAh1eezz+DCC9MU0HPOgb33htdfhzvvZFG7\ndtzSoQO7rL02t3TowKIlS7KuVpIyZ7CQluejj2D4cOjVKwWLn/wEpk6Fm29OMzuA3v36ES69lAen\nTiVceim9XVNFkvJfIKsYuECWmswHH8Dll8P116f1J044AU4/HdZeO+vKJKkginGBLKnlmTkzjaG4\n6aY08PLUU+G002CttbKuTJJKisFCrdu0afDb38Ltt8Oqq8JZZ8GQIdC5c9aVSVJJMliodXrjDbj4\nYvjDH2CNNeCCC+DEE2G11bKuTJJKmsFCrcsrr8BFF8E990C3bvC738Exx0CnTllXJkktgsFCrUNl\nZZrdcf/9sN56cM018LOfwSqrZF2ZJLUoTjdVyzZ+POy5J5SXw8svw623pmmjJ5xgqJCkJmCwUMsT\nIzz1FHz/+2kr8mnT0liKKVPgyCNdbluSmpDBQi1HjDB2LHz3u7DTTmmRq3vvTeMqDjnEDcEkqRkY\nLFT6YoQxY+Bb34Ldd4cFC+Chh2DiRNh/f2jjH3NJai7+javStWRJeiKxzTawzz5pzMQjj6RxFXvt\nBSFkXaEQU3YHAAAQ+0lEQVQktToGC5WeRYvSmInNN4cDD0yrYz71FDz9NOyyi4FCkjJksFDpWLAg\nzerYZBM47LC0Gdi4cfDoo2lchSQpc45mU/H78ku47Ta49FKYMQP226+6C0SSVFR8YqHiNXcujBwJ\nG2yQ9u/YYYc0w+MvfzFUSFKR8omFis9//wvXXQdXXAGffpq6Pc48EzbaKOvKJElfw2Ch4vHpp3D1\n1XDVVelpxZFHwi9/Ceuvn3VlkqQGMlgoe7Nnpy6Pa66BhQvhuONg2DDo2TPryiRJeTJYKDvvv592\nF73hhjRF9KST4Oc/T7uOSpJKksFCzW/GDBgxAm65JS1qdfrpcOqpsOaaWVcmSVpBBgs1nzffhEsu\ngTvvhLIyGD4cTj45fS1JahEMFmp6kyenQHH33dClS/r6+ONh1VWzrkySVGAGCzWdl1+GCy+E++6D\nHj3SAM2jj4YOHbKuTJLURFwgS4X3wgtpU7Cttkpf33BD6gYZMsRQIUktnMFChfPcc2nb8gEDYMoU\nuOMOeOMNOPZYWHnlrKuTJDUDg4VWTIzwxBPwve+lJbfffRdGj4bXXoOf/hTat8+6QklSMzJYqEFi\njJx92mnEGKtOwN//DttvD4MGpWW4778fXnoJDj4Y2rbNtmBJUiYMFmqQiooKfn/ttVS++CI88AD0\n7w8/+EF68+GH4cUXYd99oY1/pCSpNXNWiBrk3muv5fJFi7h3110p/+wz2GknePzx1AUSQtblSZKK\nhP+9VJ0uOfdcNu7alT169eKNP/yBY4DX581j93XWYeNXX+WSJ580VEiSajBYqE7Dhg/njP33p907\n73D/okUE4P4FC2i/aBFnDB/OsOHDsy5RklRkDBZavhhpf+WVHHXjjYRaa0+EsjKOGjKE9s74kCTV\nYrDQsubNg5/8BM44A848k0U9enBLhw7ssvba3NKhA4uWLMm6QklSkTJYqKa3305TSMeMgXvugYsu\none/foRLL+XBqVMJl15K7379sq5SklSkwlfrEpSQEEI/oKKiooJ+/iNXOE88AQceCN/4RppSuuWW\nWVckSSqwyspKysvLAcpjjJWFbt8nFkqLXV15Jey6K2yzTdrfw1AhSWoEg0VrN38+HHEEDB2ajr//\nHdZcM+uqJEklygWyWrN33oH99oNXXoE//jEN2JQkaQUYLFqrZ56B/feHVVZJu5I6VkWSVAB2hbQ2\nMcL118POO0PfvmmPD0OFJKlADBatyf/+B8ceCyeemI5HHoG11sq6KklSC2JXSGvx3nvw4x/DxIlw\n++1pwKYkSQVmsGgNxo1LgzTbtoWnn4YBA7KuSJLUQtkV0tLdcgvsuCNsuGEaT2GokCQ1IYNFS7Vg\nAZx0EhxzDBx1VFpVs3v3rKuSJLVwdoW0RLNmwQEHwPjxcOONacCmJEnNoFFPLEIIJ4UQpoUQ5ocQ\nxocQ+tdzbd8Qwn2565eEEE5Z0TZVjxdfhG23hf/8B5580lAhSWpWeQeLEMJBwOXAecA2wEvA2BBC\nlzpu6Qi8CfwSeL9AbWp5Ro2CHXaAHj1SwPj2t7OuSJLUyjTmicVQ4MYY46gY4xTgeGAecOTyLo4x\nvhhj/GWM8R5gQSHaVC2LFqV9Pn7607Qs91NPwTrrZF2VJKkVyitYhBDaA+XA41XnYtp3/TFgYGMK\naIo2W5WPPoLddoNrrknHrbemZbolScpAvoM3uwBtgVm1zs8CNm5kDU3RZuswaRLsuy/MmwePPZam\nlUqSlKFCTTcNQCxQW03ZZsvxpz+lMRRrrpnGUxgqJElFIN8nFh8Bi4Futc53ZdknDk3e5tChQykr\nK6txbvDgwQwePLiRpZSAxYvh7LNhxAg49FC46Sbo0CHrqiRJRWj06NGMHj26xrk5c+Y06fcMaThD\nHjeEMB54PsZ4au51AGYAV8cYL/uae6cBI2OMV69ImyGEfkBFRUUF/VrTzpyffAKDB6duj9/9Dk47\nDULIuipJUgmprKykvLwcoDzGWFno9huzQNYVwJ0hhApgAmlGR0fgDoAQwihgZozx7Nzr9kBfUtfG\nSsA6IYStgC9ijG82pE0Br7ySxlN8+imMHQvf/37WFUmStIy8g0WM8Z7c+hLnk7ovJgG7xRhn5y7p\nCSxa6pYewESqx0sMyx1PATs3sM3W7a9/hcMPT/t9PPoo9O6ddUWSJC1Xo5b0jjFeB1xXx3s713o9\nnQYMEq2vzVZryRI47zy48EI48EC47Tbo1CnrqiRJqpN7hRSrOXPS4My//Q1++1s44wzHU0iSip7B\nohhNmQL77AMffpiCxR57ZF2RJEkN4rbpxeahh2DAAGjXDiZMMFRIkkqKwaJYLFkCF1wAe+8Ngwal\nLc+/+c2sq5IkKS92hRSDzz9PG4jdfz+cfz6ccw60MfNJkkqPwSJrU6em8RTvvAMPPpieWEiSVKL8\nb3GW/vEP6N8/bXs+YYKhQpJU8gwWWYgRLr0UfvAD2H77FCo22STrqiRJWmEGi+Y2dy4cfDCceWYa\nSzFmDNTaSE2SpFLlGIvmNG1a2u/jzTfhvvvgxz/OuiJJkgrKYNFcHn88Lcu9+uppKunmm2ddkSRJ\nBWdXSFOLEUaOhF13hW23hRdeMFRIklosg0VTmj8/7Ur685/DsGHw8MOwxhpZVyVJUpOxK6SpzJgB\nP/oRTJ4Mo0enAZuSJLVwBoum8PTTsP/+0LEj/OtfsPXWWVckSVKzsCukkGKEa69Ne31svjm8+KKh\nQpLUqhgsCuXLL+Hoo+Hkk9PxyCPQpUvWVUmS1KzsCimEd99Na1JMmgR33pkGbEqS1AoZLFbUv/6V\nQkW7dvDss2lKqSRJrZRdISvi5pthp52gT580nsJQIUlq5QwWjbFgAZxwAhx7LBxzTFpVs1u3rKuS\nJClzdoXk64MP0lTSCRPSE4ujj866IkmSiobBIh8vvJAWvVqyBJ56CgYOzLoiSZKKil0hDXXHHfCd\n78C666bxFIYKSZKWYbD4OgsXwqmnws9+BoceCk8+CT16ZF2VJElFya6Q+syenbY6f/ZZuO46OP54\nCCHrqiRJKloGi7pMnAj77ptW1HziidQNIkmS6mVXyPLcfTdsvz2stVYaT2GokCSpQQwWS1u0CH7x\nCzjkkDSl9Jln0mBNSZLUIHaFVPnkEzj44NTtceWVcMopjqeQJClPBguAf/87jaeYMyftSrrzzllX\nJElSSbIr5L770poUq62WxlMYKiRJarTWGywWL4ZzzoEDDoAf/jDtUrr++llXJUlSSWudXSGffZYG\naP7jHzBiBAwb5ngKSZIKoPUFi8mTYZ990uJXDz8Mu+2WdUWSJLUYrasr5MEH4VvfgpVWShuKGSok\nSSqo1hEsliyB3/wmzfzYZRcYNw769Mm6KkmSWpyW3xXy3//C4YfDmDFwwQVw9tnQpnXkKUmSmlvL\nDhZvvJGeUrz7bgoWe+2VdUWSJLVoLfe/7g8/DAMGpG6QCRMMFZIkNYOWFyxihEsuSUHiu9+F55+H\njTfOuipJklqFlhUsvvgCDjwwjaMYPhweeADKyrKuSpKkVqPljLF46600nmLaNPjrX+FHP8q6IkmS\nWp2SfmIRY0xfPPoobLstzJ8P48cbKiRJykhJB4spkyfD5ZfD7runha8mTIDNNsu6LEmSWq2S7gp5\n9KyzOGTmTDjzTLjwQmjbNuuSJElq1Ur6icX0mTPZvVs3Nr71Vi75zW+yLkeSpFavpIPF5UB74Izh\nwxk2fHjW5UiS1OqVdFcIQCgr46ghQ7IuQ5IkUeJPLO5feWUWLVmSdRmSJCmnpJ9YhFNOoff06VmX\nIUmSchr1xCKEcFIIYVoIYX4IYXwIof/XXH9ACGFy7vqXQgh71Hr/9hDCklrHw19Xx74HH8y1f/5z\nY34ESU1s9OjRWZcgKQN5B4sQwkGkcZPnAdsALwFjQwhd6rh+IHA3cDOwNfAA8EAIoW+tS/8OdAO6\n547B+dYmqXgYLKTWqTFPLIYCN8YYR8UYpwDHA/OAI+u4/lTg7zHGK2KMr8cYzwMqgZNrXfe/GOPs\nGOOHuWNOI2qTJEkZyitYhBDaA+XA41XnYlpX+zFgYB23Dcy9v7Sxy7l+pxDCrBDClBDCdSGENfKp\nTUkp/C8xixqb8nsWqu0Vbacx9+d7Tyn8+Sp2pfB72JI+o4Vsd0Xaauy9pfgZzfeJRRegLTCr1vlZ\npO6L5enegOv/DhwO7AycAewIPBxCCHnW1+oVwx+qr9OS/tIqZNsGi9ahFH4PW9Jn1GDR/Ao1KyQA\nsbHXxxjvWeq9V0MI/wbeBHYC/rmc+1cBmDx5ct6FtnRz5syhsrIy6zLqlUWNTfk9C9X2irbTmPvz\nvSef60vhz2IWSuH3pSV9RgvZ7oq01dh7m+IzutS/navkXVBDxBgbfJAWulwI7F3r/B3A/XXcMx04\npda5XwMTv+Z7fQgcU8d7PyEFEw8PDw8PD4/GHT/JJwM09MjriUWMcWEIoQIYBIwByHVXDAKuruO2\ncct5f5fc+eUKIfQE1gTer+OSscAhwNvAlw3/CSRJavVWAdYn/VtacCH3BKDhN4RwIHAncBwwgTRL\nZH9gkxjj7BDCKGBmjPHs3PUDgaeAM4G/kaaRngn0izG+FkLoRJq6+hfgA6APcCnQCdgyxrhwhX9K\nSZLULPIeYxFjvCe3ZsX5pHUnJgG7xRhn5y7pCSxa6vpxIYTBwEW54z/APjHG13KXLAa2JA3eXB14\nj5SizjVUSJJUWvJ+YiFJklSXkt6ETJIkFReDhSRJKpgWFyxCCGUhhBdCCJUhhJdDCEdnXZOkZYUQ\nOoQQ3g4hjMi6FknVcp/LSSGEiSGEx7/+jppKetv0OvwX+E6M8csQQgfSglt/iTF+mnVhkmo4Bxif\ndRGSlrEEGBhjnN+Ym1vcE4uYVK1t0SH3q0uDS0UkhNAH2Bh4OOtaJC0jsAL5oMUFC/iqO2QSMAO4\nLMb4SdY1Sarhd8BZGPqlYrQEeDKE8HwI4Sf53lxUwSKE8J0QwpgQwrshhCUhhL2Xc81JIYRpIYT5\nIYTxIYT+ta+JMc6JMW4N9AYOCSGs1Rz1Sy1dIT6juXtejzFOrTrVHLVLLV2h/g0Fto8x9gf2Ac4O\nIWyWTx1FFSxIq21OAk4irWNeQwjhIOBy0kqd2wAvAWNzC3YtI7do18vAd5qqYKmVKcRndDvg4BDC\nW6QnF0eHEH7V1IVLrUBB/g2NMX6w1K8PA+X5FFG0C2SFEJYA+8YYxyx1bjzwfIzx1NzrALwDXB1j\nHJE71w2YG2P8IoRQBjwLHBxjfLXZfwipBWvsZ7RWGz8FNosxntFMZUutwgr8G9oRaJP7N3RV4Eng\nuBhjRUO/d7E9sahTCKE9KTV9NfUlplT0GDBwqUvXA54JIUwk7VFylaFCanp5fEYlNbM8Pp/dgGdz\n/4b+C7gjn1ABpTXdtAvQFphV6/ws0uhyAGKML5Ae8UhqXg36jC4txnhnUxclCWj4v6HTgK1X5BuV\nzBOLegSW05ckqWj4GZWKV8E/n6UULD4i7YTardb5riybwCQ1Pz+jUvFqts9nyQSL3BbqFcCgqnO5\ngSeDSP1AkjLkZ1QqXs35+SyqMRYhhE5AH6rntW8QQtgK+CTG+A5wBXBnCKECmAAMBToCd2RQrtTq\n+BmVilexfD6LarppCGFH4J8s299zZ4zxyNw1JwJnkB7nTAKGxBhfbNZCpVbKz6hUvIrl81lUwUKS\nJJW2khljIUmSip/BQpIkFYzBQpIkFYzBQpIkFYzBQpIkFYzBQpIkFYzBQpIkFYzBQpIkFYzBQpIk\nFYzBQpIkFYzBQpIkFYzBQpIkFYzBQpIkFcz/A+gfhCTOjKXCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d190ef3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVDAGC0mSVDAGC0mSVDD/H1CLcSbNmt8NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c3dc79080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKUMwYLSZKUMwYLSZKUMwYLSZKUM/8PDkriy1h80hUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c34aea438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8/9Yrj9Dg2/jWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d07b09dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c3f6bb7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_curve([accu_1000, accu_2500, accu_5000, accu_10000, accu_25000],\n",
    "              np.logspace(-3, 1.5, 6, 4), [1000, 2500,5000,10000, 25000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing(accu_1000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhkFCxEREQkMgoWIiIiEpn/D5dfQwi1nlk8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c3234d2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_2500, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFqCAYAAABGTxP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM1k66cdj7b0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c38cf5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_5000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFqCAYAAAC6Wjg+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh4iIiMRK4UNERERi9f8BOoyzK+0PHaAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d49ad2f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_10000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFqCAYAAAC6Wjg+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d190dfda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing(accu_25000, np.logspace(-3, 1.5, 6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.92272838311708e-07\n",
      "Losgistic Regression(       0/100000): loss= 13558.3986235257\n",
      "Losgistic Regression(     100/100000): loss= 9015.03859638758\n",
      "Losgistic Regression(     200/100000): loss= 8564.93889943202\n",
      "Losgistic Regression(     300/100000): loss= 8347.85518261214\n",
      "Losgistic Regression(     400/100000): loss= 8223.67747944826\n",
      "Losgistic Regression(     500/100000): loss= 8144.97019368849\n",
      "Losgistic Regression(     600/100000): loss= 8091.40659649746\n",
      "Losgistic Regression(     700/100000): loss= 8052.09745046569\n",
      "Losgistic Regression(     800/100000): loss= 8023.83028131998\n",
      "Losgistic Regression(     900/100000): loss= 8001.74640004757\n",
      "Losgistic Regression(    1000/100000): loss= 7984.34061861021\n",
      "Losgistic Regression(    1100/100000): loss= 7971.13626974165\n",
      "Losgistic Regression(    1200/100000): loss= 7961.27649743662\n",
      "Losgistic Regression(    1300/100000): loss= 7953.50760543862\n",
      "Losgistic Regression(    1400/100000): loss= 7949.1291842453\n",
      "Losgistic Regression(    1500/100000): loss= 7947.5050682421\n",
      "Losgistic Regression(    1600/100000): loss= 7947.44470976324\n",
      "Totoal number of iterations =  1600\n",
      "Loss                        =  7947.44470976\n",
      "1 0.824 0.82076\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(20000)\n",
    "cols = np.arange(train_tX9.shape[1])\n",
    "\n",
    "L = np.linalg.eigvals(train_tX9[idxes][:, cols].T @ train_tX9[idxes][:, cols]).max()\n",
    "print(np.real(1/L))\n",
    "\n",
    "lambda_ = 1\n",
    "w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L),\n",
    "                       max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso)\n",
    "\n",
    "y_pred = predict_labels(w, train_tX9[idxes][:, cols])\n",
    "tr_acc = accuracy(y_pred, transform_y_back(train_y9[idxes]))\n",
    "y_pred = predict_labels(w, cv_tX9[:, cols])\n",
    "te_acc = accuracy(y_pred, transform_y_back(cv_y9))\n",
    "print(lambda_, tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81706153846153851"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, train_tX9[30000:][:, cols])\n",
    "acc = accuracy(y_pred, transform_y_back(train_y9[30000:]))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-37bb4d1dd0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1], \n\u001b[1;32m      6\u001b[0m           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1]], [-5, 5], 'g-')\n\u001b[0;32m----> 7\u001b[0;31m plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1], \n\u001b[0m\u001b[1;32m      8\u001b[0m           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1]],\n\u001b[1;32m      9\u001b[0m          [-5, 5], 'r--')\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAFkCAYAAAB4sKK5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7Jg+iVkHHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d0699a2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w)\n",
    "\n",
    "plt.plot([poly_tX.shape[1], poly_tX.shape[1]], [-5, 5], 'k-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1], poly_tX.shape[1]+decomposed_tX.shape[1]], [-5, 5], 'r-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1], \n",
    "          poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1]], [-5, 5], 'g-')\n",
    "plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1], \n",
    "          poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1]],\n",
    "         [-5, 5], 'r--')\n",
    "# plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1], \n",
    "#           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1]],\n",
    "#          [-5, 5], 'k--')\n",
    "# plt.plot([poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] \n",
    "#           + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1]\n",
    "#           + mixed_tX.shape[1], \n",
    "#           poly_tX.shape[1]+decomposed_tX.shape[1] + log_poly_tX.shape[1] \n",
    "#           + missing_indicator_tX.shape[1] + inv_poly_tX.shape[1]\n",
    "#           + mixed_tX.shape[1]], [-5, 5], 'r:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose features that is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,   9,  12,  14,  15,  16,  23,  26,  52,  58,  65,  69,  75,\n",
       "        76,  77,  84,  88,  98, 106, 107, 108, 112, 114, 117, 123, 124,\n",
       "       136, 138, 142, 143, 144, 150, 151, 155, 156, 162, 169, 170, 171,\n",
       "       173, 175, 189, 202, 205, 210, 214, 222, 225, 226, 227, 228, 231,\n",
       "       241, 247, 256, 260, 270, 282, 287, 289, 309, 315, 322, 323, 326,\n",
       "       333, 342, 345, 346, 349, 351, 358, 366, 367, 368, 369, 371, 373,\n",
       "       376, 379, 383, 384, 386, 387, 391, 403, 404, 407, 416, 418, 421,\n",
       "       423, 424, 434, 442, 443, 447, 448, 453, 458, 462, 465, 466, 467,\n",
       "       469, 471, 480, 481, 487, 494, 497, 498, 502, 513, 516, 519, 520,\n",
       "       521, 523, 524, 525])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = np.arange(len(w_cached))[abs(w_cached) < 0.025]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225000, 529)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tX9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(train_tX9[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(np.max(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002,)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov[abs(cov)>0.99].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7532857516227975e-06\n",
      "Losgistic Regression(       0/100000): loss= 44027.643871765\n",
      "Losgistic Regression(     100/100000): loss= 7149.23068313139\n",
      "Losgistic Regression(     200/100000): loss= 5830.61785617616\n",
      "Losgistic Regression(     300/100000): loss= 5651.0690221374\n",
      "Losgistic Regression(     400/100000): loss= 5610.96407601787\n",
      "Losgistic Regression(     500/100000): loss= 5584.15653210648\n",
      "Losgistic Regression(     600/100000): loss= 5565.44509463008\n",
      "Losgistic Regression(     700/100000): loss= 5554.08864337397\n",
      "Losgistic Regression(     800/100000): loss= 5550.08004866419\n",
      "Losgistic Regression(     900/100000): loss= 5548.08541955738\n",
      "Losgistic Regression(    1000/100000): loss= 5546.07400213894\n",
      "Losgistic Regression(    1100/100000): loss= 5544.52349151197\n",
      "Losgistic Regression(    1200/100000): loss= 5543.66170086858\n",
      "Losgistic Regression(    1300/100000): loss= 5542.96201864574\n",
      "Losgistic Regression(    1400/100000): loss= 5542.35387058249\n",
      "Losgistic Regression(    1500/100000): loss= 5541.91065810279\n",
      "Losgistic Regression(    1600/100000): loss= 5541.71519164196\n",
      "Losgistic Regression(    1700/100000): loss= 5541.62394167513\n",
      "Losgistic Regression(    1800/100000): loss= 5541.48745172193\n",
      "Losgistic Regression(    1900/100000): loss= 5541.3325065692\n",
      "Losgistic Regression(    2000/100000): loss= 5541.11067539367\n",
      "Losgistic Regression(    2100/100000): loss= 5540.8999691436\n",
      "Losgistic Regression(    2200/100000): loss= 5540.70706251728\n",
      "Losgistic Regression(    2300/100000): loss= 5540.5060690598\n",
      "Losgistic Regression(    2400/100000): loss= 5540.29062187811\n",
      "Losgistic Regression(    2500/100000): loss= 5540.08415859742\n",
      "Losgistic Regression(    2600/100000): loss= 5539.88127961009\n",
      "Losgistic Regression(    2700/100000): loss= 5539.6689181606\n",
      "Losgistic Regression(    2800/100000): loss= 5539.44714530173\n",
      "Losgistic Regression(    2900/100000): loss= 5539.216574923\n",
      "Losgistic Regression(    3000/100000): loss= 5538.98013852817\n",
      "Losgistic Regression(    3100/100000): loss= 5538.75102911006\n",
      "Losgistic Regression(    3200/100000): loss= 5538.52371263354\n",
      "Losgistic Regression(    3300/100000): loss= 5538.28912437458\n",
      "Losgistic Regression(    3400/100000): loss= 5538.0534078974\n",
      "Losgistic Regression(    3500/100000): loss= 5537.8140283405\n",
      "Losgistic Regression(    3600/100000): loss= 5537.57023645811\n",
      "Losgistic Regression(    3700/100000): loss= 5537.3218375429\n",
      "Losgistic Regression(    3800/100000): loss= 5537.06784699668\n",
      "Losgistic Regression(    3900/100000): loss= 5536.80731519535\n",
      "Losgistic Regression(    4000/100000): loss= 5536.54498148511\n",
      "Losgistic Regression(    4100/100000): loss= 5536.28038103831\n",
      "Losgistic Regression(    4200/100000): loss= 5536.01240976893\n",
      "Losgistic Regression(    4300/100000): loss= 5535.74071208025\n",
      "Losgistic Regression(    4400/100000): loss= 5535.46573713438\n",
      "Losgistic Regression(    4500/100000): loss= 5535.18777530807\n",
      "Losgistic Regression(    4600/100000): loss= 5534.90763353057\n",
      "Losgistic Regression(    4700/100000): loss= 5534.62584810594\n",
      "Losgistic Regression(    4800/100000): loss= 5534.34252010805\n",
      "Losgistic Regression(    4900/100000): loss= 5534.06008181016\n",
      "Losgistic Regression(    5000/100000): loss= 5533.77558244073\n",
      "Losgistic Regression(    5100/100000): loss= 5533.48646011176\n",
      "Losgistic Regression(    5200/100000): loss= 5533.19142459606\n",
      "Losgistic Regression(    5300/100000): loss= 5532.88908367772\n",
      "Losgistic Regression(    5400/100000): loss= 5532.58130183602\n",
      "Losgistic Regression(    5500/100000): loss= 5532.26956285377\n",
      "Losgistic Regression(    5600/100000): loss= 5531.95588454197\n",
      "Losgistic Regression(    5700/100000): loss= 5531.64108535763\n",
      "Losgistic Regression(    5800/100000): loss= 5531.32588390287\n",
      "Losgistic Regression(    5900/100000): loss= 5531.00973926472\n",
      "Losgistic Regression(    6000/100000): loss= 5530.69070445144\n",
      "Losgistic Regression(    6100/100000): loss= 5530.36814401525\n",
      "Losgistic Regression(    6200/100000): loss= 5530.04175659677\n",
      "Losgistic Regression(    6300/100000): loss= 5529.7130126271\n",
      "Losgistic Regression(    6400/100000): loss= 5529.38165142565\n",
      "Losgistic Regression(    6500/100000): loss= 5529.04745201874\n",
      "Losgistic Regression(    6600/100000): loss= 5528.7110806547\n",
      "Losgistic Regression(    6700/100000): loss= 5528.3724889422\n",
      "Losgistic Regression(    6800/100000): loss= 5528.03113446775\n",
      "Losgistic Regression(    6900/100000): loss= 5527.68744216968\n",
      "Losgistic Regression(    7000/100000): loss= 5527.3418397663\n",
      "Losgistic Regression(    7100/100000): loss= 5526.99439188549\n",
      "Losgistic Regression(    7200/100000): loss= 5526.64458487969\n",
      "Losgistic Regression(    7300/100000): loss= 5526.29283405827\n",
      "Losgistic Regression(    7400/100000): loss= 5525.93974523049\n",
      "Losgistic Regression(    7500/100000): loss= 5525.58526343156\n",
      "Losgistic Regression(    7600/100000): loss= 5525.22916856464\n",
      "Losgistic Regression(    7700/100000): loss= 5524.87154170972\n",
      "Losgistic Regression(    7800/100000): loss= 5524.51261290613\n",
      "Losgistic Regression(    7900/100000): loss= 5524.15253952439\n",
      "Losgistic Regression(    8000/100000): loss= 5523.7913003972\n",
      "Losgistic Regression(    8100/100000): loss= 5523.42947475149\n",
      "Losgistic Regression(    8200/100000): loss= 5523.06646667013\n",
      "Losgistic Regression(    8300/100000): loss= 5522.70282574999\n",
      "Losgistic Regression(    8400/100000): loss= 5522.43071213075\n",
      "Losgistic Regression(    8500/100000): loss= 5521.9864399381\n",
      "Losgistic Regression(    8600/100000): loss= 5521.03663115989\n",
      "Losgistic Regression(    8700/100000): loss= 5520.16355490774\n",
      "Losgistic Regression(    8800/100000): loss= 5519.30518081302\n",
      "Losgistic Regression(    8900/100000): loss= 5518.47143132678\n",
      "Losgistic Regression(    9000/100000): loss= 5517.66730608177\n",
      "Losgistic Regression(    9100/100000): loss= 5516.8848320604\n",
      "Losgistic Regression(    9200/100000): loss= 5516.12045766804\n",
      "Losgistic Regression(    9300/100000): loss= 5515.36861301696\n",
      "Losgistic Regression(    9400/100000): loss= 5514.64873309955\n",
      "Losgistic Regression(    9500/100000): loss= 5513.95395597739\n",
      "Losgistic Regression(    9600/100000): loss= 5513.27296627707\n",
      "Losgistic Regression(    9700/100000): loss= 5512.60656585565\n",
      "Losgistic Regression(    9800/100000): loss= 5511.96209419897\n",
      "Losgistic Regression(    9900/100000): loss= 5511.35179831282\n",
      "Losgistic Regression(   10000/100000): loss= 5510.75848649488\n",
      "Losgistic Regression(   10100/100000): loss= 5510.17028203018\n",
      "Losgistic Regression(   10200/100000): loss= 5509.63955915718\n",
      "Losgistic Regression(   10300/100000): loss= 5508.97815195267\n",
      "Losgistic Regression(   10400/100000): loss= 5508.06982248843\n",
      "Losgistic Regression(   10500/100000): loss= 5507.23516172221\n",
      "Losgistic Regression(   10600/100000): loss= 5506.42303205762\n",
      "Losgistic Regression(   10700/100000): loss= 5505.64016583945\n",
      "Losgistic Regression(   10800/100000): loss= 5504.88858553373\n",
      "Losgistic Regression(   10900/100000): loss= 5504.17544005739\n",
      "Losgistic Regression(   11000/100000): loss= 5503.50012416932\n",
      "Losgistic Regression(   11100/100000): loss= 5502.84567707191\n",
      "Losgistic Regression(   11200/100000): loss= 5502.20786846475\n",
      "Losgistic Regression(   11300/100000): loss= 5501.60210248123\n",
      "Losgistic Regression(   11400/100000): loss= 5501.05982186157\n",
      "Losgistic Regression(   11500/100000): loss= 5500.45549177256\n",
      "Losgistic Regression(   11600/100000): loss= 5499.59817415762\n",
      "Losgistic Regression(   11700/100000): loss= 5498.79081573418\n",
      "Losgistic Regression(   11800/100000): loss= 5498.02629115607\n",
      "Losgistic Regression(   11900/100000): loss= 5497.3091165667\n",
      "Losgistic Regression(   12000/100000): loss= 5496.62878087498\n",
      "Losgistic Regression(   12100/100000): loss= 5495.97601418442\n",
      "Losgistic Regression(   12200/100000): loss= 5495.35697499503\n",
      "Losgistic Regression(   12300/100000): loss= 5494.76921641822\n",
      "Losgistic Regression(   12400/100000): loss= 5494.2428542026\n",
      "Losgistic Regression(   12500/100000): loss= 5493.53002657472\n",
      "Losgistic Regression(   12600/100000): loss= 5492.72952127158\n",
      "Losgistic Regression(   12700/100000): loss= 5492.00980337848\n",
      "Losgistic Regression(   12800/100000): loss= 5491.325995707\n",
      "Losgistic Regression(   12900/100000): loss= 5490.68489270183\n",
      "Losgistic Regression(   13000/100000): loss= 5490.08224042034\n",
      "Losgistic Regression(   13100/100000): loss= 5489.52004817367\n",
      "Losgistic Regression(   13200/100000): loss= 5489.011414236\n",
      "Losgistic Regression(   13300/100000): loss= 5488.2980801529\n",
      "Losgistic Regression(   13400/100000): loss= 5487.57156324623\n",
      "Losgistic Regression(   13500/100000): loss= 5486.8987038258\n",
      "Losgistic Regression(   13600/100000): loss= 5486.26718643836\n",
      "Losgistic Regression(   13700/100000): loss= 5485.67913986571\n",
      "Losgistic Regression(   13800/100000): loss= 5485.13221230525\n",
      "Losgistic Regression(   13900/100000): loss= 5484.62580920705\n",
      "Losgistic Regression(   14000/100000): loss= 5483.90751167447\n",
      "Losgistic Regression(   14100/100000): loss= 5483.03808067515\n",
      "Losgistic Regression(   14200/100000): loss= 5482.22733240992\n",
      "Losgistic Regression(   14300/100000): loss= 5481.48122856812\n",
      "Losgistic Regression(   14400/100000): loss= 5480.7871127802\n",
      "Losgistic Regression(   14500/100000): loss= 5480.13723707136\n",
      "Losgistic Regression(   14600/100000): loss= 5479.2900415124\n",
      "Losgistic Regression(   14700/100000): loss= 5478.46526880945\n",
      "Losgistic Regression(   14800/100000): loss= 5477.6947228974\n",
      "Losgistic Regression(   14900/100000): loss= 5476.98214956527\n",
      "Losgistic Regression(   15000/100000): loss= 5476.36130948395\n",
      "Losgistic Regression(   15100/100000): loss= 5475.66832117198\n",
      "Losgistic Regression(   15200/100000): loss= 5474.83064847409\n",
      "Losgistic Regression(   15300/100000): loss= 5474.07010564958\n",
      "Losgistic Regression(   15400/100000): loss= 5473.36269178611\n",
      "Losgistic Regression(   15500/100000): loss= 5472.7413802886\n",
      "Losgistic Regression(   15600/100000): loss= 5472.08778253234\n",
      "Losgistic Regression(   15700/100000): loss= 5471.30061066398\n",
      "Losgistic Regression(   15800/100000): loss= 5470.58924455943\n",
      "Losgistic Regression(   15900/100000): loss= 5469.9304088886\n",
      "Losgistic Regression(   16000/100000): loss= 5469.29335825475\n",
      "Losgistic Regression(   16100/100000): loss= 5468.58621520046\n",
      "Losgistic Regression(   16200/100000): loss= 5467.93292572775\n",
      "Losgistic Regression(   16300/100000): loss= 5467.30732093454\n",
      "Losgistic Regression(   16400/100000): loss= 5466.72801791811\n",
      "Losgistic Regression(   16500/100000): loss= 5466.00978013297\n",
      "Losgistic Regression(   16600/100000): loss= 5465.27623360792\n",
      "Losgistic Regression(   16700/100000): loss= 5464.67843660639\n",
      "Losgistic Regression(   16800/100000): loss= 5464.18513160678\n",
      "Losgistic Regression(   16900/100000): loss= 5463.53178458592\n",
      "Losgistic Regression(   17000/100000): loss= 5462.84758108584\n",
      "Losgistic Regression(   17100/100000): loss= 5462.24038847284\n",
      "Losgistic Regression(   17200/100000): loss= 5461.69561594346\n",
      "Losgistic Regression(   17300/100000): loss= 5461.05816075169\n",
      "Losgistic Regression(   17400/100000): loss= 5460.48822720874\n",
      "Losgistic Regression(   17500/100000): loss= 5459.96698087117\n",
      "Losgistic Regression(   17600/100000): loss= 5459.31511235488\n",
      "Losgistic Regression(   17700/100000): loss= 5458.71214125892\n",
      "Losgistic Regression(   17800/100000): loss= 5458.20314872269\n",
      "Losgistic Regression(   17900/100000): loss= 5457.625744967\n",
      "Losgistic Regression(   18000/100000): loss= 5457.06849608827\n",
      "Losgistic Regression(   18100/100000): loss= 5456.54301380383\n",
      "Losgistic Regression(   18200/100000): loss= 5455.95363957723\n",
      "Losgistic Regression(   18300/100000): loss= 5455.45109880843\n",
      "Losgistic Regression(   18400/100000): loss= 5454.928072419\n",
      "Losgistic Regression(   18500/100000): loss= 5454.36376239695\n",
      "Losgistic Regression(   18600/100000): loss= 5453.90506777163\n",
      "Losgistic Regression(   18700/100000): loss= 5453.32387332031\n",
      "Losgistic Regression(   18800/100000): loss= 5452.87688308764\n",
      "Losgistic Regression(   18900/100000): loss= 5452.39076550068\n",
      "Losgistic Regression(   19000/100000): loss= 5451.90605772973\n",
      "Losgistic Regression(   19100/100000): loss= 5451.40756362164\n",
      "Losgistic Regression(   19200/100000): loss= 5450.94162869991\n",
      "Losgistic Regression(   19300/100000): loss= 5450.44775571802\n",
      "Losgistic Regression(   19400/100000): loss= 5449.97108976042\n",
      "Losgistic Regression(   19500/100000): loss= 5449.52451204418\n",
      "Losgistic Regression(   19600/100000): loss= 5449.06233309833\n",
      "Losgistic Regression(   19700/100000): loss= 5448.60529507512\n",
      "Losgistic Regression(   19800/100000): loss= 5448.15871488234\n",
      "Losgistic Regression(   19900/100000): loss= 5447.73472524713\n",
      "Losgistic Regression(   20000/100000): loss= 5447.30192609267\n",
      "Losgistic Regression(   20100/100000): loss= 5446.89926029063\n",
      "Losgistic Regression(   20200/100000): loss= 5446.49957665853\n",
      "Losgistic Regression(   20300/100000): loss= 5446.06317913\n",
      "Losgistic Regression(   20400/100000): loss= 5445.63868069095\n",
      "Losgistic Regression(   20500/100000): loss= 5445.2073107274\n",
      "Losgistic Regression(   20600/100000): loss= 5444.81515958931\n",
      "Losgistic Regression(   20700/100000): loss= 5444.45464744083\n",
      "Losgistic Regression(   20800/100000): loss= 5444.08631193854\n",
      "Losgistic Regression(   20900/100000): loss= 5443.72545113849\n",
      "Losgistic Regression(   21000/100000): loss= 5443.3470035678\n",
      "Losgistic Regression(   21100/100000): loss= 5442.95857984612\n",
      "Losgistic Regression(   21200/100000): loss= 5442.57736093928\n",
      "Losgistic Regression(   21300/100000): loss= 5442.21751646783\n",
      "Losgistic Regression(   21400/100000): loss= 5441.894976955\n",
      "Losgistic Regression(   21500/100000): loss= 5441.5770093103\n",
      "Losgistic Regression(   21600/100000): loss= 5441.24443259131\n",
      "Losgistic Regression(   21700/100000): loss= 5440.8937862527\n",
      "Losgistic Regression(   21800/100000): loss= 5440.55351372247\n",
      "Losgistic Regression(   21900/100000): loss= 5440.22658917484\n",
      "Losgistic Regression(   22000/100000): loss= 5439.92030942567\n",
      "Losgistic Regression(   22100/100000): loss= 5439.62585180653\n",
      "Losgistic Regression(   22200/100000): loss= 5439.32531559103\n",
      "Losgistic Regression(   22300/100000): loss= 5439.00396032982\n",
      "Losgistic Regression(   22400/100000): loss= 5438.69286075977\n",
      "Losgistic Regression(   22500/100000): loss= 5438.40313616372\n",
      "Losgistic Regression(   22600/100000): loss= 5438.13022843492\n",
      "Losgistic Regression(   22700/100000): loss= 5437.87098243842\n",
      "Losgistic Regression(   22800/100000): loss= 5437.60326153099\n",
      "Losgistic Regression(   22900/100000): loss= 5437.32182142731\n",
      "Losgistic Regression(   23000/100000): loss= 5437.04738034934\n",
      "Losgistic Regression(   23100/100000): loss= 5436.79127550337\n",
      "Losgistic Regression(   23200/100000): loss= 5436.5410236222\n",
      "Losgistic Regression(   23300/100000): loss= 5436.29364721691\n",
      "Losgistic Regression(   23400/100000): loss= 5436.05610131404\n",
      "Losgistic Regression(   23500/100000): loss= 5435.82452963126\n",
      "Losgistic Regression(   23600/100000): loss= 5435.58704986346\n",
      "Losgistic Regression(   23700/100000): loss= 5435.35095200302\n",
      "Losgistic Regression(   23800/100000): loss= 5435.13068304116\n",
      "Losgistic Regression(   23900/100000): loss= 5434.9116005854\n",
      "Losgistic Regression(   24000/100000): loss= 5434.69996810045\n",
      "Losgistic Regression(   24100/100000): loss= 5434.49005123812\n",
      "Losgistic Regression(   24200/100000): loss= 5434.28357171052\n",
      "Losgistic Regression(   24300/100000): loss= 5434.0784951496\n",
      "Losgistic Regression(   24400/100000): loss= 5433.88310657817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-fb28b464b319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L),\n\u001b[0;32m----> 9\u001b[0;31m                        max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tX9\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mlogistic_AGDR\u001b[0;34m(y, tx, gamma, max_iters, w0, lambda_, regularizor)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Restart if the loss of new weight is larger than the original one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lie/Documents/project_one_ml/scripts/functions.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "cols  = feature_columns\n",
    "\n",
    "L     = np.linalg.eigvals(train_tX9[idxes][:, cols].T @ train_tX9[idxes][:, cols]).max()\n",
    "print(np.real(1/L))\n",
    "\n",
    "lambda_ = 1\n",
    "w,cost = logistic_AGDR(train_y9[idxes], train_tX9[idxes][:, cols], gamma=np.real(1/L),\n",
    "                       max_iters = 100000, lambda_=lambda_, regularizor=regularizor_lasso)\n",
    "\n",
    "y_pred = predict_labels(w, train_tX9[idxes][:, cols])\n",
    "tr_acc = accuracy(y_pred, transform_y_back(train_y9[idxes]))\n",
    "y_pred = predict_labels(w, cv_tX9[:, cols])\n",
    "te_acc = accuracy(y_pred, transform_y_back(cv_y9))\n",
    "y_pred = predict_labels(w, train_tX9[20000:][:, cols])\n",
    "acc = accuracy(y_pred, transform_y_back(train_y9[20000:]))\n",
    "print(lambda_, tr_acc, te_acc, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best lambda for a degree 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37656429545e-06\n",
      "Losgistic Regression(0/150000): loss=6128.661958753694\n",
      "Losgistic Regression(1000/150000): loss=4382.793973805763\n",
      "Losgistic Regression(2000/150000): loss=4321.439300698346\n",
      "Losgistic Regression(3000/150000): loss=4291.208251980238\n",
      "Losgistic Regression(4000/150000): loss=4270.574245773539\n",
      "Losgistic Regression(5000/150000): loss=4255.255208797125\n",
      "Losgistic Regression(6000/150000): loss=4244.187346417512\n",
      "Losgistic Regression(7000/150000): loss=4236.342382593083\n",
      "Losgistic Regression(8000/150000): loss=4229.2149455352765\n",
      "Losgistic Regression(9000/150000): loss=4222.549848793302\n",
      "Losgistic Regression(10000/150000): loss=4216.256821112506\n",
      "Losgistic Regression(11000/150000): loss=4210.280363793595\n",
      "Losgistic Regression(12000/150000): loss=4204.581620909371\n",
      "Losgistic Regression(13000/150000): loss=4199.130612876477\n",
      "Losgistic Regression(14000/150000): loss=4193.902700925177\n",
      "Losgistic Regression(15000/150000): loss=4188.876869375156\n",
      "Losgistic Regression(16000/150000): loss=4184.034829564825\n",
      "Losgistic Regression(17000/150000): loss=4179.3605048153\n",
      "Losgistic Regression(18000/150000): loss=4175.203975960235\n",
      "Losgistic Regression(19000/150000): loss=4172.128890451348\n",
      "Losgistic Regression(20000/150000): loss=4169.404308525439\n",
      "Losgistic Regression(21000/150000): loss=4166.945582792079\n",
      "Losgistic Regression(22000/150000): loss=4164.697910753656\n",
      "Losgistic Regression(23000/150000): loss=4162.623370955874\n",
      "Losgistic Regression(24000/150000): loss=4160.694444672941\n",
      "Losgistic Regression(25000/150000): loss=4158.890417925157\n",
      "Losgistic Regression(26000/150000): loss=4157.195232293507\n",
      "Losgistic Regression(27000/150000): loss=4155.596121161573\n",
      "Losgistic Regression(28000/150000): loss=4154.082702688859\n",
      "Losgistic Regression(29000/150000): loss=4152.646368926147\n",
      "Losgistic Regression(30000/150000): loss=4151.279853515938\n",
      "Losgistic Regression(31000/150000): loss=4149.976926771299\n",
      "Losgistic Regression(32000/150000): loss=4148.732176552424\n",
      "Losgistic Regression(33000/150000): loss=4147.540849240619\n",
      "Losgistic Regression(34000/150000): loss=4146.398730789539\n",
      "Losgistic Regression(35000/150000): loss=4145.3020601081635\n",
      "Losgistic Regression(36000/150000): loss=4144.247460850778\n",
      "Losgistic Regression(37000/150000): loss=4143.231888176198\n",
      "Losgistic Regression(38000/150000): loss=4142.252586093812\n",
      "Losgistic Regression(39000/150000): loss=4141.307052495995\n",
      "Losgistic Regression(40000/150000): loss=4140.393009940427\n",
      "Losgistic Regression(41000/150000): loss=4139.50838085982\n",
      "Losgistic Regression(42000/150000): loss=4138.65126787957\n",
      "Losgistic Regression(43000/150000): loss=4137.819930481284\n",
      "Losgistic Regression(44000/150000): loss=4137.0127732559595\n",
      "Losgistic Regression(45000/150000): loss=4136.228330432692\n",
      "Losgistic Regression(46000/150000): loss=4135.465253085196\n",
      "Losgistic Regression(47000/150000): loss=4134.1443323593485\n",
      "Losgistic Regression(48000/150000): loss=4132.836213664394\n",
      "Losgistic Regression(49000/150000): loss=4131.548598167255\n",
      "Losgistic Regression(50000/150000): loss=4130.280447452421\n",
      "Losgistic Regression(51000/150000): loss=4129.030794438789\n",
      "Losgistic Regression(52000/150000): loss=4127.7987378407415\n",
      "Losgistic Regression(53000/150000): loss=4126.583436998648\n",
      "Losgistic Regression(54000/150000): loss=4125.3841070212775\n",
      "Losgistic Regression(55000/150000): loss=4124.200014284044\n",
      "Losgistic Regression(56000/150000): loss=4123.030472284158\n",
      "Losgistic Regression(57000/150000): loss=4121.87483784062\n",
      "Losgistic Regression(58000/150000): loss=4120.732507621012\n",
      "Losgistic Regression(59000/150000): loss=4119.602914974805\n",
      "Losgistic Regression(60000/150000): loss=4118.485527050165\n",
      "Losgistic Regression(61000/150000): loss=4117.379842171625\n",
      "Losgistic Regression(62000/150000): loss=4116.28538745605\n",
      "Losgistic Regression(63000/150000): loss=4115.201716646274\n",
      "Losgistic Regression(64000/150000): loss=4114.128408140856\n",
      "Losgistic Regression(65000/150000): loss=4113.065063202537\n",
      "Losgistic Regression(66000/150000): loss=4112.011304327397\n",
      "Losgistic Regression(67000/150000): loss=4110.966773759254\n",
      "Losgistic Regression(68000/150000): loss=4109.931132134869\n",
      "Losgistic Regression(69000/150000): loss=4108.9040572471185\n",
      "Losgistic Regression(70000/150000): loss=4107.88524291448\n",
      "Losgistic Regression(71000/150000): loss=4106.874397946057\n",
      "Losgistic Regression(72000/150000): loss=4105.8712451931\n",
      "Losgistic Regression(73000/150000): loss=4104.875520678365\n",
      "Losgistic Regression(74000/150000): loss=4103.886972795643\n",
      "Losgistic Regression(75000/150000): loss=4102.905361572792\n",
      "Losgistic Regression(76000/150000): loss=4101.930457992193\n",
      "Losgistic Regression(77000/150000): loss=4100.962043363106\n",
      "Losgistic Regression(78000/150000): loss=4099.999908741166\n",
      "Losgistic Regression(79000/150000): loss=4099.043854390546\n",
      "Losgistic Regression(80000/150000): loss=4098.093689285304\n",
      "Losgistic Regression(81000/150000): loss=4097.149230644885\n",
      "Losgistic Regression(82000/150000): loss=4096.2103035032105\n",
      "Losgistic Regression(83000/150000): loss=4095.2767403064836\n",
      "Losgistic Regression(84000/150000): loss=4094.348380641769\n",
      "Losgistic Regression(85000/150000): loss=4093.4250696102904\n",
      "Losgistic Regression(86000/150000): loss=4092.5066611711864\n",
      "Losgistic Regression(87000/150000): loss=4091.593013716083\n",
      "Losgistic Regression(88000/150000): loss=4090.6839912387345\n",
      "Losgistic Regression(89000/150000): loss=4089.779463475005\n",
      "Losgistic Regression(90000/150000): loss=4088.8793054909006\n",
      "Losgistic Regression(91000/150000): loss=4087.9833973753302\n",
      "Losgistic Regression(92000/150000): loss=4087.091623991697\n",
      "Losgistic Regression(93000/150000): loss=4086.2038746801322\n",
      "Losgistic Regression(94000/150000): loss=4085.3200435459976\n",
      "Losgistic Regression(95000/150000): loss=4084.4400271378972\n",
      "Losgistic Regression(96000/150000): loss=4083.563726604002\n",
      "Losgistic Regression(97000/150000): loss=4082.6910490395876\n",
      "Losgistic Regression(98000/150000): loss=4081.821904073697\n",
      "Losgistic Regression(99000/150000): loss=4080.9562047323307\n",
      "Losgistic Regression(100000/150000): loss=4080.0938673345763\n",
      "Losgistic Regression(101000/150000): loss=4079.234811363942\n",
      "Losgistic Regression(102000/150000): loss=4078.3789593505944\n",
      "Losgistic Regression(103000/150000): loss=4077.5262367569426\n",
      "Losgistic Regression(104000/150000): loss=4076.6765718659494\n",
      "Losgistic Regression(105000/150000): loss=4075.8298956724216\n",
      "Losgistic Regression(106000/150000): loss=4074.986141777644\n",
      "Losgistic Regression(107000/150000): loss=4074.145246287765\n",
      "Losgistic Regression(108000/150000): loss=4073.3071477161557\n",
      "Losgistic Regression(109000/150000): loss=4072.471786889442\n",
      "Losgistic Regression(110000/150000): loss=4071.639106857446\n",
      "Losgistic Regression(111000/150000): loss=4070.8090528066728\n",
      "Losgistic Regression(112000/150000): loss=4069.981571977565\n",
      "Losgistic Regression(113000/150000): loss=4069.1566135849607\n",
      "Losgistic Regression(114000/150000): loss=4068.334128741864\n",
      "Losgistic Regression(115000/150000): loss=4067.514070386298\n",
      "Losgistic Regression(116000/150000): loss=4066.6963932110834\n",
      "Losgistic Regression(117000/150000): loss=4065.8810555570626\n",
      "Losgistic Regression(118000/150000): loss=4065.068013712836\n",
      "Losgistic Regression(119000/150000): loss=4064.25722701785\n",
      "Losgistic Regression(120000/150000): loss=4063.44865651399\n",
      "Losgistic Regression(121000/150000): loss=4062.642264707486\n",
      "Losgistic Regression(122000/150000): loss=4061.83801549441\n",
      "Losgistic Regression(123000/150000): loss=4061.0358741047557\n",
      "Losgistic Regression(124000/150000): loss=4060.235807051358\n",
      "Losgistic Regression(125000/150000): loss=4059.437782534584\n",
      "Losgistic Regression(126000/150000): loss=4058.641769063937\n",
      "Losgistic Regression(127000/150000): loss=4057.8477366387892\n",
      "Losgistic Regression(128000/150000): loss=4057.055656489738\n",
      "Losgistic Regression(129000/150000): loss=4056.2655008337697\n",
      "Losgistic Regression(130000/150000): loss=4055.477242900723\n",
      "Losgistic Regression(131000/150000): loss=4054.6908568974927\n",
      "Losgistic Regression(132000/150000): loss=4053.906317968484\n",
      "Losgistic Regression(133000/150000): loss=4053.1236021583495\n",
      "Losgistic Regression(134000/150000): loss=4052.3426863762284\n",
      "Losgistic Regression(135000/150000): loss=4051.5635483610204\n",
      "Losgistic Regression(136000/150000): loss=4050.7861666489475\n",
      "Losgistic Regression(137000/150000): loss=4050.010520541866\n",
      "Losgistic Regression(138000/150000): loss=4049.236590076733\n",
      "Losgistic Regression(139000/150000): loss=4048.46435599626\n",
      "Losgistic Regression(140000/150000): loss=4047.6937997206537\n",
      "Losgistic Regression(141000/150000): loss=4046.9249033204223\n",
      "Losgistic Regression(142000/150000): loss=4046.1576494901965\n",
      "Losgistic Regression(143000/150000): loss=4045.3920215234975\n",
      "Losgistic Regression(144000/150000): loss=4044.62800328853\n",
      "Losgistic Regression(145000/150000): loss=4043.8655792049185\n",
      "Losgistic Regression(146000/150000): loss=4043.1047339495753\n",
      "Losgistic Regression(147000/150000): loss=4042.3454523613855\n",
      "Losgistic Regression(148000/150000): loss=4041.587721752981\n",
      "Losgistic Regression(149000/150000): loss=4040.8315283470806\n",
      "0.8207 0.81404\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(10000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w, losses = reg_logistic_regression_GD(train_y7[idxes], train_tX7[idxes], gamma=0.0001, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], cv_tX7, cv_y7, w)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.79002650416e-06\n",
      "Losgistic Regression(0/150000): loss=7981.108636380177\n",
      "Losgistic Regression(100/150000): loss=7981.110107054736\n",
      "0.00147067455873\n",
      "Totoal number of iterations =  100\n",
      "Loss =  7981.11010705\n",
      "0.8212 0.8158\n"
     ]
    }
   ],
   "source": [
    "idxes = np.arange(20000)\n",
    "L = np.linalg.eigvals(train_tX7[idxes].T @ train_tX7[idxes]).max()\n",
    "print(1/L)\n",
    "\n",
    "w_agdr = logistic_AGDR(train_y7[idxes], train_tX7[idxes], gamma=1/L, \n",
    "                   max_iters = 150000, lambda_=0.00001, regularizor=regularizor_lasso, w0=w_agdr)\n",
    "\n",
    "tr_acc, te_acc = prediction_and_accuracy(train_tX7[idxes], train_y7[idxes], \n",
    "                                         cv_tX7, cv_y7, w_agdr)\n",
    "print(tr_acc, te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_validation_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-db02c5f6f0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m cross_validation_var(y9, tX9, \n\u001b[0m\u001b[1;32m      5\u001b[0m                      \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_up_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_validation_var' is not defined"
     ]
    }
   ],
   "source": [
    "set_up_f = lambda y, tx : logistic_AGDR(y, tx, gamma=1e-6, \n",
    "               max_iters = 10000, lambda_=0.5, regularizor=regularizor_lasso)\n",
    "\n",
    "cross_validation_var(y9, tX9, \n",
    "                     K=25, \n",
    "                     func=set_up_f, \n",
    "                     seed=3, \n",
    "                     fpred=predict_cv,\n",
    "                     faccuracy=accuracy,\n",
    "                     max_fold=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filled_test_tX_median = fill_na(tX_test, np.median)\n",
    "#\n",
    "log_test_tX               = logs_of_features(filled_test_tX_median, Features_using_log)\n",
    "decomposed_test_tX        = decompose_categorical_features(tX_test)\n",
    "missing_indicator_test_tX = missing_indicator(tX_test)\n",
    "inverse_test_tX           = inver_terms(filled_test_tX_median, Features_using_log)\n",
    "mixed_test_tX             = mixed_features(filled_test_tX_median, Features_non_categorical)\n",
    "# mixed_invese_test_tX      = mixed_inverse_features(filled_test_tX_median, Features_denominate, Features_numerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Change\n",
    "# * filled_test_tX_median\n",
    "# * w\n",
    "# * mean_x, std_x\n",
    "# * degree\n",
    "##############################\n",
    "# For non categorical features, build polynomials\n",
    "poly_test_tX     = build_polynomial_without_mixed_term(filled_test_tX_median[:, Features_non_categorical], degree=degree)\n",
    "log_poly_test_tX = build_polynomial_without_mixed_term(log_test_tX, degree=degree)\n",
    "inv_poly_test_tX = build_polynomial_without_mixed_term(inverse_test_tX, degree=degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a design matrix\n",
    "design_matrix_test = np.c_[poly_test_tX, decomposed_test_tX, missing_indicator_test_tX, log_poly_test_tX, \n",
    "                           mixed_test_tX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 478)\n"
     ]
    }
   ],
   "source": [
    "test_tX, _, _ = standardize(design_matrix_test, mean_x, std_x)\n",
    "print(test_tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y0 = predict_labels(weights=w, data=test_tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = y0\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62168668762032808"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(y1 - y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/result12.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y0, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predt_11 = pd.read_csv('../results/result11.csv')\n",
    "predt_10 = pd.read_csv('../results/result10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predt_9  = pd.read_csv('../results/result9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     485635\n",
       "False     82603\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predt_11['Prediction'] == series).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series = pd.Series(y0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0702178367f34f9ebe15ab2ad5078a77": {
     "views": []
    },
    "104197bfe94d4c0d969b505d11b5c64f": {
     "views": []
    },
    "278b55cc2b5d470b914c860b737b13dd": {
     "views": []
    },
    "3bde44762a854e17913f8a1d09a4c769": {
     "views": []
    },
    "85d5f2ee705b4c90b7174e8404fecf2f": {
     "views": []
    },
    "c1bc1dc628504eb8a05c1d397345357a": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "e03e8d3520474092b8ccdcf44afab94a": {
     "views": []
    },
    "ff9fba9a21524b73b062f1f5b3ed2858": {
     "views": []
    }
   },
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
